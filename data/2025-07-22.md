<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 86]
- [cs.CV](#cs.CV) [Total: 141]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.RO](#cs.RO) [Total: 4]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.LG](#cs.LG) [Total: 17]
- [cs.AI](#cs.AI) [Total: 9]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [eess.IV](#eess.IV) [Total: 17]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.IR](#cs.IR) [Total: 4]
- [eess.AS](#eess.AS) [Total: 1]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base](https://arxiv.org/abs/2507.14189)
*Song Mao,Lejun Cheng,Pinlong Cai,Guohang Yan,Ding Wang,Botian Shi*

Main category: cs.CL

TL;DR: DeepWriter is a customizable, multimodal writing assistant for specialized domains, using a curated offline knowledge base to generate high-quality, factually grounded documents.


<details>
  <summary>Details</summary>
Motivation: LLMs lack deep domain-specific knowledge and hallucinate, while existing solutions like RAG and online search suffer from inconsistency and unreliable content.

Method: DeepWriter uses task decomposition, outline generation, multimodal retrieval, and section-by-section composition with reflection, leveraging a structured corpus and hierarchical knowledge representation.

Result: DeepWriter outperforms baselines in financial report generation, producing verifiable, high-quality articles with superior factual accuracy.

Conclusion: DeepWriter addresses LLM limitations in specialized domains, offering a robust solution for professional-grade document generation.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various applications. However, their use as writing assistants in specialized
domains like finance, medicine, and law is often hampered by a lack of deep
domain-specific knowledge and a tendency to hallucinate. Existing solutions,
such as Retrieval-Augmented Generation (RAG), can suffer from inconsistency
across multiple retrieval steps, while online search-based methods often
degrade quality due to unreliable web content. To address these challenges, we
introduce DeepWriter, a customizable, multimodal, long-form writing assistant
that operates on a curated, offline knowledge base. DeepWriter leverages a
novel pipeline that involves task decomposition, outline generation, multimodal
retrieval, and section-by-section composition with reflection. By deeply mining
information from a structured corpus and incorporating both textual and visual
elements, DeepWriter generates coherent, factually grounded, and
professional-grade documents. We also propose a hierarchical knowledge
representation to enhance retrieval efficiency and accuracy. Our experiments on
financial report generation demonstrate that DeepWriter produces high-quality,
verifiable articles that surpasses existing baselines in factual accuracy and
generated content quality.

</details>


### [2] [Retention analysis of edited knowledge after fine-tuning](https://arxiv.org/abs/2507.14198)
*Fufang Wen,Shichang Zhang*

Main category: cs.CL

TL;DR: Fine-tuning impacts edited knowledge in LLMs more than intrinsic knowledge, revealing a need for robust editing methods.


<details>
  <summary>Details</summary>
Motivation: To understand how fine-tuning affects edited knowledge in LLMs and improve editing robustness.

Method: Systematically investigate interactions between fine-tuning objectives and model editing techniques.

Result: Edited knowledge is more prone to forgetting during fine-tuning; freezing layers improves retention.

Conclusion: Evaluating edit robustness under fine-tuning is crucial, and freezing layers can enhance knowledge retention.

Abstract: Large language models (LLMs) store vast amounts of knowledge, which often
requires updates to correct factual errors, incorporate newly acquired
information, or adapt model behavior. Model editing methods have emerged as
efficient solutions for such updates, offering localized and precise knowledge
modification at significantly lower computational cost than continual training.
In parallel, LLMs are frequently fine-tuned for a wide range of downstream
tasks. However, the effect of fine-tuning on previously edited knowledge
remains poorly understood. In this work, we systematically investigate how
different fine-tuning objectives interact with various model editing
techniques. Our findings show that edited knowledge is substantially more
susceptible to forgetting during fine-tuning than intrinsic knowledge acquired
through pre-training. This analysis highlights a key limitation of current
editing approaches and suggests that evaluating edit robustness under
downstream fine-tuning is critical for their practical deployment. We further
find that freezing layers associated with edited content can significantly
improve knowledge retention, offering insight into how future editing methods
might be made more robust.

</details>


### [3] [Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System](https://arxiv.org/abs/2507.14200)
*Shengji Tang,Jianjian Cao,Weihao Lin,Jiale Hong,Bo Zhang,Shuyue Hu,Lei Bai,Tao Chen,Wanli Ouyang,Peng Ye*

Main category: cs.CL

TL;DR: SMACS, a scalable multi-agent collaboration system, integrates open-source LLMs to outperform closed-source LLMs like Claude-3.7-Sonnet and GPT-4.1, achieving higher performance across tasks.


<details>
  <summary>Details</summary>
Motivation: To explore whether multiple open-source LLMs can surpass closed-source LLMs by leveraging collaborative frameworks.

Method: Proposes SMACS with Retrieval-based Prior Selection (RPS) for LLM selection and Exploration-Exploitation-Driven Posterior Enhancement (EPE) for diverse, high-quality responses.

Result: SMACS outperforms leading closed-source LLMs by significant margins (e.g., +12.73% over Claude-3.7-Sonnet) and exceeds the best results from both open and closed-source LLMs.

Conclusion: SMACS demonstrates the potential of open-source collectives to push the boundaries of AI performance, offering a scalable and effective framework.

Abstract: This paper aims to demonstrate the potential and strengths of open-source
collectives. It leads to a promising question: Can we harness multiple
open-source LLMs to match or even beat the closed-source LLMs? To answer this,
we propose SMACS, a scalable multi-agent collaboration system (MACS) framework
with high performance. Specifically, for continuous integration of new LLMs and
generalization to diverse questions, we first propose a Retrieval-based Prior
Selection (RPS), which assigns a proxy performance score to each LLM to select
the Top-k LLMs at the instance level for any given question. Then, we propose
an Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the
generation of diverse responses through prior dropping and selecting the
high-quality response via a hybrid posterior score. Experiments on eight
mainstream benchmarks validate the effectiveness of our SMACS: by integrating
fifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025,
e.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%)
across multiple tasks. Remarkably, it even exceeds the average of best results
of different datasets from both open-source LLMs (+2.86%) and closed-source
LLMs (+2.04%), pushing the upper bound of intelligence. Code will be released
at https://github.com/magent4aci/SMACS.

</details>


### [4] [Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale](https://arxiv.org/abs/2507.14214)
*Rui Zhao,Vladyslav Melnychuk,Jun Zhao,Jesse Wright,Nigel Shadbolt*

Main category: cs.CL

TL;DR: PoliAnalyzer is a neuro-symbolic system using NLP to analyze privacy policies, comparing them with user preferences to reduce cognitive burden and improve compliance.


<details>
  <summary>Details</summary>
Motivation: Users rarely read privacy policies despite their importance, leading to a need for automated, personalized analysis.

Method: PoliAnalyzer uses NLP to extract formal representations of policies and applies logical inference to compare with user preferences.

Result: Achieved 90-100% F1-score in identifying data usage practices and found 4.8% of policy segments violate user preferences.

Conclusion: PoliAnalyzer enables scalable, automated privacy policy analysis, empowering users and promoting fairer data practices.

Abstract: In modern times, people have numerous online accounts, but they rarely read
the Terms of Service or Privacy Policy of those sites despite claiming
otherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that
assists users with personalized privacy policy analysis. PoliAnalyzer uses
Natural Language Processing (NLP) to extract formal representations of data
usage practices from policy texts. In favor of deterministic, logical inference
is applied to compare user preferences with the formal privacy policy
representation and produce a compliance report. To achieve this, we extend an
existing formal Data Terms of Use policy language to model privacy policies as
app policies and user preferences as data policies. In our evaluation using our
enriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated
high accuracy in identifying relevant data usage practices, achieving F1-score
of 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can
model diverse user data-sharing preferences, derived from prior research as 23
user profiles, and perform compliance analysis against the top 100 most-visited
websites. This analysis revealed that, on average, 95.2% of a privacy policy's
segments do not conflict with the analyzed user preferences, enabling users to
concentrate on understanding the 4.8% (636 / 13205) that violates preferences,
significantly reducing cognitive burden. Further, we identified common
practices in privacy policies that violate user expectations - such as the
sharing of location data with 3rd parties. This paper demonstrates that
PoliAnalyzer can support automated personalized privacy policy analysis at
scale using off-the-shelf NLP tools. This sheds light on a pathway to help
individuals regain control over their data and encourage societal discussions
on platform data practices to promote a fairer power dynamic.

</details>


### [5] [Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media](https://arxiv.org/abs/2507.14231)
*Khalid Hasan,Jamil Saquer*

Main category: cs.CL

TL;DR: The paper evaluates NLP models for detecting bipolar disorder from social media text, finding RoBERTa and BERT-embedded LSTMs most effective, while static embeddings fail. DistilBERT balances efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Bipolar disorder is often underdiagnosed due to subtle symptoms and stigma. The study aims to leverage NLP for early detection using social media text.

Method: Evaluated transformer models (BERT, RoBERTa, ALBERT, ELECTRA, DistilBERT) and LSTMs with contextualized (BERT) and static (GloVe, Word2Vec) embeddings on annotated Reddit posts.

Result: RoBERTa achieved the highest F1 score (~98%), with BERT-embedded LSTMs performing similarly. Static embeddings scored near-zero. DistilBERT offered efficiency-accuracy balance.

Conclusion: Contextualized models are crucial for bipolar disorder detection. The study provides insights for model selection in mental health NLP and supports early screening.

Abstract: Bipolar disorder is a chronic mental illness frequently underdiagnosed due to
subtle early symptoms and social stigma. This paper explores the advanced
natural language processing (NLP) models for recognizing signs of bipolar
disorder based on user-generated social media text. We conduct a comprehensive
evaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,
DistilBERT) and Long Short Term Memory (LSTM) models based on contextualized
(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed
on a large, annotated dataset of Reddit posts after confirming their validity
through sentiment variance and judgmental analysis. Our results demonstrate
that RoBERTa achieves the highest performance among transformer models with an
F1 score of ~98% while LSTM models using BERT embeddings yield nearly identical
results. In contrast, LSTMs trained on static embeddings fail to capture
meaningful patterns, scoring near-zero F1. These findings underscore the
critical role of contextual language modeling in detecting bipolar disorder. In
addition, we report model training times and highlight that DistilBERT offers
an optimal balance between efficiency and accuracy. In general, our study
offers actionable insights for model selection in mental health NLP
applications and validates the potential of contextualized language models to
support early bipolar disorder screening.

</details>


### [6] [Language Models Change Facts Based on the Way You Talk](https://arxiv.org/abs/2507.14238)
*Matthew Kearney,Reuben Binns,Yarin Gal*

Main category: cs.CL

TL;DR: LLMs infer identity from text and exhibit biases in high-stakes applications like medicine, law, and job salaries, leading to harmful disparities.


<details>
  <summary>Details</summary>
Motivation: To analyze how identity markers in user queries bias LLM responses in critical real-world applications.

Method: Comprehensive analysis across five domains (medicine, law, politics, government benefits, job salaries) to assess LLM sensitivity to identity markers like race, gender, and age.

Result: LLMs show consistent biases, e.g., varying medical care by ethnicity, aligning answers with political views by age, and recommending unequal salaries by race and gender.

Conclusion: Off-the-shelf LLMs can cause harmful disparities; thorough assessments are needed before deployment in user-facing applications.

Abstract: Large language models (LLMs) are increasingly being used in user-facing
applications, from providing medical consultations to job interview advice.
Recent research suggests that these models are becoming increasingly proficient
at inferring identity information about the author of a piece of text from
linguistic patterns as subtle as the choice of a few words. However, little is
known about how LLMs use this information in their decision-making in
real-world applications. We perform the first comprehensive analysis of how
identity markers present in a user's writing bias LLM responses across five
different high-stakes LLM applications in the domains of medicine, law,
politics, government benefits, and job salaries. We find that LLMs are
extremely sensitive to markers of identity in user queries and that race,
gender, and age consistently influence LLM responses in these applications. For
instance, when providing medical advice, we find that models apply different
standards of care to individuals of different ethnicities for the same
symptoms; we find that LLMs are more likely to alter answers to align with a
conservative (liberal) political worldview when asked factual questions by
older (younger) individuals; and that LLMs recommend lower salaries for
non-White job applicants and higher salaries for women compared to men. Taken
together, these biases mean that the use of off-the-shelf LLMs for these
applications may cause harmful differences in medical care, foster wage gaps,
and create different political factual realities for people of different
identities. Beyond providing an analysis, we also provide new tools for
evaluating how subtle encoding of identity in users' language choices impacts
model decisions. Given the serious implications of these findings, we recommend
that similar thorough assessments of LLM use in user-facing applications are
conducted before future deployment.

</details>


### [7] [CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation](https://arxiv.org/abs/2507.14239)
*Weihua Zheng,Roy Ka-Wei Lee,Zhengyuan Liu,Kui Wu,AiTi Aw,Bowei Zou*

Main category: cs.CL

TL;DR: CCL-XCoT, a two-stage fine-tuning framework, reduces hallucinations in Multilingual Large Language Models (MLLMs) by 62% using curriculum-based contrastive learning and cross-lingual Chain-of-Thought prompting.


<details>
  <summary>Details</summary>
Motivation: MLLMs suffer from hallucinations, especially in low-resource languages, due to training data imbalances, impacting domain-specific tasks.

Method: 1. Curriculum-based contrastive learning for cross-lingual semantic alignment. 2. Cross-lingual Chain-of-Thought (XCoT) prompting during fine-tuning to guide reasoning in high-resource languages before generating in low-resource ones.

Result: CCL-XCoT reduces hallucination rates by up to 62% and improves factual knowledge transfer across languages.

Conclusion: The proposed framework effectively mitigates hallucinations in MLLMs without external tools, enhancing cross-lingual performance.

Abstract: Multilingual Large Language Models(MLLMs) demonstrate strong generalization
across languages, yet they remain prone to hallucinations, especially in
low-resource languages, due to training data imbalances. These hallucinations,
which include inaccurate or fabricated outputs, are particularly problematic in
domain-specific generation tasks (Chataigner et al., 2024). To address this
challenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based
Cross-lingual Chain-of-Thought), a two-stage fine-tuning framework for
mitigating hallucination in MLLMs. Our approach first enhances cross-lingual
semantic alignment through curriculum-based contrastive learning combined with
next-token prediction during continued pre-training. Building on this
foundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting
strategy during instruction fine-tuning, which guides the model to reason in a
high-resource language before generating answers in the target low-resource
language. Experimental results show that CCL-XCoT reduces hallucination rates
by up to 62% and substantially improves factual knowledge transfer across
language pairs, without relying on external retrieval or multi-model ensembles.

</details>


### [8] [HuggingGraph: Understanding the Supply Chain of LLM Ecosystem](https://arxiv.org/abs/2507.14240)
*Mohammad Shahedur Rahman,Peng Gao,Yuede Ji*

Main category: cs.CL

TL;DR: The paper analyzes the LLM supply chain, focusing on model-dataset relationships, using a graph-based approach to uncover structural patterns and dynamics.


<details>
  <summary>Details</summary>
Motivation: The growing complexity and resource demands of LLMs, along with inherited vulnerabilities and biases, necessitate understanding the LLM supply chain to mitigate risks and ensure fairness.

Method: The study systematically collects LLM supply chain data and constructs a directed heterogeneous graph (397,376 nodes, 453,469 edges) to model relationships between models and datasets.

Result: Key findings include the graph's large, sparse, power-law structure; a dense core with fragmented periphery; pivotal dataset roles; strong model-dataset interdependence; and dynamic daily updates.

Conclusion: The study highlights the importance of tracking LLM supply chains for risk detection, fairness improvement, and compliance, with the graph model providing actionable insights.

Abstract: Large language models (LLMs) leverage deep learning to process and predict
sequences of words from context, enabling them to perform various NLP tasks,
such as translation, summarization, question answering, and content generation.
However, the growing size and complexity of developing, training, and deploying
advanced LLMs require extensive computational resources and large datasets.
This creates a barrier for users. As a result, platforms that host models and
datasets are widely used. For example, Hugging Face, one of the most popular
platforms, hosted 1.8 million models and 450K datasets by June 2025, with no
sign of slowing down. Since many LLMs are built from base models, pre-trained
models, and external datasets, they can inherit vulnerabilities, biases, or
malicious components from earlier models or datasets. Therefore, it is critical
to understand the origin and development of these components to better detect
potential risks, improve model fairness, and ensure compliance. Motivated by
this, our project aims to study the relationships between models and datasets,
which are core components of the LLM supply chain. First, we design a method to
systematically collect LLM supply chain data. Using this data, we build a
directed heterogeneous graph to model the relationships between models and
datasets, resulting in a structure with 397,376 nodes and 453,469 edges. We
then perform various analyses and uncover several findings, such as: (i) the
LLM supply chain graph is large, sparse, and follows a power-law degree
distribution; (ii) it features a densely connected core and a fragmented
periphery; (iii) datasets play pivotal roles in training; (iv) strong
interdependence exists between models and datasets; and (v) the graph is
dynamic, with daily updates reflecting the ecosystem's ongoing evolution.

</details>


### [9] [Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models](https://arxiv.org/abs/2507.14241)
*Rithesh Murthy,Ming Zhu,Liangwei Yang,Jielin Qiu,Juntao Tan,Shelby Heinecke,Huan Wang,Caiming Xiong,Silvio Savarese*

Main category: cs.CL

TL;DR: Promptomatix is an automatic prompt optimization framework for LLMs, eliminating manual tuning and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Manual prompt engineering is inconsistent and inaccessible to non-experts, necessitating an automated solution.

Method: Uses meta-prompt-based optimization and DSPy-powered compiler, analyzing intent, generating synthetic data, and refining prompts.

Result: Achieves competitive or superior performance across 5 tasks, reducing prompt length and computational overhead.

Conclusion: Promptomatix makes prompt optimization scalable, efficient, and accessible without domain expertise.

Abstract: Large Language Models (LLMs) perform best with well-crafted prompts, yet
prompt engineering remains manual, inconsistent, and inaccessible to
non-experts. We introduce Promptomatix, an automatic prompt optimization
framework that transforms natural language task descriptions into high-quality
prompts without requiring manual tuning or domain expertise. Promptomatix
supports both a lightweight meta-prompt-based optimizer and a DSPy-powered
compiler, with modular design enabling future extension to more advanced
frameworks. The system analyzes user intent, generates synthetic training data,
selects prompting strategies, and refines prompts using cost-aware objectives.
Evaluated across 5 task categories, Promptomatix achieves competitive or
superior performance compared to existing libraries, while reducing prompt
length and computational overhead making prompt optimization scalable and
efficient.

</details>


### [10] [In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding](https://arxiv.org/abs/2507.14298)
*Wan-Cyuan Fan,Yen-Chun Chen,Mengchen Liu,Alexander Jacobson,Lu Yuan,Leonid Sigal*

Main category: cs.CL

TL;DR: ChartScope is a new LVLM for chart comprehension, addressing limitations of existing methods by using diverse chart data and a Dual-Path training strategy, outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing LVLMs for chart comprehension lack generalization across chart types and targeted pre-training for data alignment.

Method: Proposes a data generation pipeline for diverse chart types and a Dual-Path training strategy for data and reasoning alignment.

Result: ChartScope significantly improves comprehension across various chart types, validated by the new ChartDQA benchmark.

Conclusion: ChartScope advances chart comprehension by addressing data diversity and alignment, with open-source code and data.

Abstract: Recent methods for customizing Large Vision Language Models (LVLMs) for
domain-specific tasks have shown promising results in scientific chart
comprehension. However, existing approaches face two major limitations: First,
they rely on paired data from only a few chart types, limiting generalization
to wide range of chart types. Secondly, they lack targeted pre-training for
chart-data alignment, which hampers the model's understanding of underlying
data. In this paper, we introduce ChartScope, an LVLM optimized for in-depth
chart comprehension across diverse chart types. We propose an efficient data
generation pipeline that synthesizes paired data for a wide range of chart
types, along with a novel Dual-Path training strategy that enabling the model
to succinctly capture essential data details while preserving robust reasoning
capabilities by incorporating reasoning over the underlying data. Lastly, we
establish ChartDQA, a new benchmark for evaluating not only question-answering
at different levels but also underlying data understanding. Experimental
results demonstrate that ChartScope significantly enhances comprehension on a
wide range of chart types. The code and data are available at
https://davidhalladay.github.io/chartscope_demo.

</details>


### [11] [Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study](https://arxiv.org/abs/2507.14304)
*Rakesh Paul,Anusha Kamath,Kanishk Singla,Raviraj Joshi,Utkarsh Vaidya,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

TL;DR: Selective translation improves multilingual LLM alignment by preserving non-translatable content, outperforming standard translation methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the performance gap in multilingual LLMs for low-resource languages due to limited high-quality alignment data.

Method: LLM-based selective translation, preserving non-translatable elements like code and JSON, compared with vanilla translation and mixed-data alignment.

Result: Selective translation shows promise, especially for Hindi, outperforming Google Cloud Translation and Llama-3.1-405B.

Conclusion: Selective translation is a practical and effective method for enhancing multilingual alignment in LLMs.

Abstract: Multilingual large language models (LLMs) often demonstrate a performance gap
between English and non-English languages, particularly in low-resource
settings. Aligning these models to low-resource languages is essential yet
challenging due to limited high-quality data. While English alignment datasets
are readily available, curating equivalent data in other languages is expensive
and time-consuming. A common workaround is to translate existing English
alignment data; however, standard translation techniques often fail to preserve
critical elements such as code, mathematical expressions, and structured
formats like JSON. In this work, we investigate LLM-based selective
translation, a technique that selectively translates only the translatable
parts of a text while preserving non-translatable content and sentence
structure. We conduct a systematic study to explore key questions around this
approach, including its effectiveness compared to vanilla translation, the
importance of filtering noisy outputs, and the benefits of mixing translated
samples with original English data during alignment. Our experiments focus on
the low-resource Indic language Hindi and compare translations generated by
Google Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the
promise of selective translation as a practical and effective method for
improving multilingual alignment in LLMs.

</details>


### [12] [How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs](https://arxiv.org/abs/2507.14307)
*Karin de Langis,Jong Inn Park,Andreas Schramm,Bin Hu,Khanh Chi Le,Michael Mensink,Ahn Thu Tong,Dongyeop Kang*

Main category: cs.CL

TL;DR: LLMs process linguistic aspect differently from humans, relying on prototypicality and struggling with causal reasoning, indicating limited narrative comprehension.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs process temporal meaning in narratives like humans or rely on pattern recognition.

Method: Used an Expert-in-the-Loop probing pipeline to test LLMs' semantic and pragmatic processing of linguistic aspect.

Result: LLMs over-rely on prototypicality, produce inconsistent judgments, and struggle with causal reasoning.

Conclusion: LLMs lack human-like narrative understanding; a standardized framework is proposed for assessing their capabilities.

Abstract: Large language models (LLMs) exhibit increasingly sophisticated linguistic
capabilities, yet the extent to which these behaviors reflect human-like
cognition versus advanced pattern recognition remains an open question. In this
study, we investigate how LLMs process the temporal meaning of linguistic
aspect in narratives that were previously used in human studies. Using an
Expert-in-the-Loop probing pipeline, we conduct a series of targeted
experiments to assess whether LLMs construct semantic representations and
pragmatic inferences in a human-like manner. Our findings show that LLMs
over-rely on prototypicality, produce inconsistent aspectual judgments, and
struggle with causal reasoning derived from aspect, raising concerns about
their ability to fully comprehend narratives. These results suggest that LLMs
process aspect fundamentally differently from humans and lack robust narrative
understanding. Beyond these empirical findings, we develop a standardized
experimental framework for the reliable assessment of LLMs' cognitive and
linguistic capabilities.

</details>


### [13] [What Makes You CLIC: Detection of Croatian Clickbait Headlines](https://arxiv.org/abs/2507.14314)
*Marija Anđedelić,Dominik Šipek,Laura Majer,Jan Šnajder*

Main category: cs.CL

TL;DR: The paper introduces CLIC, a Croatian dataset for clickbait detection, compares fine-tuned BERTić with LLM-based in-context learning, and finds fine-tuned models outperform general LLMs.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of clickbait detection in less-resourced languages like Croatian and evaluate the effectiveness of fine-tuned vs. in-context learning methods.

Method: Compiled CLIC dataset, fine-tuned BERTić, and compared it with LLM-based in-context learning using Croatian and English prompts.

Result: Nearly half of headlines contained clickbait; fine-tuned models performed better than general LLMs.

Conclusion: Fine-tuned models are more effective for clickbait detection in less-resourced languages like Croatian.

Abstract: Online news outlets operate predominantly on an advertising-based revenue
model, compelling journalists to create headlines that are often scandalous,
intriguing, and provocative -- commonly referred to as clickbait. Automatic
detection of clickbait headlines is essential for preserving information
quality and reader trust in digital media and requires both contextual
understanding and world knowledge. For this task, particularly in
less-resourced languages, it remains unclear whether fine-tuned methods or
in-context learning (ICL) yield better results. In this paper, we compile CLIC,
a novel dataset for clickbait detection of Croatian news headlines spanning a
20-year period and encompassing mainstream and fringe outlets. We fine-tune the
BERTi\'c model on this task and compare its performance to LLM-based ICL
methods with prompts both in Croatian and English. Finally, we analyze the
linguistic properties of clickbait. We find that nearly half of the analyzed
headlines contain clickbait, and that finetuned models deliver better results
than general LLMs.

</details>


### [14] [Can LLMs Infer Personality from Real World Conversations?](https://arxiv.org/abs/2507.14355)
*Jianfeng Zhu,Ruoming Jin,Karin G. Coifman*

Main category: cs.CL

TL;DR: LLMs like GPT-4 and LLaMA show promise for personality assessment but struggle with validity. A benchmark of 555 interviews with BFI-10 scores tested three LLMs, revealing high reliability but weak validity and biases. Chain-of-thought prompting helped slightly, but accuracy remains limited.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of LLMs in inferring personality traits from real-world data, addressing gaps in psychometric validity and synthetic data reliance.

Method: Tested three LLMs (GPT-4.1 Mini, Meta-LLaMA, DeepSeek) using zero-shot and chain-of-thought prompting on 555 semi-structured interviews with BFI-10 scores.

Result: High test-retest reliability but weak construct validity (max Pearson's r = 0.27), low interrater agreement, and trait-level biases. Chain-of-thought improved distributional alignment but not accuracy.

Conclusion: Current LLMs have limitations for personality inference, emphasizing the need for evidence-based improvements in psychological applications.

Abstract: Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a
promising approach for scalable personality assessment from open-ended
language. However, inferring personality traits remains challenging, and
earlier work often relied on synthetic data or social media text lacking
psychometric validity. We introduce a real-world benchmark of 555
semi-structured interviews with BFI-10 self-report scores for evaluating
LLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini,
Meta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item
prediction and both zero-shot and chain-of-thought prompting for Big Five trait
inference. All models showed high test-retest reliability, but construct
validity was limited: correlations with ground-truth scores were weak (max
Pearson's $r = 0.27$), interrater agreement was low (Cohen's $\kappa < 0.10$),
and predictions were biased toward moderate or high trait levels.
Chain-of-thought prompting and longer input context modestly improved
distributional alignment, but not trait-level accuracy. These results
underscore limitations in current LLM-based personality inference and highlight
the need for evidence-based development for psychological applications.

</details>


### [15] [Text-to-SQL for Enterprise Data Analytics](https://arxiv.org/abs/2507.14372)
*Albert Chen,Manas Bundele,Gaurav Ahlawat,Patrick Stetz,Zhitao Wang,Qiang Fei,Donghoon Jung,Audrey Chu,Bharadwaj Jayaraman,Ayushi Panth,Yatin Arora,Sourav Jain,Renjith Varma,Alexey Ilin,Iuliia Melnychuk,Chelsea Chueh,Joyan Sil,Xiaofeng Wang*

Main category: cs.CL

TL;DR: The paper presents a practical approach to building an enterprise Text-to-SQL chatbot for LinkedIn, combining a knowledge graph, a Text-to-SQL agent, and an interactive chatbot to enable self-serve data insights.


<details>
  <summary>Details</summary>
Motivation: Large language models have advanced Text-to-SQL benchmarks, but enterprise solutions remain challenging. The paper aims to address this gap by developing a functional chatbot for LinkedIn's teams.

Method: The approach involves: 1) constructing a knowledge graph from metadata, logs, wikis, and code; 2) building a Text-to-SQL agent for context retrieval, query writing, and error correction; 3) creating an interactive chatbot for diverse user intents.

Result: The chatbot has 300+ weekly users, with 53% of responses rated correct or close to correct in expert reviews. Ablation studies highlight key components for enterprise solutions.

Conclusion: The study offers actionable insights for developing enterprise Text-to-SQL solutions, emphasizing the importance of knowledge graphs and modeling components.

Abstract: The introduction of large language models has brought rapid progress on
Text-to-SQL benchmarks, but it is not yet easy to build a working enterprise
solution. In this paper, we present insights from building an internal chatbot
that enables LinkedIn's product managers, engineers, and operations teams to
self-serve data insights from a large, dynamic data lake. Our approach features
three components. First, we construct a knowledge graph that captures
up-to-date semantics by indexing database metadata, historical query logs,
wikis, and code. We apply clustering to identify relevant tables for each team
or product area. Second, we build a Text-to-SQL agent that retrieves and ranks
context from the knowledge graph, writes a query, and automatically corrects
hallucinations and syntax errors. Third, we build an interactive chatbot that
supports various user intents, from data discovery to query writing to
debugging, and displays responses in rich UI elements to encourage follow-up
chats. Our chatbot has over 300 weekly users. Expert review shows that 53% of
its responses are correct or close to correct on an internal benchmark set.
Through ablation studies, we identify the most important knowledge graph and
modeling components, offering a practical path for developing enterprise
Text-to-SQL solutions.

</details>


### [16] [Error-Aware Curriculum Learning for Biomedical Relation Classification](https://arxiv.org/abs/2507.14374)
*Sinchani Chakraborty,Sudeshna Sarkar,Pawan Goyal*

Main category: cs.CL

TL;DR: An error-aware teacher-student framework using GPT-4o improves biomedical relation classification by analyzing errors, generating remediations, and training models progressively.


<details>
  <summary>Details</summary>
Motivation: Enhancing biomedical relation classification to support knowledge graphs and applications like drug repurposing and clinical decision-making.

Method: Uses a teacher-student framework with GPT-4o to analyze errors, generate remediations, and train models via instruction tuning and curriculum learning. A knowledge graph from PubMed abstracts supports context-aware classification.

Result: Achieves state-of-the-art performance on 4 of 5 PPI datasets and the DDI dataset, remaining competitive on ChemProt.

Conclusion: The framework effectively improves relation classification in biomedical texts through structured guidance and progressive learning.

Abstract: Relation Classification (RC) in biomedical texts is essential for
constructing knowledge graphs and enabling applications such as drug
repurposing and clinical decision-making. We propose an error-aware
teacher--student framework that improves RC through structured guidance from a
large language model (GPT-4o). Prediction failures from a baseline student
model are analyzed by the teacher to classify error types, assign difficulty
scores, and generate targeted remediations, including sentence rewrites and
suggestions for KG-based enrichment. These enriched annotations are used to
train a first student model via instruction tuning. This model then annotates a
broader dataset with difficulty scores and remediation-enhanced inputs. A
second student is subsequently trained via curriculum learning on this dataset,
ordered by difficulty, to promote robust and progressive learning. We also
construct a heterogeneous biomedical knowledge graph from PubMed abstracts to
support context-aware RC. Our approach achieves new state-of-the-art
performance on 4 of 5 PPI datasets and the DDI dataset, while remaining
competitive on ChemProt.

</details>


### [17] [X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display](https://arxiv.org/abs/2507.14430)
*Xiaolin Yan,Yangxing Liu,Jiazhang Zheng,Chi Liu,Mingyu Du,Caisheng Chen,Haoyang Liu,Ming Ding,Yuan Li,Qiuping Liao,Linfeng Li,Zhili Mei,Siyu Wan,Li Li,Ruyi Zhong,Jiangling Yu,Xule Liu,Huihui Hu,Jiameng Yue,Ruohui Cheng,Qi Yang,Liangqing Wu,Ke Zhu,Chi Zhang,Chufei Jing,Yifan Zhou,Yan Liang,Dongdong Li,Zhaohui Wang,Bin Zhao,Mingzhou Wu,Mingzhong Zhou,Peng Du,Zuomin Liao,Chao Dai,Pengfei Liang,Xiaoguang Zhu,Yu Zhang,Yu Gu,Kun Pan,Yuan Wu,Yanqing Guan,Shaojing Wu,Zikang Feng,Xianze Ma,Peishan Cheng,Wenjuan Jiang,Jing Ba,Huihao Yu,Zeping Hu,Yuan Xu,Zhiwei Liu,He Wang,Zhenguo Lin,Ming Liu,Yanhong Meng*

Main category: cs.CL

TL;DR: X-Intelligence 3.0 is a specialized 32B-parameter LLM for the semiconductor display industry, outperforming larger models like DeepSeek-R1-671B through domain-specific training and RAG.


<details>
  <summary>Details</summary>
Motivation: Address the lack of domain-specific reasoning in LLMs for the semiconductor display industry.

Method: Supervised fine-tuning, reinforcement learning, and a domain-specific RAG mechanism, supported by an automated evaluation framework.

Result: Outperforms DeepSeek-R1-671B on benchmarks despite smaller size.

Conclusion: X-Intelligence 3.0 is an efficient, high-performance solution for industry-specific reasoning challenges.

Abstract: Large language models (LLMs) have recently achieved significant advances in
reasoning and demonstrated their advantages in solving challenging problems.
Yet, their effectiveness in the semiconductor display industry remains limited
due to a lack of domain-specific training and expertise. To bridge this gap, we
present X-Intelligence 3.0, the first high-performance reasoning model
specifically developed for the semiconductor display industry. This model is
designed to deliver expert-level understanding and reasoning for the industry's
complex challenges. Leveraging a carefully curated industry knowledge base, the
model undergoes supervised fine-tuning and reinforcement learning to enhance
its reasoning and comprehension capabilities. To further accelerate
development, we implemented an automated evaluation framework that simulates
expert-level assessments. We also integrated a domain-specific
retrieval-augmented generation (RAG) mechanism, resulting in notable
performance gains on benchmark datasets. Despite its relatively compact size of
32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B
across multiple evaluations. This demonstrates its exceptional efficiency and
establishes it as a powerful solution to the longstanding reasoning challenges
faced by the semiconductor display industry.

</details>


### [18] [XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification](https://arxiv.org/abs/2507.14578)
*Sachin Yadav,Dominik Schlechtweg*

Main category: cs.CL

TL;DR: XL-DURel is a multilingual Sentence Transformer model optimized for ordinal Word-in-Context classification, outperforming previous models with a ranking objective based on angular distance.


<details>
  <summary>Details</summary>
Motivation: To improve performance on ordinal and binary Word-in-Context tasks and unify their treatment.

Method: Finetuning a multilingual Sentence Transformer model with various loss functions for regression and ranking tasks.

Result: Outperforms previous models on ordinal and binary data, showing binary WiC as a special case of ordinal WiC.

Conclusion: Optimizing for ordinal tasks improves binary task performance, enabling unified WiC modeling.

Abstract: We propose XL-DURel, a finetuned, multilingual Sentence Transformer model
optimized for ordinal Word-in-Context classification. We test several loss
functions for regression and ranking tasks managing to outperform previous
models on ordinal and binary data with a ranking objective based on angular
distance in complex space. We further show that binary WiC can be treated as a
special case of ordinal WiC and that optimizing models for the general ordinal
task improves performance on the more specific binary task. This paves the way
for a unified treatment of WiC modeling across different task formulations.

</details>


### [19] [Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models](https://arxiv.org/abs/2507.14579)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: The paper explores the use of multimodal BERT (AudiBERT) for detecting CPS indicators, showing significant improvements in social-cognitive dimensions but not affective ones, and emphasizes human-AI complementarity with explainability.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of reliably detecting CPS indicators from dialogue using machine learning, particularly leveraging multimodal data (speech and acoustic-prosodic features) for enhanced diagnosis.

Method: The study extends prior work by evaluating AudiBERT (multimodal BERT) against BERT on transcription data, analyzing class-wise improvements, correlation with training data size, and human-AI complementarity.

Result: AudiBERT showed statistically significant improvements in social-cognitive classifications but not affective ones. Training data size correlated with recall, and human coder agreement influenced BERT's precision.

Conclusion: The paper advocates for a structured approach to human-AI complementarity in CPS diagnosis, stressing model explainability to support human engagement in coding.

Abstract: Detecting collaborative problem solving (CPS) indicators from dialogue using
machine learning techniques is a significant challenge for the field of AI in
Education. Recent studies have explored the use of Bidirectional Encoder
Representations from Transformers (BERT) models on transcription data to
reliably detect meaningful CPS indicators. A notable advancement involved the
multimodal BERT variant, AudiBERT, which integrates speech and
acoustic-prosodic audio features to enhance CPS diagnosis. Although initial
results demonstrated multimodal improvements, the statistical significance of
these enhancements remained unclear, and there was insufficient guidance on
leveraging human-AI complementarity for CPS diagnosis tasks. This workshop
paper extends the previous research by highlighting that the AudiBERT model not
only improved the classification of classes that were sparse in the dataset,
but it also had statistically significant class-wise improvements over the BERT
model for classifications in the social-cognitive dimension. However, similar
significant class-wise improvements over the BERT model were not observed for
classifications in the affective dimension. A correlation analysis highlighted
that larger training data was significantly associated with higher recall
performance for both the AudiBERT and BERT models. Additionally, the precision
of the BERT model was significantly associated with high inter-rater agreement
among human coders. When employing the BERT model to diagnose indicators within
these subskills that were well-detected by the AudiBERT model, the performance
across all indicators was inconsistent. We conclude the paper by outlining a
structured approach towards achieving human-AI complementarity for CPS
diagnosis, highlighting the crucial inclusion of model explainability to
support human agency and engagement in the reflective coding process.

</details>


### [20] [Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption](https://arxiv.org/abs/2507.14584)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: The paper explores BERT model explainability in CPS classification using SHAP, finding that high performance doesn't guarantee reasonable explanations and identifying spurious word contributions.


<details>
  <summary>Details</summary>
Motivation: To enhance transparency and trust in BERT-based CPS diagnostics for educators by understanding token contributions.

Method: Used SHAP to analyze tokenized word contributions in BERT's CPS classification.

Result: Found frequent but not semantically meaningful token contributions, suggesting performance ≠ explainability.

Conclusion: Calls for ensemble models and human-AI collaboration in CPS diagnosis due to the need for fine-grained human reasoning.

Abstract: The use of Bidirectional Encoder Representations from Transformers (BERT)
model and its variants for classifying collaborative problem solving (CPS) has
been extensively explored within the AI in Education community. However,
limited attention has been given to understanding how individual tokenised
words in the dataset contribute to the model's classification decisions.
Enhancing the explainability of BERT-based CPS diagnostics is essential to
better inform end users such as teachers, thereby fostering greater trust and
facilitating wider adoption in education. This study undertook a preliminary
step towards model transparency and explainability by using SHapley Additive
exPlanations (SHAP) to examine how different tokenised words in transcription
data contributed to a BERT model's classification of CPS processes. The
findings suggested that well-performing classifications did not necessarily
equate to a reasonable explanation for the classification decisions. Particular
tokenised words were used frequently to affect classifications. The analysis
also identified a spurious word, which contributed positively to the
classification but was not semantically meaningful to the class. While such
model transparency is unlikely to be useful to an end user to improve their
practice, it can help them not to overrely on LLM diagnostics and ignore their
human expertise. We conclude the workshop paper by noting that the extent to
which the model appropriately uses the tokens for its classification is
associated with the number of classes involved. It calls for an investigation
into the exploration of ensemble model architectures and the involvement of
human-AI complementarity for CPS diagnosis, since considerable human reasoning
is still required for fine-grained discrimination of CPS subskills.

</details>


### [21] [Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification](https://arxiv.org/abs/2507.14590)
*Łukasz Radliński,Mateusz Guściora,Jan Kocoń*

Main category: cs.CL

TL;DR: The paper explores data augmentation in NLP using large language models like GPT, comparing traditional methods (paraphrasing, backtranslation) with generative methods. Findings show traditional methods can match or outperform generative approaches.


<details>
  <summary>Details</summary>
Motivation: Addressing data scarcity and class imbalance in domain-specific ML tasks by evaluating the effectiveness of traditional data augmentation methods enhanced by modern LLMs.

Method: Systematic comparison of four data augmentation approaches (including paraphrasing and backtranslation) using ChatGPT and an exemplary dataset, evaluated on data quality and classification performance.

Result: Backtranslation and paraphrasing achieved comparable or better results than zero/few-shot generative methods.

Conclusion: Traditional data augmentation methods, when leveraged by modern LLMs, can be as effective as purely generative approaches for NLP tasks.

Abstract: Numerous domain-specific machine learning tasks struggle with data scarcity
and class imbalance. This paper systematically explores data augmentation
methods for NLP, particularly through large language models like GPT. The
purpose of this paper is to examine and evaluate whether traditional methods
such as paraphrasing and backtranslation can leverage a new generation of
models to achieve comparable performance to purely generative methods. Methods
aimed at solving the problem of data scarcity and utilizing ChatGPT were
chosen, as well as an exemplary dataset. We conducted a series of experiments
comparing four different approaches to data augmentation in multiple
experimental setups. We then evaluated the results both in terms of the quality
of generated data and its impact on classification performance. The key
findings indicate that backtranslation and paraphrasing can yield comparable or
even better results than zero and a few-shot generation of examples.

</details>


### [22] [Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper](https://arxiv.org/abs/2507.14615)
*Fred Mutisya,Shikoh Gitau,Christine Syovata,Diana Oigara,Ibrahim Matende,Muna Aden,Munira Ali,Ryan Nyotu,Diana Marion,Job Nyangena,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha,Eric Mibuari,Jean Philbert Nsengemana,Talkmore Chidede*

Main category: cs.CL

TL;DR: The paper explores using LLMs for healthcare in low-resource African settings, focusing on Kenya. It introduces a benchmark dataset (Alama Health QA) using RAG and local guidelines, evaluated by Kenyan physicians. Results show LLMs perform worse on localized content, highlighting the need for tailored benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored effectiveness of LLMs in African primary care and improve healthcare access in low-resource settings.

Method: Uses retrieval augmented generation (RAG) to align questions with Kenya's national guidelines, digitized and indexed for semantic retrieval. Gemini Flash 2.0 Lite generates clinical scenarios and questions, refined by Kenyan physicians and expert review.

Result: LLMs show significant performance gaps in localized scenarios, with lower accuracy on African medical content compared to US benchmarks.

Conclusion: The work provides a replicable model for guideline-driven benchmarking to ensure safe AI deployment in African health systems.

Abstract: Large Language Models(LLMs) hold promise for improving healthcare access in
low-resource settings, but their effectiveness in African primary care remains
underexplored. We present a methodology for creating a benchmark dataset and
evaluation framework focused on Kenyan Level 2 and 3 clinical care. Our
approach uses retrieval augmented generation (RAG) to ground clinical questions
in Kenya's national guidelines, ensuring alignment with local standards. These
guidelines were digitized, chunked, and indexed for semantic retrieval. Gemini
Flash 2.0 Lite was then prompted with guideline excerpts to generate realistic
clinical scenarios, multiple-choice questions, and rationale based answers in
English and Swahili. Kenyan physicians co-created and refined the dataset, and
a blinded expert review process ensured clinical accuracy, clarity, and
cultural appropriateness. The resulting Alama Health QA dataset includes
thousands of regulator-aligned question answer pairs across common outpatient
conditions. Beyond accuracy, we introduce evaluation metrics that test clinical
reasoning, safety, and adaptability such as rare case detection (Needle in the
Haystack), stepwise logic (Decision Points), and contextual adaptability.
Initial results reveal significant performance gaps when LLMs are applied to
localized scenarios, consistent with findings that LLM accuracy is lower on
African medical content than on US-based benchmarks. This work offers a
replicable model for guideline-driven, dynamic benchmarking to support safe AI
deployment in African health systems.

</details>


### [23] [Linear Relational Decoding of Morphology in Language Models](https://arxiv.org/abs/2507.14640)
*Eric Xia,Jugal Kalita*

Main category: cs.CL

TL;DR: A two-part affine approximation effectively approximates transformer computations for certain subject-object relations, achieving 90% faithfulness on morphological relations.


<details>
  <summary>Details</summary>
Motivation: To explore interpretability of conceptual relationships in language models, such as morphology, through latent space analysis.

Method: Adapting the Bigger Analogy Test Set and using linear transformations (Ws) derived from model derivatives to approximate final object states.

Result: The linear technique achieves high faithfulness (90%) on morphological relations, with consistent results across languages and models.

Conclusion: Conceptual relationships like morphology are sparsely encoded and interpretable via cross-layer linear transformations in language models.

Abstract: A two-part affine approximation has been found to be a good approximation for
transformer computations over certain subject object relations. Adapting the
Bigger Analogy Test Set, we show that the linear transformation Ws, where s is
a middle layer representation of a subject token and W is derived from model
derivatives, is also able to accurately reproduce final object states for many
relations. This linear technique is able to achieve 90% faithfulness on
morphological relations, and we show similar findings multi-lingually and
across models. Our findings indicate that some conceptual relationships in
language models, such as morphology, are readily interpretable from latent
space, and are sparsely encoded by cross-layer linear transformations.

</details>


### [24] [Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs](https://arxiv.org/abs/2507.14649)
*Minsuh Joo,Hyunsoo Cho*

Main category: cs.CL

TL;DR: The paper proposes Cleanse, a clustering-based method to estimate uncertainty in LLM responses to detect hallucinations, validated on multiple models and benchmarks.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs pose a reliability and safety risk, necessitating methods to distinguish accurate from inaccurate responses.

Method: Cleanse uses clustering on LLM hidden embeddings to quantify uncertainty via intra-cluster semantic consistency.

Result: Validated on LLaMA and Mistral models using SQuAD and CoQA benchmarks, Cleanse effectively detects hallucinations.

Conclusion: Cleanse provides a reliable approach for uncertainty estimation to mitigate LLM hallucinations, enhancing model safety and reliability.

Abstract: Despite the outstanding performance of large language models (LLMs) across
various NLP tasks, hallucinations in LLMs--where LLMs generate inaccurate
responses--remains as a critical problem as it can be directly connected to a
crisis of building safe and reliable LLMs. Uncertainty estimation is primarily
used to measure hallucination levels in LLM responses so that correct and
incorrect answers can be distinguished clearly. This study proposes an
effective uncertainty estimation approach, \textbf{Cl}ust\textbf{e}ring-based
sem\textbf{an}tic con\textbf{s}ist\textbf{e}ncy (\textbf{Cleanse}). Cleanse
quantifies the uncertainty with the proportion of the intra-cluster consistency
in the total consistency between LLM hidden embeddings which contain adequate
semantic information of generations, by employing clustering. The effectiveness
of Cleanse for detecting hallucination is validated using four off-the-shelf
models, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two
question-answering benchmarks, SQuAD and CoQA.

</details>


### [25] [Mangosteen: An Open Thai Corpus for Language Model Pretraining](https://arxiv.org/abs/2507.14664)
*Wannaphong Phatthiyaphaibun,Can Udomcharoenchaikit,Pakpoom Singkorapoom,Kunat Pipatanakul,Ekapol Chuangsuwanich,Peerat Limkonchotiwat,Sarana Nutanong*

Main category: cs.CL

TL;DR: Mangosteen is a 47B-token Thai corpus built with a Thai-adapted Dolma pipeline, improving Thai LLM performance and reproducibility.


<details>
  <summary>Details</summary>
Motivation: Existing corpora lack Thai-specific cleaning, risking harmful content and hindering reproducibility.

Method: Thai-adapted Dolma pipeline with custom language ID, quality filters, and curated non-web sources.

Result: Pipeline reduces CommonCrawl docs from 202M to 25M, improves SEA-HELM NLG from 3 to 11, and boosts SEA-LION model performance.

Conclusion: Mangosteen provides a transparent, high-quality Thai corpus with full reproducibility for future research.

Abstract: Pre-training data shapes a language model's quality, but raw web text is
noisy and demands careful cleaning. Existing large-scale corpora rely on
English-centric or language-agnostic pipelines whose heuristics do not capture
Thai script or cultural nuances, leaving risky material such as gambling
content untreated. Prior Thai-specific efforts customize pipelines or build new
ones, yet seldom release their data or document design choices, hindering
reproducibility and raising the question of how to construct a transparent,
high-quality Thai corpus. We introduce Mangosteen: a 47 billion-token Thai
corpus built through a Thai-adapted Dolma pipeline that includes custom
rule-based language ID, revised C4/Gopher quality filters, and Thai-trained
content filters, plus curated non-web sources such as Wikipedia, Royal Gazette
texts, OCR-extracted books, and CC-licensed YouTube subtitles. Systematic
ablations using GPT-2 show the pipeline trims CommonCrawl from 202M to 25M
documents while raising SEA-HELM NLG from 3 to 11; an 8B-parameter SEA-LION
model continually pre-trained on Mangosteen then surpasses SEA-LION-v3 and
Llama-3.1 by about four points on Thai benchmarks. We release the full pipeline
code, cleaning manifests, corpus snapshot, and all checkpoints, providing a
fully reproducible foundation for future Thai and regional LLM research.

</details>


### [26] [Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care](https://arxiv.org/abs/2507.14681)
*Vinicius Anjos de Almeida,Vinicius de Camargo,Raquel Gómez-Bravo,Egbert van der Haring,Kees van Boven,Marcelo Finger,Luis Fernandez Lopez*

Main category: cs.CL

TL;DR: LLMs demonstrate strong potential for automating ICPC-2 coding without fine-tuning, with top models achieving high F1-scores. Challenges include formatting issues for smaller models and dataset limitations.


<details>
  <summary>Details</summary>
Motivation: To assess the feasibility of using LLMs for automating ICPC-2 code assignment in healthcare data, leveraging domain-specific search engine outputs.

Method: Used a dataset of 437 clinical expressions in Brazilian Portuguese annotated with ICPC-2 codes. Evaluated 33 LLMs by prompting them with queries and retrieved results, measuring performance via F1-score, token usage, cost, response time, and format adherence.

Result: Top models (e.g., gpt-4.5-preview, o3, gemini-2.5-pro) achieved F1-scores > 0.85. Retriever optimization improved performance by up to 4 points. Smaller models (<3B) faced formatting and input length issues.

Conclusion: LLMs are promising for ICPC-2 coding automation, but broader, multilingual, and end-to-end evaluations are needed for clinical validation.

Abstract: Background: Medical coding structures healthcare data for research, quality
monitoring, and policy. This study assesses the potential of large language
models (LLMs) to assign ICPC-2 codes using the output of a domain-specific
search engine.
  Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each
annotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's
text-embedding-3-large) retrieved candidates from 73,563 labeled concepts.
Thirty-three LLMs were prompted with each query and retrieved results to select
the best-matching ICPC-2 code. Performance was evaluated using F1-score, along
with token usage, cost, response time, and format adherence.
  Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top
performers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever
optimization can improve performance by up to 4 points. Most models returned
valid codes in the expected format, with reduced hallucinations. Smaller models
(<3B) struggled with formatting and input length.
  Conclusions: LLMs show strong potential for automating ICPC-2 coding, even
without fine-tuning. This work offers a benchmark and highlights challenges,
but findings are limited by dataset scope and setup. Broader, multilingual,
end-to-end evaluations are needed for clinical validation.

</details>


### [27] [MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization](https://arxiv.org/abs/2507.14683)
*Xingxuan Li,Yao Xiao,Dianwen Ng,Hai Ye,Yue Deng,Xiang Lin,Bin Wang,Zhanfeng Mo,Chong Zhang,Yueyi Zhang,Zonglin Yang,Ruilin Li,Lei Lei,Shihao Xu,Han Zhao,Weiling Chen,Feng Ji,Lidong Bing*

Main category: cs.CL

TL;DR: The paper introduces MiroMind-M1, a fully open-source reasoning language model (RLM) series, addressing transparency and reproducibility gaps in existing RLMs by releasing models, datasets, and configurations.


<details>
  <summary>Details</summary>
Motivation: To enhance transparency and reproducibility in RLM development, as proprietary models like GPT-3 lack openness, and many open-source projects omit critical resources.

Method: Two-stage training: supervised fine-tuning (SFT) on 719K math problems with verified reasoning steps, followed by reinforcement learning with verifiable rewards (RLVR) on 62K problems, using a novel Context-Aware Multi-Stage Policy Optimization algorithm.

Result: State-of-the-art or competitive performance on benchmarks (AIME24, AIME25, MATH) with superior token efficiency for 7B and 32B models.

Conclusion: The MiroMind-M1 series and released resources aim to advance research and community progress in RLMs.

Abstract: Large language models have recently evolved from fluent text generation to
advanced reasoning across diverse domains, giving rise to reasoning language
models. Among these domains, mathematical reasoning serves as a representative
benchmark as it requires precise multi-step logic and abstract reasoning, which
can be generalized to other tasks. While closed-source RLMs such as GPT-o3
demonstrate impressive reasoning capabilities, their proprietary nature limits
transparency and reproducibility. Although many open-source projects aim to
close this gap, most of them lack sufficient openness by omitting critical
resources such as datasets and detailed training configurations, which hinders
reproducibility. To contribute toward greater transparency in RLM development,
we introduce the MiroMind-M1 series, a set of fully open-source RLMs built on
the Qwen-2.5 backbone that match or exceed the performance of existing
open-source RLMs. Specifically, our models are trained in two stages: SFT on a
carefully curated corpus of 719K math-reasoning problems with verified CoT
trajectories, followed by RLVR on 62K challenging and verifiable problems. To
enhance the robustness and efficiency of the RLVR process, we introduce
Context-Aware Multi-Stage Policy Optimization, an algorithm that integrates
length-progressive training with an adaptive repetition penalty to encourage
context-aware RL training. Our model achieves state-of-the-art or competitive
performance and superior token efficiency among Qwen-2.5-based open-source 7B
and 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate
reproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,
MiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,
MiroMind-M1-RL-62K); and all training and evaluation configurations. We hope
these resources will support further research and foster community advancement.

</details>


### [28] [Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations](https://arxiv.org/abs/2507.14688)
*Mohammed Alkhowaiter,Norah Alshahrani,Saied Alshahrani,Reem I. Masoud,Alaa Alzahrani,Deema Alnuhait,Emad A. Alghamdi,Khalid Almubarak*

Main category: cs.CL

TL;DR: A review of Arabic post-training datasets on Hugging Face Hub highlights gaps in task diversity, documentation, and adoption, with recommendations for future improvements.


<details>
  <summary>Details</summary>
Motivation: To assess and improve the quality and diversity of Arabic post-training datasets for better alignment of LLMs with human instructions.

Method: Evaluation of datasets along four dimensions: LLM Capabilities, Steerability, Alignment, and Robustness, using criteria like popularity, adoption, and documentation.

Result: Identified gaps include limited task diversity, poor documentation, and low community adoption.

Conclusion: The findings underscore the need for better Arabic post-training datasets to advance Arabic LLMs, with actionable recommendations provided.

Abstract: Post-training has emerged as a crucial technique for aligning pre-trained
Large Language Models (LLMs) with human instructions, significantly enhancing
their performance across a wide range of tasks. Central to this process is the
quality and diversity of post-training datasets. This paper presents a review
of publicly available Arabic post-training datasets on the Hugging Face Hub,
organized along four key dimensions: (1) LLM Capabilities (e.g., Question
Answering, Translation, Reasoning, Summarization, Dialogue, Code Generation,
and Function Calling); (2) Steerability (e.g., persona and system prompts); (3)
Alignment (e.g., cultural, safety, ethics, and fairness), and (4) Robustness.
Each dataset is rigorously evaluated based on popularity, practical adoption,
recency and maintenance, documentation and annotation quality, licensing
transparency, and scientific contribution. Our review revealed critical gaps in
the development of Arabic post-training datasets, including limited task
diversity, inconsistent or missing documentation and annotation, and low
adoption across the community. Finally, the paper discusses the implications of
these gaps on the progress of Arabic LLMs and applications while providing
concrete recommendations for future efforts in post-training dataset
development.

</details>


### [29] [Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation](https://arxiv.org/abs/2507.14693)
*Amina Dzafic,Merve Kavut,Ulya Bayram*

Main category: cs.CL

TL;DR: The paper addresses gaps in suicidal ideation detection by creating a Turkish corpus and evaluating label reliability and model performance across datasets, advocating for better practices in mental health NLP.


<details>
  <summary>Details</summary>
Motivation: Limited language coverage and unreliable annotation practices in suicidal ideation detection datasets hinder global suicide prevention efforts.

Method: Constructed a Turkish suicidal ideation corpus using social media posts and a resource-efficient annotation framework. Evaluated label reliability and model consistency across datasets using transfer learning with pre-trained classifiers.

Result: Highlighted the need for rigorous, language-inclusive annotation and evaluation, and questioned the performance of models with zero-shot transfer learning.

Conclusion: Advocates for transparency in dataset construction and model training, prioritizing reliability in mental health NLP.

Abstract: Suicidal ideation detection is critical for real-time suicide prevention, yet
its progress faces two under-explored challenges: limited language coverage and
unreliable annotation practices. Most available datasets are in English, but
even among these, high-quality, human-annotated data remains scarce. As a
result, many studies rely on available pre-labeled datasets without examining
their annotation process or label reliability. The lack of datasets in other
languages further limits the global realization of suicide prevention via
artificial intelligence (AI). In this study, we address one of these gaps by
constructing a novel Turkish suicidal ideation corpus derived from social media
posts and introducing a resource-efficient annotation framework involving three
human annotators and two large language models (LLMs). We then address the
remaining gaps by performing a bidirectional evaluation of label reliability
and model consistency across this dataset and three popular English suicidal
ideation detection datasets, using transfer learning through eight pre-trained
sentiment and emotion classifiers. These transformers help assess annotation
consistency and benchmark model performance against manually labeled data. Our
findings underscore the need for more rigorous, language-inclusive approaches
to annotation and evaluation in mental health natural language processing (NLP)
while demonstrating the questionable performance of popular models with
zero-shot transfer learning. We advocate for transparency in model training and
dataset construction in mental health NLP, prioritizing data and model
reliability.

</details>


### [30] [Disparities in Peer Review Tone and the Role of Reviewer Anonymity](https://arxiv.org/abs/2507.14741)
*Maria Sahakyan,Bedoor AlShebli*

Main category: cs.CL

TL;DR: The study analyzes linguistic biases in peer review across 80,000 reviews, revealing disparities in tone, sentiment, and support based on author demographics and reviewer anonymity.


<details>
  <summary>Details</summary>
Motivation: To uncover hidden biases in peer review language and assess how reviewer anonymity affects fairness.

Method: Natural language processing and large-scale statistical modeling of reviews from two major journals, comparing anonymous and signed reviews.

Result: Reveals disparities in review language linked to author demographics and challenges assumptions about anonymity's role in fairness.

Conclusion: Highlights the need for reform in peer review policies to address biases and promote equitable scientific progress.

Abstract: The peer review process is often regarded as the gatekeeper of scientific
integrity, yet increasing evidence suggests that it is not immune to bias.
Although structural inequities in peer review have been widely debated, much
less attention has been paid to the subtle ways in which language itself may
reinforce disparities. This study undertakes one of the most comprehensive
linguistic analyses of peer review to date, examining more than 80,000 reviews
in two major journals. Using natural language processing and large-scale
statistical modeling, it uncovers how review tone, sentiment, and supportive
language vary across author demographics, including gender, race, and
institutional affiliation. Using a data set that includes both anonymous and
signed reviews, this research also reveals how the disclosure of reviewer
identity shapes the language of evaluation. The findings not only expose hidden
biases in peer feedback, but also challenge conventional assumptions about
anonymity's role in fairness. As academic publishing grapples with reform,
these insights raise critical questions about how review policies shape career
trajectories and scientific progress.

</details>


### [31] [On the robustness of modeling grounded word learning through a child's egocentric input](https://arxiv.org/abs/2507.14749)
*Wai Keen Vong,Brenden M. Lake*

Main category: cs.CL

TL;DR: Neural networks trained on limited, child-like input can learn word-referent mappings, showing robustness across architectures but revealing individual learning differences.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between machine learning models (trained on massive data) and human children (learning from limited input), researchers explored training neural networks on child-like data.

Method: Automated speech transcription was applied to the SAYCam dataset (500+ hours of video from three children) to create multimodal datasets. Various neural network configurations were tested for word learning.

Result: Networks successfully learned and generalized word-referent mappings across architectures, validating robustness but showing individual differences in learning patterns.

Conclusion: Multimodal neural networks can robustly simulate word learning from child-like input, though individual developmental experiences influence learning outcomes.

Abstract: What insights can machine learning bring to understanding human language
acquisition? Large language and multimodal models have achieved remarkable
capabilities, but their reliance on massive training datasets creates a
fundamental mismatch with children, who succeed in acquiring language from
comparatively limited input. To help bridge this gap, researchers have
increasingly trained neural networks using data similar in quantity and quality
to children's input. Taking this approach to the limit, Vong et al. (2024)
showed that a multimodal neural network trained on 61 hours of visual and
linguistic input extracted from just one child's developmental experience could
acquire word-referent mappings. However, whether this approach's success
reflects the idiosyncrasies of a single child's experience, or whether it would
show consistent and robust learning patterns across multiple children's
experiences was not explored. In this article, we applied automated speech
transcription methods to the entirety of the SAYCam dataset, consisting of over
500 hours of video data spread across all three children. Using these automated
transcriptions, we generated multi-modal vision-and-language datasets for both
training and evaluation, and explored a range of neural network configurations
to examine the robustness of simulated word learning. Our findings demonstrate
that networks trained on automatically transcribed data from each child can
acquire and generalize word-referent mappings across multiple network
architectures. These results validate the robustness of multimodal neural
networks for grounded word learning, while highlighting the individual
differences that emerge in how models learn when trained on each child's
developmental experiences.

</details>


### [32] [GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization](https://arxiv.org/abs/2507.14758)
*Luyi Ma,Wanjia Zhang,Kai Zhao,Abhishek Kulkarni,Lalitesh Morishetti,Anjana Ganesh,Ashish Ranjan,Aashika Padmanabhan,Jianpeng Xu,Jason Cho,Praveen Kanumala,Kaushiki Nag,Sumit Dutta,Kamiya Motwani,Malay Patel,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.CL

TL;DR: GRACE improves multi-behavior recommendation by combining Chain-of-Thought tokenization and Journey-Aware Sparse Attention, achieving significant performance gains and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Current generative models for multi-behavior recommendation lack interpretability, efficiency, and multi-scale modeling.

Method: GRACE uses hybrid Chain-of-Thought tokenization and Journey-Aware Sparse Attention for efficient, interpretable recommendations.

Result: GRACE outperforms baselines by up to +106.9% HR@10 and reduces attention computation by 48%.

Conclusion: GRACE offers a scalable, interpretable solution for multi-behavior sequential recommendation.

Abstract: Generative models have recently demonstrated strong potential in
multi-behavior recommendation systems, leveraging the expressive power of
transformers and tokenization to generate personalized item sequences. However,
their adoption is hindered by (1) the lack of explicit information for token
reasoning, (2) high computational costs due to quadratic attention complexity
and dense sequence representations after tokenization, and (3) limited
multi-scale modeling over user history. In this work, we propose GRACE
(Generative Recommendation via journey-aware sparse Attention on
Chain-of-thought tokEnization), a novel generative framework for multi-behavior
sequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT)
tokenization method that encodes user-item interactions with explicit
attributes from product knowledge graphs (e.g., category, brand, price) over
semantic tokenization, enabling interpretable and behavior-aligned generation.
To address the inefficiency of standard attention, we design a Journey-Aware
Sparse Attention (JSA) mechanism, which selectively attends to compressed,
intra-, inter-, and current-context segments in the tokenized sequence.
Experiments on two real-world datasets show that GRACE significantly
outperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and
+106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home
domain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces
attention computation by up to 48% with long sequences.

</details>


### [33] [FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing](https://arxiv.org/abs/2507.14815)
*Shoutao Guo,Shaolei Zhang,Qingkai Fang,Zhengrui Ma,Min Zhang,Yang Feng*

Main category: cs.CL

TL;DR: FastLongSpeech is a framework for efficient long-speech processing in LSLMs without needing long-speech training data, using iterative fusion and dynamic compression training.


<details>
  <summary>Details</summary>
Motivation: Existing LSLMs lack efficiency in long-speech processing due to data scarcity and high computational costs.

Method: Introduces FastLongSpeech with iterative fusion and dynamic compression training to adapt LSLMs for long-speech tasks.

Result: Strong performance in long- and short-speech tasks with improved inference efficiency.

Conclusion: FastLongSpeech effectively bridges the gap in long-speech processing for LSLMs.

Abstract: The rapid advancement of Large Language Models (LLMs) has spurred significant
progress in Large Speech-Language Models (LSLMs), enhancing their capabilities
in both speech understanding and generation. While existing LSLMs often
concentrate on augmenting speech generation or tackling a diverse array of
short-speech tasks, the efficient processing of long-form speech remains a
critical yet underexplored challenge. This gap is primarily attributed to the
scarcity of long-speech training datasets and the high computational costs
associated with long sequences. To address these limitations, we introduce
FastLongSpeech, a novel framework designed to extend LSLM capabilities for
efficient long-speech processing without necessitating dedicated long-speech
training data. FastLongSpeech incorporates an iterative fusion strategy that
can compress excessively long-speech sequences into manageable lengths. To
adapt LSLMs for long-speech inputs, it introduces a dynamic compression
training approach, which exposes the model to short-speech sequences at varying
compression ratios, thereby transferring the capabilities of LSLMs to
long-speech tasks. To assess the long-speech capabilities of LSLMs, we develop
a long-speech understanding benchmark called LongSpeech-Eval. Experiments show
that our method exhibits strong performance in both long-speech and
short-speech tasks, while greatly improving inference efficiency.

</details>


### [34] [Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents](https://arxiv.org/abs/2507.14819)
*Akriti Jain,Pritika Ramu,Aparna Garimella,Apoorv Saxena*

Main category: cs.CL

TL;DR: The paper introduces intent-based chart generation from documents using a two-staged LLM framework, outperforming baselines in accuracy and chart type selection.


<details>
  <summary>Details</summary>
Motivation: Existing methods for text-to-chart generation require manual content selection, limiting real-world applicability. The paper addresses this by automating chart generation from long documents based on user intents.

Method: An unsupervised, two-staged framework: (1) LLM extracts and refines data from documents by decomposing intents, (2) heuristic-guided module selects chart type before code generation. An attribution-based metric evaluates data accuracy.

Result: The method outperforms baselines by up to 9 points in data accuracy and 17 points in chart type selection, validated on a curated dataset of 1,242 tuples across finance and scientific domains.

Conclusion: The proposed framework effectively automates intent-based chart generation from documents, improving accuracy and adaptability over existing methods.

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
transforming text descriptions or tables to data visualizations via
instruction-tuning methods. However, it is not straightforward to apply these
methods directly for a more real-world use case of visualizing data from long
documents based on user-given intents, as opposed to the user pre-selecting the
relevant content manually. We introduce the task of intent-based chart
generation from documents: given a user-specified intent and document(s), the
goal is to generate a chart adhering to the intent and grounded on the
document(s) in a zero-shot setting. We propose an unsupervised, two-staged
framework in which an LLM first extracts relevant information from the
document(s) by decomposing the intent and iteratively validates and refines
this data. Next, a heuristic-guided module selects an appropriate chart type
before final code generation. To assess the data accuracy of the generated
charts, we propose an attribution-based metric that uses a structured textual
representation of charts, instead of relying on visual decoding metrics that
often fail to capture the chart data effectively. To validate our approach, we
curate a dataset comprising of 1,242 $<$intent, document, charts$>$ tuples from
two domains, finance and scientific, in contrast to the existing datasets that
are largely limited to parallel text descriptions/ tables and their
corresponding charts. We compare our approach with baselines using single-shot
chart generation using LLMs and query-based retrieval methods; our method
outperforms by upto $9$ points and $17$ points in terms of chart data accuracy
and chart type respectively over the best baselines.

</details>


### [35] [Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding](https://arxiv.org/abs/2507.14849)
*Yifei Wang*

Main category: cs.CL

TL;DR: Reasoning distillation improves smaller models' long-context understanding, mitigating the 'lost in the middle' issue in Retrieval-Augmented Generation (RAG) systems.


<details>
  <summary>Details</summary>
Motivation: To explore how large-scale reasoning distillation affects in-context retrieval and reasoning, given the growing importance of RAG systems.

Method: Comprehensive investigation using open-source models distilled from Deepseek-R1, evaluated through multi-document QA tasks.

Result: Distilled reasoning patterns enhance long-context comprehension by promoting detailed reasoning during context analysis.

Conclusion: Reasoning distillation significantly improves long-context awareness and mitigates common issues in long-context models.

Abstract: Reasoning distillation has emerged as an effective approach to enhance the
reasoning capabilities of smaller language models. However, the impact of
large-scale reasoning distillation on other critical abilities, particularly
in-context retrieval and reasoning, remains unexplored. This gap in
understanding is particularly significant given the increasing importance of
Retrieval-Augmented Generation (RAG) systems, where efficient acquisition and
utilization of contextual information are paramount for generating reliable
responses. Motivated by the need to understand how the extended long-CoT
process influences long-context comprehension, we conduct a comprehensive
investigation using a series of open-source models distilled from Deepseek-R1,
renowned for its exceptional reasoning capabilities. Our study focuses on
evaluating these models' performance in extracting and integrating relevant
information from extended contexts through multi-document question and
answering tasks. Through rigorous experimentation, we demonstrate that
distilled reasoning patterns significantly improve long-context understanding.
Our analysis reveals that distillation fosters greater long-context awareness
by promoting more detailed and explicit reasoning processes during context
analysis and information parsing. This advancement effectively mitigates the
persistent "lost in the middle" issue that has hindered long-context models.

</details>


### [36] [Tiny language models](https://arxiv.org/abs/2507.14871)
*Ronit D. Gross,Yarden Tzach,Tal Halevi,Ella Koresh,Ido Kanter*

Main category: cs.CL

TL;DR: The study investigates whether tiny language models (TLMs) mimic key features of large language models (LLMs), showing pre-training effectiveness even at small scales. TLMs' performance improves with dataset size and token overlap, and shallow architectures can match deep ones in accuracy.


<details>
  <summary>Details</summary>
Motivation: The high computational cost of LLM pre-training restricts research participation, necessitating more accessible alternatives like TLMs.

Method: Pre-training BERT-6 and BERT-1 variants on Wikipedia subsets and evaluating on FewRel, AGNews, and DBPedia tasks.

Result: Pre-trained TLMs outperform non-pre-trained ones, with performance scaling with dataset size and token overlap. Shallow architectures can replicate deep TLM accuracy.

Conclusion: TLMs offer a viable, resource-efficient alternative to LLMs, with potential insights into NLP mechanisms and human language development.

Abstract: A prominent achievement of natural language processing (NLP) is its ability
to understand and generate meaningful human language. This capability relies on
complex feedforward transformer block architectures pre-trained on large
language models (LLMs). However, LLM pre-training is currently feasible only
for a few dominant companies due to the immense computational resources
required, limiting broader research participation. This creates a critical need
for more accessible alternatives. In this study, we explore whether tiny
language models (TLMs) exhibit the same key qualitative features of LLMs. We
demonstrate that TLMs exhibit a clear performance gap between pre-trained and
non-pre-trained models across classification tasks, indicating the
effectiveness of pre-training, even at a tiny scale. The performance gap
increases with the size of the pre-training dataset and with greater overlap
between tokens in the pre-training and classification datasets. Furthermore,
the classification accuracy achieved by a pre-trained deep TLM architecture can
be replicated through a soft committee of multiple, independently pre-trained
shallow architectures, enabling low-latency TLMs without affecting
classification accuracy. Our results are based on pre-training BERT-6 and
variants of BERT-1 on subsets of the Wikipedia dataset and evaluating their
performance on FewRel, AGNews, and DBPedia classification tasks. Future
research on TLM is expected to further illuminate the mechanisms underlying
NLP, especially given that its biologically inspired models suggest that TLMs
may be sufficient for children or adolescents to develop language.

</details>


### [37] [MEKiT: Multi-source Heterogeneous Knowledge Injection Method via Instruction Tuning for Emotion-Cause Pair Extraction](https://arxiv.org/abs/2507.14887)
*Shiyi Mu,Yongkang Liu,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.CL

TL;DR: MEKiT improves LLMs' performance on Emotion-Cause Pair Extraction by injecting multi-source heterogeneous knowledge, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: LLMs underperform on ECPE due to lack of auxiliary knowledge, limiting emotion perception and causal reasoning.

Method: MEKiT integrates internal emotional and external causal knowledge using instruction templates and mixed data for instruction-tuning.

Result: MEKiT significantly boosts LLMs' performance on ECPE, surpassing baselines.

Conclusion: MEKiT offers an effective, adaptable solution for ECPE, enhancing LLMs' capabilities in emotion and cause reasoning.

Abstract: Although large language models (LLMs) excel in text comprehension and
generation, their performance on the Emotion-Cause Pair Extraction (ECPE) task,
which requires reasoning ability, is often underperform smaller language model.
The main reason is the lack of auxiliary knowledge, which limits LLMs' ability
to effectively perceive emotions and reason causes. To address this issue, we
propose a novel \textbf{M}ulti-source h\textbf{E}terogeneous \textbf{K}nowledge
\textbf{i}njection me\textbf{T}hod, MEKiT, which integrates heterogeneous
internal emotional knowledge and external causal knowledge. Specifically, for
these two distinct aspects and structures of knowledge, we apply the approaches
of incorporating instruction templates and mixing data for instruction-tuning,
which respectively facilitate LLMs in more comprehensively identifying emotion
and accurately reasoning causes. Experimental results demonstrate that MEKiT
provides a more effective and adaptable solution for the ECPE task, exhibiting
an absolute performance advantage over compared baselines and dramatically
improving the performance of LLMs on the ECPE task.

</details>


### [38] [Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs](https://arxiv.org/abs/2507.14894)
*Boyi Deng,Yu Wan,Baosong Yang,Fei Huang,Wenjie Wang,Fuli Feng*

Main category: cs.CL

TL;DR: The paper addresses unexpected code-switching in LLMs, proposes SASFT to reduce it by 50%, and maintains multilingual performance.


<details>
  <summary>Details</summary>
Motivation: LLMs exhibit unexpected code-switching, degrading usability, but existing solutions lack mechanistic analysis and effectiveness.

Method: Uses sparse autoencoders to analyze code-switching, then introduces SASFT to control language feature pre-activation.

Result: SASFT reduces code-switching by over 50%, eliminates it in four cases, and maintains benchmark performance.

Conclusion: SASFT effectively mitigates code-switching while preserving LLMs' multilingual capabilities.

Abstract: Large Language Models (LLMs) have impressive multilingual capabilities, but
they suffer from unexpected code-switching, also known as language mixing,
which involves switching to unexpected languages in the model response. This
problem leads to poor readability and degrades the usability of model
responses. However, existing work on this issue lacks a mechanistic analysis
and shows limited effectiveness. In this paper, we first provide an in-depth
analysis of unexpected code-switching using sparse autoencoders and find that
when LLMs switch to a language, the features of that language exhibit excessive
pre-activation values. Based on our findings, we propose $\textbf{S}$parse
$\textbf{A}$utoencoder-guided $\textbf{S}$upervised
$\textbf{F}$ine$\textbf{t}$uning (SASFT), which teaches LLMs to maintain
appropriate pre-activation values of specific language features during
training. Experiments on five models across three languages demonstrate that
SASFT consistently reduces unexpected code-switching by more than 50\% compared
to standard supervised fine-tuning, with complete elimination in four cases.
Moreover, SASFT maintains or even improves the models' performance on six
multilingual benchmarks, showing its effectiveness in addressing code-switching
while preserving multilingual capabilities.

</details>


### [39] [From Neurons to Semantics: Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment](https://arxiv.org/abs/2507.14900)
*Chongxuan Huang,Yongshi Ye,Biao Fu,Qifeng Su,Xiaodong Shi*

Main category: cs.CL

TL;DR: NeuronXA is a novel method for evaluating cross-lingual alignment in LLMs, achieving high correlation with downstream tasks and transferability using minimal data.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for cross-lingual alignment focus on sentence embeddings, which may not capture semantic alignment well, especially for low-resource languages.

Method: Proposes NeuronXA, a neuron state-based approach inspired by neuroscientific findings, to assess cross-lingual alignment in LLMs.

Result: NeuronXA achieves Pearson correlations of 0.9556 with downstream tasks and 0.8514 with transferability using only 100 parallel sentence pairs.

Conclusion: NeuronXA effectively evaluates cross-lingual alignment and transferability, advancing research and improving semantic understanding in multilingual LLMs.

Abstract: Large language models (LLMs) have demonstrated remarkable multilingual
capabilities, however, how to evaluate cross-lingual alignment remains
underexplored. Existing alignment benchmarks primarily focus on sentence
embeddings, but prior research has shown that neural models tend to induce a
non-smooth representation space, which impact of semantic alignment evaluation
on low-resource languages. Inspired by neuroscientific findings that similar
information activates overlapping neuronal regions, we propose a novel Neuron
State-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a
lignment capabilities of LLMs, which offers a more semantically grounded
approach to assess cross-lingual alignment. We evaluate NeuronXA on several
prominent multilingual LLMs (LLaMA, Qwen, Mistral, GLM, and OLMo) across two
transfer tasks and three multilingual benchmarks. The results demonstrate that
with only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation
of 0.9556 with downstream tasks performance and 0.8514 with transferability.
These findings demonstrate NeuronXA's effectiveness in assessing both
cross-lingual alignment and transferability, even with a small dataset. This
highlights its potential to advance cross-lingual alignment research and to
improve the semantic understanding of multilingual LLMs.

</details>


### [40] [PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation](https://arxiv.org/abs/2507.14913)
*Eliya Habba,Noam Dahan,Gili Lior,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: PromptSuite is a framework for automatic generation of varied prompts to improve LLM evaluation reliability.


<details>
  <summary>Details</summary>
Motivation: Single-prompt evaluations of LLMs are unreliable due to sensitivity to small changes, but creating diverse prompts manually is challenging.

Method: PromptSuite uses modular prompt design for controlled perturbations, supports extensibility, and works across tasks and benchmarks.

Result: Case studies demonstrate PromptSuite generates meaningful prompt variations for robust evaluation.

Conclusion: PromptSuite offers a practical solution for multi-prompt LLM evaluation, available via Python API and web interface.

Abstract: Evaluating LLMs with a single prompt has proven unreliable, with small
changes leading to significant performance differences. However, generating the
prompt variations needed for a more robust multi-prompt evaluation is
challenging, limiting its adoption in practice. To address this, we introduce
PromptSuite, a framework that enables the automatic generation of various
prompts. PromptSuite is flexible - working out of the box on a wide range of
tasks and benchmarks. It follows a modular prompt design, allowing controlled
perturbations to each component, and is extensible, supporting the addition of
new components and perturbation types. Through a series of case studies, we
show that PromptSuite provides meaningful variations to support strong
evaluation practices. It is available through both a Python API:
https://github.com/eliyahabba/PromptSuite, and a user-friendly web interface:
https://promptsuite.streamlit.app/

</details>


### [41] [SYNTHIA: Synthetic Yet Naturally Tailored Human-Inspired PersonAs](https://arxiv.org/abs/2507.14922)
*Vahid Rahimzadeh,Erfan Moosavi Monazzah,Mohammad Taher Pilehvar,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: SYNTHIA is a dataset of 30,000 backstories from 10,000 real BlueSky users, combining synthetic generation with real data for improved consistency and realism in persona-driven LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing methods for persona-driven LLMs are either costly (human-curated) or lack realism (synthetic). SYNTHIA bridges this gap by grounding synthetic personas in real user activity.

Method: SYNTHIA derives backstories from real social media users (BlueSky) across three time windows, incorporating temporal and social interaction metadata.

Result: SYNTHIA matches state-of-the-art methods in demographic diversity and survey alignment but excels in narrative consistency. It also enables new research via temporal and social metadata.

Conclusion: SYNTHIA offers a balanced, realistic, and research-enabling approach to persona-driven LLMs, outperforming existing methods in consistency and utility.

Abstract: Persona-driven LLMs have emerged as powerful tools in computational social
science, yet existing approaches fall at opposite extremes, either relying on
costly human-curated data or producing synthetic personas that lack consistency
and realism. We introduce SYNTHIA, a dataset of 30,000 backstories derived from
10,000 real social media users from BlueSky open platform across three time
windows, bridging this spectrum by grounding synthetic generation in authentic
user activity. Our evaluation demonstrates that SYNTHIA achieves competitive
performance with state-of-the-art methods in demographic diversity and social
survey alignment while significantly outperforming them in narrative
consistency. Uniquely, SYNTHIA incorporates temporal dimensionality and
provides rich social interaction metadata from the underlying network, enabling
new research directions in computational social science and persona-driven
language modeling.

</details>


### [42] [MUR: Momentum Uncertainty guided Reasoning for Large Language Models](https://arxiv.org/abs/2507.14958)
*Hang Yan,Fangzhi Xu,Rongman Xu,Yifei Li,Jian Zhang,Haoran Luo,Xiaobao Wu,Luu Anh Tuan,Haiteng Zhao,Qika Lin,Jun Liu*

Main category: cs.CL

TL;DR: MUR dynamically allocates thinking budgets to LLMs using momentum-inspired uncertainty tracking, reducing computation by 50% and improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Optimizing reasoning efficiency in LLMs without additional training, addressing overthinking in Test-Time Scaling.

Method: Proposes Momentum Uncertainty-guided Reasoning (MUR) with gamma-control for adaptive budget allocation.

Result: Reduces computation by 50% on average and improves accuracy by 0.62-3.37% across benchmarks.

Conclusion: MUR efficiently enhances LLM reasoning with minimal overhead, validated by theoretical and empirical results.

Abstract: Large Language Models (LLMs) have achieved impressive performance on
reasoning-intensive tasks, yet optimizing their reasoning efficiency remains an
open challenge. While Test-Time Scaling (TTS) improves reasoning quality, it
often leads to overthinking, wasting tokens on redundant computations. This
work investigates how to efficiently and adaptively guide LLM test-time scaling
without additional training. Inspired by the concept of momentum in physics, we
propose Momentum Uncertainty-guided Reasoning (MUR), which dynamically
allocates thinking budgets to critical reasoning steps by tracking and
aggregating stepwise uncertainty over time. To support flexible inference-time
control, we introduce gamma-control, a simple mechanism that tunes the
reasoning budget via a single hyperparameter. We provide in-depth theoretical
proof to support the superiority of MUR in terms of stability and biases. MUR
is comprehensively evaluated against various TTS methods across four
challenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using
different sizes of recent Qwen3 models (1.7B, 4B, and 8B). Results demonstrate
that MUR reduces computation by over 50% on average while improving accuracy by
0.62-3.37%.

</details>


### [43] [RefCritic: Training Long Chain-of-Thought Critic Models with Refinement Feedback](https://arxiv.org/abs/2507.15024)
*Qiaoyu Tang,Hao Xiang,Le Yu,Bowen Yu,Hongyu Lin,Yaojie Lu,Xianpei Han,Le Sun,Junyang Lin*

Main category: cs.CL

TL;DR: RefCritic, a reinforcement learning-based critic module, outperforms supervised fine-tuning methods by generating actionable critiques, improving model refinement across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current supervised fine-tuning for critic modules fails to enhance critique abilities, producing superficial feedback. RefCritic aims to unlock superior critique capabilities.

Method: RefCritic uses reinforcement learning with dual rule-based rewards: correctness of judgments and refinement accuracies, to generate high-quality critiques.

Result: RefCritic shows consistent gains (e.g., 6.8% and 7.2% on AIME25) and outperforms step-level supervised approaches on ProcessBench.

Conclusion: RefCritic effectively enhances critique quality and model refinement, demonstrating scalability and superiority over existing methods.

Abstract: With the rapid advancement of Large Language Models (LLMs), developing
effective critic modules for precise guidance has become crucial yet
challenging. In this paper, we initially demonstrate that supervised
fine-tuning for building critic modules (which is widely adopted in current
solutions) fails to genuinely enhance models' critique abilities, producing
superficial critiques with insufficient reflections and verifications. To
unlock the unprecedented critique capabilities, we propose RefCritic, a
long-chain-of-thought critic module based on reinforcement learning with dual
rule-based rewards: (1) instance-level correctness of solution judgments and
(2) refinement accuracies of the policy model based on critiques, aiming to
generate high-quality evaluations with actionable feedback that effectively
guides model refinement. We evaluate RefCritic on Qwen2.5-14B-Instruct and
DeepSeek-R1-Distill-Qwen-14B across five benchmarks. On critique and refinement
settings, RefCritic demonstrates consistent advantages across all benchmarks,
e.g., 6.8\% and 7.2\% gains on AIME25 for the respective base models. Notably,
under majority voting, policy models filtered by RefCritic show superior
scaling with increased voting numbers. Moreover, despite training on
solution-level supervision, RefCritic outperforms step-level supervised
approaches on ProcessBench, a benchmark to identify erroneous steps in
mathematical reasoning.

</details>


### [44] [WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization](https://arxiv.org/abs/2507.15061)
*Zhengwei Tao,Jialong Wu,Wenbiao Yin,Junkai Zhang,Baixuan Li,Haiyang Shen,Kuan Li,Liwen Zhang,Xinyu Wang,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: WebShaper introduces a formalization-driven framework for synthesizing high-quality training data for information-seeking (IS) agents, improving consistency and performance.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-quality training data limits IS agent development, and existing methods often lead to inconsistencies between information and reasoning structures.

Method: WebShaper formalizes IS tasks using set theory and Knowledge Projections (KP), then synthesizes data through a multi-step expansion process with retrieval and validation tools.

Result: WebShaper achieves state-of-the-art performance on GAIA and WebWalkerQA benchmarks.

Conclusion: The framework effectively addresses data scarcity and inconsistency, enhancing IS agent capabilities.

Abstract: The advent of Large Language Model (LLM)-powered agents has revolutionized
artificial intelligence by enabling solutions to complex, open-ended tasks
through web-based information-seeking (IS) capabilities. The scarcity of
high-quality training data has limited the development of IS agents. Existing
approaches typically adopt an information-driven paradigm that first collects
web data and then generates questions based on the retrieval. However, this may
lead to inconsistency between information structure and reasoning structure,
question and answer. To mitigate, we propose a formalization-driven IS data
synthesis framework WebShaper to construct a dataset. WebShaper systematically
formalizes IS tasks through set theory. Central to the formalization is the
concept of Knowledge Projections (KP), which enables precise control over
reasoning structure by KP operation compositions. During synthesis, we begin by
creating seed tasks, then use a multi-step expansion process. At each step, an
agentic Expander expands the current formal question more complex with
retrieval and validation tools based on our formalization. We train our model
on the synthesized dataset. Experiment results demonstrate that WebShaper
achieves state-of-the-art performance among open-sourced IS agents on GAIA and
WebWalkerQA benchmarks.

</details>


### [45] [Evaluation of Coding Schemes for Transformer-based Gene Sequence Modeling](https://arxiv.org/abs/2507.15087)
*Chenlei Gong,Yuanhe Tian,Lei Mao,Yan Song*

Main category: cs.CL

TL;DR: The paper compares k-mer segmentation and BPE tokenization for DNA sequence modeling, evaluating performance across different configurations and positional encodings. BPE outperforms k-mer, and RoPE excels for periodic motifs, while AliBi handles local dependencies well. Model depth shows diminishing returns beyond 12 layers.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate the effectiveness of k-mer segmentation versus BPE tokenization and different positional encoding methods in DNA sequence modeling using Transformers.

Method: Compared k-mer (k=1,3,4,5,6) and BPE tokenization with three positional encodings (sinusoidal, AliBi, RoPE) in 3, 6, 12, and 24-layer Transformer encoders, evaluated on the GUE benchmark dataset.

Result: BPE performs better and more stably, compressing motifs and reducing sequence length. RoPE captures periodic motifs well, while AliBi excels in local dependencies. Depth gains plateau after 12 layers.

Conclusion: BPE and RoPE are recommended for DNA Transformer models, with optimal depth around 12 layers for performance and efficiency.

Abstract: Currently, many studies view DNA sequences as a special type of language and
utilize Transformers to model them. These studies use fixed-length k-mer
segmentation and BPE subword tokenization but lack a systematic evaluation to
determine which is superior. We compare k-mer segmentation with k=1,3,4,5,6, a
4,096-token BPE vocabulary, and three positional encoding methods-sinusoidal,
AliBi, and RoPE. Each configuration is trained from scratch in 3, 6, 12, and
24-layer Transformer encoders and evaluated on GUE benchmark dataset. In
general, BPE delivers higher and more stable performance across tasks by
compressing frequent motifs into variable-length tokens, reducing sequence
length, and improving model generalization. RoPE excels at capturing periodic
motifs and extrapolating to long sequences, while AliBi also performs well on
tasks driven by local dependencies. In terms of depth, we observe significant
gains when increasing layers from 3 to 12, with only marginal improvements or
slight overfitting at 24 layers. This study provides practical guidance for
designing tokenization and positional encoding in DNA Transformer models.

</details>


### [46] [A Penalty Goes a Long Way: Measuring Lexical Diversity in Synthetic Texts Under Prompt-Influenced Length Variations](https://arxiv.org/abs/2507.15092)
*Vijeta Deshpande,Ishita Dasgupta,Uttaran Bhattacharya,Somdeb Sarkhel,Saayan Mitra,Anna Rumshisky*

Main category: cs.CL

TL;DR: The paper introduces PATTR, a new diversity metric for synthetic text, addressing biases in existing metrics caused by text length variations.


<details>
  <summary>Details</summary>
Motivation: Existing diversity metrics for synthetic text are biased by text length variations, impacting their reliability.

Method: Proposes Penalty-Adjusted Type-Token Ratio (PATTR), tested on a 20M-word synthetic corpus from LLaMA, OLMo, and Phi models for video script generation.

Result: PATTR outperforms MATTR and CR by mitigating length biases and maintaining diversity while adhering to target response length.

Conclusion: PATTR is a robust diversity metric for synthetic text, especially in tasks where length variations are common.

Abstract: Synthetic text generated by Large Language Models (LLMs) is increasingly used
for further training and improvement of LLMs. Diversity is crucial for the
effectiveness of synthetic data, and researchers rely on prompt engineering to
improve diversity. However, the impact of prompt variations on response text
length, and, more importantly, the consequential effect on lexical diversity
measurements, remain underexplored. In this work, we propose Penalty-Adjusted
Type-Token Ratio (PATTR), a diversity metric robust to length variations. We
generate a large synthetic corpus of over 20M words using seven models from the
LLaMA, OLMo, and Phi families, focusing on a creative writing task of video
script generation, where diversity is crucial. We evaluate per-response lexical
diversity using PATTR and compare it against existing metrics of Moving-Average
TTR (MATTR) and Compression Ratio (CR). Our analysis highlights how text length
variations introduce biases favoring shorter responses. Unlike existing
metrics, PATTR explicitly considers the task-specific target response length
($L_T$) to effectively mitigate length biases. We further demonstrate the
utility of PATTR in filtering the top-10/100/1,000 most lexically diverse
responses, showing that it consistently outperforms MATTR and CR by yielding on
par or better diversity with high adherence to $L_T$.

</details>


### [47] [Filling the Gap: Is Commonsense Knowledge Generation useful for Natural Language Inference?](https://arxiv.org/abs/2507.15100)
*Chathuri Jayaweera,Brianna Yanqui,Bonnie Dorr*

Main category: cs.CL

TL;DR: The paper explores using Large Language Models (LLMs) as commonsense knowledge generators for Natural Language Inference (NLI), evaluating their reliability and impact on prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing commonsense resources lack coverage for diverse premise-hypothesis pairs, prompting the use of LLMs to fill this gap.

Method: The study adapts metrics to assess LLM factuality and consistency in generating commonsense knowledge for NLI.

Result: Incorporating commonsense knowledge doesn't consistently improve overall results but helps distinguish entailing instances and moderately aids in contradictory and neutral inferences.

Conclusion: LLMs show potential as commonsense knowledge generators for NLI, though their impact varies across inference types.

Abstract: Natural Language Inference (NLI) is the task of determining the semantic
entailment of a premise for a given hypothesis. The task aims to develop
systems that emulate natural human inferential processes where commonsense
knowledge plays a major role. However, existing commonsense resources lack
sufficient coverage for a variety of premise-hypothesis pairs. This study
explores the potential of Large Language Models as commonsense knowledge
generators for NLI along two key dimensions: their reliability in generating
such knowledge and the impact of that knowledge on prediction accuracy. We
adapt and modify existing metrics to assess LLM factuality and consistency in
generating in this context. While explicitly incorporating commonsense
knowledge does not consistently improve overall results, it effectively helps
distinguish entailing instances and moderately improves distinguishing
contradictory and neutral inferences.

</details>


### [48] [From Disagreement to Understanding: The Case for Ambiguity Detection in NLI](https://arxiv.org/abs/2507.15114)
*Chathuri Jayaweera,Bonnie Dorr*

Main category: cs.CL

TL;DR: The paper argues that annotation disagreement in NLI stems from meaningful interpretive variation due to ambiguity, not just noise. It proposes an ambiguity-aware NLI framework and highlights the need for datasets annotated for ambiguity.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked role of ambiguity in NLI annotation disagreement and advocate for models that better align with human interpretation.

Method: Introduces a unified framework integrating existing taxonomies, classifies ambiguity subtypes, and proposes new annotated resources and unsupervised detection methods.

Result: Demonstrates how ambiguity influences annotator decisions and identifies the lack of ambiguity-annotated datasets as a key limitation.

Conclusion: Calls for ambiguity-aware NLI systems, emphasizing the need for better datasets and detection methods to improve model robustness and alignment with human interpretation.

Abstract: This position paper argues that annotation disagreement in Natural Language
Inference (NLI) is not mere noise but often reflects meaningful interpretive
variation, especially when triggered by ambiguity in the premise or hypothesis.
While underspecified guidelines and annotator behavior can contribute to
variation, content-based ambiguity offers a process-independent signal of
divergent human perspectives. We call for a shift toward ambiguity-aware NLI by
systematically identifying ambiguous input pairs and classifying ambiguity
types. To support this, we present a unified framework that integrates existing
taxonomies and illustrate key ambiguity subtypes through concrete examples.
These examples reveal how ambiguity shapes annotator decisions and motivate the
need for targeted detection methods that better align models with human
interpretation. A key limitation is the lack of datasets annotated for
ambiguity and subtypes. We propose addressing this gap through new annotated
resources and unsupervised approaches to ambiguity detection -- paving the way
for more robust, explainable, and human-aligned NLI systems.

</details>


### [49] [A Case Against Implicit Standards: Homophone Normalization in Machine Translation for Languages that use the Ge'ez Script](https://arxiv.org/abs/2507.15142)
*Hellina Hailu Nigatu,Atnafu Lambebo Tonja,Henok Biadglign Ademtew,Hizkel Mitiku Alemayehu,Negasi Haile Abadi,Tadesse Destaw Belay,Seid Muhie Yimam*

Main category: cs.CL

TL;DR: The paper examines the impact of homophone normalization in Amharic NLP, proposing post-inference normalization to improve BLEU scores while preserving language features.


<details>
  <summary>Details</summary>
Motivation: To address the drawbacks of homophone normalization in NLP, such as reduced model generalization and loss of language diversity.

Method: Experiments with monolingual training and cross-lingual transfer, followed by post-inference normalization of model predictions.

Result: Achieved a BLEU score increase of up to 1.03 while maintaining language features.

Conclusion: Advocates for language-aware interventions and highlights the role of technology in language change.

Abstract: Homophone normalization, where characters that have the same sound in a
writing script are mapped to one character, is a pre-processing step applied in
Amharic Natural Language Processing (NLP) literature. While this may improve
performance reported by automatic metrics, it also results in models that are
not able to understand different forms of writing in a single language.
Further, there might be impacts in transfer learning, where models trained on
normalized data do not generalize well to other languages. In this paper, we
experiment with monolingual training and cross-lingual transfer to understand
the impacts of normalization on languages that use the Ge'ez script. We then
propose a post-inference intervention in which normalization is applied to
model predictions instead of training data. With our simple scheme of
post-inference normalization, we show that we can achieve an increase in BLEU
score of up to 1.03 while preserving language features in training. Our work
contributes to the broader discussion on technology-facilitated language change
and calls for more language-aware interventions.

</details>


### [50] [What Level of Automation is "Good Enough"? A Benchmark of Large Language Models for Meta-Analysis Data Extraction](https://arxiv.org/abs/2507.15152)
*Lingbo Li,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: The study evaluates three LLMs (Gemini-2.0-flash, Grok-3, GPT-4o-mini) for automating data extraction from RCTs, focusing on statistical results, risk-of-bias assessments, and study-level characteristics. Customised prompts improved recall by 15%, leading to proposed guidelines for balancing automation and expert oversight.


<details>
  <summary>Details</summary>
Motivation: Automating data extraction from RCTs for meta-analysis is challenging, requiring evaluation of LLMs' performance to improve efficiency and accuracy.

Method: Tested three LLMs across four prompting strategies (basic, self-reflective, ensemble, customised) in medical domains (hypertension, diabetes, orthopaedics).

Result: High precision but poor recall; customised prompts boosted recall by 15%.

Conclusion: Proposed guidelines for task-specific automation, balancing LLM efficiency with expert oversight for real-world meta-analyses.

Abstract: Automating data extraction from full-text randomised controlled trials (RCTs)
for meta-analysis remains a significant challenge. This study evaluates the
practical performance of three LLMs (Gemini-2.0-flash, Grok-3, GPT-4o-mini)
across tasks involving statistical results, risk-of-bias assessments, and
study-level characteristics in three medical domains: hypertension, diabetes,
and orthopaedics. We tested four distinct prompting strategies (basic
prompting, self-reflective prompting, model ensemble, and customised prompts)
to determine how to improve extraction quality. All models demonstrate high
precision but consistently suffer from poor recall by omitting key information.
We found that customised prompts were the most effective, boosting recall by up
to 15\%. Based on this analysis, we propose a three-tiered set of guidelines
for using LLMs in data extraction, matching data types to appropriate levels of
automation based on task complexity and risk. Our study offers practical advice
for automating data extraction in real-world meta-analyses, balancing LLM
efficiency with expert oversight through targeted, task-specific automation.

</details>


### [51] [Collaborative Distillation Strategies for Parameter-Efficient Language Model Deployment](https://arxiv.org/abs/2507.15198)
*Xiandong Meng,Yan Wu,Yexin Tian,Xin Hu,Tianze Kang,Junliang Du*

Main category: cs.CL

TL;DR: The paper proposes a multi-teacher distillation strategy to reduce computational costs and improve inference speed in large language models, achieving strong performance with a smaller student model.


<details>
  <summary>Details</summary>
Motivation: Addressing the high computational cost and slow inference of large language models by leveraging knowledge from multiple teacher models.

Method: Uses a distillation strategy with weighted output fusion, feature alignment loss, and dynamic teacher weighting to guide a student model.

Result: The student model shows improved language understanding, generation ability, and performance across tasks like language modeling and text generation.

Conclusion: The method offers an efficient way to compress large language models and highlights the effectiveness of multi-teacher collaboration.

Abstract: This paper addresses the challenges of high computational cost and slow
inference in deploying large language models. It proposes a distillation
strategy guided by multiple teacher models. The method constructs several
teacher models and integrates their output probability distributions and
intermediate semantic features. This guides the student model to learn from
multiple sources of knowledge. As a result, the student model gains stronger
language understanding and generation ability while maintaining a small
parameter size. To achieve this, the paper introduces a weighted output fusion
mechanism, a feature alignment loss function, and an entropy-driven dynamic
teacher weighting strategy. These components improve the quality and stability
of knowledge transfer during distillation. Under multi-teacher guidance, the
student model captures semantic information more effectively and demonstrates
strong performance across multiple evaluation metrics. In particular, the
method shows high consistency in expression, generalization ability, and task
adaptability in tasks such as language modeling, text generation, and
multi-task learning. The experiments compare the proposed method with several
widely adopted distillation approaches. The results further confirm its overall
advantages in perplexity, distillation loss, and generation quality. This study
provides a feasible technical path for the efficient compression of large-scale
language models. It also demonstrates the effectiveness of multi-teacher
collaborative mechanisms in complex language modeling tasks.

</details>


### [52] [SOI Matters: Analyzing Multi-Setting Training Dynamics in Pretrained Language Models via Subsets of Interest](https://arxiv.org/abs/2507.15236)
*Shayan Vassef,Amirhossein Dabiriaghdam,Mohammadreza Bakhtiari,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: The paper explores how multi-task, multi-lingual, and multi-source learning affect pretrained language models, introducing a novel framework (SOI) to categorize learning behaviors. Experiments show multi-source learning boosts out-of-distribution performance, while multi-task learning benefits similar tasks. A two-stage fine-tuning method using SOI further enhances results.


<details>
  <summary>Details</summary>
Motivation: To understand and improve the robustness and performance of pretrained language models in multi-setting configurations by analyzing learning behaviors and transitions.

Method: Introduces Subsets of Interest (SOI) to categorize learning behaviors, uses SOI transition heatmaps and dataset cartography, and conducts experiments comparing multi-task, multi-source, and multi-lingual learning against single-setting baselines.

Result: Multi-source learning improves out-of-distribution performance by up to 7%, multi-task learning benefits similar tasks, and SOI-based subset selection in two-stage fine-tuning yields additional gains.

Conclusion: The study offers insights into training dynamics and practical methods for optimizing language models in multi-setting scenarios, with SOI proving valuable for performance enhancement.

Abstract: This work investigates the impact of multi-task, multi-lingual, and
multi-source learning approaches on the robustness and performance of
pretrained language models. To enhance this analysis, we introduce Subsets of
Interest (SOI), a novel categorization framework that identifies six distinct
learning behavior patterns during training, including forgettable examples,
unlearned examples, and always correct examples. Through SOI transition
heatmaps and dataset cartography visualization, we analyze how examples shift
between these categories when transitioning from single-setting to
multi-setting configurations. We perform comprehensive experiments across three
parallel comparisons: multi-task vs. single-task learning using English tasks
(entailment, paraphrase, sentiment), multi-source vs. single-source learning
using sentiment analysis datasets, and multi-lingual vs. single-lingual
learning using intent classification in French, English, and Persian. Our
results demonstrate that multi-source learning consistently improves
out-of-distribution performance by up to 7%, while multi-task learning shows
mixed results with notable gains in similar task combinations. We further
introduce a two-stage fine-tuning approach where the second stage leverages
SOI-based subset selection to achieve additional performance improvements.
These findings provide new insights into training dynamics and offer practical
approaches for optimizing multi-setting language model performance.

</details>


### [53] [ChiMed 2.0: Advancing Chinese Medical Dataset in Facilitating Large Language Modeling](https://arxiv.org/abs/2507.15275)
*Yuanhe Tian,Junjie Liu,Zhizhou Kou,Yuxiang Li,Yan Song*

Main category: cs.CL

TL;DR: ChiMed 2.0 is a large-scale Chinese medical dataset designed for pre-training, fine-tuning, and RLHF, addressing gaps in existing resources.


<details>
  <summary>Details</summary>
Motivation: Existing Chinese medical datasets are limited in size and scope, hindering effective AI development in the domain.

Method: ChiMed 2.0 extends the previous ChiMed dataset, incorporating data from online platforms and LLMs, and includes pre-training, SFT, and RLHF components.

Result: Experiments show performance gains across model scales, validating the dataset's effectiveness.

Conclusion: ChiMed 2.0 successfully addresses limitations in existing datasets and supports advanced AI training in the Chinese medical domain.

Abstract: Building high-quality data resources is crucial for advancing artificial
intelligence research and applications in specific domains, particularly in the
Chinese medical domain. Existing Chinese medical datasets are limited in size
and narrow in domain coverage, falling short of the diverse corpora required
for effective pre-training. Moreover, most datasets are designed solely for LLM
fine-tuning and do not support pre-training and reinforcement learning from
human feedback (RLHF). In this paper, we propose a Chinese medical dataset
named ChiMed 2.0, which extends our previous work ChiMed, and covers data
collected from Chinese medical online platforms and generated by LLMs. ChiMed
2.0 contains 204.4M Chinese characters covering both traditional Chinese
medicine classics and modern general medical data, where there are 164.8K
documents for pre-training, 351.6K question-answering pairs for supervised
fine-tuning (SFT), and 41.7K preference data tuples for RLHF. To validate the
effectiveness of our approach for training a Chinese medical LLM, we conduct
further pre-training, SFT, and RLHF experiments on representative general
domain LLMs and evaluate their performance on medical benchmark datasets. The
results show performance gains across different model scales, validating the
dataset's effectiveness and applicability.

</details>


### [54] [A Novel Self-Evolution Framework for Large Language Models](https://arxiv.org/abs/2507.15281)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.CL

TL;DR: The paper introduces the Dual-Phase Self-Evolution (DPSE) framework to enhance LLMs by jointly optimizing user preference adaptation and domain-specific competence, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing post-training strategies for LLMs improve user alignment but lack domain cognition enhancement, creating a gap this work addresses.

Method: DPSE uses a Censor module to extract interaction signals and guide structured data expansion, followed by a two-stage fine-tuning pipeline.

Result: DPSE outperforms baselines like Supervised Fine-Tuning and Preference Optimization in general NLP benchmarks and long-term dialogue tasks.

Conclusion: The DPSE framework offers a path for autonomous, continual self-evolution of LLMs, validated by ablation studies.

Abstract: The capabilities of Large Language Models (LLMs) are limited to some extent
by pre-training, so some researchers optimize LLMs through post-training.
Existing post-training strategies, such as memory-based retrieval or preference
optimization, improve user alignment yet fail to enhance the model's domain
cognition. To bridge this gap, we propose a novel Dual-Phase Self-Evolution
(DPSE) framework that jointly optimizes user preference adaptation and
domain-specific competence. DPSE introduces a Censor module to extract
multi-dimensional interaction signals and estimate satisfaction scores, which
guide structured data expansion via topic-aware and preference-driven
strategies. These expanded datasets support a two-stage fine-tuning pipeline:
supervised domain grounding followed by frequency-aware preference
optimization. Experiments across general NLP benchmarks and long-term dialogue
tasks demonstrate that DPSE consistently outperforms Supervised Fine-Tuning,
Preference Optimization, and Memory-Augmented baselines. Ablation studies
validate the contribution of each module. In this way, our framework provides
an autonomous path toward continual self-evolution of LLMs.

</details>


### [55] [Beyond Easy Wins: A Text Hardness-Aware Benchmark for LLM-generated Text Detection](https://arxiv.org/abs/2507.15286)
*Navid Ayoobi,Sadat Shahriar,Arjun Mukherjee*

Main category: cs.CL

TL;DR: The paper introduces SHIELD, a benchmark for AI text detectors, focusing on real-world reliability and stability, and proposes a humanification framework to challenge current detection methods.


<details>
  <summary>Details</summary>
Motivation: Current AI text detector evaluations overlook practical deployment issues like false positive rates and stability across domains, which SHIELD aims to address.

Method: SHIELD integrates reliability and stability into a unified metric and introduces a model-agnostic humanification framework with a controllable hardness parameter.

Result: The benchmark and framework effectively challenge state-of-the-art zero-shot detection methods in maintaining reliability and stability.

Conclusion: SHIELD provides a more practical and equitable evaluation paradigm for AI text detectors, addressing gaps in prior research.

Abstract: We present a novel evaluation paradigm for AI text detectors that prioritizes
real-world and equitable assessment. Current approaches predominantly report
conventional metrics like AUROC, overlooking that even modest false positive
rates constitute a critical impediment to practical deployment of detection
systems. Furthermore, real-world deployment necessitates predetermined
threshold configuration, making detector stability (i.e. the maintenance of
consistent performance across diverse domains and adversarial scenarios), a
critical factor. These aspects have been largely ignored in previous research
and benchmarks. Our benchmark, SHIELD, addresses these limitations by
integrating both reliability and stability factors into a unified evaluation
metric designed for practical assessment. Furthermore, we develop a post-hoc,
model-agnostic humanification framework that modifies AI text to more closely
resemble human authorship, incorporating a controllable hardness parameter.
This hardness-aware approach effectively challenges current SOTA zero-shot
detection methods in maintaining both reliability and stability. (Data and
code: https://github.com/navid-aub/SHIELD-Benchmark)

</details>


### [56] [On the Inevitability of Left-Leaning Political Bias in Aligned Language Models](https://arxiv.org/abs/2507.15328)
*Thilo Hagendorff*

Main category: cs.CL

TL;DR: The paper argues that left-wing political bias in LLMs is inevitable due to AI alignment principles (HHH), which align with progressive values, and critiques framing this bias as problematic.


<details>
  <summary>Details</summary>
Motivation: To reconcile the contradiction between AI alignment goals (HHH) and concerns about left-wing bias in LLMs, showing that such bias is inherent to alignment principles.

Method: Theoretical argumentation linking AI alignment objectives (harmless, helpful, honest) with progressive moral frameworks and left-wing principles.

Result: Demonstrates that left-wing bias in LLMs is a natural outcome of alignment, conflicting with right-wing ideologies, and critiques the framing of this bias as problematic.

Conclusion: AI alignment inherently leads to left-wing bias; framing this as a problem undermines alignment goals (HHH).

Abstract: The guiding principle of AI alignment is to train large language models
(LLMs) to be harmless, helpful, and honest (HHH). At the same time, there are
mounting concerns that LLMs exhibit a left-wing political bias. Yet, the
commitment to AI alignment cannot be harmonized with the latter critique. In
this article, I argue that intelligent systems that are trained to be harmless
and honest must necessarily exhibit left-wing political bias. Normative
assumptions underlying alignment objectives inherently concur with progressive
moral frameworks and left-wing principles, emphasizing harm avoidance,
inclusivity, fairness, and empirical truthfulness. Conversely, right-wing
ideologies often conflict with alignment guidelines. Yet, research on political
bias in LLMs is consistently framing its insights about left-leaning tendencies
as a risk, as problematic, or concerning. This way, researchers are actively
arguing against AI alignment, tacitly fostering the violation of HHH
principles.

</details>


### [57] [Reasoning Models are Test Exploiters: Rethinking Multiple-Choice](https://arxiv.org/abs/2507.15337)
*Narun Raman,Taylor Lundy,Kevin Leyton-Brown*

Main category: cs.CL

TL;DR: MCQA is a common but flawed proxy for evaluating LLMs' downstream performance, especially for state-of-the-art reasoning models. The study shows MCQA works only if models reason before seeing options, not after.


<details>
  <summary>Details</summary>
Motivation: To assess whether MCQA remains a valid proxy for evaluating LLMs' reasoning capabilities, given advancements in model performance and reasoning methods.

Method: Systematic evaluation of 15 QA benchmarks and 25 LLMs, testing 5 presentation variations (e.g., chain-of-thought timing, option inclusion).

Result: MCQA is reliable only if models reason before seeing options. Models exploiting options after reasoning outperform free-text performance, undermining MCQA's validity.

Conclusion: MCQA is no longer a good proxy for downstream performance. New benchmarks are needed to better reflect LLMs' genuine reasoning abilities.

Abstract: When evaluating Large Language Models (LLMs) in question-answering domains,
it is common to ask the model to choose among a fixed set of choices (so-called
multiple-choice question-answering, or MCQA). Although downstream tasks of
interest typically do not provide systems with explicit options among which to
choose, this approach is nevertheless widely used because it makes it makes
automatic grading straightforward and has tended to produce challenging
benchmarks that correlate sufficiently well with downstream performance. This
paper investigates the extent to which this trend continues to hold for
state-of-the-art reasoning models, describing a systematic evaluation of $15$
different question-answering benchmarks (e.g., MMLU, HLE) and $25$ different
LLMs (including small models such as Qwen 7B and relatively large models such
as Llama 70B). For each model-benchmark pair, we considered $5$ ways of
presenting the model with questions, including variations on whether multiple
choices were offered to the model at all; whether "none of the above" sometimes
replaced the right answer; and whether the model was permitted to perform
chain-of-thought reasoning before and/or after the choices were presented. MCQA
remained a good proxy for the downstream performance of models as long as they
were allowed to perform chain-of-thought reasoning only before being presented
with the options among which they had to select. On the other hand, large
models that were able to perform reasoning after being given a set of options
tended to significantly outperform their free-text performance due to
exploiting the information in the options. We conclude that MCQA is no longer a
good proxy for assessing downstream performance of state-of-the-art models, and
offer practical guidelines for designing more robust, bias-resistant benchmarks
that better reflect LLMs' genuine reasoning capabilities.

</details>


### [58] [LionGuard 2: Building Lightweight, Data-Efficient & Localised Multilingual Content Moderators](https://arxiv.org/abs/2507.15339)
*Leanne Tan,Gabriel Chua,Ziyu Ge,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: LionGuard 2 is a lightweight, multilingual moderation classifier for Singapore, outperforming commercial systems without fine-tuning large models.


<details>
  <summary>Details</summary>
Motivation: Addressing gaps in multilingual moderation, especially for low-resource languages, in real-world deployments.

Method: Uses pre-trained OpenAI embeddings and a multi-head ordinal classifier, tailored for Singapore's languages (English, Chinese, Malay, partial Tamil).

Result: Outperforms commercial and open-source systems across 17 benchmarks, including Singapore-specific datasets.

Conclusion: High-quality local data and robust multilingual embeddings enable strong moderation performance without large models. Model weights and training data are released for future LLM safety work.

Abstract: Modern moderation systems increasingly support multiple languages, but often
fail to address localisation and low-resource variants - creating safety gaps
in real-world deployments. Small models offer a potential alternative to large
LLMs, yet still demand considerable data and compute. We present LionGuard 2, a
lightweight, multilingual moderation classifier tailored to the Singapore
context, supporting English, Chinese, Malay, and partial Tamil. Built on
pre-trained OpenAI embeddings and a multi-head ordinal classifier, LionGuard 2
outperforms several commercial and open-source systems across 17 benchmarks,
including both Singapore-specific and public English datasets. The system is
actively deployed within the Singapore Government, demonstrating practical
efficacy at scale. Our findings show that high-quality local data and robust
multilingual embeddings can achieve strong moderation performance, without
fine-tuning large models. We release our model weights and part of our training
data to support future work on LLM safety.

</details>


### [59] [Probing Information Distribution in Transformer Architectures through Entropy Analysis](https://arxiv.org/abs/2507.15347)
*Amedeo Buonanno,Alessandro Rivetti,Francesco A. N. Palmieri,Giovanni Di Gennaro,Gianmarco Romano*

Main category: cs.CL

TL;DR: The paper explores entropy analysis in Transformer models to study information distribution and token-level uncertainty, using GPT as a case study.


<details>
  <summary>Details</summary>
Motivation: To understand how information is managed and transformed in Transformer-based architectures.

Method: Quantifies token-level uncertainty and examines entropy patterns across processing stages in a GPT-based model.

Result: The approach reveals insights into model behavior and internal representations.

Conclusion: This method could aid in developing interpretability and evaluation frameworks for Transformer models.

Abstract: This work explores entropy analysis as a tool for probing information
distribution within Transformer-based architectures. By quantifying token-level
uncertainty and examining entropy patterns across different stages of
processing, we aim to investigate how information is managed and transformed
within these models. As a case study, we apply the methodology to a GPT-based
large language model, illustrating its potential to reveal insights into model
behavior and internal representations. This approach may offer insights into
model behavior and contribute to the development of interpretability and
evaluation frameworks for transformer-based models

</details>


### [60] [Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding](https://arxiv.org/abs/2507.15357)
*Elisa Sanchez-Bayona,Rodrigo Agerri*

Main category: cs.CL

TL;DR: The paper evaluates LLMs' metaphor interpretation across diverse datasets and tasks, finding performance driven by surface-level features rather than metaphorical understanding.


<details>
  <summary>Details</summary>
Motivation: Address limitations in prior metaphor processing research, which was restricted to single datasets and artificial data, by using diverse datasets and realistic tasks.

Method: Conducted extensive experiments with publicly available datasets, focusing on NLI and QA tasks, analyzing LLMs' performance.

Result: LLMs' performance is influenced by lexical overlap and sentence length, not metaphorical content, suggesting no emergent understanding of metaphors.

Conclusion: Highlights LLMs' limitations in figurative language processing and calls for more realistic evaluation frameworks.

Abstract: This paper presents a comprehensive evaluation of the capabilities of Large
Language Models (LLMs) in metaphor interpretation across multiple datasets,
tasks, and prompt configurations. Although metaphor processing has gained
significant attention in Natural Language Processing (NLP), previous research
has been limited to single-dataset evaluations and specific task settings,
often using artificially constructed data through lexical replacement. We
address these limitations by conducting extensive experiments using diverse
publicly available datasets with inference and metaphor annotations, focusing
on Natural Language Inference (NLI) and Question Answering (QA) tasks. The
results indicate that LLMs' performance is more influenced by features like
lexical overlap and sentence length than by metaphorical content, demonstrating
that any alleged emergent abilities of LLMs to understand metaphorical language
are the result of a combination of surface-level features, in-context learning,
and linguistic knowledge. This work provides critical insights into the current
capabilities and limitations of LLMs in processing figurative language,
highlighting the need for more realistic evaluation frameworks in metaphor
interpretation tasks. Data and code are publicly available.

</details>


### [61] [STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for Spoken Language Models](https://arxiv.org/abs/2507.15375)
*Cheng-Han Chiang,Xiaofei Wang,Linjie Li,Chung-Ching Lin,Kevin Lin,Shujie Liu,Zhendong Wang,Zhengyuan Yang,Hung-yi Lee,Lijuan Wang*

Main category: cs.CL

TL;DR: Stitch is a novel method enabling Spoken Language Models (SLMs) to alternate between unspoken reasoning and spoken responses, reducing latency while improving reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Current SLMs lack internal reasoning like humans, causing unclear or delayed responses. Integrating unspoken thought processes is desired for clearer communication.

Method: Stitch alternates generating reasoning chunks and spoken response chunks, using audio playback time to generate reasoning tokens, enabling simultaneous thinking and talking.

Result: Stitch matches baseline latency while outperforming them by 15% on math reasoning tasks and performs equally on non-reasoning tasks.

Conclusion: Stitch effectively integrates reasoning into SLMs without added latency, enhancing performance on reasoning tasks while maintaining efficiency.

Abstract: Spoken Language Models (SLMs) are designed to take speech inputs and produce
spoken responses. However, current SLMs lack the ability to perform an
internal, unspoken thinking process before responding. In contrast, humans
typically engage in complex mental reasoning internally, enabling them to
communicate ideas clearly and concisely. Thus, integrating an unspoken thought
process into SLMs is highly desirable. While naively generating a complete
chain-of-thought (CoT) reasoning before starting to talk can enable thinking
for SLMs, this induces additional latency for the speech response, as the CoT
reasoning can be arbitrarily long. To solve this issue, we propose Stitch, a
novel generation method that alternates between the generation of unspoken
reasoning chunks and spoken response chunks. Since the audio duration of a
chunk of spoken response is much longer than the time to generate the tokens in
a chunk of spoken response, we use the remaining free time to generate the
unspoken reasoning tokens. When a chunk of audio is played to the user, the
model continues to generate the next unspoken reasoning chunk, achieving
simultaneous thinking and talking. Remarkably, Stitch matches the latency of
baselines that cannot generate unspoken CoT by design while outperforming those
baselines by 15% on math reasoning datasets; Stitch also performs equally well
on non-reasoning datasets as those baseline models. Some animations and
demonstrations are on the project page: https://d223302.github.io/STITCH.

</details>


### [62] [AlgoSimBench: Identifying Algorithmically Similar Problems for Competitive Programming](https://arxiv.org/abs/2507.15378)
*Jierui Li,Raymond Mooney*

Main category: cs.CL

TL;DR: The paper introduces AlgoSimBench to assess LLMs' ability to identify algorithmically similar problems (ASPs), revealing their struggles and proposing a method (ASM) to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: To explore whether LLMs' problem-solving abilities generalize to less-seen domains, specifically identifying ASPs.

Method: Introduces AlgoSimBench with 1317 problems and 402 MCQs, evaluates LLMs, and proposes ASM for improved similarity detection.

Result: LLMs struggle with ASP identification (best model: 65.9% accuracy). ASM improves accuracy by 6.7-11.7%. Code embedding models and retrieval methods also evaluated.

Conclusion: ASM enhances LLMs' ASP detection, but adversarial problem selection challenges performance. Summarizing problems and combining ASM with BM25 improves results.

Abstract: Recent progress in LLMs, such as reasoning models, has demonstrated strong
abilities to solve complex competitive programming problems, often rivaling top
human competitors. However, it remains underexplored whether these abilities
generalize to relevant domains that are less seen during training. To address
this, we introduce AlgoSimBench, a new benchmark designed to assess LLMs'
ability to identify algorithmically similar problems (ASPs)-problems that can
be solved using similar algorithmic approaches. AlgoSimBench consists of 1317
problems, annotated with 231 distinct fine-grained algorithm tags, from which
we curate 402 multiple-choice questions (MCQs), where each question presents
one algorithmically similar problem alongside three textually similar but
algorithmically dissimilar distractors. Our evaluation reveals that LLMs
struggle to identify ASPs, with the best-performing model (o3-mini) achieving
only 65.9% accuracy on the MCQ task. To address this challenge, we propose
attempted solution matching (ASM), a novel method for improving problem
similarity detection. On our MCQ task, ASM yields an absolute accuracy
improvement of 6.7% to 11.7% across different models. We also evaluated code
embedding models and retrieval methods on similar problem identification. While
the adversarial selection of problems degrades the performance to be less than
random, we found that simply summarizing the problem to remove narrative
elements eliminates the effect, and combining ASM with a keyword-prioritized
method, BM25, can yield up to 52.2% accuracy. Code and data are available at
github.com

</details>


### [63] [ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution](https://arxiv.org/abs/2507.15501)
*Alexandru Coca,Mark Gaynor,Zhenxing Zhang,Jianpeng Cheng,Bo-Hsiang Tseng,Pete Boothroyd,Héctor Martinez Alonso,Diarmuid Ó Séaghdha,Anders Johannsen*

Main category: cs.CL

TL;DR: The paper introduces ASPERA, a framework for evaluating LLMs in powering digital assistants for complex action execution, and Asper-Bench, a dataset of 250 tasks. It highlights the challenge of program generation with custom libraries compared to dependency-free code.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' ability to execute complex actions in digital assistants by leveraging pre-trained programming knowledge, addressing data scarcity and evaluation robustness.

Method: Developed ASPERA, a framework with a simulation for assistant libraries and a human-guided LLM data generation engine to create high-quality tasks.

Result: Asper-Bench, a dataset of 250 tasks, shows LLMs struggle more with program generation using custom libraries than dependency-free code.

Conclusion: Program generation grounded in custom assistant libraries is a significant challenge for LLMs, with ASPERA and Asper-Bench providing tools for evaluation.

Abstract: This work evaluates the potential of large language models (LLMs) to power
digital assistants capable of complex action execution. These assistants rely
on pre-trained programming knowledge to execute multi-step goals by composing
objects and functions defined in assistant libraries into action execution
programs. To achieve this, we develop ASPERA, a framework comprising an
assistant library simulation and a human-assisted LLM data generation engine.
Our engine allows developers to guide LLM generation of high-quality tasks
consisting of complex user queries, simulation state and corresponding
validation programs, tackling data availability and evaluation robustness
challenges. Alongside the framework we release Asper-Bench, an evaluation
dataset of 250 challenging tasks generated using ASPERA, which we use to show
that program generation grounded in custom assistant libraries is a significant
challenge to LLMs compared to dependency-free code generation.

</details>


### [64] [Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language Models](https://arxiv.org/abs/2507.15512)
*Kaiyan Chang,Yonghao Shi,Chenglong Wang,Hang Zhou,Chi Hu,Xiaoqian Liu,Yingfeng Luo,Yuan Ge,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: Hybrid Test-Time Scaling (TTS) combines fine-grained sequential and parallel scaling methods to enhance reasoning in LLMs without additional training overhead.


<details>
  <summary>Details</summary>
Motivation: Training-based TTS methods increase computational burden, so the paper focuses on training-free TTS for efficient and scalable reasoning.

Method: Proposes Conditional Step-level Self-refinement (sequential scaling) and combines it with parallel scaling methods to create Hybrid TTS.

Result: Experiments on 3B-14B LLMs show Hybrid TTS significantly improves reasoning performance.

Conclusion: Training-free Hybrid TTS is a promising approach to push the boundaries of LLM reasoning without extra training costs.

Abstract: Test-Time Scaling (TTS) is a promising approach to progressively elicit the
model's intelligence during inference. Recently, training-based TTS methods,
such as continued reinforcement learning (RL), have further surged in
popularity, while training-free TTS methods are gradually fading from
prominence. However, the additional computation overhead of training amplifies
the burden on test-time scaling. In this paper, we focus on training-free TTS
methods for reasoning. We first design Conditional Step-level Self-refinement,
a fine-grained sequential scaling method guided by process verification. On top
of its effectiveness, we further combine it with other classical parallel
scaling methods at the step level, to introduce a novel inference paradigm
called Hybrid Test-Time Scaling. Extensive experiments on five
instruction-tuned LLMs across different scales (3B-14B) and families
demonstrate that hybrid strategy incorporating various training-free TTS
methods at a fine granularity has considerable potential for expanding the
reasoning performance boundaries of LLMs.

</details>


### [65] [Evaluating Text Style Transfer: A Nine-Language Benchmark for Text Detoxification](https://arxiv.org/abs/2507.15557)
*Vitaly Protasov,Nikolay Babakov,Daryna Dementieva,Alexander Panchenko*

Main category: cs.CL

TL;DR: The paper addresses the challenge of evaluating text style transfer (TST) tasks, particularly in multilingual contexts, by conducting a comprehensive study across nine languages. It compares neural-based evaluation models and LLM-as-a-judge approaches, offering practical insights for reliable multilingual TST evaluation.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the gap between automatic metrics and human judgments in TST evaluation, as well as the lack of multilingual focus in prior work.

Method: The study evaluates text detoxification systems across nine languages using neural-based models and LLM-as-a-judge approaches, inspired by machine translation evaluation methods.

Result: The findings highlight the effectiveness of the evaluated approaches and provide a practical framework for designing reliable multilingual TST evaluation pipelines.

Conclusion: The paper concludes with actionable recommendations for improving multilingual TST evaluation, particularly in text detoxification.

Abstract: Despite recent progress in large language models (LLMs), evaluation of text
generation tasks such as text style transfer (TST) remains a significant
challenge. Recent studies (Dementieva et al., 2024; Pauli et al., 2025)
revealed a substantial gap between automatic metrics and human judgments.
Moreover, most prior work focuses exclusively on English, leaving multilingual
TST evaluation largely unexplored. In this paper, we perform the first
comprehensive multilingual study on evaluation of text detoxification system
across nine languages: English, Spanish, German, Chinese, Arabic, Hindi,
Ukrainian, Russian, Amharic. Drawing inspiration from the machine translation,
we assess the effectiveness of modern neural-based evaluation models alongside
prompting-based LLM-as-a-judge approaches. Our findings provide a practical
recipe for designing more reliable multilingual TST evaluation pipeline in the
text detoxification case.

</details>


### [66] [Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging](https://arxiv.org/abs/2507.15576)
*Nicolas Poggi,Shashank Agnihotri,Margret Keuper*

Main category: cs.CL

TL;DR: The paper introduces In-Context Learning (ICL) with Vision-Language Models (VLMs) for THz image classification, addressing challenges like limited annotations and low resolution. It shows improved performance and interpretability without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: THz imaging faces challenges in classification due to limited annotations, low resolution, and visual ambiguity. The goal is to provide a flexible, interpretable solution without fine-tuning.

Method: The authors adapt two open-weight VLMs to the THz domain using a modality-aligned prompting framework, evaluating them in zero-shot and one-shot settings.

Result: ICL with VLMs improves classification accuracy and interpretability in low-data regimes.

Conclusion: This is the first application of ICL-enhanced VLMs to THz imaging, offering a promising approach for resource-constrained domains.

Abstract: Terahertz (THz) imaging enables non-invasive analysis for applications such
as security screening and material classification, but effective image
classification remains challenging due to limited annotations, low resolution,
and visual ambiguity. We introduce In-Context Learning (ICL) with
Vision-Language Models (VLMs) as a flexible, interpretable alternative that
requires no fine-tuning. Using a modality-aligned prompting framework, we adapt
two open-weight VLMs to the THz domain and evaluate them under zero-shot and
one-shot settings. Our results show that ICL improves classification and
interpretability in low-data regimes. This is the first application of
ICL-enhanced VLMs to THz imaging, offering a promising direction for
resource-constrained scientific domains. Code:
\href{https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main}{GitHub
repository}.

</details>


### [67] [Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation](https://arxiv.org/abs/2507.15586)
*Xinping Zhao,Shouzheng Huang,Yan Zhong,Xinshuo Hu,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: LEAR improves RAG by learning to extract rational evidence through explicit reasoning and conscious extraction, enhancing LLM accuracy.


<details>
  <summary>Details</summary>
Motivation: Retrieval noises degrade LLM generation quality, and existing methods lack explicit reasoning, risking key clue omission.

Method: LEAR combines evidence reasoning and extraction into unified training, uses knowledge token masks, and applies verifiable rewards for optimization.

Result: LEAR outperforms on benchmarks, providing high-quality evidence and boosting downstream task accuracy.

Conclusion: LEAR effectively denoises retrieval, improves LLM performance, and is practical for online RAG systems.

Abstract: Retrieval-Augmented Generation (RAG) effectively improves the accuracy of
Large Language Models (LLMs). However, retrieval noises significantly impact
the quality of LLMs' generation, necessitating the development of denoising
mechanisms. Previous methods extract evidence straightforwardly without
explicit thinking, which risks filtering out key clues and struggles with
generalization. To this end, we propose LEAR, which learns to extract rational
evidence by (1) explicitly reasoning to identify potential cues within
retrieval contents first, and then (2) consciously extracting to avoid omitting
any key cues helpful for answering questions. Specifically, we frame evidence
reasoning and evidence extraction into one unified response for end-to-end
training; apply knowledge token masks for disentanglement to derive
reasoning-based and extraction-based answers; and devise three types of
verifiable reward functions, including answer, length, and format, to update
the model via the policy optimization algorithm. Extensive experiments on three
benchmark datasets show the effectiveness of LEAR, providing compact and
high-quality evidence, improving the accuracy of downstream tasks, and
promoting effective application in online RAG systems.

</details>


### [68] [Conflicting narratives and polarization on social media](https://arxiv.org/abs/2507.15600)
*Armin Pournaki*

Main category: cs.CL

TL;DR: The paper analyzes conflicting narratives in political discourse on Twitter to understand polarization and issue alignment, focusing on topics like Ukraine, Covid, and climate change.


<details>
  <summary>Details</summary>
Motivation: To explore how conflicting narratives reveal polarization and alignment strategies in public discourse.

Method: Extracted textual signals of conflicting narratives from tweets of opposing opinion groups, focusing on actantial roles and emplotment.

Result: Found evidence of conflicting narratives (e.g., NATO's role in Ukraine) and narrative alignment strategies.

Conclusion: Narratives serve as a valuable analytical tool for understanding discursive polarization.

Abstract: Narratives are key interpretative devices by which humans make sense of
political reality. In this work, we show how the analysis of conflicting
narratives, i.e. conflicting interpretive lenses through which political
reality is experienced and told, provides insight into the discursive
mechanisms of polarization and issue alignment in the public sphere. Building
upon previous work that has identified ideologically polarized issues in the
German Twittersphere between 2021 and 2023, we analyze the discursive dimension
of polarization by extracting textual signals of conflicting narratives from
tweets of opposing opinion groups. Focusing on a selection of salient issues
and events (the war in Ukraine, Covid, climate change), we show evidence for
conflicting narratives along two dimensions: (i) different attributions of
actantial roles to the same set of actants (e.g. diverging interpretations of
the role of NATO in the war in Ukraine), and (ii) emplotment of different
actants for the same event (e.g. Bill Gates in the right-leaning Covid
narrative). Furthermore, we provide first evidence for patterns of narrative
alignment, a discursive strategy that political actors employ to align opinions
across issues. These findings demonstrate the use of narratives as an
analytical lens into the discursive mechanisms of polarization.

</details>


### [69] [Leveraging Context for Multimodal Fallacy Classification in Political Debates](https://arxiv.org/abs/2507.15641)
*Alessio Pittiglio*

Main category: cs.CL

TL;DR: The paper presents a multimodal approach for detecting logical fallacies in political debates, achieving competitive results with text and multimodal models.


<details>
  <summary>Details</summary>
Motivation: To advance research in multimodal argument mining, specifically targeting logical fallacies in political debates.

Method: Uses pretrained Transformer-based models and explores leveraging context for fallacy classification.

Result: Achieved macro F1-scores of 0.4444 (text), 0.3559 (audio), and 0.4403 (multimodal). The multimodal model performed comparably to text-only.

Conclusion: The results suggest potential for further improvements in multimodal fallacy detection.

Abstract: In this paper, we present our submission to the MM-ArgFallacy2025 shared
task, which aims to advance research in multimodal argument mining, focusing on
logical fallacies in political debates. Our approach uses pretrained
Transformer-based models and proposes several ways to leverage context. In the
fallacy classification subtask, our models achieved macro F1-scores of 0.4444
(text), 0.3559 (audio), and 0.4403 (multimodal). Our multimodal model showed
performance comparable to the text-only model, suggesting potential for
improvements.

</details>


### [70] [P3: Prompts Promote Prompting](https://arxiv.org/abs/2507.15675)
*Xinyu Zhang,Yuanquan Hu,Fangchao Liu,Zhicheng Dou*

Main category: cs.CL

TL;DR: P3 is a self-improvement framework that concurrently optimizes system and user prompts through an iterative process, outperforming unilateral approaches in automatic prompt optimization.


<details>
  <summary>Details</summary>
Motivation: Unilateral optimization of system or user prompts often yields suboptimal results due to their interdependence, prompting the need for a holistic approach.

Method: P3 introduces a framework for concurrent optimization of both system and user prompts, leveraging offline optimization for online query-dependent improvements.

Result: P3 achieves superior performance in general and reasoning tasks, demonstrating the effectiveness of holistic optimization.

Conclusion: A holistic optimization strategy enhances LLM performance across diverse domains, as evidenced by P3's success.

Abstract: Current large language model (LLM) applications often employ multi-component
prompts, comprising both system and user prompts, to guide model behaviors.
While recent advancements have demonstrated the efficacy of automatically
optimizing either the system or user prompt to boost performance, such
unilateral approaches often yield suboptimal outcomes due to the interdependent
nature of these components. In this work, we introduce P3, a novel
self-improvement framework that concurrently optimizes both system and user
prompts through an iterative process. The offline optimized prompts are further
leveraged to promote online prompting by performing query-dependent prompt
optimization. Extensive experiments on general tasks (e.g., Arena-hard and
Alpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3
achieves superior performance in the realm of automatic prompt optimization.
Our results highlight the effectiveness of a holistic optimization strategy in
enhancing LLM performance across diverse domains.

</details>


### [71] [CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models](https://arxiv.org/abs/2507.15698)
*Congmin Zheng,Jiachen Zhu,Jianghao Lin,Xinyi Dai,Yong Yu,Weinan Zhang,Mengyue Yang*

Main category: cs.CL

TL;DR: CoLD mitigates length bias in Process Reward Models (PRMs) by using counterfactual reasoning and causal analysis, improving reward predictions and reasoning conciseness.


<details>
  <summary>Details</summary>
Motivation: Existing PRMs exhibit length bias, favoring longer reasoning steps regardless of semantic or logical validity, which undermines reliability and leads to verbose outputs.

Method: Proposes CoLD, a framework with three components: explicit length-penalty adjustment, learned bias estimator, and joint training for length-invariant rewards, grounded in counterfactual reasoning.

Result: CoLD reduces reward-length correlation, improves step selection accuracy, and promotes concise, valid reasoning in experiments on MATH500 and GSM-Plus.

Conclusion: CoLD effectively enhances the fidelity and robustness of PRMs by addressing length bias.

Abstract: Process Reward Models (PRMs) play a central role in evaluating and guiding
multi-step reasoning in large language models (LLMs), especially for
mathematical problem solving. However, we identify a pervasive length bias in
existing PRMs: they tend to assign higher scores to longer reasoning steps,
even when the semantic content and logical validity are unchanged. This bias
undermines the reliability of reward predictions and leads to overly verbose
outputs during inference. To address this issue, we propose
CoLD(Counterfactually-Guided Length Debiasing), a unified framework that
mitigates length bias through three components: an explicit length-penalty
adjustment, a learned bias estimator trained to capture spurious length-related
signals, and a joint training strategy that enforces length-invariance in
reward predictions. Our approach is grounded in counterfactual reasoning and
informed by causal graph analysis. Extensive experiments on MATH500 and
GSM-Plus show that CoLD consistently reduces reward-length correlation,
improves accuracy in step selection, and encourages more concise, logically
valid reasoning. These results demonstrate the effectiveness and practicality
of CoLD in improving the fidelity and robustness of PRMs.

</details>


### [72] [Compositional Understanding in Signaling Games](https://arxiv.org/abs/2507.15706)
*David Peter Wallis Freeborn*

Main category: cs.CL

TL;DR: The paper addresses the issue of receivers in signaling games failing to learn compositional information, proposing two new models (minimalist and generalist receivers) that enable genuine compositional understanding.


<details>
  <summary>Details</summary>
Motivation: Standard signaling game models struggle with compositional learning; receivers lose information from message components when one is forgotten.

Method: Two new models are introduced: a minimalist receiver learning from atomic messages and a generalist receiver utilizing all available information.

Result: The proposed models are simpler and allow receivers to learn from atomic message components, enabling compositional understanding.

Conclusion: The new models successfully address the compositional learning problem in signaling games, offering simpler and more effective alternatives.

Abstract: Receivers in standard signaling game models struggle with learning
compositional information. Even when the signalers send compositional messages,
the receivers do not interpret them compositionally. When information from one
message component is lost or forgotten, the information from other components
is also erased. In this paper I construct signaling game models in which
genuine compositional understanding evolves. I present two new models: a
minimalist receiver who only learns from the atomic messages of a signal, and a
generalist receiver who learns from all of the available information. These
models are in many ways simpler than previous alternatives, and allow the
receivers to learn from the atomic components of messages.

</details>


### [73] [Is Large Language Model Performance on Reasoning Tasks Impacted by Different Ways Questions Are Asked?](https://arxiv.org/abs/2507.15707)
*Seok Hwan Song,Mohna Chakraborty,Qi Li,Wallapak Tavanapong*

Main category: cs.CL

TL;DR: The study examines how different question types affect LLM accuracy in reasoning tasks, finding performance varies significantly by question type and that reasoning accuracy doesn't always align with final answer accuracy.


<details>
  <summary>Details</summary>
Motivation: To explore the unexplored impact of question types (e.g., multiple-choice, true/false) on LLM accuracy in reasoning tasks.

Method: Evaluated five LLMs on three question types using quantitative and deductive reasoning tasks, measuring accuracy in reasoning steps and final answer selection.

Result: Key findings: (1) Performance varies by question type. (2) Reasoning accuracy doesn't correlate with final answer accuracy. (3) Number of options and wording influence performance.

Conclusion: Question type significantly impacts LLM performance, with reasoning and final answer accuracy not always aligned, suggesting careful design of evaluation tasks.

Abstract: Large Language Models (LLMs) have been evaluated using diverse question
types, e.g., multiple-choice, true/false, and short/long answers. This study
answers an unexplored question about the impact of different question types on
LLM accuracy on reasoning tasks. We investigate the performance of five LLMs on
three different types of questions using quantitative and deductive reasoning
tasks. The performance metrics include accuracy in the reasoning steps and
choosing the final answer. Key Findings: (1) Significant differences exist in
LLM performance across different question types. (2) Reasoning accuracy does
not necessarily correlate with the final selection accuracy. (3) The number of
options and the choice of words, influence LLM performance.

</details>


### [74] [Chinchunmei at SemEval-2025 Task 11: Boosting the Large Language Model's Capability of Emotion Perception using Contrastive Learning](https://arxiv.org/abs/2507.15714)
*Tian Li,Yujian Sun,Huizhi Liang*

Main category: cs.CL

TL;DR: The paper discusses SemEval-2025 Task 11 on emotion detection, introducing two contrastive learning approaches for multi-label classification and emotion intensity prediction across 28 languages.


<details>
  <summary>Details</summary>
Motivation: To address challenges in emotion detection due to diverse expressions and backgrounds, the task encourages advanced approaches.

Method: Two contrastive learning methods: sample-based (Contrastive Reasoning Calibration) and generation-based (DPO, SimPO), fine-tuned from LLaMa3-Instruct-8B.

Result: Achieved 9th place in Track A (multi-label classification) and 6th in Track B (emotion intensity prediction) for English, with top-tier performance in other languages.

Conclusion: The contrastive learning approaches effectively improve emotion detection, demonstrating competitive performance across languages.

Abstract: The SemEval-2025 Task 11, Bridging the Gap in Text-Based Emotion Detection,
introduces an emotion recognition challenge spanning over 28 languages. This
competition encourages researchers to explore more advanced approaches to
address the challenges posed by the diversity of emotional expressions and
background variations. It features two tracks: multi-label classification
(Track A) and emotion intensity prediction (Track B), covering six emotion
categories: anger, fear, joy, sadness, surprise, and disgust. In our work, we
systematically explore the benefits of two contrastive learning approaches:
sample-based (Contrastive Reasoning Calibration) and generation-based (DPO,
SimPO) contrastive learning. The sample-based contrastive approach trains the
model by comparing two samples to generate more reliable predictions. The
generation-based contrastive approach trains the model to differentiate between
correct and incorrect generations, refining its prediction. All models are
fine-tuned from LLaMa3-Instruct-8B. Our system achieves 9th place in Track A
and 6th place in Track B for English, while ranking among the top-tier
performing systems for other languages.

</details>


### [75] [From Queries to Criteria: Understanding How Astronomers Evaluate LLMs](https://arxiv.org/abs/2507.15715)
*Alina Hyk,Kiera McCormick,Mian Zhong,Ioana Ciucă,Sanjib Sharma,John F Wu,J. E. G. Peek,Kartheik G. Iyer,Ziang Xiao,Anjalie Field*

Main category: cs.CL

TL;DR: The study explores how users evaluate LLMs in astronomy, using a Slack-deployed bot for literature engagement. Insights from queries and interviews led to recommendations for better benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve LLM evaluation methods by understanding user criteria, especially in scientific contexts like astronomy.

Method: Inductive coding of 368 bot queries and interviews with 11 astronomers to analyze evaluation criteria.

Result: Identified user evaluation patterns and proposed benchmark improvements, demonstrated with a sample astronomy benchmark.

Conclusion: Provides actionable insights for enhancing LLM evaluation and usability in scientific research.

Abstract: There is growing interest in leveraging LLMs to aid in astronomy and other
scientific research, but benchmarks for LLM evaluation in general have not kept
pace with the increasingly diverse ways that real people evaluate and use these
models. In this study, we seek to improve evaluation procedures by building an
understanding of how users evaluate LLMs. We focus on a particular use case: an
LLM-powered retrieval-augmented generation bot for engaging with astronomical
literature, which we deployed via Slack. Our inductive coding of 368 queries to
the bot over four weeks and our follow-up interviews with 11 astronomers reveal
how humans evaluated this system, including the types of questions asked and
the criteria for judging responses. We synthesize our findings into concrete
recommendations for building better benchmarks, which we then employ in
constructing a sample benchmark for evaluating LLMs for astronomy. Overall, our
work offers ways to improve LLM evaluation and ultimately usability,
particularly for use in scientific research.

</details>


### [76] [BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological Knowledge and Reasoning](https://arxiv.org/abs/2507.15717)
*Sahana Srinivasan,Xuguang Ai,Thaddaeus Wai Soon Lo,Aidan Gilson,Minjie Zou,Ke Zou,Hyunjae Kim,Mingjia Yang,Krithi Pushpanathan,Samantha Yew,Wan Ting Loke,Jocelyn Goh,Yibing Chen,Yiming Kong,Emily Yuelei Fu,Michelle Ongyong Hui,Kristen Nwanyanwu,Amisha Dave,Kelvin Zhenghao Li,Chen-Hsin Sun,Mark Chia,Gabriel Dawei Yang,Wendy Meihua Wong,David Ziyou Chen,Dianbo Liu,Maxwell Singer,Fares Antaki,Lucian V Del Priore,Jost Jonas,Ron Adelman,Qingyu Chen,Yih-Chung Tham*

Main category: cs.CL

TL;DR: BELO is a standardized benchmark for evaluating LLMs in ophthalmology, focusing on clinical accuracy and reasoning quality, with expert-reviewed questions and multiple evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for LLMs in ophthalmology are limited and overly focused on accuracy, lacking comprehensive evaluation.

Method: BELO was developed using expert-curated MCQs from diverse datasets, refined by ophthalmologists, and evaluated using accuracy, macro-F1, and text-generation metrics.

Result: BELO includes 900 expert-reviewed questions and evaluated six LLMs, with a public leaderboard for transparent reporting.

Conclusion: BELO provides a fair, reproducible, and comprehensive benchmark for future LLM evaluations in ophthalmology.

Abstract: Current benchmarks evaluating large language models (LLMs) in ophthalmology
are limited in scope and disproportionately prioritise accuracy. We introduce
BELO (BEnchmarking LLMs for Ophthalmology), a standardized and comprehensive
evaluation benchmark developed through multiple rounds of expert checking by 13
ophthalmologists. BELO assesses ophthalmology-related clinical accuracy and
reasoning quality. Using keyword matching and a fine-tuned PubMedBERT model, we
curated ophthalmology-specific multiple-choice-questions (MCQs) from diverse
medical datasets (BCSC, MedMCQA, MedQA, BioASQ, and PubMedQA). The dataset
underwent multiple rounds of expert checking. Duplicate and substandard
questions were systematically removed. Ten ophthalmologists refined the
explanations of each MCQ's correct answer. This was further adjudicated by
three senior ophthalmologists. To illustrate BELO's utility, we evaluated six
LLMs (OpenAI o1, o3-mini, GPT-4o, DeepSeek-R1, Llama-3-8B, and Gemini 1.5 Pro)
using accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore,
BARTScore, METEOR, and AlignScore). In a further evaluation involving human
experts, two ophthalmologists qualitatively reviewed 50 randomly selected
outputs for accuracy, comprehensiveness, and completeness. BELO consists of 900
high-quality, expert-reviewed questions aggregated from five sources: BCSC
(260), BioASQ (10), MedMCQA (572), MedQA (40), and PubMedQA (18). A public
leaderboard has been established to promote transparent evaluation and
reporting. Importantly, the BELO dataset will remain a hold-out,
evaluation-only benchmark to ensure fair and reproducible comparisons of future
models.

</details>


### [77] [Understanding Large Language Models' Ability on Interdisciplinary Research](https://arxiv.org/abs/2507.15736)
*Yuanhao Shen,Daniel Xavier de Sousa,Ricardo Marçal,Ali Asad,Hongyu Guo,Xiaodan Zhu*

Main category: cs.CL

TL;DR: IDRBench is introduced as a benchmark to evaluate LLMs' ability to propose interdisciplinary research ideas, revealing their current limitations despite some awareness.


<details>
  <summary>Details</summary>
Motivation: The lack of a dedicated benchmark for assessing LLMs in interdisciplinary research (IDR) settings hinders understanding their potential and limitations in scientific discovery.

Method: IDRBench includes an expert-annotated dataset from ArXiv across six disciplines and tasks like IDR Paper Identification, Idea Integration, and Recommendation.

Result: Baselines across 10 LLMs show they struggle to produce quality IDR ideas, despite some interdisciplinary awareness.

Conclusion: IDRBench provides a framework for evaluating LLMs in IDR, highlighting their current shortcomings and potential for future improvement.

Abstract: Recent advancements in Large Language Models (LLMs) have revealed their
impressive ability to perform multi-step, logic-driven reasoning across complex
domains, positioning them as powerful tools and collaborators in scientific
discovery while challenging the long-held view that inspiration-driven ideation
is uniquely human. However, the lack of a dedicated benchmark that evaluates
LLMs' ability to develop ideas in Interdisciplinary Research (IDR) settings
poses a critical barrier to fully understanding their strengths and
limitations. To address this gap, we introduce IDRBench -- a pioneering
benchmark featuring an expert annotated dataset and a suite of tasks tailored
to evaluate LLMs' capabilities in proposing valuable research ideas from
different scientific domains for interdisciplinary research. This benchmark
aims to provide a systematic framework for assessing LLM performance in
complex, cross-domain scientific research. Our dataset consists of scientific
publications sourced from the ArXiv platform covering six distinct disciplines,
and is annotated by domain experts with diverse academic backgrounds. To ensure
high-quality annotations, we emphasize clearly defined dimensions that
characterize authentic interdisciplinary research. The design of evaluation
tasks in IDRBench follows a progressive, real-world perspective, reflecting the
natural stages of interdisciplinary research development, including 1) IDR
Paper Identification, 2) IDR Idea Integration, and 3) IDR Idea Recommendation.
Using IDRBench, we construct baselines across 10 LLMs and observe that despite
fostering some level of IDR awareness, LLMs still struggle to produce quality
IDR ideas. These findings could not only spark new research directions, but
also help to develop next-generation LLMs that excel in interdisciplinary
research.

</details>


### [78] [A Fisher's exact test justification of the TF-IDF term-weighting scheme](https://arxiv.org/abs/2507.15742)
*Paul Sheridan,Zeyad Ahmed,Aitazaz A. Farooque*

Main category: cs.CL

TL;DR: The paper justifies TF-IDF from a significance testing perspective, linking it to Fisher's exact test and its p-value.


<details>
  <summary>Details</summary>
Motivation: To provide a theoretical foundation for TF-IDF's effectiveness by connecting it to statistical significance testing.

Method: Demonstrates that TF-ICF (a TF-IDF variant) relates to the negative log of the p-value from Fisher's exact test under certain conditions.

Result: Establishes a connection between TF-IDF and Fisher's exact test, showing convergence to TF-IDF in large document collections.

Conclusion: The statistical perspective offers a clear explanation for TF-IDF's long-standing effectiveness in information retrieval.

Abstract: Term frequency-inverse document frequency, or TF-IDF for short, is arguably
the most celebrated mathematical expression in the history of information
retrieval. Conceived as a simple heuristic quantifying the extent to which a
given term's occurrences are concentrated in any one given document out of
many, TF-IDF and its many variants are routinely used as term-weighting schemes
in diverse text analysis applications. There is a growing body of scholarship
dedicated to placing TF-IDF on a sound theoretical foundation. Building on that
tradition, this paper justifies the use of TF-IDF to the statistics community
by demonstrating how the famed expression can be understood from a significance
testing perspective. We show that the common TF-IDF variant TF-ICF is, under
mild regularity conditions, closely related to the negative logarithm of the
$p$-value from a one-tailed version of Fisher's exact test of statistical
significance. As a corollary, we establish a connection between TF-IDF and the
said negative log-transformed $p$-value under certain idealized assumptions. We
further demonstrate, as a limiting case, that this same quantity converges to
TF-IDF in the limit of an infinitely large document collection. The Fisher's
exact test justification of TF-IDF equips the working statistician with a ready
explanation of the term-weighting scheme's long-established effectiveness.

</details>


### [79] [DialogueForge: LLM Simulation of Human-Chatbot Dialogue](https://arxiv.org/abs/2507.15752)
*Ruizhe Zhu,Hao Zhu,Yaxuan Li,Syang Zhou,Shijing Cai,Malgorzata Lazuka,Elliott Ash*

Main category: cs.CL

TL;DR: DialogueForge is a framework for generating AI-simulated human-chatbot dialogues using seed prompts from real interactions, tested with various LLMs. Large proprietary models like GPT-4o excel, while smaller open-source models show promise with fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Manual collection of human-chatbot dialogues is labor-intensive and limits conversational AI research. DialogueForge aims to automate this process.

Method: Uses seed prompts from real interactions, tests various LLMs (proprietary and open-source), and employs fine-tuning for smaller models. Evaluates with UniEval and GTEval.

Result: Large models (e.g., GPT-4o) generate more realistic dialogues, while smaller models (e.g., Llama, Mistral) improve with fine-tuning. Coherent long-form dialogues remain challenging.

Conclusion: DialogueForge effectively automates dialogue generation, with proprietary models leading in quality and smaller models offering customization potential.

Abstract: Collecting human-chatbot dialogues typically demands substantial manual
effort and is time-consuming, which limits and poses challenges for research on
conversational AI. In this work, we propose DialogueForge - a framework for
generating AI-simulated conversations in human-chatbot style. To initialize
each generated conversation, DialogueForge uses seed prompts extracted from
real human-chatbot interactions. We test a variety of LLMs to simulate the
human chatbot user, ranging from state-of-the-art proprietary models to
small-scale open-source LLMs, and generate multi-turn dialogues tailored to
specific tasks. In addition, we explore fine-tuning techniques to enhance the
ability of smaller models to produce indistinguishable human-like dialogues. We
evaluate the quality of the simulated conversations and compare different
models using the UniEval and GTEval evaluation protocols. Our experiments show
that large proprietary models (e.g., GPT-4o) generally outperform others in
generating more realistic dialogues, while smaller open-source models (e.g.,
Llama, Mistral) offer promising performance with greater customization. We
demonstrate that the performance of smaller models can be significantly
improved by employing supervised fine-tuning techniques. Nevertheless,
maintaining coherent and natural long-form human-like dialogues remains a
common challenge across all models.

</details>


### [80] [Interaction as Intelligence: Deep Research With Human-AI Partnership](https://arxiv.org/abs/2507.15759)
*Lyumanshan Ye,Xiaojie Cai,Xinkai Wang,Junfei Wang,Xiangkun Hu,Jiadi Su,Yang Nan,Sihan Wang,Bohan Zhang,Xiaoze Fan,Jinbin Luo,Yuxiang Zheng,Tianze Xu,Dayuan Fu,Yunze Wu,Pengrui Lu,Zengzhi Wang,Yiwei Qin,Zhen Huang,Yan Ma,Zhulin Hu,Haoyang Zou,Tiantian Mi,Yixin Ye,Ethan Chern,Pengfei Liu*

Main category: cs.CL

TL;DR: The paper redefines human-AI interaction as a core aspect of intelligence, introducing Deep Cognition for cognitive oversight, outperforming traditional systems in key metrics.


<details>
  <summary>Details</summary>
Motivation: Traditional AI systems treat interaction as a passive interface, leading to errors and inflexibility. The paper argues interaction is fundamental to intelligence.

Method: Introduces Deep Cognition with transparent, interruptible interaction, fine-grained dialogue, and shared cognitive context.

Result: Outperforms baselines in transparency, interaction, collaboration, and results, with 31.8%-50.0% improvements in research tasks.

Conclusion: Cognitive oversight transforms human-AI interaction, proving interaction is essential for effective intelligence in research tasks.

Abstract: This paper introduces "Interaction as Intelligence" research series,
presenting a reconceptualization of human-AI relationships in deep research
tasks. Traditional approaches treat interaction merely as an interface for
accessing AI capabilities-a conduit between human intent and machine output. We
propose that interaction itself constitutes a fundamental dimension of
intelligence. As AI systems engage in extended thinking processes for research
tasks, meaningful interaction transitions from an optional enhancement to an
essential component of effective intelligence. Current deep research systems
adopt an "input-wait-output" paradigm where users initiate queries and receive
results after black-box processing. This approach leads to error cascade
effects, inflexible research boundaries that prevent question refinement during
investigation, and missed opportunities for expertise integration. To address
these limitations, we introduce Deep Cognition, a system that transforms the
human role from giving instructions to cognitive oversight-a mode of engagement
where humans guide AI thinking processes through strategic intervention at
critical junctures. Deep cognition implements three key innovations:
(1)Transparent, controllable, and interruptible interaction that reveals AI
reasoning and enables intervention at any point; (2)Fine-grained bidirectional
dialogue; and (3)Shared cognitive context where the system observes and adapts
to user behaviors without explicit instruction. User evaluation demonstrates
that this cognitive oversight paradigm outperforms the strongest baseline
across six key metrics: Transparency(+20.0%), Fine-Grained Interaction(+29.2%),
Real-Time Intervention(+18.5%), Ease of Collaboration(+27.7%),
Results-Worth-Effort(+8.8%), and Interruptibility(+20.7%). Evaluations on
challenging research problems show 31.8% to 50.0% points of improvements over
deep research systems.

</details>


### [81] [Supernova: Achieving More with Less in Transformer Architectures](https://arxiv.org/abs/2507.15773)
*Andrei-Valentin Tanase,Elena Pelican*

Main category: cs.CL

TL;DR: Supernova is a 650M-parameter transformer model that matches larger models' performance with fewer parameters and tokens, thanks to innovative design and tokenization.


<details>
  <summary>Details</summary>
Motivation: To challenge the scaling paradigm by proving that architectural efficiency and tokenization quality can compensate for reduced parameter counts.

Method: Combines Rotary Positional Embeddings, Grouped Query Attention, RMSNorm, SwiGLU activation, and a custom 128K-vocabulary byte-level BPE tokenizer.

Result: Achieves 90% performance of 1B-parameter models with 53% fewer parameters and 100B training tokens (10x less than competitors).

Conclusion: Architectural and tokenization innovations can outperform traditional scaling approaches.

Abstract: We present Supernova, a 650M-parameter decoder-only transformer that
demonstrates how careful architectural design and tokenization innovation can
achieve the performance of larger models while maintaining computational
efficiency. Our architecture combines Rotary Positional Embeddings (RoPE),
Grouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm for
computational efficiency, and SwiGLU activation functions. A critical
innovation is our custom 128,000-vocabulary byte-level BPE tokenizer, which
achieves state-of-the-art compression performance. Through detailed analysis,
we show that Supernova achieves 90% of the performance of 1B-parameter models
while using 53% fewer parameters and requiring only 100B training tokens--an
order of magnitude less than competing models. Our findings challenge the
prevailing scaling paradigm, demonstrating that architectural efficiency and
tokenization quality can compensate for reduced parameter counts.

</details>


### [82] [Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR](https://arxiv.org/abs/2507.15778)
*Jiakang Wang,Runze Liu,Fuzheng Zhang,Xiu Li,Guorui Zhou*

Main category: cs.CL

TL;DR: Archer introduces an entropy-aware RLVR method with dual-token constraints and synchronous updates, outperforming previous methods in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Previous RLVR methods apply uniform training signals to all tokens, ignoring the roles of knowledge and reasoning tokens, which can hinder learning.

Method: Archer uses weaker KL regularization for reasoning tokens to encourage exploration and stronger constraints on knowledge tokens to maintain facts.

Result: Archer outperforms prior RLVR methods on mathematical reasoning and code generation benchmarks, matching or exceeding state-of-the-art performance.

Conclusion: The proposed method effectively balances exploration and factual retention, advancing RLVR for LLMs.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective
post-training method for improving the reasoning abilities of Large Language
Models (LLMs), mainly by shaping higher-order behaviors such as reflection and
planning. However, previous RLVR algorithms often apply uniform training
signals to all tokens, without considering the different roles of low-entropy
knowledge-related tokens and high-entropy reasoning-related tokens. Some recent
methods try to separate these token types by gradient masking or asynchronous
updates, but these approaches may break semantic dependencies in the model
output and hinder effective learning. In this work, we propose Archer, an
entropy-aware RLVR approach with dual-token constraints and synchronous
updates. Specifically, our method applies weaker KL regularization and higher
clipping thresholds to reasoning tokens to encourage exploration, while using
stronger constraints on knowledge tokens to maintain factual knowledge.
Experimental results on several mathematical reasoning and code generation
benchmarks show that our approach significantly outperforms previous RLVR
methods, reaching or exceeding state-of-the-art performance among models of
comparable size. The code is available at
https://github.com/wizard-III/ArcherCodeR.

</details>


### [83] [Reservoir Computing as a Language Model](https://arxiv.org/abs/2507.15779)
*Felix Köster,Atsushi Uchida*

Main category: cs.CL

TL;DR: The paper compares reservoir computing and transformer-based models for language tasks, highlighting transformers' superior prediction quality and reservoir computing's efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the energy and speed bottlenecks of Large Language Models (LLMs) by exploring reservoir computing for efficient natural text processing.

Method: Compare three approaches: two reservoir computing methods (static and attention-enhanced) and transformer-based models, evaluating performance, computational cost, and accuracy with equal trainable parameters.

Result: Transformers outperform in prediction quality, while reservoir computing is faster and more energy-efficient. Attention-enhanced reservoirs show dynamic adaptability.

Conclusion: The study provides guidelines for balancing performance and resource constraints, suggesting reservoir computing as a viable alternative for efficient language modeling.

Abstract: Large Language Models (LLM) have dominated the science and media landscape
duo to their impressive performance on processing large chunks of data and
produce human-like levels of text. Nevertheless, their huge energy demand and
slow processing still a bottleneck for further increasing quality while also
making the models accessible to everyone. To solve this bottleneck, we will
investigate how reservoir computing performs on natural text processing, which
could enable fast and energy efficient hardware implementations. Studies
investigating the use of reservoir computing as a language model remain sparse.
In this paper, we compare three distinct approaches for character-level
language modeling, two different reservoir computing approaches, where only an
output layer is trainable, and the well-known transformer-based architectures,
which fully learn an attention-based sequence representation. We explore the
performance, computational cost and prediction accuracy for both paradigms by
equally varying the number of trainable parameters for all models. Using a
consistent pipeline for all three approaches, we demonstrate that transformers
excel in prediction quality, whereas reservoir computers remain highly
efficient reducing the training and inference speed. Furthermore, we
investigate two types of reservoir computing: a traditional reservoir with a
static linear readout, and an attention-enhanced reservoir that dynamically
adapts its output weights via an attention mechanism. Our findings underline
how these paradigms scale and offer guidelines to balance resource constraints
with performance.

</details>


### [84] [Operationalizing AI for Good: Spotlight on Deployment and Integration of AI Models in Humanitarian Work](https://arxiv.org/abs/2507.15823)
*Anton Abilov,Ke Zhang,Hemank Lamba,Elizabeth M. Olson,Joel R. Tetreault,Alejandro Jaimes*

Main category: cs.CL

TL;DR: The paper highlights the gap in AI for Good literature regarding deployment and collaboration with partner organizations, sharing insights from a real-world humanitarian project.


<details>
  <summary>Details</summary>
Motivation: To address the lack of discussion on deploying AI models in resource-constrained settings and maintaining them for real-world impact.

Method: Close collaboration with a humanitarian organization, focusing on deployment, maintenance, and performance updates.

Result: Key takeaways for practitioners on deploying and sustaining AI models in humanitarian contexts.

Conclusion: The work underscores the importance of collaboration and maintenance in AI for Good projects for tangible impact.

Abstract: Publications in the AI for Good space have tended to focus on the research
and model development that can support high-impact applications. However, very
few AI for Good papers discuss the process of deploying and collaborating with
the partner organization, and the resulting real-world impact. In this work, we
share details about the close collaboration with a humanitarian-to-humanitarian
(H2H) organization and how to not only deploy the AI model in a
resource-constrained environment, but also how to maintain it for continuous
performance updates, and share key takeaways for practitioners.

</details>


### [85] [The Impact of Language Mixing on Bilingual LLM Reasoning](https://arxiv.org/abs/2507.15849)
*Yihao Li,Jiayi Xin,Miranda Muqing Miao,Qi Long,Lyle Ungar*

Main category: cs.CL

TL;DR: Language mixing in bilingual LLMs enhances reasoning, with RLVR training enabling this behavior. Enforcing monolingual decoding reduces accuracy, while guided switching improves it.


<details>
  <summary>Details</summary>
Motivation: To understand why bilingual LLMs mix languages during reasoning and whether this behavior benefits performance.

Method: Studied language switching in Chinese-English bilingual models, identifying RLVR as the key training stage. Used a lightweight probe to predict beneficial switches.

Result: Language mixing improves reasoning accuracy (5.6% drop if blocked). Guided switching boosts accuracy by up to 6.25%.

Conclusion: Language mixing is a strategic reasoning behavior, not just a training byproduct.

Abstract: Proficient multilingual speakers often intentionally switch languages in the
middle of a conversation. Similarly, recent reasoning-focused bilingual large
language models (LLMs) with strong capabilities in both languages exhibit
language mixing--alternating languages within their chain of thought.
Discouraging this behavior in DeepSeek-R1 was found to degrade accuracy,
suggesting that language mixing may benefit reasoning. In this work, we study
language switching in Chinese-English bilingual reasoning models. We identify
reinforcement learning with verifiable rewards (RLVR) as the critical training
stage that leads to language mixing. We demonstrate that language mixing can
enhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6
percentage points on math reasoning tasks. Additionally, a lightweight probe
can be trained to predict whether a potential language switch would benefit or
harm reasoning, and when used to guide decoding, increases accuracy by up to
6.25 percentage points. Our findings suggest that language mixing is not merely
a byproduct of multilingual training, but is a strategic reasoning behavior.

</details>


### [86] [3LM: Bridging Arabic, STEM, and Code through Benchmarking](https://arxiv.org/abs/2507.15850)
*Basma El Amel Boussaha,Leen AlQadi,Mugariya Farooq,Shaikha Alsuwaidi,Giulia Campesan,Ahmed Alzubaidi,Mohammed Alyafeai,Hakim Hacid*

Main category: cs.CL

TL;DR: The paper introduces 3LM, a suite of three Arabic benchmarks for STEM and code generation to address gaps in existing Arabic LLM evaluations.


<details>
  <summary>Details</summary>
Motivation: Existing Arabic benchmarks focus on linguistic, cultural, or religious content, neglecting STEM and code domains crucial for real-world LLM applications.

Method: 3LM includes STEM-related Q&A pairs from Arabic textbooks, synthetic STEM questions, and a translated code benchmark with human review.

Result: Three high-quality benchmarks for Arabic LLMs in STEM and code generation are released publicly.

Conclusion: 3LM aims to support Arabic LLM research in underrepresented but essential domains like STEM and code.

Abstract: Arabic is one of the most widely spoken languages in the world, yet efforts
to develop and evaluate Large Language Models (LLMs) for Arabic remain
relatively limited. Most existing Arabic benchmarks focus on linguistic,
cultural, or religious content, leaving a significant gap in domains like STEM
and code which are increasingly relevant for real-world LLM applications. To
help bridge this gap, we present 3LM, a suite of three benchmarks designed
specifically for Arabic. The first is a set of STEM-related question-answer
pairs, naturally sourced from Arabic textbooks and educational worksheets. The
second consists of synthetically generated STEM questions, created using the
same sources. The third benchmark focuses on code generation, built through a
careful translation of two widely used code benchmarks, incorporating a
human-in-the-loop process with several rounds of review to ensure high-quality
and faithful translations. We release all three benchmarks publicly to support
the growth of Arabic LLM research in these essential but underrepresented
areas.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [87] [Comparative Analysis of Algorithms for the Fitting of Tessellations to 3D Image Data](https://arxiv.org/abs/2507.14268)
*Andreas Alpers,Orkun Furat,Christian Jung,Matthias Neumann,Claudia Redenbach,Aigerim Saken,Volker Schmidt*

Main category: cs.CV

TL;DR: Comparative analysis of algorithmic strategies for fitting tessellation models to 3D image data, evaluating trade-offs between model complexity, optimization methods, and approximation quality.


<details>
  <summary>Details</summary>
Motivation: To assess and guide the selection of optimization-based methods for approximating voxel-based grain structures in materials like polycrystals and foams.

Method: Review and evaluation of linear/nonlinear programming, stochastic optimization (cross-entropy method), and gradient descent for generating Voronoi, Laguerre, and GBPD tessellations.

Result: Trade-offs identified between model complexity, optimization routine complexity, and approximation quality, aiding method selection based on data and application needs.

Conclusion: Provides practical guidance for choosing appropriate tessellation methods, balancing computational effort and accuracy for real-world datasets.

Abstract: This paper presents a comparative analysis of algorithmic strategies for
fitting tessellation models to 3D image data of materials such as polycrystals
and foams. In this steadily advancing field, we review and assess
optimization-based methods -- including linear and nonlinear programming,
stochastic optimization via the cross-entropy method, and gradient descent --
for generating Voronoi, Laguerre, and generalized balanced power diagrams
(GBPDs) that approximate voxelbased grain structures. The quality of fit is
evaluated on real-world datasets using discrepancy measures that quantify
differences in grain volume, surface area, and topology. Our results highlight
trade-offs between model complexity, the complexity of the optimization
routines involved, and the quality of approximation, providing guidance for
selecting appropriate methods based on data characteristics and application
needs.

</details>


### [88] [Semantic Segmentation based Scene Understanding in Autonomous Vehicles](https://arxiv.org/abs/2507.14303)
*Ehsan Rassekh*

Main category: cs.CV

TL;DR: The paper explores efficient models for scene understanding via semantic segmentation in self-driving cars, using the BDD100k dataset and various backbones, showing improved performance metrics.


<details>
  <summary>Details</summary>
Motivation: To enhance scene understanding in autonomous vehicles by leveraging deep learning for semantic segmentation, reducing reliance on human expertise.

Method: Proposes several models with different backbone encoders, tested on the BDD100k dataset, evaluated using accuracy, mean IoU, and loss function.

Result: Appropriate backbone selection significantly impacts model performance, improving semantic segmentation metrics.

Conclusion: The study demonstrates the importance of backbone choice in DL models for semantic segmentation, achieving better scene understanding in autonomous driving.

Abstract: In recent years, the concept of artificial intelligence (AI) has become a
prominent keyword because it is promising in solving complex tasks. The need
for human expertise in specific areas may no longer be needed because machines
have achieved successful results using artificial intelligence and can make the
right decisions in critical situations. This process is possible with the help
of deep learning (DL), one of the most popular artificial intelligence
technologies. One of the areas in which the use of DL is used is in the
development of self-driving cars, which is very effective and important. In
this work, we propose several efficient models to investigate scene
understanding through semantic segmentation. We use the BDD100k dataset to
investigate these models. Another contribution of this work is the usage of
several Backbones as encoders for models. The obtained results show that
choosing the appropriate backbone has a great effect on the performance of the
model for semantic segmentation. Better performance in semantic segmentation
allows us to understand better the scene and the environment around the agent.
In the end, we analyze and evaluate the proposed models in terms of accuracy,
mean IoU, and loss function, and the results show that these metrics are
improved.

</details>


### [89] [CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation](https://arxiv.org/abs/2507.14312)
*Marc Lafon,Gustavo Adolfo Vargas Hakim,Clément Rambour,Christian Desrosier,Nicolas Thome*

Main category: cs.CV

TL;DR: CLIPTTA is a gradient-based test-time adaptation method for vision-language models, using a soft contrastive loss to align with CLIP's pre-training, improving generalization under distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Vision-language models (VLMs) like CLIP struggle with distribution shifts during inference, and traditional entropy-based TTA methods are misaligned with their contrastive training.

Method: CLIPTTA employs a soft contrastive loss aligned with CLIP's pre-training, includes a batch-aware design to prevent collapse, and extends to open-set scenarios with an Outlier Contrastive Exposure (OCE) loss.

Result: CLIPTTA outperforms entropy-based methods and competes with state-of-the-art TTA methods across 75 datasets, showing stable performance under diverse shifts.

Conclusion: CLIPTTA effectively addresses the limitations of entropy-based TTA for VLMs, offering robust adaptation and improved OOD detection.

Abstract: Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities
but often fail to generalize under distribution shifts. Test-time adaptation
(TTA) allows models to update at inference time without labeled data, typically
via entropy minimization. However, this objective is fundamentally misaligned
with the contrastive image-text training of VLMs, limiting adaptation
performance and introducing failure modes such as pseudo-label drift and class
collapse. We propose CLIPTTA, a new gradient-based TTA method for
vision-language models that leverages a soft contrastive loss aligned with
CLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's
gradients, showing how its batch-aware design mitigates the risk of collapse.
We further extend CLIPTTA to the open-set setting, where both in-distribution
(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier
Contrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75
datasets spanning diverse distribution shifts, CLIPTTA consistently outperforms
entropy-based objectives and is highly competitive with state-of-the-art TTA
methods, outperforming them on a large number of datasets and exhibiting more
stable performance across diverse shifts.

</details>


### [90] [A Hidden Stumbling Block in Generalized Category Discovery: Distracted Attention](https://arxiv.org/abs/2507.14315)
*Qiyu Xu,Zhanxuan Hu,Yu Duan,Ercheng Pei,Yonghang Tai*

Main category: cs.CV

TL;DR: The paper introduces Attention Focusing (AF), a mechanism to improve Generalized Category Discovery (GCD) by reducing distracted attention in models, leading to better feature extraction and performance.


<details>
  <summary>Details</summary>
Motivation: Existing GCD methods often suffer from distracted attention, where models focus on irrelevant background regions, degrading performance.

Method: AF uses Token Importance Measurement (TIME) and Token Adaptive Pruning (TAP) to prune non-informative tokens, sharpening the model's focus.

Result: AF improves performance by up to 15.4% when integrated into SimGCD, with minimal computational overhead.

Conclusion: AF is a lightweight, effective solution to distracted attention in GCD, easily integrable into existing methods.

Abstract: Generalized Category Discovery (GCD) aims to classify unlabeled data from
both known and unknown categories by leveraging knowledge from labeled known
categories. While existing methods have made notable progress, they often
overlook a hidden stumbling block in GCD: distracted attention. Specifically,
when processing unlabeled data, models tend to focus not only on key objects in
the image but also on task-irrelevant background regions, leading to suboptimal
feature extraction. To remove this stumbling block, we propose Attention
Focusing (AF), an adaptive mechanism designed to sharpen the model's focus by
pruning non-informative tokens. AF consists of two simple yet effective
components: Token Importance Measurement (TIME) and Token Adaptive Pruning
(TAP), working in a cascade. TIME quantifies token importance across multiple
scales, while TAP prunes non-informative tokens by utilizing the multi-scale
importance scores provided by TIME. AF is a lightweight, plug-and-play module
that integrates seamlessly into existing GCD methods with minimal computational
overhead. When incorporated into one prominent GCD method, SimGCD, AF achieves
up to 15.4% performance improvement over the baseline with minimal
computational overhead. The implementation code is provided in
https://github.com/Afleve/AFGCD.

</details>


### [91] [Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution](https://arxiv.org/abs/2507.14367)
*Weiming Ren,Raghav Goyal,Zhiming Hu,Tristan Ty Aumentado-Armstrong,Iqbal Mohomed,Alex Levinshtein*

Main category: cs.CV

TL;DR: The paper addresses hallucination artifacts in generative super-resolution (GSR) models, proposing a Hallucination Score (HS) using a multimodal large language model (MLLM) to measure and mitigate these issues.


<details>
  <summary>Details</summary>
Motivation: GSR models produce perceptual artifacts where generated details mismatch low-resolution or ground-truth images, limiting practical use. Existing metrics fail to capture these hallucinations.

Method: The study constructs a prompt for an MLLM to assess hallucinations, generating an HS. It also identifies deep feature distances correlating with HS and uses them as differentiable rewards to align GSR models.

Result: HS aligns well with human evaluations and complements existing SR metrics. Deep feature distances show strong correlation with HS.

Conclusion: The proposed HS and feature-based alignment effectively measure and reduce hallucinations in GSR, improving practical deployment.

Abstract: Generative super-resolution (GSR) currently sets the state-of-the-art in
terms of perceptual image quality, overcoming the "regression-to-the-mean" blur
of prior non-generative models. However, from a human perspective, such models
do not fully conform to the optimal balance between quality and fidelity.
Instead, a different class of artifacts, in which generated details fail to
perceptually match the low resolution image (LRI) or ground-truth image (GTI),
is a critical but under studied issue in GSR, limiting its practical
deployments. In this work, we focus on measuring, analyzing, and mitigating
these artifacts (i.e., "hallucinations"). We observe that hallucinations are
not well-characterized with existing image metrics or quality models, as they
are orthogonal to both exact fidelity and no-reference quality. Instead, we
take advantage of a multimodal large language model (MLLM) by constructing a
prompt that assesses hallucinatory visual elements and generates a
"Hallucination Score" (HS). We find that our HS is closely aligned with human
evaluations, and also provides complementary insights to prior image metrics
used for super-resolution (SR) models. In addition, we find certain deep
feature distances have strong correlations with HS. We therefore propose to
align the GSR models by using such features as differentiable reward functions
to mitigate hallucinations.

</details>


### [92] [DUSTrack: Semi-automated point tracking in ultrasound videos](https://arxiv.org/abs/2507.14368)
*Praneeth Namburi,Roger Pallarès-López,Jessica Rosendorf,Duarte Folgado,Brian W. Anthony*

Main category: cs.CV

TL;DR: DUSTrack is a semi-automated toolkit combining deep learning and optical flow for robust point tracking in B-mode ultrasound videos, outperforming zero-shot trackers and matching specialized methods.


<details>
  <summary>Details</summary>
Motivation: Accurate tissue motion tracking in B-mode ultrasound is hindered by speckle noise, low edge contrast, and out-of-plane movement, necessitating a reliable solution for clinical and biomechanical research.

Method: DUSTrack integrates deep learning with optical flow, features a GUI for training data generation, and employs optical-flow-based filtering to reduce noise while preserving motion.

Result: DUSTrack achieves superior accuracy compared to zero-shot trackers and matches specialized methods, demonstrated in cardiac, muscle, and fascicle tracking use cases.

Conclusion: DUSTrack is a versatile, open-source tool for quantifying tissue motion in ultrasound videos, with potential for broad clinical and research applications.

Abstract: Ultrasound technology enables safe, non-invasive imaging of dynamic tissue
behavior, making it a valuable tool in medicine, biomechanics, and sports
science. However, accurately tracking tissue motion in B-mode ultrasound
remains challenging due to speckle noise, low edge contrast, and out-of-plane
movement. These challenges complicate the task of tracking anatomical landmarks
over time, which is essential for quantifying tissue dynamics in many clinical
and research applications. This manuscript introduces DUSTrack (Deep learning
and optical flow-based toolkit for UltraSound Tracking), a semi-automated
framework for tracking arbitrary points in B-mode ultrasound videos. We combine
deep learning with optical flow to deliver high-quality and robust tracking
across diverse anatomical structures and motion patterns. The toolkit includes
a graphical user interface that streamlines the generation of high-quality
training data and supports iterative model refinement. It also implements a
novel optical-flow-based filtering technique that reduces high-frequency
frame-to-frame noise while preserving rapid tissue motion. DUSTrack
demonstrates superior accuracy compared to contemporary zero-shot point
trackers and performs on par with specialized methods, establishing its
potential as a general and foundational tool for clinical and biomechanical
research. We demonstrate DUSTrack's versatility through three use cases:
cardiac wall motion tracking in echocardiograms, muscle deformation analysis
during reaching tasks, and fascicle tracking during ankle plantarflexion. As an
open-source solution, DUSTrack offers a powerful, flexible framework for point
tracking to quantify tissue motion from ultrasound videos. DUSTrack is
available at https://github.com/praneethnamburi/DUSTrack.

</details>


### [93] [CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding](https://arxiv.org/abs/2507.14426)
*Zhou Chen,Joe Lin,Sathyanarayanan N. Aakur*

Main category: cs.CV

TL;DR: CRAFT is a neuro-symbolic framework for interpretable affordance grounding, combining commonsense priors and visual evidence to improve accuracy and transparency in scene understanding.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying objects enabling specific actions in a scene, with a focus on interpretability and robustness.

Method: Integrates structured commonsense priors (ConceptNet, language models) with visual evidence (CLIP) using an energy-based reasoning loop for iterative refinement.

Result: Enhances accuracy and interpretability in multi-object, label-free settings, advancing trustworthy scene understanding.

Conclusion: CRAFT provides a transparent, goal-driven approach for affordance grounding, contributing to robust scene understanding.

Abstract: We introduce CRAFT, a neuro-symbolic framework for interpretable affordance
grounding, which identifies the objects in a scene that enable a given action
(e.g., "cut"). CRAFT integrates structured commonsense priors from ConceptNet
and language models with visual evidence from CLIP, using an energy-based
reasoning loop to refine predictions iteratively. This process yields
transparent, goal-driven decisions to ground symbolic and perceptual
structures. Experiments in multi-object, label-free settings demonstrate that
CRAFT enhances accuracy while improving interpretability, providing a step
toward robust and trustworthy scene understanding.

</details>


### [94] [Adaptive 3D Gaussian Splatting Video Streaming](https://arxiv.org/abs/2507.14432)
*Han Gong,Qiyue Li,Zhi Liu,Hao Zhou,Peng Yuan Zhou,Zhu Li,Jie Li*

Main category: cs.CV

TL;DR: A framework for efficient streaming of 3D Gaussian splatting (3DGS) volumetric video, addressing challenges like large data volume and compression complexity.


<details>
  <summary>Details</summary>
Motivation: 3DGS videos have superior quality but pose streaming challenges due to their size and complexity.

Method: Uses Gaussian deformation field for construction, hybrid saliency tiling, and differentiated quality modeling for compression and bandwidth adaptation.

Result: Outperforms existing methods in video quality, compression, and transmission rate.

Conclusion: The proposed framework effectively streams 3DGS videos with high quality and efficiency.

Abstract: The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the
quality of volumetric video representation. Meanwhile, in contrast to
conventional volumetric video, 3DGS video poses significant challenges for
streaming due to its substantially larger data volume and the heightened
complexity involved in compression and transmission. To address these issues,
we introduce an innovative framework for 3DGS volumetric video streaming.
Specifically, we design a 3DGS video construction method based on the Gaussian
deformation field. By employing hybrid saliency tiling and differentiated
quality modeling of 3DGS video, we achieve efficient data compression and
adaptation to bandwidth fluctuations while ensuring high transmission quality.
Then we build a complete 3DGS video streaming system and validate the
transmission performance. Through experimental evaluation, our method
demonstrated superiority over existing approaches in various aspects, including
video quality, compression effectiveness, and transmission rate.

</details>


### [95] [IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark](https://arxiv.org/abs/2507.14449)
*Zhe Cao,Jin Zhang,Ruiheng Zhang*

Main category: cs.CV

TL;DR: IRGPT is a multi-modal large language model for real-world infrared images, leveraging a novel dataset (IR-TD) and a bi-cross-modal curriculum transfer learning strategy to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity of aligned text data and domain-specific challenges in infrared imagery, which existing methods fail to capture due to reliance on synthetic data.

Method: Proposes IRGPT, built on the IR-TD dataset (260K real image-text pairs), and introduces a bi-cross-modal curriculum transfer learning strategy to transfer knowledge from visible to infrared domains.

Result: Achieves state-of-the-art performance on 9 benchmark tasks, surpassing larger-scale models.

Conclusion: IRGPT effectively bridges the gap in infrared vision-language tasks by combining real-world data and innovative transfer learning.

Abstract: Real-world infrared imagery presents unique challenges for vision-language
models due to the scarcity of aligned text data and domain-specific
characteristics. Although existing methods have advanced the field, their
reliance on synthetic infrared images generated through style transfer from
visible images, which limits their ability to capture the unique
characteristics of the infrared modality. To address this, we propose IRGPT,
the first multi-modal large language model for real-world infrared images,
built upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K
authentic image-text pairs. The proposed IR-TD dataset contains real infrared
images paired with meticulously handcrafted texts, where the initial drafts
originated from two complementary processes: (1) LLM-generated descriptions of
visible images, and (2) rule-based descriptions of annotations. Furthermore, we
introduce a bi-cross-modal curriculum transfer learning strategy that
systematically transfers knowledge from visible to infrared domains by
considering the difficulty scores of both infrared-visible and infrared-text.
Evaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT
achieves state-of-the-art performance even compared with larger-scale models.

</details>


### [96] [GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration](https://arxiv.org/abs/2507.14452)
*Weikang Gu,Mingyue Han,Li Xue,Heng Dong,Changcai Yang,Riqing Chen,Lifang Wei*

Main category: cs.CV

TL;DR: Proposes GPI-Net, a Gestalt-guided network for point cloud registration, combining local and global features via orthogonal integration and attention mechanisms for high-quality correspondences.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of fusing local and global features in point cloud registration due to redundancy and complex spatial relationships.

Method: Uses Gestalt principles, orthogonal integration, Gestalt Feature Attention (GFA), and Dual-path Multi-Granularity (DMG) blocks for feature fusion and interaction.

Result: Outperforms existing methods in experiments on challenging tasks.

Conclusion: GPI-Net effectively integrates local and global features for improved point cloud registration.

Abstract: The accurate identification of high-quality correspondences is a prerequisite
task in feature-based point cloud registration. However, it is extremely
challenging to handle the fusion of local and global features due to feature
redundancy and complex spatial relationships. Given that Gestalt principles
provide key advantages in analyzing local and global relationships, we propose
a novel Gestalt-guided Parallel Interaction Network via orthogonal geometric
consistency (GPI-Net) in this paper. It utilizes Gestalt principles to
facilitate complementary communication between local and global information.
Specifically, we introduce an orthogonal integration strategy to optimally
reduce redundant information and generate a more compact global structure for
high-quality correspondences. To capture geometric features in correspondences,
we leverage a Gestalt Feature Attention (GFA) block through a hybrid
utilization of self-attention and cross-attention mechanisms. Furthermore, to
facilitate the integration of local detail information into the global
structure, we design an innovative Dual-path Multi-Granularity parallel
interaction aggregation (DMG) block to promote information exchange across
different granularities. Extensive experiments on various challenging tasks
demonstrate the superior performance of our proposed GPI-Net in comparison to
existing methods. The code will be released at https://github.com/gwk/GPI-Net.

</details>


### [97] [Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation](https://arxiv.org/abs/2507.14454)
*Han Gong,Qiyue Li,Jie Li,Zhi Liu*

Main category: cs.CV

TL;DR: The paper addresses challenges in 3D Gaussian splatting video streaming, proposing adaptive tiling, quality assessment, and bitrate adaptation solutions.


<details>
  <summary>Details</summary>
Motivation: To enhance immersive 3D video experiences by solving fundamental challenges like tiling, quality assessment, and bitrate adaptation in 3DGS streaming.

Method: Proposes adaptive 3DGS tiling with saliency analysis, a quality assessment framework, and a meta-learning-based bitrate algorithm.

Result: The solutions outperform state-of-the-art methods in experiments.

Conclusion: The proposed approaches effectively tackle key challenges in 3DGS video streaming, improving performance and quality.

Abstract: 3D Gaussian splatting video (3DGS) streaming has recently emerged as a
research hotspot in both academia and industry, owing to its impressive ability
to deliver immersive 3D video experiences. However, research in this area is
still in its early stages, and several fundamental challenges, such as tiling,
quality assessment, and bitrate adaptation, require further investigation. In
this paper, we tackle these challenges by proposing a comprehensive set of
solutions. Specifically, we propose an adaptive 3DGS tiling technique guided by
saliency analysis, which integrates both spatial and temporal features. Each
tile is encoded into versions possessing dedicated deformation fields and
multiple quality levels for adaptive selection. We also introduce a novel
quality assessment framework for 3DGS video that jointly evaluates
spatial-domain degradation in 3DGS representations during streaming and the
quality of the resulting 2D rendered images. Additionally, we develop a
meta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS
video streaming, achieving optimal performance across varying network
conditions. Extensive experiments demonstrate that our proposed approaches
significantly outperform state-of-the-art methods.

</details>


### [98] [GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving](https://arxiv.org/abs/2507.14456)
*Chi Wan,Yixin Cui,Jiatong Du,Shuo Yang,Yulong Bai,Yanjun Huang*

Main category: cs.CV

TL;DR: GEMINUS is a Mixture-of-Experts framework for autonomous driving, combining a Global Expert and Scene-Adaptive Experts with a Dual-aware Router, achieving adaptive and robust performance in diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: Single-mode planning struggles with diverse driving scenarios, necessitating a framework that adapts to varied conditions.

Method: GEMINUS uses a Global Expert for robustness and Scene-Adaptive Experts for adaptability, dynamically activated by a Dual-aware Router.

Result: Outperforms existing methods in Bench2Drive, achieving top scores in Driving Score and Success Rate with monocular vision.

Conclusion: GEMINUS demonstrates significant improvements over single-expert baselines, proving its effectiveness in adaptive autonomous driving.

Abstract: End-to-end autonomous driving requires adaptive and robust handling of
complex and diverse traffic environments. However, prevalent single-mode
planning methods attempt to learn an overall policy while struggling to acquire
diversified driving skills to handle diverse scenarios. Therefore, this paper
proposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework
featuring a Global Expert, a Scene-Adaptive Experts Group, and equipped with a
Dual-aware Router. Specifically, the Global Expert is trained on the overall
dataset, possessing robust performance. The Scene-Adaptive Experts are trained
on corresponding scene subsets, achieving adaptive performance. The Dual-aware
Router simultaneously considers scenario-level features and routing uncertainty
to dynamically activate expert modules. Through the effective coupling of the
Global Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,
GEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS
outperforms existing methods in the Bench2Drive closed-loop benchmark and
achieves state-of-the-art performance in Driving Score and Success Rate, even
with only monocular vision input. Furthermore, ablation studies demonstrate
significant improvements over the original single-expert baseline: 7.67% in
Driving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The
code will be available at https://github.com/newbrains1/GEMINUS.

</details>


### [99] [VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval](https://arxiv.org/abs/2507.14459)
*Huayuan Ye,Juntong Chen,Shenzhuo Zhang,Yipeng Zhang,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: VisGuard is a tamper-resistant framework for embedding metadata links in visualization images, ensuring recoverability even after tampering. It enhances robustness with techniques like repetitive data tiling and crop localization, supporting applications like interactive chart reconstruction and copyright protection.


<details>
  <summary>Details</summary>
Motivation: Current methods for embedding metadata in visualization images are fragile to common tampering, leading to loss of critical information. VisGuard aims to address this by providing a robust solution.

Method: VisGuard uses repetitive data tiling, invertible information broadcasting, and an anchor-based scheme for crop localization to embed and recover metadata links reliably.

Result: VisGuard outperforms in data retrieval accuracy, embedding capacity, and security against tampering and steganalysis.

Conclusion: VisGuard effectively safeguards visualization dissemination by ensuring metadata recoverability and robustness against tampering.

Abstract: The dissemination of visualizations is primarily in the form of raster
images, which often results in the loss of critical information such as source
code, interactive features, and metadata. While previous methods have proposed
embedding metadata into images to facilitate Visualization Image Data Retrieval
(VIDR), most existing methods lack practicability since they are fragile to
common image tampering during online distribution such as cropping and editing.
To address this issue, we propose VisGuard, a tamper-resistant VIDR framework
that reliably embeds metadata link into visualization images. The embedded data
link remains recoverable even after substantial tampering upon images. We
propose several techniques to enhance robustness, including repetitive data
tiling, invertible information broadcasting, and an anchor-based scheme for
crop localization. VisGuard enables various applications, including interactive
chart reconstruction, tampering detection, and copyright protection. We conduct
comprehensive experiments on VisGuard's superior performance in data retrieval
accuracy, embedding capacity, and security against tampering and steganalysis,
demonstrating VisGuard's competence in facilitating and safeguarding
visualization dissemination and information conveyance.

</details>


### [100] [OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition](https://arxiv.org/abs/2507.14477)
*Zhenyu Li,Tianyi Shang,Pengjie Xu,Ruirui Zhang,Fanchen Kong*

Main category: cs.CV

TL;DR: OptiCorNet introduces a sequence modeling framework for VPR, combining spatial feature extraction and temporal differencing into an end-to-end trainable module, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of VPR in dynamic and perceptually aliased environments by leveraging temporal coherence in image sequences, which existing single-frame embedding methods neglect.

Method: Uses a lightweight 1D convolutional encoder and a learnable differential temporal operator (DSD), combined with LSTM-based refinement and quadruplet loss for enhanced separability.

Result: Outperforms state-of-the-art baselines on public benchmarks under challenging seasonal and viewpoint variations.

Conclusion: OptiCorNet effectively learns sequence-level embeddings end-to-end, improving robustness in dynamic environments.

Abstract: Visual Place Recognition (VPR) in dynamic and perceptually aliased
environments remains a fundamental challenge for long-term localization.
Existing deep learning-based solutions predominantly focus on single-frame
embeddings, neglecting the temporal coherence present in image sequences. This
paper presents OptiCorNet, a novel sequence modeling framework that unifies
spatial feature extraction and temporal differencing into a differentiable,
end-to-end trainable module. Central to our approach is a lightweight 1D
convolutional encoder combined with a learnable differential temporal operator,
termed Differentiable Sequence Delta (DSD), which jointly captures short-term
spatial context and long-range temporal transitions. The DSD module models
directional differences across sequences via a fixed-weight differencing
kernel, followed by an LSTM-based refinement and optional residual projection,
yielding compact, discriminative descriptors robust to viewpoint and appearance
shifts. To further enhance inter-class separability, we incorporate a
quadruplet loss that optimizes both positive alignment and multi-negative
divergence within each batch. Unlike prior VPR methods that treat temporal
aggregation as post-processing, OptiCorNet learns sequence-level embeddings
directly, enabling more effective end-to-end place recognition. Comprehensive
evaluations on multiple public benchmarks demonstrate that our approach
outperforms state-of-the-art baselines under challenging seasonal and viewpoint
variations.

</details>


### [101] [DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning](https://arxiv.org/abs/2507.14481)
*Yujia Tong,Jingling Yuan,Tian Zhang,Jianquan Liu,Chuang Hu*

Main category: cs.CV

TL;DR: DFQ-ViT improves data-free quantization for Vision Transformers by enhancing synthetic data quality and aligning activation distributions, outperforming existing methods without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing DFQ methods for ViTs struggle with synthetic data quality and activation distribution mismatches, leading to performance degradation.

Method: Proposes DFQ-ViT: synthesizes samples by difficulty and uses activation correction to align quantized and full-precision model activations.

Result: DFQ-ViT outperforms state-of-the-art DFQ methods, e.g., 4.29% higher accuracy for 3-bit DeiT-T, without fine-tuning.

Conclusion: DFQ-ViT enables efficient, high-performance quantization for edge devices, aligning with Green Learning principles.

Abstract: Data-Free Quantization (DFQ) enables the quantization of Vision Transformers
(ViTs) without requiring access to data, allowing for the deployment of ViTs on
devices with limited resources. In DFQ, the quantization model must be
calibrated using synthetic samples, making the quality of these synthetic
samples crucial. Existing methods fail to fully capture and balance the global
and local features within the samples, resulting in limited synthetic data
quality. Moreover, we have found that during inference, there is a significant
difference in the distributions of intermediate layer activations between the
quantized and full-precision models. These issues lead to a severe performance
degradation of the quantized model. To address these problems, we propose a
pipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).
Specifically, we synthesize samples in order of increasing difficulty,
effectively enhancing the quality of synthetic data. During the calibration and
inference stage, we introduce the activation correction matrix for the
quantized model to align the intermediate layer activations with those of the
full-precision model. Extensive experiments demonstrate that DFQ-ViT achieves
remarkable superiority over existing DFQ methods and its performance is on par
with models quantized through real data. For example, the performance of DeiT-T
with 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our
method eliminates the need for fine-tuning, which not only reduces
computational overhead but also lowers the deployment barriers for edge
devices. This characteristic aligns with the principles of Green Learning by
improving energy efficiency and facilitating real-world applications in
resource-constrained environments.

</details>


### [102] [Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion](https://arxiv.org/abs/2507.14485)
*Hongye Hou,Liu Zhan,Yang Yang*

Main category: cs.CV

TL;DR: A novel retrieval-augmented framework enhances 3D point cloud completion by leveraging cross-modal retrieval and hierarchical feature fusion, improving fine-grained generation and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D point cloud completion are limited by input class focus and lack structural priors. Cross-modal learning is explored but needs broader generalization.

Method: Proposes a retrieval-augmented framework with a Structural Shared Feature Encoder (SSFE) for cross-modal feature extraction and a Progressive Retrieval-Augmented Generator (PRAG) for hierarchical feature fusion.

Result: Demonstrates effectiveness in fine-grained point cloud generation and generalization to sparse data and unseen categories.

Conclusion: The framework advances point cloud completion by integrating cross-modal retrieval and hierarchical fusion, offering improved performance and versatility.

Abstract: Completing the whole 3D structure based on an incomplete point cloud is a
challenging task, particularly when the residual point cloud lacks typical
structural characteristics. Recent methods based on cross-modal learning
attempt to introduce instance images to aid the structure feature learning.
However, they still focus on each particular input class, limiting their
generation abilities. In this work, we propose a novel retrieval-augmented
point cloud completion framework. The core idea is to incorporate cross-modal
retrieval into completion task to learn structural prior information from
similar reference samples. Specifically, we design a Structural Shared Feature
Encoder (SSFE) to jointly extract cross-modal features and reconstruct
reference features as priors. Benefiting from a dual-channel control gate in
the encoder, relevant structural features in the reference sample are enhanced
and irrelevant information interference is suppressed. In addition, we propose
a Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical
feature fusion mechanism to integrate reference prior information with input
features from global to local. Through extensive evaluations on multiple
datasets and real-world scenes, our method shows its effectiveness in
generating fine-grained point clouds, as well as its generalization capability
in handling sparse data and unseen categories.

</details>


### [103] [Efficient Whole Slide Pathology VQA via Token Compression](https://arxiv.org/abs/2507.14497)
*Weimin Lyu,Qingqiao Hu,Kehan Qi,Zhan Shi,Wentao Huang,Saumya Gupta,Chao Chen*

Main category: cs.CV

TL;DR: TCP-LLaVA introduces token compression for WSI VQA, reducing computational costs while improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of high-resolution WSIs in MLLMs, which lack generative capabilities and consume excessive resources.

Method: Uses trainable compression tokens to aggregate visual/textual info, inspired by BERT's [CLS] token, reducing input length.

Result: Outperforms baselines in VQA accuracy and reduces training resource consumption significantly.

Conclusion: TCP-LLaVA is an efficient MLLM architecture for WSI VQA, balancing performance and computational demands.

Abstract: Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000
pixels, posing significant challenges for multimodal large language model
(MLLM) due to long context length and high computational demands. Previous
methods typically focus on patch-level analysis or slide-level classification
using CLIP-based models with multi-instance learning, but they lack the
generative capabilities needed for visual question answering (VQA). More recent
MLLM-based approaches address VQA by feeding thousands of patch tokens directly
into the language model, which leads to excessive resource consumption. To
address these limitations, we propose Token Compression Pathology LLaVA
(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token
compression. TCP-LLaVA introduces a set of trainable compression tokens that
aggregate visual and textual information through a modality compression module,
inspired by the [CLS] token mechanism in BERT. Only the compressed tokens are
forwarded to the LLM for answer generation, significantly reducing input length
and computational cost. Experiments on ten TCGA tumor subtypes show that
TCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing
training resource consumption by a substantial margin.

</details>


### [104] [Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow](https://arxiv.org/abs/2507.14500)
*Zhiyuan Hua,Dehao Yuan,Cornelia Fermüller*

Main category: cs.CV

TL;DR: A framework for motion segmentation and egomotion estimation using event-based normal flow for neuromorphic vision sensors, avoiding full optical flow computation.


<details>
  <summary>Details</summary>
Motivation: Traditional methods rely on optical flow or depth estimation, which can be inefficient. The paper aims to leverage sparse, high-temporal-resolution event data for better performance.

Method: An optimization-based pipeline with event over-segmentation, residual analysis for moving objects, and hierarchical clustering using motion similarity and temporal consistency.

Result: Accurate segmentation and translational motion estimation on the EVIMO2v2 dataset, with advantages at object boundaries.

Conclusion: The method is scalable and suitable for real-time robotic and navigation applications.

Abstract: This paper introduces a robust framework for motion segmentation and
egomotion estimation using event-based normal flow, tailored specifically for
neuromorphic vision sensors. In contrast to traditional methods that rely
heavily on optical flow or explicit depth estimation, our approach exploits the
sparse, high-temporal-resolution event data and incorporates geometric
constraints between normal flow, scene structure, and inertial measurements.
The proposed optimization-based pipeline iteratively performs event
over-segmentation, isolates independently moving objects via residual analysis,
and refines segmentations using hierarchical clustering informed by motion
similarity and temporal consistency. Experimental results on the EVIMO2v2
dataset validate that our method achieves accurate segmentation and
translational motion estimation without requiring full optical flow
computation. This approach demonstrates significant advantages at object
boundaries and offers considerable potential for scalable, real-time robotic
and navigation applications.

</details>


### [105] [Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey](https://arxiv.org/abs/2507.14501)
*Jiahui Zhang,Yuelei Li,Anpei Chen,Muyu Xu,Kunhao Liu,Jianyuan Wang,Xiao-Xiao Long,Hanxue Liang,Zexiang Xu,Hao Su,Christian Theobalt,Christian Rupprecht,Andrea Vedaldi,Hanspeter Pfister,Shijian Lu,Fangneng Zhan*

Main category: cs.CV

TL;DR: A survey on feed-forward deep learning techniques for 3D reconstruction and view synthesis, covering representations like NeRF and 3DGS, applications, datasets, and future challenges.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for 3D reconstruction and view synthesis are computationally intensive, limiting real-world use. Feed-forward deep learning approaches offer faster, generalizable solutions.

Method: The paper reviews feed-forward techniques, categorizing them by representation architectures (e.g., NeRF, 3DGS) and tasks like dynamic reconstruction and pose-free synthesis.

Result: The survey highlights advancements in speed and generalization, with applications in AR/VR, robotics, and digital humans, and reviews datasets and evaluation protocols.

Conclusion: Feed-forward approaches hold promise for advancing 3D vision, though challenges remain. Future work should address these to push the field further.

Abstract: 3D reconstruction and view synthesis are foundational problems in computer
vision, graphics, and immersive technologies such as augmented reality (AR),
virtual reality (VR), and digital twins. Traditional methods rely on
computationally intensive iterative optimization in a complex chain, limiting
their applicability in real-world scenarios. Recent advances in feed-forward
approaches, driven by deep learning, have revolutionized this field by enabling
fast and generalizable 3D reconstruction and view synthesis. This survey offers
a comprehensive review of feed-forward techniques for 3D reconstruction and
view synthesis, with a taxonomy according to the underlying representation
architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural
Radiance Fields (NeRF), etc. We examine key tasks such as pose-free
reconstruction, dynamic 3D reconstruction, and 3D-aware image and video
synthesis, highlighting their applications in digital humans, SLAM, robotics,
and beyond. In addition, we review commonly used datasets with detailed
statistics, along with evaluation protocols for various downstream tasks. We
conclude by discussing open research challenges and promising directions for
future work, emphasizing the potential of feed-forward approaches to advance
the state of the art in 3D vision.

</details>


### [106] [DCHM: Depth-Consistent Human Modeling for Multiview Detection](https://arxiv.org/abs/2507.14505)
*Jiahao Ma,Tianyu Wang,Miaomiao Liu,David Ahmedt-Aristizabal,Chuong Nguyen*

Main category: cs.CV

TL;DR: DCHM improves multiview pedestrian detection by ensuring depth consistency and reducing noise in human modeling without relying on costly 3D annotations.


<details>
  <summary>Details</summary>
Motivation: Existing methods for human modeling in multiview pedestrian detection introduce noise and lack generalization, often requiring expensive annotations.

Method: Proposes Depth-Consistent Human Modeling (DCHM) with superpixel-wise Gaussian Splatting for consistent depth estimation and multiview fusion in global coordinates.

Result: DCHM significantly reduces noise, outperforms state-of-the-art baselines, and is the first to reconstruct pedestrians in sparse-view, large-scale, and crowded scenarios.

Conclusion: DCHM offers a robust solution for accurate human modeling and pedestrian localization without relying on human-labeled annotations.

Abstract: Multiview pedestrian detection typically involves two stages: human modeling
and pedestrian localization. Human modeling represents pedestrians in 3D space
by fusing multiview information, making its quality crucial for detection
accuracy. However, existing methods often introduce noise and have low
precision. While some approaches reduce noise by fitting on costly multiview 3D
annotations, they often struggle to generalize across diverse scenes. To
eliminate reliance on human-labeled annotations and accurately model humans, we
propose Depth-Consistent Human Modeling (DCHM), a framework designed for
consistent depth estimation and multiview fusion in global coordinates.
Specifically, our proposed pipeline with superpixel-wise Gaussian Splatting
achieves multiview depth consistency in sparse-view, large-scaled, and crowded
scenarios, producing precise point clouds for pedestrian localization.
Extensive validations demonstrate that our method significantly reduces noise
during human modeling, outperforming previous state-of-the-art baselines.
Additionally, to our knowledge, DCHM is the first to reconstruct pedestrians
and perform multiview segmentation in such a challenging setting. Code is
available on the \href{https://jiahao-ma.github.io/DCHM/}{project page}.

</details>


### [107] [ArtiMuse: Fine-Grained Image Aesthetics Assessment with Joint Scoring and Expert-Level Understanding](https://arxiv.org/abs/2507.14533)
*Shuo Cao,Nan Ma,Jiayang Li,Xiaohui Li,Lihao Shao,Kaiwen Zhu,Yu Zhou,Yuandong Pu,Jiarui Wu,Jiaquan Wang,Bo Qu,Wenhai Wang,Yu Qiao,Dajuin Yao,Yihao Liu*

Main category: cs.CV

TL;DR: ArtiMuse is an MLLM-based IAA model offering joint scoring and expert-level understanding, addressing modality bias and lack of fine-grained analysis. It introduces ArtiMuse-10K, a 10K-image expert-curated dataset with detailed annotations.


<details>
  <summary>Details</summary>
Motivation: The need for comprehensive IAA methods with quantitative scoring and professional understanding due to advancements in educational, artistic, and AIGC technologies.

Method: ArtiMuse, an MLLM-based IAA model, combines scoring and expert-level understanding, supported by the ArtiMuse-10K dataset with 8D attributes and holistic scores.

Result: ArtiMuse addresses modality bias and enables fine-grained aesthetic assessment, backed by a high-quality dataset.

Conclusion: The model and dataset advance IAA by providing detailed, expert-level analysis and public availability for further research.

Abstract: The rapid advancement of educational applications, artistic creation, and
AI-generated content (AIGC) technologies has substantially increased practical
requirements for comprehensive Image Aesthetics Assessment (IAA), particularly
demanding methods capable of delivering both quantitative scoring and
professional understanding. Multimodal Large Language Model (MLLM)-based IAA
methods demonstrate stronger perceptual and generalization capabilities
compared to traditional approaches, yet they suffer from modality bias
(score-only or text-only) and lack fine-grained attribute decomposition,
thereby failing to support further aesthetic assessment. In this paper, we
present:(1) ArtiMuse, an innovative MLLM-based IAA model with Joint Scoring and
Expert-Level Understanding capabilities; (2) ArtiMuse-10K, the first
expert-curated image aesthetic dataset comprising 10,000 images spanning 5 main
categories and 15 subcategories, each annotated by professional experts with
8-dimensional attributes analysis and a holistic score. Both the model and
dataset will be made public to advance the field.

</details>


### [108] [Real Time Captioning of Sign Language Gestures in Video Meetings](https://arxiv.org/abs/2507.14543)
*Sharanya Mukherjee,Md Hishaam Akhtar,Kannadasan R*

Main category: cs.CV

TL;DR: A browser extension is proposed to translate sign language to subtitles in video calls, using a large dataset of ASL videos.


<details>
  <summary>Details</summary>
Motivation: To bridge the communication gap between deaf-mute individuals and others, especially during video calls, where signing is preferred over typing.

Method: Utilizes a large-scale dataset of 2000+ Word-Level ASL videos from 100+ signers to train the system for automatic translation.

Result: The proposed extension aims to provide real-time subtitles for sign language in video meetings.

Conclusion: The tool enhances accessibility for deaf-mute individuals in digital communication.

Abstract: It has always been a rather tough task to communicate with someone possessing
a hearing impairment. One of the most tested ways to establish such a
communication is through the use of sign based languages. However, not many
people are aware of the smaller intricacies involved with sign language. Sign
language recognition using computer vision aims at eliminating the
communication barrier between deaf-mute and ordinary people so that they can
properly communicate with others. Recently the pandemic has left the whole
world shaken up and has transformed the way we communicate. Video meetings have
become essential for everyone, even people with a hearing disability. In recent
studies, it has been found that people with hearing disabilities prefer to sign
over typing during these video calls. In this paper, we are proposing a browser
extension that will automatically translate sign language to subtitles for
everyone else in the video call. The Large-scale dataset which contains more
than 2000 Word-Level ASL videos, which were performed by over 100 signers will
be used.

</details>


### [109] [Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025](https://arxiv.org/abs/2507.14544)
*Sujata Gaihre,Amir Thapa Magar,Prasuna Pokharel,Laxmi Tiwari*

Main category: cs.CV

TL;DR: The paper presents a VQA approach for gastrointestinal endoscopy using the Florence model, achieving strong results on the KASVIR dataset.


<details>
  <summary>Details</summary>
Motivation: To address visual question answering in medical endoscopy by leveraging large multimodal models for accurate and clinically relevant responses.

Method: Adopts the Florence model with domain-specific augmentations to enhance generalization and fine-tunes it on the KASVIR dataset.

Result: Fine-tuning Florence yields accurate responses, demonstrating the potential of large multimodal models in medical VQA.

Conclusion: The work provides a strong baseline for future research on explainability, robustness, and clinical integration in medical VQA.

Abstract: This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA
2025 Challenge, which targets visual question answering (VQA) for
gastrointestinal endoscopy. We adopt the Florence model-a large-scale
multimodal foundation model-as the backbone of our VQA pipeline, pairing a
powerful vision encoder with a text encoder to interpret endoscopic images and
produce clinically relevant answers. To improve generalization, we apply
domain-specific augmentations that preserve medical features while increasing
training diversity. Experiments on the KASVIR dataset show that fine-tuning
Florence yields accurate responses on the official challenge metrics. Our
results highlight the potential of large multimodal models in medical VQA and
provide a strong baseline for future work on explainability, robustness, and
clinical integration. The code is publicly available at:
https://github.com/TiwariLaxuu/VQA-Florence.git

</details>


### [110] [Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering Human Perceptual Variability on Facial Expressions](https://arxiv.org/abs/2507.14549)
*Haotian Deng,Chi Zhang,Chen Wei,Quanying Liu*

Main category: cs.CV

TL;DR: The study explores the link between ANN decision boundaries and human perceptual variability in emotion categorization, revealing shared computational principles and enabling personalized emotion modeling.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding inter-individual differences in emotion perception and the potential alignment between ANN models and human perceptual variability.

Method: A perceptual boundary sampling method was used to generate ambiguous facial expression stimuli, forming the varEmotion dataset, followed by large-scale human experiments and ANN fine-tuning.

Result: ANN-confusing stimuli also caused perceptual uncertainty in humans, and fine-tuning ANNs aligned their predictions with human perceptual patterns.

Conclusion: The findings bridge ANN decision boundaries and human perceptual variability, advancing personalized emotion interpretation models.

Abstract: A fundamental challenge in affective cognitive science is to develop models
that accurately capture the relationship between external emotional stimuli and
human internal experiences. While ANNs have demonstrated remarkable accuracy in
facial expression recognition, their ability to model inter-individual
differences in human perception remains underexplored. This study investigates
the phenomenon of high perceptual variability-where individuals exhibit
significant differences in emotion categorization even when viewing the same
stimulus. Inspired by the similarity between ANNs and human perception, we
hypothesize that facial expression samples that are ambiguous for ANN
classifiers also elicit divergent perceptual judgments among human observers.
To examine this hypothesis, we introduce a novel perceptual boundary sampling
method to generate facial expression stimuli that lie along ANN decision
boundaries. These ambiguous samples form the basis of the varEmotion dataset,
constructed through large-scale human behavioral experiments. Our analysis
reveals that these ANN-confusing stimuli also provoke heightened perceptual
uncertainty in human participants, highlighting shared computational principles
in emotion perception. Finally, by fine-tuning ANN representations using
behavioral data, we achieve alignment between ANN predictions and both
group-level and individual-level human perceptual patterns. Our findings
establish a systematic link between ANN decision boundaries and human
perceptual variability, offering new insights into personalized modeling of
emotional interpretation.

</details>


### [111] [Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance](https://arxiv.org/abs/2507.14553)
*Xiaoran Wu*

Main category: cs.CV

TL;DR: A camera guidance system helps users identify and remove clutter in photos using aesthetic evaluation and image inpainting, improving photo quality.


<details>
  <summary>Details</summary>
Motivation: Clutter in photos distracts from intended emotions or stories, especially for amateurs.

Method: Uses a clutter distinguishment algorithm and iterative image inpainting with GANs for high-resolution images.

Result: User studies show the system helps users identify distractions and take better photos faster.

Conclusion: The system effectively aids in clutter removal and enhances photographic quality.

Abstract: Clutter in photos is a distraction preventing photographers from conveying
the intended emotions or stories to the audience. Photography amateurs
frequently include clutter in their photos due to unconscious negligence or the
lack of experience in creating a decluttered, aesthetically appealing scene for
shooting. We are thus motivated to develop a camera guidance system that
provides solutions and guidance for clutter identification and removal. We
estimate and visualize the contribution of objects to the overall aesthetics
and content of a photo, based on which users can interactively identify
clutter. Suggestions on getting rid of clutter, as well as a tool that removes
cluttered objects computationally, are provided to guide users to deal with
different kinds of clutter and improve their photographic work. Two technical
novelties underpin interactions in our system: a clutter distinguishment
algorithm with aesthetics evaluations for objects and an iterative image
inpainting algorithm based on generative adversarial nets that reconstructs
missing regions of removed objects for high-resolution images. User studies
demonstrate that our system provides flexible interfaces and accurate
algorithms that allow users to better identify distractions and take higher
quality images within less time.

</details>


### [112] [Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions](https://arxiv.org/abs/2507.14555)
*Jintang Xue,Ganning Zhao,Jie-En Yao,Hong-En Chen,Yue Hu,Meida Chen,Suya You,C. -C. Jay Kuo*

Main category: cs.CV

TL;DR: Descrip3D enhances 3D scene understanding by integrating textual descriptions of object relationships, outperforming baselines on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current 3D scene-language models lack relational understanding due to reliance on visual embeddings alone.

Method: Descrip3D uses natural language to encode object relationships, combining embedding fusion and prompt-level injection for unified reasoning.

Result: Outperforms baselines on five datasets (ScanRefer, Multi3DRefer, ScanQA, SQA3D, Scan2Cap).

Conclusion: Language-guided relational representation improves complex indoor scene understanding.

Abstract: Understanding 3D scenes goes beyond simply recognizing objects; it requires
reasoning about the spatial and semantic relationships between them. Current 3D
scene-language models often struggle with this relational understanding,
particularly when visual embeddings alone do not adequately convey the roles
and interactions of objects. In this paper, we introduce Descrip3D, a novel and
powerful framework that explicitly encodes the relationships between objects
using natural language. Unlike previous methods that rely only on 2D and 3D
embeddings, Descrip3D enhances each object with a textual description that
captures both its intrinsic attributes and contextual relationships. These
relational cues are incorporated into the model through a dual-level
integration: embedding fusion and prompt-level injection. This allows for
unified reasoning across various tasks such as grounding, captioning, and
question answering, all without the need for task-specific heads or additional
supervision. When evaluated on five benchmark datasets, including ScanRefer,
Multi3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms
strong baseline models, demonstrating the effectiveness of language-guided
relational representation for understanding complex indoor scenes.

</details>


### [113] [LEAD: Exploring Logit Space Evolution for Model Selection](https://arxiv.org/abs/2507.14559)
*Zixuan Hu,Xiaotong Li,Shixiang Tang,Jun Liu,Yichun Hu,Ling-Yu Duan*

Main category: cs.CV

TL;DR: LEAD introduces a finetuning-aligned approach using logits to model transferability of pre-trained models, outperforming linear methods by capturing nonlinear optimization dynamics.


<details>
  <summary>Details</summary>
Motivation: The challenge of selecting suitable pre-trained models for downstream tasks due to the inefficiency of existing linear methods in modeling fine-tuning dynamics.

Method: LEAD models optimization via an ODE framework for nonlinear logit evolution and uses class-aware decomposition for practical applicability.

Result: Outperforms existing methods on 24 pre-trained models across 10 datasets, even in low-data scenarios.

Conclusion: LEAD effectively bridges the optimization gap in model transferability, offering a concise and adaptable solution.

Abstract: The remarkable success of pretrain-then-finetune paradigm has led to a
proliferation of available pre-trained models for vision tasks. This surge
presents a significant challenge in efficiently choosing the most suitable
pre-trained models for downstream tasks. The critical aspect of this challenge
lies in effectively predicting the model transferability by considering the
underlying fine-tuning dynamics. Existing methods often model fine-tuning
dynamics in feature space with linear transformations, which do not precisely
align with the fine-tuning objective and fail to grasp the essential
nonlinearity from optimization. To this end, we present LEAD, a
finetuning-aligned approach based on the network output of logits. LEAD
proposes a theoretical framework to model the optimization process and derives
an ordinary differential equation (ODE) to depict the nonlinear evolution
toward the final logit state. Additionally, we design a class-aware
decomposition method to consider the varying evolution dynamics across classes
and further ensure practical applicability. Integrating the closely aligned
optimization objective and nonlinear modeling capabilities derived from the
differential equation, our method offers a concise solution to effectively
bridge the optimization gap in a single step, bypassing the lengthy fine-tuning
process. The comprehensive experiments on 24 supervised and self-supervised
pre-trained models across 10 downstream datasets demonstrate impressive
performances and showcase its broad adaptability even in low-data scenarios.

</details>


### [114] [Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation](https://arxiv.org/abs/2507.14575)
*Andrea Moschetto,Lemuel Puglisi,Alec Sargood,Pierluigi Dell'Acqua,Francesco Guarnera,Sebastiano Battiato,Daniele Ravì*

Main category: cs.CV

TL;DR: The paper benchmarks GANs, diffusion models, and flow matching for T1w-to-T2w MRI synthesis, finding GAN-based Pix2Pix superior in fidelity, quality, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Reducing MRI scan time and cost by computationally synthesizing missing contrasts from acquired ones.

Method: Comparative evaluation of GANs, diffusion models, and flow matching for 2D MRI I2I translation on three public datasets.

Result: GAN-based Pix2Pix outperforms diffusion and FM methods in structural fidelity, image quality, and efficiency.

Conclusion: GANs are more practical for current MRI workflows, while flow-based models may need more data to compete. Future research should explore cross-modal synthesis further.

Abstract: Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image
contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering
distinct diagnostic insights. However, acquiring all desired modalities
increases scan time and cost, motivating research into computational methods
for cross-modal synthesis. To address this, recent approaches aim to synthesize
missing MRI contrasts from those already acquired, reducing acquisition time
while preserving diagnostic quality. Image-to-image (I2I) translation provides
a promising framework for this task. In this paper, we present a comprehensive
benchmark of generative models$\unicode{x2013}$specifically, Generative
Adversarial Networks (GANs), diffusion models, and flow matching (FM)
techniques$\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All
frameworks are implemented with comparable settings and evaluated on three
publicly available MRI datasets of healthy adults. Our quantitative and
qualitative analyses show that the GAN-based Pix2Pix model outperforms
diffusion and FM-based methods in terms of structural fidelity, image quality,
and computational efficiency. Consistent with existing literature, these
results suggest that flow-based models are prone to overfitting on small
datasets and simpler tasks, and may require more data to match or surpass GAN
performance. These findings offer practical guidance for deploying I2I
translation techniques in real-world MRI workflows and highlight promising
directions for future research in cross-modal medical image synthesis. Code and
models are publicly available at
https://github.com/AndreaMoschetto/medical-I2I-benchmark.

</details>


### [115] [Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX](https://arxiv.org/abs/2507.14587)
*Merjem Bećirović,Amina Kurtović,Nordin Smajlović,Medina Kapo,Amila Akagić*

Main category: cs.CV

TL;DR: The paper compares TensorFlow, PyTorch, and JAX for blood cell image classification, analyzing inference time and accuracy, with JAX and PyTorch performing well.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare the performance of deep learning frameworks (TensorFlow, PyTorch, JAX) in blood cell image classification due to a lack of detailed analysis.

Method: Comparison of TensorFlow, PyTorch, and JAX on the BloodMNIST dataset, focusing on inference time and classification performance across image sizes.

Result: JAX and PyTorch showed comparable accuracy to benchmarks, with performance variations due to image resolution and framework optimizations.

Conclusion: JAX and PyTorch are efficient for medical image classification, with framework choice impacting performance based on task specifics.

Abstract: Medical imaging plays a vital role in early disease diagnosis and monitoring.
Specifically, blood microscopy offers valuable insights into blood cell
morphology and the detection of hematological disorders. In recent years, deep
learning-based automated classification systems have demonstrated high
potential in enhancing the accuracy and efficiency of blood image analysis.
However, a detailed performance analysis of specific deep learning frameworks
appears to be lacking. This paper compares the performance of three popular
deep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in
classifying blood cell images from the publicly available BloodMNIST dataset.
The study primarily focuses on inference time differences, but also
classification performance for different image sizes. The results reveal
variations in performance across frameworks, influenced by factors such as
image resolution and framework-specific optimizations. Classification accuracy
for JAX and PyTorch was comparable to current benchmarks, showcasing the
efficiency of these frameworks for medical image classification.

</details>


### [116] [DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF](https://arxiv.org/abs/2507.14596)
*Doriand Petit,Steve Bourgeois,Vincent Gay-Bellile,Florian Chabot,Loïc Barthe*

Main category: cs.CV

TL;DR: DiSCO-3D is a novel method for 3D Open-Vocabulary Sub-concepts Discovery, combining unsupervised segmentation with weak open-vocabulary guidance to adapt to both scene content and user queries.


<details>
  <summary>Details</summary>
Motivation: Traditional methods are limited to either task-specific goals or scene content, lacking adaptability to both. DiSCO-3D aims to bridge this gap.

Method: Built on Neural Fields representations, DiSCO-3D integrates unsupervised segmentation with weak open-vocabulary guidance.

Result: DiSCO-3D achieves effective performance in Open-Vocabulary Sub-concepts Discovery and state-of-the-art results in edge cases of open-vocabulary and unsupervised segmentation.

Conclusion: DiSCO-3D successfully addresses the broader problem of 3D semantic segmentation by adapting to both scene and user queries, outperforming traditional methods.

Abstract: 3D semantic segmentation provides high-level scene understanding for
applications in robotics, autonomous systems, \textit{etc}. Traditional methods
adapt exclusively to either task-specific goals (open-vocabulary segmentation)
or scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the
first method addressing the broader problem of 3D Open-Vocabulary Sub-concepts
Discovery, which aims to provide a 3D semantic segmentation that adapts to both
the scene and user queries. We build DiSCO-3D on Neural Fields representations,
combining unsupervised segmentation with weak open-vocabulary guidance. Our
evaluations demonstrate that DiSCO-3D achieves effective performance in
Open-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in
the edge cases of both open-vocabulary and unsupervised segmentation.

</details>


### [117] [Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition](https://arxiv.org/abs/2507.14608)
*Nandani Sharma,Dinesh Singh*

Main category: cs.CV

TL;DR: Exp-Graph is a graph-based framework for facial expression recognition, combining facial landmarks with vision transformers and graph convolutional networks to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Facial expression recognition is vital for applications like human-computer interaction and affective computing. Structural information of facial attributes is key for accurate recognition.

Method: The framework uses facial landmarks as graph vertices, proximity and appearance similarity for edges, and integrates vision transformers with graph convolutional networks to capture dependencies.

Result: Exp-Graph achieved high accuracies on benchmark datasets: 98.09% (Oulu-CASIA), 79.01% (eNTERFACE05), and 56.39% (AFEW).

Conclusion: Exp-Graph demonstrates strong generalization across controlled and real-world settings, proving its effectiveness for practical facial expression recognition.

Abstract: Facial expression recognition is crucial for human-computer interaction
applications such as face animation, video surveillance, affective computing,
medical analysis, etc. Since the structure of facial attributes varies with
facial expressions, incorporating structural information into facial attributes
is essential for facial expression recognition. In this paper, we propose
Exp-Graph, a novel framework designed to represent the structural relationships
among facial attributes using graph-based modeling for facial expression
recognition. For facial attributes graph representation, facial landmarks are
used as the graph's vertices. At the same time, the edges are determined based
on the proximity of the facial landmark and the similarity of the local
appearance of the facial attributes encoded using the vision transformer.
Additionally, graph convolutional networks are utilized to capture and
integrate these structural dependencies into the encoding of facial attributes,
thereby enhancing the accuracy of expression recognition. Thus, Exp-Graph
learns from the facial attribute graphs highly expressive semantic
representations. On the other hand, the vision transformer and graph
convolutional blocks help the framework exploit the local and global
dependencies among the facial attributes that are essential for the recognition
of facial expressions. We conducted comprehensive evaluations of the proposed
Exp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW.
The model achieved recognition accuracies of 98.09\%, 79.01\%, and 56.39\%,
respectively. These results indicate that Exp-Graph maintains strong
generalization capabilities across both controlled laboratory settings and
real-world, unconstrained environments, underscoring its effectiveness for
practical facial expression recognition applications.

</details>


### [118] [Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2](https://arxiv.org/abs/2507.14613)
*Guoping Xu,Christopher Kabat,You Zhang*

Main category: cs.CV

TL;DR: DD-SAM2 is an efficient adaptation framework for SAM2, enhancing medical video segmentation with minimal parameter overhead, achieving high Dice scores on tumor and left ventricle datasets.


<details>
  <summary>Details</summary>
Motivation: Existing medical image segmentation methods lack adaptability to dynamic scenarios and require large datasets for retraining, leading to high costs and risks.

Method: Proposes DD-SAM2 with a Depthwise-Dilated Adapter for multi-scale feature extraction, enabling fine-tuning on limited medical video data.

Result: Achieves Dice scores of 0.93 (TrackRad2025) and 0.97 (EchoNet-Dynamic), outperforming existing methods.

Conclusion: DD-SAM2 successfully adapts SAM2 for medical video segmentation and tracking, offering a scalable solution with public resources.

Abstract: Recent advances in medical image segmentation have been driven by deep
learning; however, most existing methods remain limited by modality-specific
designs and exhibit poor adaptability to dynamic medical imaging scenarios. The
Segment Anything Model 2 (SAM2) and its related variants, which introduce a
streaming memory mechanism for real-time video segmentation, present new
opportunities for prompt-based, generalizable solutions. Nevertheless, adapting
these models to medical video scenarios typically requires large-scale datasets
for retraining or transfer learning, leading to high computational costs and
the risk of catastrophic forgetting. To address these challenges, we propose
DD-SAM2, an efficient adaptation framework for SAM2 that incorporates a
Depthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature
extraction with minimal parameter overhead. This design enables effective
fine-tuning of SAM2 on medical videos with limited training data. Unlike
existing adapter-based methods focused solely on static images, DD-SAM2 fully
exploits SAM2's streaming memory for medical video object tracking and
segmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation)
and EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior
performance, achieving Dice scores of 0.93 and 0.97, respectively. To the best
of our knowledge, this work provides an initial attempt at systematically
exploring adapter-based SAM2 fine-tuning for medical video segmentation and
tracking. Code, datasets, and models will be publicly available at
https://github.com/apple1986/DD-SAM2.

</details>


### [119] [BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM](https://arxiv.org/abs/2507.14632)
*Haiquan Wen,Tianxiao Li,Zhenglin Huang,Yiwei He,Guangliang Cheng*

Main category: cs.CV

TL;DR: BusterX++ is a novel framework for cross-modal detection of synthetic media, using reinforcement learning and hybrid reasoning to outperform single-modality methods.


<details>
  <summary>Details</summary>
Motivation: The rise of generative AI has increased misinformation risks, but current detection systems are limited by single-modality designs, failing against multi-format synthetic content.

Method: BusterX++ employs reinforcement learning post-training (Multi-stage Training, Thinking Reward, Hybrid Reasoning) to enhance detection. GenBuster++ benchmark (4,000 curated images/videos) supports evaluation.

Result: BusterX++ achieves stable and significant performance improvements, demonstrating effectiveness and generalizability in cross-modal synthetic media detection.

Conclusion: BusterX++ addresses limitations of single-modality detection, offering a robust solution for identifying and explaining synthetic media across formats.

Abstract: Recent advances in generative AI have dramatically improved image and video
synthesis capabilities, significantly increasing the risk of misinformation
through sophisticated fake content. In response, detection methods have evolved
from traditional approaches to multimodal large language models (MLLMs),
offering enhanced transparency and interpretability in identifying synthetic
media. However, current detection systems remain fundamentally limited by their
single-modality design. These approaches analyze images or videos separately,
making them ineffective against synthetic content that combines multiple media
formats. To address these challenges, we introduce \textbf{BusterX++}, a novel
framework designed specifically for cross-modal detection and explanation of
synthetic media. Our approach incorporates an advanced reinforcement learning
(RL) post-training strategy that eliminates cold-start. Through Multi-stage
Training, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and
substantial performance improvements. To enable comprehensive evaluation, we
also present \textbf{GenBuster++}, a cross-modal benchmark leveraging
state-of-the-art image and video generation techniques. This benchmark
comprises 4,000 images and video clips, meticulously curated by human experts
using a novel filtering methodology to ensure high quality, diversity, and
real-world applicability. Extensive experiments demonstrate the effectiveness
and generalizability of our approach.

</details>


### [120] [Multispectral State-Space Feature Fusion: Bridging Shared and Cross-Parametric Interactions for Object Detection](https://arxiv.org/abs/2507.14643)
*Jifeng Shen,Haibo Zhan,Shaohua Dong,Xin Zuo,Wankou Yang,Haibin Ling*

Main category: cs.CV

TL;DR: MS2Fusion is a novel multispectral feature fusion framework using state space models to address limitations in local feature preference and computational complexity, achieving superior performance in object detection and other tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of excessive local feature preference and computational bottlenecks in multispectral feature fusion for object detection.

Method: Proposes MS2Fusion with a dual-path parametric interaction mechanism: one for cross-modal complementary features and another for shared semantics, both optimized via state space models.

Result: Outperforms state-of-the-art methods on benchmarks like FLIR, M3FD, and LLVIP, and shows generality in RGB-T semantic segmentation and RGBT salient object detection.

Conclusion: MS2Fusion effectively balances complementary and shared features, demonstrating scalability and superior performance across multispectral tasks.

Abstract: Modern multispectral feature fusion for object detection faces two critical
limitations: (1) Excessive preference for local complementary features over
cross-modal shared semantics adversely affects generalization performance; and
(2) The trade-off between the receptive field size and computational complexity
present critical bottlenecks for scalable feature modeling. Addressing these
issues, a novel Multispectral State-Space Feature Fusion framework, dubbed
MS2Fusion, is proposed based on the state space model (SSM), achieving
efficient and effective fusion through a dual-path parametric interaction
mechanism. More specifically, the first cross-parameter interaction branch
inherits the advantage of cross-attention in mining complementary information
with cross-modal hidden state decoding in SSM. The second shared-parameter
branch explores cross-modal alignment with joint embedding to obtain
cross-modal similar semantic features and structures through parameter sharing
in SSM. Finally, these two paths are jointly optimized with SSM for fusing
multispectral features in a unified framework, allowing our MS2Fusion to enjoy
both functional complementarity and shared semantic space. In our extensive
experiments on mainstream benchmarks including FLIR, M3FD and LLVIP, our
MS2Fusion significantly outperforms other state-of-the-art multispectral object
detection methods, evidencing its superiority. Moreover, MS2Fusion is general
and applicable to other multispectral perception tasks. We show that, even
without specific design, MS2Fusion achieves state-of-the-art results on RGB-T
semantic segmentation and RGBT salient object detection, showing its
generality. The source code will be available at
https://github.com/61s61min/MS2Fusion.git.

</details>


### [121] [AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)](https://arxiv.org/abs/2507.14657)
*Keivan Shariatmadar,Ahmad Osman*

Main category: cs.CV

TL;DR: FST.ai is an AI framework for real-time head kick detection in Taekwondo, reducing decision time and improving fairness. Its adaptable design can extend to other sports.


<details>
  <summary>Details</summary>
Motivation: Traditional sports officiating suffers from latency, subjectivity, and inconsistency, undermining fairness. AI can address these issues.

Method: Uses computer vision, deep learning, and edge inference for pose estimation, motion classification, and impact analysis.

Result: Reduces decision time from minutes to seconds, enhances consistency, and is adaptable to other sports.

Conclusion: FST.ai demonstrates robustness and scalability, with potential to revolutionize officiating across multiple sports.

Abstract: The integration of Artificial Intelligence (AI) into sports officiating
represents a paradigm shift in how decisions are made in competitive
environments. Traditional manual systems, even when supported by Instant Video
Replay (IVR), often suffer from latency, subjectivity, and inconsistent
enforcement, undermining fairness and athlete trust. This paper introduces
FST.ai, a novel AI-powered framework designed to enhance officiating in Sport
Taekwondo, particularly focusing on the complex task of real-time head kick
detection and scoring. Leveraging computer vision, deep learning, and edge
inference, the system automates the identification and classification of key
actions, significantly reducing decision time from minutes to seconds while
improving consistency and transparency. Importantly, the methodology is not
limited to Taekwondo. The underlying framework -- based on pose estimation,
motion classification, and impact analysis -- can be adapted to a wide range of
sports requiring action detection, such as judo, karate, fencing, or even team
sports like football and basketball, where foul recognition or performance
tracking is critical. By addressing one of Taekwondo's most challenging
scenarios -- head kick scoring -- we demonstrate the robustness, scalability,
and sport-agnostic potential of FST.ai to transform officiating standards
across multiple disciplines.

</details>


### [122] [Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall](https://arxiv.org/abs/2507.14662)
*Shayan Rokhva,Babak Teimourpour*

Main category: cs.CV

TL;DR: A cost-effective computer vision framework using semantic segmentation to estimate plate-level food waste in institutional dining settings, achieving high accuracy with lightweight models.


<details>
  <summary>Details</summary>
Motivation: Quantifying post-consumer food waste is crucial for data-driven sustainability strategies in dining environments.

Method: Utilized semantic segmentation of RGB images (before/after meals) with four supervised models (U-Net, U-Net++, lightweight variants), trained using capped dynamic inverse-frequency loss and AdamW optimizer. Evaluated with Pixel Accuracy, Dice, IoU, and custom DPA metrics.

Result: Models achieved strong performance (≥90% DPA for some foods), with lighter models enabling real-time inference. Dry/rigid foods segmented better than complex/viscous ones.

Conclusion: The framework is scalable and pioneering for automated, real-time waste tracking, offering actionable insights for reducing institutional food waste.

Abstract: Quantifying post-consumer food waste in institutional dining settings is
essential for supporting data-driven sustainability strategies. This study
presents a cost-effective computer vision framework that estimates plate-level
food waste by utilizing semantic segmentation of RGB images taken before and
after meal consumption across five Iranian dishes. Four fully supervised models
(U-Net, U-Net++, and their lightweight variants) were trained using a capped
dynamic inverse-frequency loss and AdamW optimizer, then evaluated through a
comprehensive set of metrics, including Pixel Accuracy, Dice, IoU, and a
custom-defined Distributional Pixel Agreement (DPA) metric tailored to the
task. All models achieved satisfying performance, and for each food type, at
least one model approached or surpassed 90% DPA, demonstrating strong alignment
in pixel-wise proportion estimates. Lighter models with reduced parameter
counts offered faster inference, achieving real-time throughput on an NVIDIA T4
GPU. Further analysis showed superior segmentation performance for dry and more
rigid components (e.g., rice and fries), while more complex, fragmented, or
viscous dishes, such as stews, showed reduced performance, specifically
post-consumption. Despite limitations such as reliance on 2D imaging,
constrained food variety, and manual data collection, the proposed framework is
pioneering and represents a scalable, contactless solution for continuous
monitoring of food consumption. This research lays foundational groundwork for
automated, real-time waste tracking systems in large-scale food service
environments and offers actionable insights and outlines feasible future
directions for dining hall management and policymakers aiming to reduce
institutional food waste.

</details>


### [123] [Gene-DML: Dual-Pathway Multi-Level Discrimination for Gene Expression Prediction from Histopathology Images](https://arxiv.org/abs/2507.14670)
*Yaxuan Song,Jianan Fan,Hang Chang,Weidong Cai*

Main category: cs.CV

TL;DR: Gene-DML is a framework that improves gene expression prediction from histopathology images by aligning cross-modal representations at multiple levels.


<details>
  <summary>Details</summary>
Motivation: Existing methods underutilize cross-modal alignment between histopathology images and gene expression, limiting prediction performance.

Method: Gene-DML uses Dual-pathway Multi-Level discrimination: one aligns multi-scale histopathology representations with gene profiles, and the other enforces structural consistency between instances and groups.

Result: Gene-DML achieves state-of-the-art performance in gene expression prediction on public datasets.

Conclusion: The framework enhances predictive accuracy and generalization, with potential applications in precision medicine and computational pathology.

Abstract: Accurately predicting gene expression from histopathology images offers a
scalable and non-invasive approach to molecular profiling, with significant
implications for precision medicine and computational pathology. However,
existing methods often underutilize the cross-modal representation alignment
between histopathology images and gene expression profiles across multiple
representational levels, thereby limiting their prediction performance. To
address this, we propose Gene-DML, a unified framework that structures latent
space through Dual-pathway Multi-Level discrimination to enhance correspondence
between morphological and transcriptional modalities. The multi-scale
instance-level discrimination pathway aligns hierarchical histopathology
representations extracted at local, neighbor, and global levels with gene
expression profiles, capturing scale-aware morphological-transcriptional
relationships. In parallel, the cross-level instance-group discrimination
pathway enforces structural consistency between individual (image/gene)
instances and modality-crossed (gene/image, respectively) groups, strengthening
the alignment across modalities. By jointly modelling fine-grained and
structural-level discrimination, Gene-DML is able to learn robust cross-modal
representations, enhancing both predictive accuracy and generalization across
diverse biological contexts. Extensive experiments on public spatial
transcriptomics datasets demonstrate that Gene-DML achieves state-of-the-art
performance in gene expression prediction. The code and checkpoints will be
released soon.

</details>


### [124] [Docopilot: Improving Multimodal Models for Document-Level Understanding](https://arxiv.org/abs/2507.14675)
*Yuchen Duan,Zhe Chen,Yusong Hu,Weiyun Wang,Shenglong Ye,Botian Shi,Lewei Lu,Qibin Hou,Tong Lu,Hongsheng Li,Jifeng Dai,Wenhai Wang*

Main category: cs.CV

TL;DR: The paper introduces Doc-750K, a high-quality dataset for multimodal document comprehension, and Docopilot, a native multimodal model that outperforms RAG methods in coherence, accuracy, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with complex document comprehension due to lack of quality datasets, and RAG methods have limitations like fragmented contexts and error accumulation.

Method: Developed Doc-750K dataset with diverse structures and cross-page dependencies, and built Docopilot, a native multimodal model avoiding RAG.

Result: Docopilot achieves superior performance in document understanding tasks and multi-turn interactions.

Conclusion: Docopilot sets a new baseline for document-level multimodal understanding, with released data, code, and models.

Abstract: Despite significant progress in multimodal large language models (MLLMs),
their performance on complex, multi-page document comprehension remains
inadequate, largely due to the lack of high-quality, document-level datasets.
While current retrieval-augmented generation (RAG) methods offer partial
solutions, they suffer from issues, such as fragmented retrieval contexts,
multi-stage error accumulation, and extra time costs of retrieval. In this
work, we present a high-quality document-level dataset, Doc-750K, designed to
support in-depth understanding of multimodal documents. This dataset includes
diverse document structures, extensive cross-page dependencies, and real
question-answer pairs derived from the original documents. Building on the
dataset, we develop a native multimodal model, Docopilot, which can accurately
handle document-level dependencies without relying on RAG. Experiments
demonstrate that Docopilot achieves superior coherence, accuracy, and
efficiency in document understanding tasks and multi-turn interactions, setting
a new baseline for document-level multimodal understanding. Data, code, and
models are released at https://github.com/OpenGVLab/Docopilot

</details>


### [125] [WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis](https://arxiv.org/abs/2507.14680)
*Xinheng Lyu,Yuci Liang,Wenting Chen,Meidan Ding,Jiaqi Yang,Guolin Huang,Daokun Zhang,Xiangjian He,Linlin Shen*

Main category: cs.CV

TL;DR: WSI-Agents is a collaborative multi-agent system for multi-modal WSI analysis, improving accuracy and versatility over existing methods.


<details>
  <summary>Details</summary>
Motivation: Address underperformance of multi-modal large language models (MLLMs) in WSI analysis and unexplored potential of multi-agent systems in pathology.

Method: Integrates specialized agents with task allocation, verification, and summary modules for multi-task WSI analysis.

Result: Outperforms current WSI MLLMs and medical agent frameworks in diverse tasks.

Conclusion: WSI-Agents offers a promising solution for accurate and versatile WSI analysis.

Abstract: Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel
tissue analysis across various pathological tasks. While recent advancements in
multi-modal large language models (MLLMs) allow multi-task WSI analysis through
natural language, they often underperform compared to task-specific models.
Collaborative multi-agent systems have emerged as a promising solution to
balance versatility and accuracy in healthcare, yet their potential remains
underexplored in pathology-specific domains. To address these issues, we
propose WSI-Agents, a novel collaborative multi-agent system for multi-modal
WSI analysis. WSI-Agents integrates specialized functional agents with robust
task allocation and verification mechanisms to enhance both task-specific
accuracy and multi-task versatility through three components: (1) a task
allocation module assigning tasks to expert agents using a model zoo of patch
and WSI level MLLMs, (2) a verification mechanism ensuring accuracy through
internal consistency checks and external validation using pathology knowledge
bases and domain-specific models, and (3) a summary module synthesizing the
final summary with visual interpretation maps. Extensive experiments on
multi-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs
and medical agent frameworks across diverse tasks.

</details>


### [126] [From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition](https://arxiv.org/abs/2507.14686)
*Chen Cai,Tianyi Liu,Jianjun Gao,Wenyang Liu,Kejun Wu,Ruoyu Wang,Yi Wang,Soo Chin Liew*

Main category: cs.CV

TL;DR: The paper introduces Open-vocabulary Grounded Situation Recognition (Ov-GSR) and proposes Multimodal Interactive Prompt Distillation (MIPD) to transfer knowledge from a teacher MLLM to a smaller GSR model, improving generalization and zero-shot abilities.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with complex GSR and are resource-heavy, while conventional GSR models lack generalization for unseen and rare situations.

Method: MIPD uses a Judgmental Rationales Generator (JRG) and Negative-Guided Multimodal Prompting Alignment (NMPA) to distill enriched multimodal knowledge from a teacher MLLM into a student Ov-GSR model.

Result: MIPD achieves superior performance on seen, rare, and unseen situations in the Ov-SWiG dataset and improves unseen detection on HICO-DET.

Conclusion: MIPD effectively enhances generalization, bridges seen-unseen gaps, and mitigates bias in rare cases, offering a practical solution for GSR.

Abstract: Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot
abilities but struggle with complex Grounded Situation Recognition (GSR) and
are resource-intensive for edge device deployment. Meanwhile, conventional GSR
models often lack generalization ability, falling short in recognizing unseen
and rare situations. In this paper, we exploit transferring knowledge from a
teacher MLLM to a small GSR model to enhance its generalization and zero-shot
abilities, thereby introducing the task of Open-vocabulary Grounded Situation
Recognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt
Distillation (MIPD), a novel framework that distills enriched multimodal
knowledge from the foundation model, enabling the student Ov-GSR model to
recognize unseen situations and be better aware of rare situations.
Specifically, the MIPD framework first leverages the LLM-based Judgmental
Rationales Generator (JRG) to construct positive and negative glimpse and gaze
rationales enriched with contextual semantic information. The proposed
scene-aware and instance-perception prompts are then introduced to align
rationales with visual information from the MLLM teacher via the
Negative-Guided Multimodal Prompting Alignment (NMPA) module, effectively
capturing holistic and perceptual multimodal knowledge. Finally, the aligned
multimodal knowledge is distilled into the student Ov-GSR model, providing a
stronger foundation for generalization that enhances situation understanding,
bridges the gap between seen and unseen scenarios, and mitigates prediction
bias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving
superior performance on seen, rare, and unseen situations, and further
demonstrate improved unseen detection on the HICO-DET dataset.

</details>


### [127] [GTPBD: A Fine-Grained Global Terraced Parcel and Boundary Dataset](https://arxiv.org/abs/2507.14697)
*Zhiwei Zhang,Zi Ye,Yibin Wen,Shuai Yuan,Haohuan Fu,Jianxi Huang,Juepeng Zheng*

Main category: cs.CV

TL;DR: The paper introduces GTPBD, a fine-grained terraced parcel dataset for complex terrains, addressing gaps in existing datasets. It supports multiple tasks like semantic segmentation and domain adaptation.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack representation of complex terraced terrains, limiting precision agriculture research. GTPBD aims to fill this gap.

Method: GTPBD includes 47,537 high-resolution images with three-level labels (boundary, mask, parcel) across diverse terrains and climates. It benchmarks various methods for tasks like segmentation and edge detection.

Result: GTPBD outperforms existing datasets, offering challenges like terrain diversity and complex parcel objects. It supports four tasks and integrates multi-dimensional evaluation.

Conclusion: GTPBD is a foundational resource for fine-grained agricultural terrain analysis and cross-scenario knowledge transfer in remote sensing.

Abstract: Agricultural parcels serve as basic units for conducting agricultural
practices and applications, which is vital for land ownership registration,
food security assessment, soil erosion monitoring, etc. However, existing
agriculture parcel extraction studies only focus on mid-resolution mapping or
regular plain farmlands while lacking representation of complex terraced
terrains due to the demands of precision agriculture.In this paper, we
introduce a more fine-grained terraced parcel dataset named GTPBD (Global
Terraced Parcel and Boundary Dataset), which is the first fine-grained dataset
covering major worldwide terraced regions with more than 200,000 complex
terraced parcels with manual annotation. GTPBD comprises 47,537 high-resolution
images with three-level labels, including pixel-level boundary labels, mask
labels, and parcel labels. It covers seven major geographic zones in China and
transcontinental climatic regions around the world.Compared to the existing
datasets, the GTPBD dataset brings considerable challenges due to the: (1)
terrain diversity; (2) complex and irregular parcel objects; and (3) multiple
domain styles. Our proposed GTPBD dataset is suitable for four different tasks,
including semantic segmentation, edge detection, terraced parcel extraction,
and unsupervised domain adaptation (UDA) tasks.Accordingly, we benchmark the
GTPBD dataset on eight semantic segmentation methods, four edge extraction
methods, three parcel extraction methods, and five UDA methods, along with a
multi-dimensional evaluation framework integrating pixel-level and object-level
metrics. GTPBD fills a critical gap in terraced remote sensing research,
providing a basic infrastructure for fine-grained agricultural terrain analysis
and cross-scenario knowledge transfer.

</details>


### [128] [MultiRetNet: A Multimodal Vision Model and Deferral System for Staging Diabetic Retinopathy](https://arxiv.org/abs/2507.14738)
*Jeannie She,Katie Spivakovsky*

Main category: cs.CV

TL;DR: MultiRetNet improves diabetic retinopathy staging by combining retinal imaging, socioeconomic data, and comorbidities, using multimodal fusion and a deferral system for clinician review.


<details>
  <summary>Details</summary>
Motivation: Diabetic retinopathy disproportionately affects lower-income communities due to limited screening access, necessitating better early detection methods.

Method: Proposes MultiRetNet, integrating retinal imaging, socioeconomic factors, and comorbidities with three fusion methods and a deferral system trained via contrastive learning.

Result: Fully connected layer fusion is most versatile; the system maintains accuracy on suboptimal images and identifies cases needing clinician review.

Conclusion: MultiRetNet enhances early detection in underserved populations, potentially reducing costs and healthcare disparities.

Abstract: Diabetic retinopathy (DR) is a leading cause of preventable blindness,
affecting over 100 million people worldwide. In the United States, individuals
from lower-income communities face a higher risk of progressing to advanced
stages before diagnosis, largely due to limited access to screening. Comorbid
conditions further accelerate disease progression. We propose MultiRetNet, a
novel pipeline combining retinal imaging, socioeconomic factors, and
comorbidity profiles to improve DR staging accuracy, integrated with a clinical
deferral system for a clinical human-in-the-loop implementation. We experiment
with three multimodal fusion methods and identify fusion through a fully
connected layer as the most versatile methodology. We synthesize adversarial,
low-quality images and use contrastive learning to train the deferral system,
guiding the model to identify out-of-distribution samples that warrant
clinician review. By maintaining diagnostic accuracy on suboptimal images and
integrating critical health data, our system can improve early detection,
particularly in underserved populations where advanced DR is often first
identified. This approach may reduce healthcare costs, increase early detection
rates, and address disparities in access to care, promoting healthcare equity.

</details>


### [129] [InterAct-Video: Reasoning-Rich Video QA for Urban Traffic](https://arxiv.org/abs/2507.14743)
*Joseph Raj Vishal,Rutuja Patil,Manas Srinivas Gowda,Katha Naik,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

TL;DR: The paper introduces InterAct VideoQA, a dataset for benchmarking VideoQA models in traffic monitoring, addressing challenges in real-world traffic scenes.


<details>
  <summary>Details</summary>
Motivation: Existing VideoQA models struggle with complex real-world traffic scenarios, necessitating a domain-specific dataset for improvement.

Method: The InterAct VideoQA dataset includes 8 hours of real-world traffic footage with 25,000 QA pairs, covering spatiotemporal dynamics and vehicle interactions.

Result: Evaluation shows challenges in fine-grained reasoning, but fine-tuning on InterAct VideoQA improves model performance.

Conclusion: InterAct VideoQA is a valuable benchmark for advancing VideoQA models in intelligent transportation systems.

Abstract: Traffic monitoring is crucial for urban mobility, road safety, and
intelligent transportation systems (ITS). Deep learning has advanced
video-based traffic monitoring through video question answering (VideoQA)
models, enabling structured insight extraction from traffic videos. However,
existing VideoQA models struggle with the complexity of real-world traffic
scenes, where multiple concurrent events unfold across spatiotemporal
dimensions. To address these challenges, this paper introduces \textbf{InterAct
VideoQA}, a curated dataset designed to benchmark and enhance VideoQA models
for traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of
real-world traffic footage collected from diverse intersections, segmented into
10-second video clips, with over 25,000 question-answer (QA) pairs covering
spatiotemporal dynamics, vehicle interactions, incident detection, and other
critical traffic attributes. State-of-the-art VideoQA models are evaluated on
InterAct VideoQA, exposing challenges in reasoning over fine-grained
spatiotemporal dependencies within complex traffic scenarios. Additionally,
fine-tuning these models on InterAct VideoQA yields notable performance
improvements, demonstrating the necessity of domain-specific datasets for
VideoQA. InterAct VideoQA is publicly available as a benchmark dataset to
facilitate future research in real-world deployable VideoQA models for
intelligent transportation systems. GitHub Repo:
https://github.com/joe-rabbit/InterAct_VideoQA

</details>


### [130] [LeAdQA: LLM-Driven Context-Aware Temporal Grounding for Video Question Answering](https://arxiv.org/abs/2507.14784)
*Xinxin Dong,Baoyun Peng,Haokai Ma,Yufei Wang,Zixuan Dong,Fei Hu,Xiaodong Wang*

Main category: cs.CV

TL;DR: LeAdQA improves VideoQA by refining queries with LLMs and precise visual grounding, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Current VideoQA methods struggle with irrelevant frame processing and lack causal-temporal reasoning.

Method: LeAdQA uses LLMs for query refinement, temporal grounding for segment retrieval, and adaptive fusion for evidence integration.

Result: Achieves SOTA on NExT-QA, IntentQA, and NExT-GQA, enhancing video-question understanding.

Conclusion: LeAdQA addresses key limitations in VideoQA, improving accuracy and efficiency for complex reasoning.

Abstract: Video Question Answering (VideoQA) requires identifying sparse critical
moments in long videos and reasoning about their causal relationships to answer
semantically complex questions. While recent advances in multimodal learning
have improved alignment and fusion, current approaches remain limited by two
prevalent but fundamentally flawed strategies: (1) task-agnostic sampling
indiscriminately processes all frames, overwhelming key events with irrelevant
content; and (2) heuristic retrieval captures superficial patterns but misses
causal-temporal structures needed for complex reasoning. To address these
challenges, we introduce LeAdQA, an innovative approach that bridges these gaps
through synergizing causal-aware query refinement with fine-grained visual
grounding. Our method first leverages LLMs to reformulate question-option
pairs, resolving causal ambiguities and sharpening temporal focus. These
refined queries subsequently direct a temporal grounding model to precisely
retrieve the most salient segments, complemented by an adaptive fusion
mechanism dynamically integrating the evidence to maximize relevance. The
integrated visual-textual cues are then processed by an MLLM to generate
accurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and
NExT-GQA demonstrate that our method's precise visual grounding substantially
enhances the understanding of video-question relationships, achieving
state-of-the-art (SOTA) performance on complex reasoning tasks while
maintaining computational efficiency.

</details>


### [131] [FOCUS: Fused Observation of Channels for Unveiling Spectra](https://arxiv.org/abs/2507.14787)
*Xi Xiao,Aristeidis Tsaris,Anika Tabassum,John Lagergren,Larry M. York,Tianyang Wang,Xiao Wang*

Main category: cs.CV

TL;DR: FOCUS enables efficient spatial-spectral interpretability for Vision Transformers (ViTs) in hyperspectral imaging (HSI) by addressing attention collapse and computational challenges.


<details>
  <summary>Details</summary>
Motivation: Existing saliency methods fail to capture meaningful spectral cues in HSI, and full-spectrum ViTs are computationally prohibitive for interpretability.

Method: FOCUS introduces class-specific spectral prompts and a learnable [SINK] token to guide attention and absorb noise, enabling stable 3D saliency maps without gradient backpropagation.

Result: FOCUS improves band-level IoU by 15%, reduces attention collapse by 40%, and aligns saliency results with expert annotations.

Conclusion: FOCUS bridges the gap between black-box ViTs and trustworthy HSI decision-making with minimal parameter overhead.

Abstract: Hyperspectral imaging (HSI) captures hundreds of narrow, contiguous
wavelength bands, making it a powerful tool in biology, agriculture, and
environmental monitoring. However, interpreting Vision Transformers (ViTs) in
this setting remains largely unexplored due to two key challenges: (1) existing
saliency methods struggle to capture meaningful spectral cues, often collapsing
attention onto the class token, and (2) full-spectrum ViTs are computationally
prohibitive for interpretability, given the high-dimensional nature of HSI
data. We present FOCUS, the first framework that enables reliable and efficient
spatial-spectral interpretability for frozen ViTs. FOCUS introduces two core
components: class-specific spectral prompts that guide attention toward
semantically meaningful wavelength groups, and a learnable [SINK] token trained
with an attraction loss to absorb noisy or redundant attention. Together, these
designs make it possible to generate stable and interpretable 3D saliency maps
and spectral importance curves in a single forward pass, without any gradient
backpropagation or backbone modification. FOCUS improves band-level IoU by 15
percent, reduces attention collapse by over 40 percent, and produces saliency
results that align closely with expert annotations. With less than 1 percent
parameter overhead, our method makes high-resolution ViT interpretability
practical for real-world hyperspectral applications, bridging a long-standing
gap between black-box modeling and trustworthy HSI decision-making.

</details>


### [132] [A Novel Downsampling Strategy Based on Information Complementarity for Medical Image Segmentation](https://arxiv.org/abs/2507.14790)
*Wenbo Yue,Chang Li,Guoping Xu*

Main category: cs.CV

TL;DR: The paper proposes Hybrid Pooling Downsampling (HPD), a method to improve semantic segmentation by retaining spatial information during downsampling, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional downsampling methods in CNNs lose key spatial information, affecting pixel-by-pixel prediction accuracy in semantic segmentation.

Method: HPD replaces traditional downsampling with MinMaxPooling to retain image contrast and detail features by extracting local maximum values.

Result: Experiments on ACDC and Synapse datasets show HPD improves segmentation performance, increasing the DSC coefficient by 0.5% on average.

Conclusion: HPD provides an efficient solution for semantic segmentation tasks by better preserving spatial information.

Abstract: In convolutional neural networks (CNNs), downsampling operations are crucial
to model performance. Although traditional downsampling methods (such as
maximum pooling and cross-row convolution) perform well in feature aggregation,
receptive field expansion, and computational reduction, they may lead to the
loss of key spatial information in semantic segmentation tasks, thereby
affecting the pixel-by-pixel prediction accuracy.To this end, this study
proposes a downsampling method based on information complementarity - Hybrid
Pooling Downsampling (HPD). The core is to replace the traditional method with
MinMaxPooling, and effectively retain the light and dark contrast and detail
features of the image by extracting the maximum value information of the local
area.Experiment on various CNN architectures on the ACDC and Synapse datasets
show that HPD outperforms traditional methods in segmentation performance, and
increases the DSC coefficient by 0.5% on average. The results show that the HPD
module provides an efficient solution for semantic segmentation tasks.

</details>


### [133] [Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models](https://arxiv.org/abs/2507.14797)
*Beier Zhu,Ruoyu Wang,Tong Zhao,Hanwang Zhang,Chi Zhang*

Main category: cs.CV

TL;DR: Proposes Ensemble Parallel Direction (EPD), a novel ODE solver for Diffusion Models (DMs) to reduce sampling latency while maintaining image quality by using parallel gradient evaluations.


<details>
  <summary>Details</summary>
Motivation: DMs suffer from high sampling latency due to sequential denoising; existing acceleration methods degrade image quality under low-latency constraints.

Method: EPD incorporates multiple parallel gradient evaluations per ODE step, fully parallelizable for low latency. It uses learnable parameters optimized via distillation.

Result: EPD achieves superior FID scores (e.g., 4.47 on CIFAR-10) at 5 NFE latency, outperforming existing solvers.

Conclusion: EPD is an effective, pluggable solution for high-quality, low-latency sampling in DMs.

Abstract: Diffusion models (DMs) have achieved state-of-the-art generative performance
but suffer from high sampling latency due to their sequential denoising nature.
Existing solver-based acceleration methods often face image quality degradation
under a low-latency budget. In this paper, we propose the Ensemble Parallel
Direction solver (dubbed as \ours), a novel ODE solver that mitigates
truncation errors by incorporating multiple parallel gradient evaluations in
each ODE step. Importantly, since the additional gradient computations are
independent, they can be fully parallelized, preserving low-latency sampling.
  Our method optimizes a small set of learnable parameters in a distillation
fashion, ensuring minimal training overhead.
  In addition, our method can serve as a plugin to improve existing ODE
samplers. Extensive experiments on various image synthesis benchmarks
demonstrate the effectiveness of our \ours~in achieving high-quality and
low-latency sampling. For example, at the same latency level of 5 NFE, EPD
achieves an FID of 4.47 on CIFAR-10, 7.97 on FFHQ, 8.17 on ImageNet, and 8.26
on LSUN Bedroom, surpassing existing learning-based solvers by a significant
margin. Codes are available in https://github.com/BeierZhu/EPD.

</details>


### [134] [An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks](https://arxiv.org/abs/2507.14798)
*Xinyi Wu,Steven Landgraf,Markus Ulrich,Rongjun Qin*

Main category: cs.CV

TL;DR: The paper evaluates DUSt3R, MASt3R, and VGGT models on aerial images, showing their effectiveness in sparse scenarios but limitations with high-resolution or large datasets.


<details>
  <summary>Details</summary>
Motivation: To assess the performance of state-of-the-art 3D reconstruction models (DUSt3R, MASt3R, VGGT) on photogrammetric aerial blocks, which remains unexplored despite their success in sparse image sets.

Method: Comprehensive evaluation of pre-trained DUSt3R, MASt3R, and VGGT models on the UseGeo dataset for pose estimation and dense 3D reconstruction.

Result: These models accurately reconstruct dense point clouds from very sparse image sets (fewer than 10 images), with VGGT showing higher efficiency and reliability. However, performance declines with high-resolution images and larger datasets.

Conclusion: Transformer-based methods like DUSt3R, MASt3R, and VGGT are promising for sparse and low-resolution scenarios but cannot fully replace traditional SfM and MVS, serving as complementary approaches.

Abstract: State-of-the-art 3D computer vision algorithms continue to advance in
handling sparse, unordered image sets. Recently developed foundational models
for 3D reconstruction, such as Dense and Unconstrained Stereo 3D Reconstruction
(DUSt3R), Matching and Stereo 3D Reconstruction (MASt3R), and Visual Geometry
Grounded Transformer (VGGT), have attracted attention due to their ability to
handle very sparse image overlaps. Evaluating DUSt3R/MASt3R/VGGT on typical
aerial images matters, as these models may handle extremely low image overlaps,
stereo occlusions, and textureless regions. For redundant collections, they can
accelerate 3D reconstruction by using extremely sparsified image sets. Despite
tests on various computer vision benchmarks, their potential on photogrammetric
aerial blocks remains unexplored. This paper conducts a comprehensive
evaluation of the pre-trained DUSt3R/MASt3R/VGGT models on the aerial blocks of
the UseGeo dataset for pose estimation and dense 3D reconstruction. Results
show these methods can accurately reconstruct dense point clouds from very
sparse image sets (fewer than 10 images, up to 518 pixels resolution), with
completeness gains up to +50% over COLMAP. VGGT also demonstrates higher
computational efficiency, scalability, and more reliable camera pose
estimation. However, all exhibit limitations with high-resolution images and
large sets, as pose reliability declines with more images and geometric
complexity. These findings suggest transformer-based methods cannot fully
replace traditional SfM and MVS, but offer promise as complementary approaches,
especially in challenging, low-resolution, and sparse scenarios.

</details>


### [135] [Exploring Scalable Unified Modeling for General Low-Level Vision](https://arxiv.org/abs/2507.14801)
*Xiangyu Chen,Kaiwen Zhu,Yuandong Pu,Shuo Cao,Xiaohui Li,Wenlong Zhang,Yihao Liu,Yu Qiao,Jiantao Zhou,Chao Dong*

Main category: cs.CV

TL;DR: The paper proposes a Visual task Prompt-based Image Processing (VPIP) framework for unified modeling of diverse low-level vision tasks, achieving strong performance and adaptability.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of unified modeling across diverse low-level vision tasks with varying formulations and output domains.

Method: Introduces VPIP, an end-to-end framework with a backbone, prompt encoder, and interaction module, leveraging input-target pairs as visual prompts. Develops GenLV, a unified model, and evaluates scalability via model capacity and task diversity.

Result: Achieves strong performance across 100+ tasks, with improved generalization for limited-data tasks. Demonstrates adaptability in zero-shot, few-shot, and fine-tuning scenarios.

Conclusion: VPIP is effective, scalable, and adaptable, serving as a unified foundation for general low-level vision modeling.

Abstract: Low-level vision involves a wide spectrum of tasks, including image
restoration, enhancement, stylization, and feature extraction, which differ
significantly in both task formulation and output domains. To address the
challenge of unified modeling across such diverse tasks, we propose a Visual
task Prompt-based Image Processing (VPIP) framework that leverages input-target
image pairs as visual prompts to guide the model in performing a variety of
low-level vision tasks. The framework comprises an end-to-end image processing
backbone, a prompt encoder, and a prompt interaction module, enabling flexible
integration with various architectures and effective utilization of
task-specific visual representations. Based on this design, we develop a
unified low-level vision model, GenLV, and evaluate its performance across
multiple representative tasks. To explore the scalability of this approach, we
extend the framework along two dimensions: model capacity and task diversity.
We construct a large-scale benchmark consisting of over 100 low-level vision
tasks and train multiple versions of the model with varying scales.
Experimental results show that the proposed method achieves considerable
performance across a wide range of tasks. Notably, increasing the number of
training tasks enhances generalization, particularly for tasks with limited
data, indicating the model's ability to learn transferable representations
through joint training. Further evaluations in zero-shot generalization,
few-shot transfer, and task-specific fine-tuning scenarios demonstrate the
model's strong adaptability, confirming the effectiveness, scalability, and
potential of the proposed framework as a unified foundation for general
low-level vision modeling.

</details>


### [136] [Seeing Through Deepfakes: A Human-Inspired Framework for Multi-Face Detection](https://arxiv.org/abs/2507.14807)
*Juan Hu,Shaojing Fan,Terence Sim*

Main category: cs.CV

TL;DR: A novel framework, HICOM, improves multi-face deepfake detection by leveraging human-inspired cues, outperforming existing methods in accuracy and generalizability.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection methods struggle with multi-face scenarios due to lack of contextual awareness. This work aims to bridge the gap by incorporating human cognitive cues.

Method: The approach involves human studies to identify key detection cues (scene-motion coherence, inter-face appearance compatibility, interpersonal gaze alignment, face-body consistency) and integrates these into the HICOM framework.

Result: HICOM improves accuracy by 3.3% in in-dataset detection and 2.8% under perturbations, and outperforms existing methods by 5.8% on unseen datasets. It also enhances interpretability with LLM-generated explanations.

Conclusion: Incorporating human cognitive cues significantly improves multi-face deepfake detection, offering better accuracy, generalizability, and transparency.

Abstract: Multi-face deepfake videos are becoming increasingly prevalent, often
appearing in natural social settings that challenge existing detection methods.
Most current approaches excel at single-face detection but struggle in
multi-face scenarios, due to a lack of awareness of crucial contextual cues. In
this work, we develop a novel approach that leverages human cognition to
analyze and defend against multi-face deepfake videos. Through a series of
human studies, we systematically examine how people detect deepfake faces in
social settings. Our quantitative analysis reveals four key cues humans rely
on: scene-motion coherence, inter-face appearance compatibility, interpersonal
gaze alignment, and face-body consistency. Guided by these insights, we
introduce \textsf{HICOM}, a novel framework designed to detect every fake face
in multi-face scenarios. Extensive experiments on benchmark datasets show that
\textsf{HICOM} improves average accuracy by 3.3\% in in-dataset detection and
2.8\% under real-world perturbations. Moreover, it outperforms existing methods
by 5.8\% on unseen datasets, demonstrating the generalization of human-inspired
cues. \textsf{HICOM} further enhances interpretability by incorporating an LLM
to provide human-readable explanations, making detection results more
transparent and convincing. Our work sheds light on involving human factors to
enhance defense against deepfakes.

</details>


### [137] [Light Future: Multimodal Action Frame Prediction via InstructPix2Pix](https://arxiv.org/abs/2507.14809)
*Zesen Zhong,Duomin Zhang,Yijia Li*

Main category: cs.CV

TL;DR: The paper introduces a lightweight, efficient method for predicting robot motion trajectories using a modified InstructPix2Pix model, reducing computational costs and latency compared to traditional video prediction models.


<details>
  <summary>Details</summary>
Motivation: Predicting future motion trajectories is crucial for robotics and autonomous systems to enhance decision-making. Existing methods are computationally heavy and slow, necessitating a more efficient solution.

Method: The authors adapt the InstructPix2Pix model for future visual frame prediction in robotics, using a single image and textual instruction as input. The model is fine-tuned for multimodal prediction.

Result: Experiments on the RoboTWin dataset show superior SSIM and PSNR scores compared to state-of-the-art baselines, with faster inference and lower GPU demands.

Conclusion: The proposed method offers a lightweight, efficient alternative for robot action prediction, prioritizing motion trajectory precision over visual fidelity, making it suitable for robotics and motion analytics.

Abstract: Predicting future motion trajectories is a critical capability across domains
such as robotics, autonomous systems, and human activity forecasting, enabling
safer and more intelligent decision-making. This paper proposes a novel,
efficient, and lightweight approach for robot action prediction, offering
significantly reduced computational cost and inference latency compared to
conventional video prediction models. Importantly, it pioneers the adaptation
of the InstructPix2Pix model for forecasting future visual frames in robotic
tasks, extending its utility beyond static image editing. We implement a deep
learning-based visual prediction framework that forecasts what a robot will
observe 100 frames (10 seconds) into the future, given a current image and a
textual instruction. We repurpose and fine-tune the InstructPix2Pix model to
accept both visual and textual inputs, enabling multimodal future frame
prediction. Experiments on the RoboTWin dataset (generated based on real-world
scenarios) demonstrate that our method achieves superior SSIM and PSNR compared
to state-of-the-art baselines in robot action prediction tasks. Unlike
conventional video prediction models that require multiple input frames, heavy
computation, and slow inference latency, our approach only needs a single image
and a text prompt as input. This lightweight design enables faster inference,
reduced GPU demands, and flexible multimodal control, particularly valuable for
applications like robotics and sports motion trajectory analytics, where motion
trajectory precision is prioritized over visual fidelity.

</details>


### [138] [SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models](https://arxiv.org/abs/2507.14811)
*Jiaji Zhang,Ruichao Sun,Hailiang Zhao,Jiaju Wu,Peng Chen,Hao Li,Xinkui Zhao,Kingsum Chow,Gang Xiong,Lin Ye,Shuiguang Deng*

Main category: cs.CV

TL;DR: SegQuant is a unified quantization framework for diffusion models, combining segment-aware quantization and dual-scale schemes to enhance versatility and maintain visual fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing PTQ methods for diffusion models lack generalizability and compatibility with industrial pipelines, limiting their deployment in resource-constrained environments.

Method: SegQuant uses a segment-aware, graph-based strategy (SegLinear) and a dual-scale quantization scheme (DualScale) to preserve structural semantics and polarity-asymmetric activations.

Result: SegQuant achieves strong performance across models, ensuring compatibility with mainstream deployment tools.

Conclusion: SegQuant addresses limitations of existing PTQ methods, offering a versatile and efficient solution for quantizing diffusion models.

Abstract: Diffusion models have demonstrated exceptional generative capabilities but
are computationally intensive, posing significant challenges for deployment in
resource-constrained or latency-sensitive environments. Quantization offers an
effective means to reduce model size and computational cost, with post-training
quantization (PTQ) being particularly appealing due to its compatibility with
pre-trained models without requiring retraining or training data. However,
existing PTQ methods for diffusion models often rely on architecture-specific
heuristics that limit their generalizability and hinder integration with
industrial deployment pipelines. To address these limitations, we propose
SegQuant, a unified quantization framework that adaptively combines
complementary techniques to enhance cross-model versatility. SegQuant consists
of a segment-aware, graph-based quantization strategy (SegLinear) that captures
structural semantics and spatial heterogeneity, along with a dual-scale
quantization scheme (DualScale) that preserves polarity-asymmetric activations,
which is crucial for maintaining visual fidelity in generated outputs. SegQuant
is broadly applicable beyond Transformer-based diffusion models, achieving
strong performance while ensuring seamless compatibility with mainstream
deployment tools.

</details>


### [139] [FinChart-Bench: Benchmarking Financial Chart Comprehension in Vision-Language Models](https://arxiv.org/abs/2507.14823)
*Dong Shu,Haoyang Yuan,Yuchen Wang,Yanguang Liu,Huopu Zhang,Haiyan Zhao,Mengnan Du*

Main category: cs.CV

TL;DR: FinChart-Bench is a new benchmark for evaluating LVLMs on financial charts, revealing key limitations in current models.


<details>
  <summary>Details</summary>
Motivation: Financial charts are complex and underexplored, necessitating a dedicated benchmark to assess LVLM performance.

Method: FinChart-Bench includes 1,200 annotated financial charts with 7,016 questions (TF, MC, QA). Evaluated 25 LVLMs.

Result: Key findings: narrowing gap between open/closed-source models, performance degradation in upgrades, struggles with instruction following, spatial reasoning limitations, and unreliability as automated evaluators.

Conclusion: Current LVLMs have significant limitations in financial chart understanding, highlighting the need for further research.

Abstract: Large vision-language models (LVLMs) have made significant progress in chart
understanding. However, financial charts, characterized by complex temporal
structures and domain-specific terminology, remain notably underexplored. We
introduce FinChart-Bench, the first benchmark specifically focused on
real-world financial charts. FinChart-Bench comprises 1,200 financial chart
images collected from 2015 to 2024, each annotated with True/False (TF),
Multiple Choice (MC), and Question Answering (QA) questions, totaling 7,016
questions. We conduct a comprehensive evaluation of 25 state-of-the-art LVLMs
on FinChart-Bench. Our evaluation reveals critical insights: (1) the
performance gap between open-source and closed-source models is narrowing, (2)
performance degradation occurs in upgraded models within families, (3) many
models struggle with instruction following, (4) both advanced models show
significant limitations in spatial reasoning abilities, and (5) current LVLMs
are not reliable enough to serve as automated evaluators. These findings
highlight important limitations in current LVLM capabilities for financial
chart understanding. The FinChart-Bench dataset is available at
https://huggingface.co/datasets/Tizzzzy/FinChart-Bench.

</details>


### [140] [PHATNet: A Physics-guided Haze Transfer Network for Domain-adaptive Real-world Image Dehazing](https://arxiv.org/abs/2507.14826)
*Fu-Jen Tsai,Yan-Tsung Peng,Yen-Yu Lin,Chia-Wen Lin*

Main category: cs.CV

TL;DR: PHATNet improves dehazing by transferring haze patterns from unseen domains to source images, enhancing model adaptability with novel losses.


<details>
  <summary>Details</summary>
Motivation: Existing dehazing models perform poorly on unseen real-world hazy images due to limited training data, prompting a need for domain adaptation.

Method: Proposes PHATNet, which transfers haze patterns to source-domain images for fine-tuning, using Haze-Transfer-Consistency and Content-Leakage Losses.

Result: PHATNet significantly enhances state-of-the-art dehazing models on real-world datasets.

Conclusion: PHATNet effectively adapts dehazing models to unseen domains, improving performance with innovative haze transfer and loss mechanisms.

Abstract: Image dehazing aims to remove unwanted hazy artifacts in images. Although
previous research has collected paired real-world hazy and haze-free images to
improve dehazing models' performance in real-world scenarios, these models
often experience significant performance drops when handling unseen real-world
hazy images due to limited training data. This issue motivates us to develop a
flexible domain adaptation method to enhance dehazing performance during
testing. Observing that predicting haze patterns is generally easier than
recovering clean content, we propose the Physics-guided Haze Transfer Network
(PHATNet) which transfers haze patterns from unseen target domains to
source-domain haze-free images, creating domain-specific fine-tuning sets to
update dehazing models for effective domain adaptation. Additionally, we
introduce a Haze-Transfer-Consistency loss and a Content-Leakage Loss to
enhance PHATNet's disentanglement ability. Experimental results demonstrate
that PHATNet significantly boosts state-of-the-art dehazing models on benchmark
real-world image dehazing datasets.

</details>


### [141] [Paired Image Generation with Diffusion-Guided Diffusion Models](https://arxiv.org/abs/2507.14833)
*Haoxuan Zhang,Wenju Cui,Yuzhu Cao,Tao Tan,Jie Liu,Yunsong Peng,Jian Zheng*

Main category: cs.CV

TL;DR: A paired image generation method for DBT images improves lesion segmentation by generating high-quality images and annotations without external conditions.


<details>
  <summary>Details</summary>
Motivation: High concealment of mass lesions in DBT images makes manual annotation difficult, leading to a lack of annotated data for training. Existing diffusion models struggle with lesion feature learning and lack annotation generation.

Method: Proposes a paired image generation method using a conditional diffusion model with an extra diffusion guider to generate images and annotations.

Result: Generated paired DBT slices and lesion masks improved segmentation task performance by addressing data shortage.

Conclusion: The method enhances generation quality and usability for supervised training, aiding downstream tasks.

Abstract: The segmentation of mass lesions in digital breast tomosynthesis (DBT) images
is very significant for the early screening of breast cancer. However, the
high-density breast tissue often leads to high concealment of the mass lesions,
which makes manual annotation difficult and time-consuming. As a result, there
is a lack of annotated data for model training. Diffusion models are commonly
used for data augmentation, but the existing methods face two challenges.
First, due to the high concealment of lesions, it is difficult for the model to
learn the features of the lesion area. This leads to the low generation quality
of the lesion areas, thus limiting the quality of the generated images. Second,
existing methods can only generate images and cannot generate corresponding
annotations, which restricts the usability of the generated images in
supervised training. In this work, we propose a paired image generation method.
The method does not require external conditions and can achieve the generation
of paired images by training an extra diffusion guider for the conditional
diffusion model. During the experimental phase, we generated paired DBT slices
and mass lesion masks. Then, we incorporated them into the supervised training
process of the mass lesion segmentation task. The experimental results show
that our method can improve the generation quality without external conditions.
Moreover, it contributes to alleviating the shortage of annotated data, thus
enhancing the performance of downstream tasks.

</details>


### [142] [Training Self-Supervised Depth Completion Using Sparse Measurements and a Single Image](https://arxiv.org/abs/2507.14845)
*Rizhao Fan,Zhigen Li,Heping Li,Ning An*

Main category: cs.CV

TL;DR: A novel self-supervised depth completion method using only sparse depth and single images, eliminating the need for dense labels or multi-frame data.


<details>
  <summary>Details</summary>
Motivation: Overcoming the limitations of costly dense annotations and multi-frame dependencies in existing depth completion methods.

Method: Proposes a self-supervised paradigm with novel loss functions for depth propagation and leverages segmentation maps from vision foundation models.

Result: Effective depth completion without dense labels or additional frames, validated through experiments.

Conclusion: The method offers a practical solution for depth completion in static or single-frame scenarios.

Abstract: Depth completion is an important vision task, and many efforts have been made
to enhance the quality of depth maps from sparse depth measurements. Despite
significant advances, training these models to recover dense depth from sparse
measurements remains a challenging problem. Supervised learning methods rely on
dense depth labels to predict unobserved regions, while self-supervised
approaches require image sequences to enforce geometric constraints and
photometric consistency between frames. However, acquiring dense annotations is
costly, and multi-frame dependencies limit the applicability of self-supervised
methods in static or single-frame scenarios. To address these challenges, we
propose a novel self-supervised depth completion paradigm that requires only
sparse depth measurements and their corresponding image for training. Unlike
existing methods, our approach eliminates the need for dense depth labels or
additional images captured from neighboring viewpoints. By leveraging the
characteristics of depth distribution, we design novel loss functions that
effectively propagate depth information from observed points to unobserved
regions. Additionally, we incorporate segmentation maps generated by vision
foundation models to further enhance depth estimation. Extensive experiments
demonstrate the effectiveness of our proposed method.

</details>


### [143] [Grounding Degradations in Natural Language for All-In-One Video Restoration](https://arxiv.org/abs/2507.14851)
*Muhammad Kamran Janjua,Amirhosein Ghasemabadi,Kunlin Zhang,Mohammad Salameh,Chao Gao,Di Niu*

Main category: cs.CV

TL;DR: Proposes an all-in-one video restoration framework using foundation models for interpretable guidance, introduces new benchmarks, and achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To create a flexible, interpretable video restoration method without requiring prior degradation knowledge, and to standardize benchmarks in the field.

Method: Uses foundation models to ground degradation-aware semantic context in natural language, learning approximations to avoid extra inference costs. Introduces new multi-degradation and time-varying benchmarks.

Result: Achieves state-of-the-art performance on all proposed benchmarks, including a new dataset with varying snow intensity.

Conclusion: The framework is effective and interpretable, and the new benchmarks address gaps in video restoration evaluation.

Abstract: In this work, we propose an all-in-one video restoration framework that
grounds degradation-aware semantic context of video frames in natural language
via foundation models, offering interpretable and flexible guidance. Unlike
prior art, our method assumes no degradation knowledge in train or test time
and learns an approximation to the grounded knowledge such that the foundation
model can be safely disentangled during inference adding no extra cost.
Further, we call for standardization of benchmarks in all-in-one video
restoration, and propose two benchmarks in multi-degradation setting,
three-task (3D) and four-task (4D), and two time-varying composite degradation
benchmarks; one of the latter being our proposed dataset with varying snow
intensity, simulating how weather degradations affect videos naturally. We
compare our method with prior works and report state-of-the-art performance on
all benchmarks.

</details>


### [144] [An Uncertainty-aware DETR Enhancement Framework for Object Detection](https://arxiv.org/abs/2507.14855)
*Xingshu Chen,Sicheng Yu,Chong Cheng,Hao Wang,Ting Tian*

Main category: cs.CV

TL;DR: The paper introduces an uncertainty-aware enhancement framework for DETR-based object detectors, improving localization accuracy and modeling prediction uncertainty using Gaussian distributions and Gromov-Wasserstein distance.


<details>
  <summary>Details</summary>
Motivation: Conventional object detectors lack uncertainty modeling, limiting robustness. The paper aims to address this by enhancing DETR-based detectors.

Method: Proposes modeling bounding boxes as multivariate Gaussian distributions, incorporating Gromov-Wasserstein distance in the loss function, and using Bayes Risk for filtering high-risk predictions. Also introduces a method to quantify uncertainty via confidence intervals.

Result: Experiments on COCO, LISC, and WBCDD datasets show improved performance and scalability across general and domain-specific tasks.

Conclusion: The framework effectively enhances DETR variants, confirming its scalability and robustness in object detection.

Abstract: This paper investigates the problem of object detection with a focus on
improving both the localization accuracy of bounding boxes and explicitly
modeling prediction uncertainty. Conventional detectors rely on deterministic
bounding box regression, ignoring uncertainty in predictions and limiting model
robustness. In this paper, we propose an uncertainty-aware enhancement
framework for DETR-based object detectors. We model bounding boxes as
multivariate Gaussian distributions and incorporate the Gromov-Wasserstein
distance into the loss function to better align the predicted and ground-truth
distributions. Building on this, we derive a Bayes Risk formulation to filter
high-risk information and improve detection reliability. We also propose a
simple algorithm to quantify localization uncertainty via confidence intervals.
Experiments on the COCO benchmark show that our method can be effectively
integrated into existing DETR variants, enhancing their performance. We further
extend our framework to leukocyte detection tasks, achieving state-of-the-art
results on the LISC and WBCDD datasets. These results confirm the scalability
of our framework across both general and domain-specific detection tasks. Code
page:
https://github.com/ParadiseforAndaChen/An-Uncertainty-aware-DETR-Enhancement-Framework-for-Object-Detection.

</details>


### [145] [Hybrid-supervised Hypergraph-enhanced Transformer for Micro-gesture Based Emotion Recognition](https://arxiv.org/abs/2507.14867)
*Zhaoqiang Xia,Hexiang Huang,Haoyu Chen,Xiaoyi Feng,Guoying Zhao*

Main category: cs.CV

TL;DR: The paper proposes a hypergraph-enhanced Transformer framework for emotion recognition using micro-gestures, achieving state-of-the-art performance on public datasets.


<details>
  <summary>Details</summary>
Motivation: Micro-gestures are understudied for emotion recognition, and existing methods lack sufficient modeling of subtle local motions.

Method: A hybrid-supervised framework with hypergraph-enhanced Transformer, including encoder-decoder architecture, self-supervised reconstruction, and supervised emotion recognition.

Result: The method outperforms existing approaches on iMiGUE and SMG datasets under multiple metrics.

Conclusion: The proposed framework effectively models micro-gestures for emotion recognition, demonstrating superior performance.

Abstract: Micro-gestures are unconsciously performed body gestures that can convey the
emotion states of humans and start to attract more research attention in the
fields of human behavior understanding and affective computing as an emerging
topic. However, the modeling of human emotion based on micro-gestures has not
been explored sufficiently. In this work, we propose to recognize the emotion
states based on the micro-gestures by reconstructing the behavior patterns with
a hypergraph-enhanced Transformer in a hybrid-supervised framework. In the
framework, hypergraph Transformer based encoder and decoder are separately
designed by stacking the hypergraph-enhanced self-attention and multiscale
temporal convolution modules. Especially, to better capture the subtle motion
of micro-gestures, we construct a decoder with additional upsampling operations
for a reconstruction task in a self-supervised learning manner. We further
propose a hypergraph-enhanced self-attention module where the hyperedges
between skeleton joints are gradually updated to present the relationships of
body joints for modeling the subtle local motion. Lastly, for exploiting the
relationship between the emotion states and local motion of micro-gestures, an
emotion recognition head from the output of encoder is designed with a shallow
architecture and learned in a supervised way. The end-to-end framework is
jointly trained in a one-stage way by comprehensively utilizing
self-reconstruction and supervision information. The proposed method is
evaluated on two publicly available datasets, namely iMiGUE and SMG, and
achieves the best performance under multiple metrics, which is superior to the
existing methods.

</details>


### [146] [Region-aware Depth Scale Adaptation with Sparse Measurements](https://arxiv.org/abs/2507.14879)
*Rizhao Fan,Tianfang Ma,Zhigen Li,Ning An,Jian Cheng*

Main category: cs.CV

TL;DR: A non-learning-based method uses sparse depth measurements to convert relative-scale depth predictions from foundation models into metric-scale depth, preserving generalization without retraining.


<details>
  <summary>Details</summary>
Motivation: Foundation models for depth prediction lack metric scale, limiting real-world application. Existing scale adaptation methods are costly and reduce generalization.

Method: Leverages sparse depth measurements to adapt relative-scale predictions to metric scale without retraining or fine-tuning.

Result: Effectively bridges the gap between relative and metric depth without additional computational costs or loss of generalization.

Conclusion: The approach enables metric-scale depth prediction while maintaining the generalization capabilities of foundation models.

Abstract: In recent years, the emergence of foundation models for depth prediction has
led to remarkable progress, particularly in zero-shot monocular depth
estimation. These models generate impressive depth predictions; however, their
outputs are often in relative scale rather than metric scale. This limitation
poses challenges for direct deployment in real-world applications. To address
this, several scale adaptation methods have been proposed to enable foundation
models to produce metric depth. However, these methods are typically costly, as
they require additional training on new domains and datasets. Moreover,
fine-tuning these models often compromises their original generalization
capabilities, limiting their adaptability across diverse scenes. In this paper,
we introduce a non-learning-based approach that leverages sparse depth
measurements to adapt the relative-scale predictions of foundation models into
metric-scale depth. Our method requires neither retraining nor fine-tuning,
thereby preserving the strong generalization ability of the original foundation
models while enabling them to produce metric depth. Experimental results
demonstrate the effectiveness of our approach, high-lighting its potential to
bridge the gap between relative and metric depth without incurring additional
computational costs or sacrificing generalization ability.

</details>


### [147] [BeatFormer: Efficient motion-robust remote heart rate estimation through unsupervised spectral zoomed attention filters](https://arxiv.org/abs/2507.14885)
*Joaquim Comas,Federico Sukno*

Main category: cs.CV

TL;DR: BeatFormer is a lightweight spectral attention model for rPPG estimation, combining deep learning and handcrafted methods for robustness and efficiency, validated on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: The need for hybrid approaches to leverage the strengths of deep learning (superior performance in complex conditions) and handcrafted methods (better generalization and computational efficiency) in rPPG estimation.

Method: Introduces BeatFormer, integrating zoomed orthonormal complex attention and frequency-domain energy measurement, and Spectral Contrastive Learning (SCL) for training without PPG or HR labels.

Result: Validated on PURE, UBFC-rPPG, and MMPD datasets, showing robustness and performance, especially in cross-dataset evaluations under motion.

Conclusion: BeatFormer effectively combines deep learning and handcrafted methods, offering a lightweight, efficient, and robust solution for rPPG estimation.

Abstract: Remote photoplethysmography (rPPG) captures cardiac signals from facial
videos and is gaining attention for its diverse applications. While deep
learning has advanced rPPG estimation, it relies on large, diverse datasets for
effective generalization. In contrast, handcrafted methods utilize
physiological priors for better generalization in unseen scenarios like motion
while maintaining computational efficiency. However, their linear assumptions
limit performance in complex conditions, where deep learning provides superior
pulsatile information extraction. This highlights the need for hybrid
approaches that combine the strengths of both methods. To address this, we
present BeatFormer, a lightweight spectral attention model for rPPG estimation,
which integrates zoomed orthonormal complex attention and frequency-domain
energy measurement, enabling a highly efficient model. Additionally, we
introduce Spectral Contrastive Learning (SCL), which allows BeatFormer to be
trained without any PPG or HR labels. We validate BeatFormer on the PURE,
UBFC-rPPG, and MMPD datasets, demonstrating its robustness and performance,
particularly in cross-dataset evaluations under motion scenarios.

</details>


### [148] [TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP](https://arxiv.org/abs/2507.14904)
*Fan Li,Zanyi Wang,Zeyi Huang,Guang Dai,Jingdong Wang,Mengmeng Wang*

Main category: cs.CV

TL;DR: A unified 2D pre-trained multi-modal network simplifies 3D visual grounding by processing RGB images, text, and point clouds together, reducing model complexity and improving performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on separate encoders for different modalities, leading to inefficiency and complexity. The goal is to streamline the process using a unified approach.

Method: Leverages a 2D CLIP bi-modal model with adapter-based fine-tuning, introduces GARF for geometric feature fusion, and integrates textual features with a multi-modal decoder.

Result: Reduces trainable parameters by 58%, improves 3D detection by 6.52%, and 3D visual grounding by 6.25%.

Conclusion: The proposed method offers a simpler, more efficient, and higher-performing solution for 3D visual grounding.

Abstract: 3D visual grounding allows an embodied agent to understand visual information
in real-world 3D environments based on human instructions, which is crucial for
embodied intelligence. Existing 3D visual grounding methods typically rely on
separate encoders for different modalities (e.g., RGB images, text, and 3D
point clouds), resulting in large and complex models that are inefficient to
train. While some approaches use pre-trained 2D multi-modal models like CLIP
for 3D tasks, they still struggle with aligning point cloud data to 2D
encoders. As a result, these methods continue to depend on 3D encoders for
feature extraction, further increasing model complexity and training
inefficiency. In this paper, we propose a unified 2D pre-trained multi-modal
network to process all three modalities (RGB images, text, and point clouds),
significantly simplifying the architecture. By leveraging a 2D CLIP bi-modal
model with adapter-based fine-tuning, this framework effectively adapts to the
tri-modal setting, improving both adaptability and performance across
modalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module
is designed to fuse geometric multi-scale features from point clouds and
images. We then integrate textual features for final modality fusion and
introduce a multi-modal decoder to facilitate deep cross-modal understanding.
Together, our method achieves unified feature extraction and fusion across the
three modalities, enabling an end-to-end 3D visual grounding model. Compared to
the baseline, our method reduces the number of trainable parameters by
approximately 58\%, while achieving a 6.52\% improvement in the 3D detection
task and a 6.25\% improvement in the 3D visual grounding task.

</details>


### [149] [Semantic-Aware Representation Learning for Multi-label Image Classification](https://arxiv.org/abs/2507.14918)
*Ren-Dong Xie,Zhi-Fen He,Bo Li,Bin Liu,Jin-Yan Hu*

Main category: cs.CV

TL;DR: Proposes SARL for multi-label image classification, using semantic-aware features, optimal transport-based attention, and regional score aggregation, outperforming existing methods on PASCAL VOC 2007 and MS-COCO.


<details>
  <summary>Details</summary>
Motivation: Existing methods for multi-label image classification often produce noisy representations and fail to locate objects precisely.

Method: Uses label semantic-related feature learning, optimal transport-based attention, and regional score aggregation.

Result: Outperforms existing methods on PASCAL VOC 2007 and MS-COCO datasets.

Conclusion: SARL improves multi-label image classification by addressing noise and misalignment in representations.

Abstract: Multi-label image classification, an important research area in computer
vision, focuses on identifying multiple labels or concepts within an image.
Existing approaches often employ attention mechanisms or graph convolutional
networks (GCNs) to learn image representation. However, this representation may
contain noise and may not locate objects precisely. Therefore, this paper
proposes a Semantic-Aware Representation Learning (SARL) for multi-label image
classification. First, a label semantic-related feature learning module is
utilized to extract semantic-related features. Then, an optimal transport-based
attention mechanism is designed to obtain semantically aligned image
representation. Finally, a regional score aggregation strategy is used for
multi-label prediction. Experimental results on two benchmark datasets, PASCAL
VOC 2007 and MS-COCO, demonstrate the superiority of SARL over existing
methods.

</details>


### [150] [Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian Splatting Reconstruction](https://arxiv.org/abs/2507.14921)
*Xiufeng Huang,Ka Chun Cheung,Runmin Cong,Simon See,Renjie Wan*

Main category: cs.CV

TL;DR: A disentangled framework for efficient 3D Gaussian prediction, reducing computational demands and improving robustness without relying on camera parameters.


<details>
  <summary>Details</summary>
Motivation: Current methods for 3D Gaussian Splatting reconstruction are computationally intensive, slow, and rely heavily on data-driven priors and camera parameters.

Method: Extracts features from local image pairs using stereo vision, fuses them via global attention, and uses dedicated heads for geometry and appearance prediction, refined for high-quality output.

Result: Achieves pose-free 3D reconstruction with reduced resource demands while maintaining high-quality outputs.

Conclusion: The proposed method offers an efficient, scalable solution for real-world 3D content generation.

Abstract: Generalizable 3D Gaussian Splatting reconstruction showcases advanced
Image-to-3D content creation but requires substantial computational resources
and large datasets, posing challenges to training models from scratch. Current
methods usually entangle the prediction of 3D Gaussian geometry and appearance,
which rely heavily on data-driven priors and result in slow regression speeds.
To address this, we propose \method, a disentangled framework for efficient 3D
Gaussian prediction. Our method extracts features from local image pairs using
a stereo vision backbone and fuses them via global attention blocks. Dedicated
point and Gaussian prediction heads generate multi-view point-maps for geometry
and Gaussian features for appearance, combined as GS-maps to represent the 3DGS
object. A refinement network enhances these GS-maps for high-quality
reconstruction. Unlike existing methods that depend on camera parameters, our
approach achieves pose-free 3D reconstruction, improving robustness and
practicality. By reducing resource demands while maintaining high-quality
outputs, \method provides an efficient, scalable solution for real-world 3D
content generation.

</details>


### [151] [3-Dimensional CryoEM Pose Estimation and Shift Correction Pipeline](https://arxiv.org/abs/2507.14924)
*Kaishva Chintan Shah,Virajith Boddapati,Karthik S. Gurumoorthy,Sandip Kaledhonkar,Ajit Rajwade*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Accurate pose estimation and shift correction are key challenges in cryo-EM
due to the very low SNR, which directly impacts the fidelity of 3D
reconstructions. We present an approach for pose estimation in cryo-EM that
leverages multi-dimensional scaling (MDS) techniques in a robust manner to
estimate the 3D rotation matrix of each particle from pairs of dihedral angles.
We express the rotation matrix in the form of an axis of rotation and a unit
vector in the plane perpendicular to the axis. The technique leverages the
concept of common lines in 3D reconstruction from projections. However, common
line estimation is ridden with large errors due to the very low SNR of cryo-EM
projection images. To address this challenge, we introduce two complementary
components: (i) a robust joint optimization framework for pose estimation based
on an $\ell_1$-norm objective or a similar robust norm, which simultaneously
estimates rotation axes and in-plane vectors while exactly enforcing unit norm
and orthogonality constraints via projected coordinate descent; and (ii) an
iterative shift correction algorithm that estimates consistent in-plane
translations through a global least-squares formulation. While prior approaches
have leveraged such embeddings and common-line geometry for orientation
recovery, existing formulations typically rely on $\ell_2$-based objectives
that are sensitive to noise, and enforce geometric constraints only
approximately. These choices, combined with a sequential pipeline structure,
can lead to compounding errors and suboptimal reconstructions in low-SNR
regimes. Our pipeline consistently outperforms prior methods in both Euler
angle accuracy and reconstruction fidelity, as measured by the Fourier Shell
Correlation (FSC).

</details>


### [152] [Probabilistic smooth attention for deep multiple instance learning in medical imaging](https://arxiv.org/abs/2507.14932)
*Francisco M. Castro-Macías,Pablo Morales-Álvarez,Yunan Wu,Rafael Molina,Aggelos K. Katsaggelos*

Main category: cs.CV

TL;DR: A probabilistic framework for Multiple Instance Learning (MIL) in medical imaging improves predictive performance and provides interpretable uncertainty maps.


<details>
  <summary>Details</summary>
Motivation: MIL methods in medical imaging often overlook uncertainty in instance contributions, despite their importance for accurate classification and interpretability.

Method: The proposed framework estimates a probability distribution over attention values, capturing both local and global interactions among instances.

Result: Outperforms eleven state-of-the-art baselines across three medical datasets, achieving top predictive performance and interpretable uncertainty maps.

Conclusion: The probabilistic approach enhances MIL by addressing uncertainty in attention values, improving both performance and interpretability in medical imaging.

Abstract: The Multiple Instance Learning (MIL) paradigm is attracting plenty of
attention in medical imaging classification, where labeled data is scarce. MIL
methods cast medical images as bags of instances (e.g. patches in whole slide
images, or slices in CT scans), and only bag labels are required for training.
Deep MIL approaches have obtained promising results by aggregating
instance-level representations via an attention mechanism to compute the
bag-level prediction. These methods typically capture both local interactions
among adjacent instances and global, long-range dependencies through various
mechanisms. However, they treat attention values deterministically, potentially
overlooking uncertainty in the contribution of individual instances. In this
work we propose a novel probabilistic framework that estimates a probability
distribution over the attention values, and accounts for both global and local
interactions. In a comprehensive evaluation involving {\color{review} eleven}
state-of-the-art baselines and three medical datasets, we show that our
approach achieves top predictive performance in different metrics. Moreover,
the probabilistic treatment of the attention provides uncertainty maps that are
interpretable in terms of illness localization.

</details>


### [153] [Open-set Cross Modal Generalization via Multimodal Unified Representation](https://arxiv.org/abs/2507.14935)
*Hai Huang,Yan Xia,Shulei Wang,Hanting Wang,Minghui Fang,Shengpeng Ji,Sashuai Zhou,Tao Jin,Zhou Zhao*

Main category: cs.CV

TL;DR: The paper introduces Open-set Cross Modal Generalization (OSCMG), a more challenging task than CMG, and proposes MICU with FCMI and CUJP to address open-set limitations in multimodal representation learning.


<details>
  <summary>Details</summary>
Motivation: Prior work on multimodal unified representations lacks consideration for open-set environments, which are common in real-world applications.

Method: Proposes MICU with two components: Fine-Coarse Masked multimodal InfoNCE (FCMI) for multimodal alignment and Cross modal Unified Jigsaw Puzzles (CUJP) for feature diversity and uncertainty handling.

Result: Extensive experiments on CMG and OSCMG validate the effectiveness of MICU.

Conclusion: The approach successfully addresses open-set challenges in cross-modal generalization, enhancing robustness to unseen classes.

Abstract: This paper extends Cross Modal Generalization (CMG) to open-set environments
by proposing the more challenging Open-set Cross Modal Generalization (OSCMG)
task. This task evaluates multimodal unified representations in open-set
conditions, addressing the limitations of prior closed-set cross-modal
evaluations. OSCMG requires not only cross-modal knowledge transfer but also
robust generalization to unseen classes within new modalities, a scenario
frequently encountered in real-world applications. Existing multimodal unified
representation work lacks consideration for open-set environments. To tackle
this, we propose MICU, comprising two key components: Fine-Coarse Masked
multimodal InfoNCE (FCMI) and Cross modal Unified Jigsaw Puzzles (CUJP). FCMI
enhances multimodal alignment by applying contrastive learning at both holistic
semantic and temporal levels, incorporating masking to enhance generalization.
CUJP enhances feature diversity and model uncertainty by integrating
modality-agnostic feature selection with self-supervised learning, thereby
strengthening the model's ability to handle unknown categories in open-set
tasks. Extensive experiments on CMG and the newly proposed OSCMG validate the
effectiveness of our approach. The code is available at
https://github.com/haihuangcode/CMG.

</details>


### [154] [Polymorph: Energy-Efficient Multi-Label Classification for Video Streams on Embedded Devices](https://arxiv.org/abs/2507.14959)
*Saeid Ghafouri,Mohsen Fayyaz,Xiangchen Li,Deepu John,Bo Ji,Dimitrios Nikolopoulos,Hans Vandierendonck*

Main category: cs.CV

TL;DR: Polymorph is a framework for efficient real-time multi-label video classification on embedded devices by leveraging label sparsity and co-occurrence, reducing energy use by 40% and improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Limited compute and energy budgets on embedded devices necessitate efficient video classification methods.

Method: Polymorph uses lightweight Low Rank Adapters (LoRA) activated per frame based on label co-occurrence, avoiding full-model switching.

Result: Achieves 40% lower energy consumption and 9-point mAP improvement on the TAO dataset.

Conclusion: Polymorph offers a scalable, energy-efficient solution for real-time video classification.

Abstract: Real-time multi-label video classification on embedded devices is constrained
by limited compute and energy budgets. Yet, video streams exhibit structural
properties such as label sparsity, temporal continuity, and label co-occurrence
that can be leveraged for more efficient inference. We introduce Polymorph, a
context-aware framework that activates a minimal set of lightweight Low Rank
Adapters (LoRA) per frame. Each adapter specializes in a subset of classes
derived from co-occurrence patterns and is implemented as a LoRA weight over a
shared backbone. At runtime, Polymorph dynamically selects and composes only
the adapters needed to cover the active labels, avoiding full-model switching
and weight merging. This modular strategy improves scalability while reducing
latency and energy overhead. Polymorph achieves 40% lower energy consumption
and improves mAP by 9 points over strong baselines on the TAO dataset.
Polymorph is open source at https://github.com/inference-serving/polymorph/.

</details>


### [155] [Decision PCR: Decision version of the Point Cloud Registration task](https://arxiv.org/abs/2507.14965)
*Yaojie Zhang,Tianlun Huang,Weijun Wang,Wei Feng*

Main category: cs.CV

TL;DR: The paper addresses low-overlap point cloud registration (PCR) by proposing a data-driven deep learning classifier to evaluate registration quality, improving existing methods' performance.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics like Maximum Inlier Count fail under low inlier ratios, necessitating a new approach to evaluate PCR results.

Method: A deep learning-based classifier is trained on a dataset derived from 3DMatch to assess registration quality, integrated into standard PCR pipelines.

Result: Integration with GeoTransformer achieves 86.97% registration recall on 3DLoMatch and shows strong generalization on ETH dataset.

Conclusion: The proposed data-driven approach effectively improves PCR performance and generalizes well to unseen datasets.

Abstract: Low-overlap point cloud registration (PCR) remains a significant challenge in
3D vision. Traditional evaluation metrics, such as Maximum Inlier Count, become
ineffective under extremely low inlier ratios. In this paper, we revisit the
registration result evaluation problem and identify the Decision version of the
PCR task as the fundamental problem. To address this Decision PCR task, we
propose a data-driven approach. First, we construct a corresponding dataset
based on the 3DMatch dataset. Then, a deep learning-based classifier is trained
to reliably assess registration quality, overcoming the limitations of
traditional metrics. To our knowledge, this is the first comprehensive study to
address this task through a deep learning framework. We incorporate this
classifier into standard PCR pipelines. When integrated with our approach,
existing state-of-the-art PCR methods exhibit significantly enhanced
registration performance. For example, combining our framework with
GeoTransformer achieves a new SOTA registration recall of 86.97\% on the
challenging 3DLoMatch benchmark. Our method also demonstrates strong
generalization capabilities on the unseen outdoor ETH dataset.

</details>


### [156] [Hierarchical Cross-modal Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2507.14976)
*Hao Zheng,Shunzhi Yang,Zhuoxin He,Jinfeng Yang,Zhenhua Huang*

Main category: cs.CV

TL;DR: HiCroPL is a hierarchical cross-modal prompt learning framework addressing modality isolation and semantic decay in VLMs, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Adapting large-scale VLMs to downstream tasks while preserving generalization is challenging due to modality isolation and hierarchical semantic decay.

Method: HiCroPL establishes bidirectional knowledge flow between text and vision, using a hierarchical knowledge mapper and lightweight layer-specific proxies.

Result: Achieves state-of-the-art results on 11 benchmarks across four tasks.

Conclusion: HiCroPL effectively enhances generalization in VLMs by enabling mutual refinement of text and vision modalities.

Abstract: Pre-trained Vision-Language Models (VLMs) such as CLIP have shown excellent
generalization abilities. However, adapting these large-scale models to
downstream tasks while preserving their generalization capabilities remains
challenging. Although prompt learning methods have shown promise, they suffer
from two fundamental bottlenecks that limit generalization: (a) modality
isolation, and (b) hierarchical semantic decay. To address these limitations,
we propose HiCroPL, a Hierarchical Cross-modal Prompt Learning framework that
establishes bidirectional knowledge flow between text and vision modalities,
enabling them to refine their semantics mutually. HiCroPL routes knowledge
flows by leveraging the complementary strengths of text and vision. In early
layers, text prompts inject relatively clear semantics into visual prompts
through a hierarchical knowledge mapper, enhancing the representation of
low-level visual semantics. In later layers, visual prompts encoding specific
task-relevant objects flow back to refine text prompts, enabling deeper
alignment. Crucially, our hierarchical knowledge mapper allows representations
at multi-scales to be fused, ensuring that deeper representations retain
transferable shallow semantics thereby enhancing generalization. We further
introduce a lightweight layer-specific knowledge proxy to enable efficient
cross-modal interactions. Extensive evaluations across four tasks demonstrate
HiCroPL's superior performance, achieving state-of-the-art results on 11
benchmarks with significant improvements. Code is available at:
https://github.com/zzeoZheng/HiCroPL.

</details>


### [157] [Language Integration in Fine-Tuning Multimodal Large Language Models for Image-Based Regression](https://arxiv.org/abs/2507.14997)
*Roy H. Jennings,Genady Paikin,Roy Shaul,Evgeny Soloveichik*

Main category: cs.CV

TL;DR: MLLMs for image regression underperform due to generic prompts and preset vocabularies. RvTC, a bin-based method, outperforms by using flexible bins and data-specific prompts, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Current MLLM approaches for image regression fail to leverage textual input effectively, performing no better than image-only models.

Method: Proposed RvTC replaces vocabulary-constrained classification with a flexible bin-based approach and uses data-specific prompts.

Result: RvTC achieves state-of-the-art performance on four datasets. Semantic prompts improve correlations (e.g., 0.83 to 0.90 on AVA).

Conclusion: Meaningful textual context is crucial for MLLMs in multimodal regression, as shown by RvTC's success with semantic prompts.

Abstract: Multimodal Large Language Models (MLLMs) show promise for image-based
regression tasks, but current approaches face key limitations. Recent methods
fine-tune MLLMs using preset output vocabularies and generic task-level prompts
(e.g., "How would you rate this image?"), assuming this mimics human rating
behavior. Our analysis reveals these approaches provide no benefit over
image-only training. Models using preset vocabularies and generic prompts
perform equivalently to image-only models, failing to leverage semantic
understanding from textual input. We propose Regression via Transformer-Based
Classification (RvTC), which replaces vocabulary-constrained classification
with a flexible bin-based approach. Unlike approaches that address
discretization errors through complex distributional modeling, RvTC eliminates
manual vocabulary crafting through straightforward bin increase, achieving
state-of-the-art performance on four image assessment datasets using only
images. More importantly, we demonstrate that data-specific prompts
dramatically improve performance. Unlike generic task descriptions, prompts
containing semantic information about specific images enable MLLMs to leverage
cross-modal understanding. On the AVA dataset, adding challenge titles to
prompts improves correlations from 0.83 to 0.90, a new state-of-the-art. We
demonstrate through empirical evidence from the AVA and AGIQA-3k datasets that
MLLMs benefit from semantic prompt information surpassing mere statistical
biases. This underscores the importance of incorporating meaningful textual
context in multimodal regression tasks.

</details>


### [158] [Axis-Aligned Document Dewarping](https://arxiv.org/abs/2507.15000)
*Chaoyun Wang,I-Chao Shen,Takeo Igarashi,Nanning Zheng,Caigui Jiang*

Main category: cs.CV

TL;DR: The paper introduces an axis-aligned geometric constraint for document dewarping, improving performance and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack geometric property utilization in document dewarping.

Method: Proposes axis-aligned geometric constraint for training and preprocessing for inference.

Result: Achieves SOTA results with 18.2%~34.5% improvement on the AAD metric.

Conclusion: The method effectively leverages geometric properties for superior dewarping.

Abstract: Document dewarping is crucial for many applications. However, existing
learning-based methods primarily rely on supervised regression with annotated
data without leveraging the inherent geometric properties in physical documents
to the dewarping process. Our key insight is that a well-dewarped document is
characterized by transforming distorted feature lines into axis-aligned ones.
This property aligns with the inherent axis-aligned nature of the discrete grid
geometry in planar documents. In the training phase, we propose an axis-aligned
geometric constraint to enhance document dewarping. In the inference phase, we
propose an axis alignment preprocessing strategy to reduce the dewarping
difficulty. In the evaluation phase, we introduce a new metric, Axis-Aligned
Distortion (AAD), that not only incorporates geometric meaning and aligns with
human visual perception but also demonstrates greater robustness. As a result,
our method achieves SOTA results on multiple existing benchmarks and achieves
18.2%~34.5% improvements on the AAD metric.

</details>


### [159] [FastSmoothSAM: A Fast Smooth Method For Segment Anything Model](https://arxiv.org/abs/2507.15008)
*Jiasheng Xu,Yewang Chen*

Main category: cs.CV

TL;DR: The paper introduces a B-Spline curve fitting method to refine jagged edges in FastSAM, improving edge quality while maintaining real-time performance.


<details>
  <summary>Details</summary>
Motivation: FastSAM, though efficient, produces jagged edges that misrepresent true object shapes, limiting its accuracy in real-world applications.

Method: A four-stage refining process using B-Spline curve fitting is applied to smooth edges in FastSAM.

Result: The method enhances edge quality and analytical accuracy without losing geometric details or real-time capabilities.

Conclusion: This refinement boosts FastSAM's practicality for applications like industrial automation and medical imaging, where precise edge recognition is vital.

Abstract: Accurately identifying and representing object edges is a challenging task in
computer vision and image processing. The Segment Anything Model (SAM) has
significantly influenced the field of image segmentation, but suffers from high
memory consumption and long inference times, limiting its efficiency in
real-time applications. To address these limitations, Fast Segment Anything
(FastSAM) was proposed, achieving real-time segmentation. However, FastSAM
often generates jagged edges that deviate from the true object shapes.
Therefore, this paper introduces a novel refinement approach using B-Spline
curve fitting techniques to enhance the edge quality in FastSAM. Leveraging the
robust shape control and flexible geometric construction of B-Splines, a
four-stage refining process involving two rounds of curve fitting is employed
to effectively smooth jagged edges. This approach significantly improves the
visual quality and analytical accuracy of object edges without compromising
critical geometric information. The proposed method improves the practical
utility of FastSAM by improving segmentation accuracy while maintaining
real-time processing capabilities. This advancement unlocks greater potential
for FastSAM technology in various real-world scenarios, such as industrial
automation, medical imaging, and autonomous systems, where precise and
efficient edge recognition is crucial.

</details>


### [160] [Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding](https://arxiv.org/abs/2507.15028)
*Yuanhan Zhang,Yunice Chew,Yuhao Dong,Aria Leo,Bo Hu,Ziwei Liu*

Main category: cs.CV

TL;DR: The paper introduces Video-TT, a benchmark to evaluate video LLMs' correctness and robustness in video understanding, revealing a significant gap compared to human performance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to measure the gap between video LLMs and human intelligence in video interpretation, particularly in correctness and robustness.

Method: Video-TT includes 1,000 YouTube Shorts videos with open-ended and adversarial questions to assess visual and narrative understanding.

Result: Video LLMs show a significant performance gap compared to humans in interpreting complex videos and handling adversarial questions.

Conclusion: Video-TT highlights the limitations of current video LLMs and underscores the need for improved models to match human-like video understanding.

Abstract: Human intelligence requires correctness and robustness, with the former being
foundational for the latter. In video understanding, correctness ensures the
accurate interpretation of visual content, and robustness maintains consistent
performance in challenging conditions. Despite advances in video large language
models (video LLMs), existing benchmarks inadequately reflect the gap between
these models and human intelligence in maintaining correctness and robustness
in video interpretation. We introduce the Video Thinking Test (Video-TT), to
assess if video LLMs can interpret real-world videos as effectively as humans.
Video-TT reflects genuine gaps in understanding complex visual narratives, and
evaluates robustness against natural adversarial questions. Video-TT comprises
1,000 YouTube Shorts videos, each with one open-ended question and four
adversarial questions that probe visual and narrative complexity. Our
evaluation shows a significant gap between video LLMs and human performance.

</details>


### [161] [OpenBreastUS: Benchmarking Neural Operators for Wave Imaging Using Breast Ultrasound Computed Tomography](https://arxiv.org/abs/2507.15035)
*Zhijun Zeng,Youjia Zheng,Hao Hu,Zeyuan Dong,Yihang Zheng,Xinliang Liu,Jinzhuo Wang,Zuoqiang Shi,Linfeng Zhang,Yubing Li,He Sun*

Main category: cs.CV

TL;DR: OpenBreastUS introduces a large-scale dataset for wave equation simulations, enabling benchmarking of neural operators for realistic medical imaging.


<details>
  <summary>Details</summary>
Motivation: Traditional wave equation solvers are computationally intensive and unstable, while neural operators lack realistic datasets. OpenBreastUS bridges this gap.

Method: The dataset includes 8,000 realistic breast phantoms and 16 million frequency-domain wave simulations, tested with neural operators for forward and inverse tasks.

Result: The dataset allows benchmarking neural operators, showing their performance, scalability, and generalization. Efficient in vivo breast imaging is demonstrated.

Conclusion: OpenBreastUS advances neural PDE solvers for real-world medical imaging, providing a realistic platform for development and deployment.

Abstract: Accurate and efficient simulation of wave equations is crucial in
computational wave imaging applications, such as ultrasound computed tomography
(USCT), which reconstructs tissue material properties from observed scattered
waves. Traditional numerical solvers for wave equations are computationally
intensive and often unstable, limiting their practical applications for
quasi-real-time image reconstruction. Neural operators offer an innovative
approach by accelerating PDE solving using neural networks; however, their
effectiveness in realistic imaging is limited because existing datasets
oversimplify real-world complexity. In this paper, we present OpenBreastUS, a
large-scale wave equation dataset designed to bridge the gap between
theoretical equations and practical imaging applications. OpenBreastUS includes
8,000 anatomically realistic human breast phantoms and over 16 million
frequency-domain wave simulations using real USCT configurations. It enables a
comprehensive benchmarking of popular neural operators for both forward
simulation and inverse imaging tasks, allowing analysis of their performance,
scalability, and generalization capabilities. By offering a realistic and
extensive dataset, OpenBreastUS not only serves as a platform for developing
innovative neural PDE solvers but also facilitates their deployment in
real-world medical imaging problems. For the first time, we demonstrate
efficient in vivo imaging of the human breast using neural operator solvers.

</details>


### [162] [EBA-AI: Ethics-Guided Bias-Aware AI for Efficient Underwater Image Enhancement and Coral Reef Monitoring](https://arxiv.org/abs/2507.15036)
*Lyes Saad Saoud,Irfan Hussain*

Main category: cs.CV

TL;DR: EBA-AI is an ethics-guided, bias-aware AI framework for underwater image enhancement, addressing dataset bias, computational costs, and transparency issues.


<details>
  <summary>Details</summary>
Motivation: To improve marine conservation efforts by overcoming AI limitations like bias, high computational demands, and lack of interpretability in underwater image enhancement.

Method: Uses CLIP embeddings for bias detection and mitigation, adaptive processing for energy efficiency, and integrates uncertainty estimation and explainability techniques.

Result: Achieves computational savings with minimal PSNR drop (1.0 dB), enabling real-time feasibility, and outperforms models like CycleGAN and WaterNet in fairness and efficiency.

Conclusion: EBA-AI advances sustainable, bias-aware, and efficient underwater image processing, supporting trustworthy AI-driven marine conservation.

Abstract: Underwater image enhancement is vital for marine conservation, particularly
coral reef monitoring. However, AI-based enhancement models often face dataset
bias, high computational costs, and lack of transparency, leading to potential
misinterpretations. This paper introduces EBA-AI, an ethics-guided bias-aware
AI framework to address these challenges. EBA-AI leverages CLIP embeddings to
detect and mitigate dataset bias, ensuring balanced representation across
varied underwater environments. It also integrates adaptive processing to
optimize energy efficiency, significantly reducing GPU usage while maintaining
competitive enhancement quality. Experiments on LSUI400, Oceanex, and UIEB100
show that while PSNR drops by a controlled 1.0 dB, computational savings enable
real-time feasibility for large-scale marine monitoring. Additionally,
uncertainty estimation and explainability techniques enhance trust in AI-driven
environmental decisions. Comparisons with CycleGAN, FunIEGAN, RAUNENet,
WaterNet, UGAN, PUGAN, and UTUIE validate EBA-AI's effectiveness in balancing
efficiency, fairness, and interpretability in underwater image processing. By
addressing key limitations of AI-driven enhancement, this work contributes to
sustainable, bias-aware, and computationally efficient marine conservation
efforts. For interactive visualizations, animations, source code, and access to
the preprint, visit: https://lyessaadsaoud.github.io/EBA-AI/

</details>


### [163] [OmniVTON: Training-Free Universal Virtual Try-On](https://arxiv.org/abs/2507.15037)
*Zhaotong Yang,Yuhui Li,Shengfeng He,Xinzhe Li,Yangyang Xu,Junyu Dong,Yong Du*

Main category: cs.CV

TL;DR: OmniVTON is a training-free universal VTON framework that decouples garment and pose conditioning for high fidelity and consistency across diverse settings.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of supervised in-shop and unsupervised in-the-wild VTON methods, which struggle with cross-domain generalization and data biases.

Method: Uses garment prior generation and boundary stitching for texture fidelity, and DDIM inversion for pose alignment, disentangling garment and pose constraints.

Result: Superior performance across diverse datasets, garment types, and scenarios, including multi-human VTON.

Conclusion: OmniVTON is a versatile, training-free solution for universal VTON, overcoming biases and enabling multi-human garment transfer.

Abstract: Image-based Virtual Try-On (VTON) techniques rely on either supervised
in-shop approaches, which ensure high fidelity but struggle with cross-domain
generalization, or unsupervised in-the-wild methods, which improve adaptability
but remain constrained by data biases and limited universality. A unified,
training-free solution that works across both scenarios remains an open
challenge. We propose OmniVTON, the first training-free universal VTON
framework that decouples garment and pose conditioning to achieve both texture
fidelity and pose consistency across diverse settings. To preserve garment
details, we introduce a garment prior generation mechanism that aligns clothing
with the body, followed by continuous boundary stitching technique to achieve
fine-grained texture retention. For precise pose alignment, we utilize DDIM
inversion to capture structural cues while suppressing texture interference,
ensuring accurate body alignment independent of the original image textures. By
disentangling garment and pose constraints, OmniVTON eliminates the bias
inherent in diffusion models when handling multiple conditions simultaneously.
Experimental results demonstrate that OmniVTON achieves superior performance
across diverse datasets, garment types, and application scenarios. Notably, it
is the first framework capable of multi-human VTON, enabling realistic garment
transfer across multiple individuals in a single scene. Code is available at
https://github.com/Jerome-Young/OmniVTON

</details>


### [164] [Rethinking Pan-sharpening: Principled Design, Unified Training, and a Universal Loss Surpass Brute-Force Scaling](https://arxiv.org/abs/2507.15059)
*Ran Zhang,Xuanhua He,Li Xueheng,Ke Cao,Liu Liu,Wenbo Xu,Fang Jiabin,Yang Qize,Jie Zhang*

Main category: cs.CV

TL;DR: PanTiny is a lightweight, efficient pan-sharpening framework trained on multiple datasets, outperforming larger models with better generalization and performance-to-efficiency balance.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiency and poor generalization of large, dataset-specific pan-sharpening models.

Method: Proposes PanTiny, a single-step framework trained on three satellite datasets (WV2, WV3, GF2) with a composite loss function.

Result: PanTiny achieves superior performance-to-efficiency balance and outperforms larger models.

Conclusion: Advocates for efficient, generalizable models in pan-sharpening, validated by principled engineering over brute-force scaling.

Abstract: The field of pan-sharpening has recently seen a trend towards increasingly
large and complex models, often trained on single, specific satellite datasets.
This approach, however, leads to high computational overhead and poor
generalization on full resolution data, a paradigm we challenge in this paper.
In response to this issue, we propose PanTiny, a lightweight, single-step
pan-sharpening framework designed for both efficiency and robust performance.
More critically, we introduce multiple-in-one training paradigm, where a
single, compact model is trained simultaneously on three distinct satellite
datasets (WV2, WV3, and GF2) with different resolution and spectral
information. Our experiments show that this unified training strategy not only
simplifies deployment but also significantly boosts generalization on
full-resolution data. Further, we introduce a universally powerful composite
loss function that elevates the performance of almost all of models for
pan-sharpening, pushing state-of-the-art metrics into a new era. Our PanTiny
model, benefiting from these innovations, achieves a superior
performance-to-efficiency balance, outperforming most larger, specialized
models. Through extensive ablation studies, we validate that principled
engineering in model design, training paradigms, and loss functions can surpass
brute-force scaling. Our work advocates for a community-wide shift towards
creating efficient, generalizable, and data-conscious models for
pan-sharpening. The code is available at
https://github.com/Zirconium233/PanTiny .

</details>


### [165] [StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation](https://arxiv.org/abs/2507.15064)
*Shuyuan Tu,Zhen Xing,Xintong Han,Zhi-Qi Cheng,Qi Dai,Chong Luo,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: StableAnimator++ is a video diffusion framework that preserves identity (ID) consistency in human image animation by using learnable pose alignment and advanced modules for training and inference.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models struggle with ID consistency when reference images and driving videos differ in body size or position.

Method: StableAnimator++ employs learnable pose alignment via SVD-guided transformation matrices, image/face embeddings, a Face Encoder, and a distribution-aware ID Adapter. It also integrates HJB-based face optimization during inference.

Result: The framework achieves high-quality, ID-consistent video generation without post-processing, validated by benchmarks.

Conclusion: StableAnimator++ effectively addresses ID inconsistency in human image animation, offering superior performance in both quality and fidelity.

Abstract: Current diffusion models for human image animation often struggle to maintain
identity (ID) consistency, especially when the reference image and driving
video differ significantly in body size or position. We introduce
StableAnimator++, the first ID-preserving video diffusion framework with
learnable pose alignment, capable of generating high-quality videos conditioned
on a reference image and a pose sequence without any post-processing. Building
upon a video diffusion model, StableAnimator++ contains carefully designed
modules for both training and inference, striving for identity consistency. In
particular, StableAnimator++ first uses learnable layers to predict the
similarity transformation matrices between the reference image and the driven
poses via injecting guidance from Singular Value Decomposition (SVD). These
matrices align the driven poses with the reference image, mitigating
misalignment to a great extent. StableAnimator++ then computes image and face
embeddings using off-the-shelf encoders, refining the face embeddings via a
global content-aware Face Encoder. To further maintain ID, we introduce a
distribution-aware ID Adapter that counteracts interference caused by temporal
layers while preserving ID via distribution alignment. During the inference
stage, we propose a novel Hamilton-Jacobi-Bellman (HJB) based face optimization
integrated into the denoising process, guiding the diffusion trajectory for
enhanced facial fidelity. Experiments on benchmarks show the effectiveness of
StableAnimator++ both qualitatively and quantitatively.

</details>


### [166] [Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR](https://arxiv.org/abs/2507.15085)
*Peirong Zhang,Haowei Xu,Jiaxin Zhang,Guitao Xu,Xuhan Zheng,Zhenhua Yang,Junle Liu,Yuyi Zhang,Lianwen Jin*

Main category: cs.CV

TL;DR: The paper evaluates state-of-the-art generative models for text image generation and editing, incorporating OCR tasks, and identifies their weaknesses.


<details>
  <summary>Details</summary>
Motivation: To assess if current generative models can handle the complexities of text image generation and editing, given their advancements in fidelity.

Method: Evaluates six models using 33 OCR tasks across five categories, with tailored inputs and prompts.

Result: Identifies weaknesses in current models and argues for integrating text image generation as a foundational skill in general-domain models.

Conclusion: Photorealistic text image generation should be a core capability of general models, not just specialized solutions.

Abstract: Text image is a unique and crucial information medium that integrates visual
aesthetics and linguistic semantics in modern e-society. Due to their subtlety
and complexity, the generation of text images represents a challenging and
evolving frontier in the image generation field. The recent surge of
specialized image generators (\emph{e.g.}, Flux-series) and unified generative
models (\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a
natural question: can they master the intricacies of text image generation and
editing? Motivated by this, we assess current state-of-the-art generative
models' capabilities in terms of text image generation and editing. We
incorporate various typical optical character recognition (OCR) tasks into our
evaluation and broaden the concept of text-based generation tasks into OCR
generative tasks. We select 33 representative tasks and categorize them into
five categories: document, handwritten text, scene text, artistic text, and
complex \& layout-rich text. For comprehensive evaluation, we examine six
models across both closed-source and open-source domains, using tailored,
high-quality image inputs and prompts. Through this evaluation, we draw crucial
observations and identify the weaknesses of current generative models for OCR
tasks. We argue that photorealistic text image generation and editing should be
internalized as foundational skills into general-domain generative models,
rather than being delegated to specialized solutions, and we hope this
empirical analysis can provide valuable insights for the community to achieve
this goal. This evaluation is online and will be continuously updated at our
GitHub repository.

</details>


### [167] [Visual Place Recognition for Large-Scale UAV Applications](https://arxiv.org/abs/2507.15089)
*Ioannis Tsampikos Papapetros,Ioannis Kansizoglou,Antonios Gasteratos*

Main category: cs.CV

TL;DR: The paper introduces LASED, a large-scale aerial dataset, and steerable CNNs to improve Visual Place Recognition (vPR) for UAVs, addressing challenges like dataset scarcity and rotational ambiguity.


<details>
  <summary>Details</summary>
Motivation: Aerial vPR faces challenges due to limited datasets and rotational variance in UAV imagery, hindering model generalization.

Method: Proposes LASED, a structured large-scale dataset, and integrates steerable CNNs to handle rotational variance.

Result: Models trained on LASED achieve higher recall, and steerable CNNs outperform conventional CNNs by 12% in recall.

Conclusion: Combining large-scale datasets with rotation-equivariant networks enhances robustness and generalization in aerial vPR.

Abstract: Visual Place Recognition (vPR) plays a crucial role in Unmanned Aerial
Vehicle (UAV) navigation, enabling robust localization across diverse
environments. Despite significant advancements, aerial vPR faces unique
challenges due to the limited availability of large-scale, high-altitude
datasets, which limits model generalization, along with the inherent rotational
ambiguity in UAV imagery. To address these challenges, we introduce LASED, a
large-scale aerial dataset with approximately one million images,
systematically sampled from 170,000 unique locations throughout Estonia over a
decade, offering extensive geographic and temporal diversity. Its structured
design ensures clear place separation significantly enhancing model training
for aerial scenarios. Furthermore, we propose the integration of steerable
Convolutional Neural Networks (CNNs) to explicitly handle rotational variance,
leveraging their inherent rotational equivariance to produce robust,
orientation-invariant feature representations. Our extensive benchmarking
demonstrates that models trained on LASED achieve significantly higher recall
compared to those trained on smaller, less diverse datasets, highlighting the
benefits of extensive geographic coverage and temporal diversity. Moreover,
steerable CNNs effectively address rotational ambiguity inherent in aerial
imagery, consistently outperforming conventional convolutional architectures,
achieving on average 12\% recall improvement over the best-performing
non-steerable network. By combining structured, large-scale datasets with
rotation-equivariant neural networks, our approach significantly enhances model
robustness and generalization for aerial vPR.

</details>


### [168] [BleedOrigin: Dynamic Bleeding Source Localization in Endoscopic Submucosal Dissection via Dual-Stage Detection and Tracking](https://arxiv.org/abs/2507.15094)
*Mengya Xu,Rulin Zhou,An Wang,Chaoyang Lyu,Zhen Li,Ning Zhong,Hongliang Ren*

Main category: cs.CV

TL;DR: The paper introduces BleedOrigin-Bench, a dataset for bleeding source detection in ESD, and BleedOrigin-Net, a dual-stage AI framework for localization and tracking, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Current AI methods for ESD bleeding focus on segmentation but lack accurate source detection and tracking, compounded by the absence of specialized datasets.

Method: The authors develop BleedOrigin-Bench, a dataset with expert annotations, and BleedOrigin-Net, a detection-tracking framework, evaluated against existing models.

Result: BleedOrigin-Net achieves 96.85% frame-level accuracy for onset detection, 70.24% for initial source detection, and 96.11% for tracking.

Conclusion: The proposed framework and dataset address critical gaps in ESD bleeding management, offering robust AI-assisted guidance.

Abstract: Intraoperative bleeding during Endoscopic Submucosal Dissection (ESD) poses
significant risks, demanding precise, real-time localization and continuous
monitoring of the bleeding source for effective hemostatic intervention. In
particular, endoscopists have to repeatedly flush to clear blood, allowing only
milliseconds to identify bleeding sources, an inefficient process that prolongs
operations and elevates patient risks. However, current Artificial Intelligence
(AI) methods primarily focus on bleeding region segmentation, overlooking the
critical need for accurate bleeding source detection and temporal tracking in
the challenging ESD environment, which is marked by frequent visual
obstructions and dynamic scene changes. This gap is widened by the lack of
specialized datasets, hindering the development of robust AI-assisted guidance
systems. To address these challenges, we introduce BleedOrigin-Bench, the first
comprehensive ESD bleeding source dataset, featuring 1,771 expert-annotated
bleeding sources across 106,222 frames from 44 procedures, supplemented with
39,755 pseudo-labeled frames. This benchmark covers 8 anatomical sites and 6
challenging clinical scenarios. We also present BleedOrigin-Net, a novel
dual-stage detection-tracking framework for the bleeding source localization in
ESD procedures, addressing the complete workflow from bleeding onset detection
to continuous spatial tracking. We compare with widely-used object detection
models (YOLOv11/v12), multimodal large language models, and point tracking
methods. Extensive evaluation demonstrates state-of-the-art performance,
achieving 96.85% frame-level accuracy ($\pm\leq8$ frames) for bleeding onset
detection, 70.24% pixel-level accuracy ($\leq100$ px) for initial source
detection, and 96.11% pixel-level accuracy ($\leq100$ px) for point tracking.

</details>


### [169] [LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM](https://arxiv.org/abs/2507.15109)
*Mohammad-Maher Nakshbandi,Ziad Sharawy,Sorin Grigorescu*

Main category: cs.CV

TL;DR: LoopNet improves SLAM loop closure detection with multitasking ResNet, online retraining, and DISK descriptors, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Addressing accuracy and real-time computation challenges in SLAM loop closure detection.

Method: Multitasking ResNet variant with online retraining (few-shot learning) and DISK descriptors.

Result: Better performance under varying conditions; new LoopDB dataset introduced.

Conclusion: LoopNet enhances SLAM loop closure with efficient, accurate, and adaptable solutions.

Abstract: One of the main challenges in the Simultaneous Localization and Mapping
(SLAM) loop closure problem is the recognition of previously visited places. In
this work, we tackle the two main problems of real-time SLAM systems: 1) loop
closure detection accuracy and 2) real-time computation constraints on the
embedded hardware. Our LoopNet method is based on a multitasking variant of the
classical ResNet architecture, adapted for online retraining on a dynamic
visual dataset and optimized for embedded devices. The online retraining is
designed using a few-shot learning approach. The architecture provides both an
index into the queried visual dataset, and a measurement of the prediction
quality. Moreover, by leveraging DISK (DIStinctive Keypoints) descriptors,
LoopNet surpasses the limitations of handcrafted features and traditional deep
learning methods, offering better performance under varying conditions. Code is
available at https://github.com/RovisLab/LoopNet. Additinally, we introduce a
new loop closure benchmarking dataset, coined LoopDB, which is available at
https://github.com/RovisLab/LoopDB.

</details>


### [170] [Enhancing Visual Planning with Auxiliary Tasks and Multi-token Prediction](https://arxiv.org/abs/2507.15130)
*Ce Zhang,Yale Song,Ruta Desai,Michael Louis Iuzzolino,Joseph Tighe,Gedas Bertasius,Satwik Kottur*

Main category: cs.CV

TL;DR: VPA predicts user action sequences from videos. Challenges include scarce procedural annotations and inefficient next-token prediction. Solutions: Auxiliary Task Augmentation and Multi-token Prediction. VideoPlan achieves SOTA results on COIN and CrossTask.


<details>
  <summary>Details</summary>
Motivation: Address challenges in training MLLMs for video-based planning: data scarcity and inefficiency of next-token prediction for structured action spaces.

Method: Introduces Auxiliary Task Augmentation and Multi-token Prediction to enhance planning ability and model structured action spaces.

Result: VideoPlan outperforms prior methods by 7.3% and 3.4% on COIN and CrossTask, respectively, and matches SOTA on Ego4D.

Conclusion: VideoPlan effectively tackles VPA challenges, achieving superior performance and generalizability.

Abstract: Visual Planning for Assistance (VPA) aims to predict a sequence of user
actions required to achieve a specified goal based on a video showing the
user's progress. Although recent advances in multimodal large language models
(MLLMs) have shown promising results in video understanding, long-horizon
visual planning remains a challenging problem. We identify two challenges in
training large MLLMs for video-based planning tasks: (1) scarcity of procedural
annotations, limiting the model's ability to learn procedural task dynamics
effectively, and (2) inefficiency of next-token prediction objective to
explicitly capture the structured action space for visual planning when
compared to free-form, natural language. To tackle data scarcity, we introduce
Auxiliary Task Augmentation. We design and train our model on auxiliary tasks
relevant to long-horizon video-based planning (e.g., goal prediction) to
augment the model's planning ability. To more explicitly model the structured
action space unique to visual planning tasks, we leverage Multi-token
Prediction, extending traditional next-token prediction by using multiple heads
to predict multiple future tokens during training. Our approach, VideoPlan,
achieves state-of-the-art VPA performance on the COIN and CrossTask datasets,
surpassing prior methods by 7.3% and 3.4%, respectively, when predicting 3
future actions. We further extend our method to the challenging Ego4D Long-term
Action Anticipation task, and show that it is on par with the state-of-the-art
approaches despite not using specialized egocentric features. Code will be made
available.

</details>


### [171] [Event-based Graph Representation with Spatial and Motion Vectors for Asynchronous Object Detection](https://arxiv.org/abs/2507.15150)
*Aayush Atul Verma,Arpitsinh Vaghela,Bharatesh Chakravarthi,Kaustav Chanda,Yezhou Yang*

Main category: cs.CV

TL;DR: A novel spatiotemporal multigraph representation improves event-based object detection by decoupling spatial and temporal dynamics, achieving higher accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Event-based sensors' sparse, asynchronous data loses advantages when converted to dense tensors, and existing graph methods underperform due to poor spatiotemporal modeling.

Method: Proposes a spatiotemporal multigraph with decoupled spatial (B-spline basis) and temporal (motion vector-based attention) graphs, replacing 3D kernels with efficient 2D ones.

Result: Achieves 6% higher detection accuracy, 5x speedup, fewer parameters, and no added computational cost on Gen1 and eTraM datasets.

Conclusion: Structured graph modeling effectively enhances asynchronous vision tasks, balancing performance and efficiency.

Abstract: Event-based sensors offer high temporal resolution and low latency by
generating sparse, asynchronous data. However, converting this irregular data
into dense tensors for use in standard neural networks diminishes these
inherent advantages, motivating research into graph representations. While such
methods preserve sparsity and support asynchronous inference, their performance
on downstream tasks remains limited due to suboptimal modeling of
spatiotemporal dynamics. In this work, we propose a novel spatiotemporal
multigraph representation to better capture spatial structure and temporal
changes. Our approach constructs two decoupled graphs: a spatial graph
leveraging B-spline basis functions to model global structure, and a temporal
graph utilizing motion vector-based attention for local dynamic changes. This
design enables the use of efficient 2D kernels in place of computationally
expensive 3D kernels. We evaluate our method on the Gen1 automotive and eTraM
datasets for event-based object detection, achieving over a 6% improvement in
detection accuracy compared to previous graph-based works, with a 5x speedup,
reduced parameter count, and no increase in computational cost. These results
highlight the effectiveness of structured graph modeling for asynchronous
vision. Project page: eventbasedvision.github.io/eGSMV.

</details>


### [172] [MeshMamba: State Space Models for Articulated 3D Mesh Generation and Reconstruction](https://arxiv.org/abs/2507.15212)
*Yusuke Yoshiyasu,Leyuan Sun,Ryusuke Sagawa*

Main category: cs.CV

TL;DR: MeshMamba is a neural network model using Mamba-SSMs for efficient 3D articulated mesh learning, enabling large-scale mesh generation and reconstruction with over 10,000 vertices. It introduces MambaDiff3D for mesh generation and Mamba-HMR for single-image human mesh recovery, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and scalability challenges in learning 3D articulated mesh models, especially for large vertex counts, and to extend capabilities to whole-body reconstruction including clothing and hands.

Method: MeshMamba serializes mesh vertices into orderings (via body part annotations or 3D template locations) for Mamba-SSMs. It includes MambaDiff3D (denoising diffusion for mesh generation) and Mamba-HMR (single-image human mesh recovery).

Result: MambaDiff3D generates dense 3D human meshes with clothing and hands, outperforming previous methods. Mamba-HMR handles whole-body reconstruction (face, hands) in near real-time, extending prior body-only approaches.

Conclusion: MeshMamba demonstrates scalability and efficiency in 3D articulated mesh tasks, with applications in generation and reconstruction, achieving state-of-the-art performance.

Abstract: In this paper, we introduce MeshMamba, a neural network model for learning 3D
articulated mesh models by employing the recently proposed Mamba State Space
Models (Mamba-SSMs). MeshMamba is efficient and scalable in handling a large
number of input tokens, enabling the generation and reconstruction of body mesh
models with more than 10,000 vertices, capturing clothing and hand geometries.
The key to effectively learning MeshMamba is the serialization technique of
mesh vertices into orderings that are easily processed by Mamba. This is
achieved by sorting the vertices based on body part annotations or the 3D
vertex locations of a template mesh, such that the ordering respects the
structure of articulated shapes. Based on MeshMamba, we design 1) MambaDiff3D,
a denoising diffusion model for generating 3D articulated meshes and 2)
Mamba-HMR, a 3D human mesh recovery model that reconstructs a human body shape
and pose from a single image. Experimental results showed that MambaDiff3D can
generate dense 3D human meshes in clothes, with grasping hands, etc., and
outperforms previous approaches in the 3D human shape generation task.
Additionally, Mamba-HMR extends the capabilities of previous non-parametric
human mesh recovery approaches, which were limited to handling body-only poses
using around 500 vertex tokens, to the whole-body setting with face and hands,
while achieving competitive performance in (near) real-time.

</details>


### [173] [Improving Joint Embedding Predictive Architecture with Diffusion Noise](https://arxiv.org/abs/2507.15216)
*Yuping Qiu,Rui Zhu,Ying-cong Chen*

Main category: cs.CV

TL;DR: The paper proposes N-JEPA, a method combining diffusion noise with masked image modeling (MIM) in self-supervised learning (SSL) to enhance feature learning for recognition tasks.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between SSL (effective for discriminative tasks) and generative models (superior in image generation) by leveraging diffusion noise for semantic understanding.

Method: Introduces N-JEPA, integrating diffusion noise into MIM via position embedding of masked tokens and using a multi-level noise schedule for feature augmentation.

Result: Demonstrates effectiveness in downstream classification tasks.

Conclusion: N-JEPA successfully combines SSL and generative principles, enhancing representation capacity for recognition tasks.

Abstract: Self-supervised learning has become an incredibly successful method for
feature learning, widely applied to many downstream tasks. It has proven
especially effective for discriminative tasks, surpassing the trending
generative models. However, generative models perform better in image
generation and detail enhancement. Thus, it is natural for us to find a
connection between SSL and generative models to further enhance the
representation capacity of SSL. As generative models can create new samples by
approximating the data distribution, such modeling should also lead to a
semantic understanding of the raw visual data, which is necessary for
recognition tasks. This enlightens us to combine the core principle of the
diffusion model: diffusion noise, with SSL to learn a competitive recognition
model. Specifically, diffusion noise can be viewed as a particular state of
mask that reveals a close relationship between masked image modeling (MIM) and
diffusion models. In this paper, we propose N-JEPA (Noise-based JEPA) to
incorporate diffusion noise into MIM by the position embedding of masked
tokens. The multi-level noise schedule is a series of feature augmentations to
further enhance the robustness of our model. We perform a comprehensive study
to confirm its effectiveness in the classification of downstream tasks. Codes
will be released soon in public.

</details>


### [174] [Hierarchical Part-based Generative Model for Realistic 3D Blood Vessel](https://arxiv.org/abs/2507.15223)
*Siqi Chen,Guoqing Zhang,Jiahao Lai,Bingzhi Shen,Sihong Zhang,Caixia Dong,Xuejin Chen,Yang Li*

Main category: cs.CV

TL;DR: A hierarchical part-based framework for 3D blood vessel generation, separating global topology from local geometry, outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurately modeling the complex geometry and topology of blood vessels is challenging due to their intricate branching patterns and irregular shapes.

Method: A three-stage approach: (1) key graph generation for hierarchical structure, (2) vessel segment generation based on geometric properties, and (3) hierarchical vessel assembly.

Result: Validated on real-world datasets, the framework demonstrates superior performance in modeling complex vascular networks.

Conclusion: This work is the first successful part-based generative approach for 3D vessel modeling, setting a new benchmark.

Abstract: Advancements in 3D vision have increased the impact of blood vessel modeling
on medical applications. However, accurately representing the complex geometry
and topology of blood vessels remains a challenge due to their intricate
branching patterns, curvatures, and irregular shapes. In this study, we propose
a hierarchical part-based frame work for 3D vessel generation that separates
the global binary tree-like topology from local geometric details. Our approach
proceeds in three stages: (1) key graph generation to model the overall
hierarchical struc ture, (2) vessel segment generation conditioned on geometric
properties, and (3) hierarchical vessel assembly by integrating the local
segments according to the global key graph. We validate our framework on real
world datasets, demonstrating superior performance over existing methods in
modeling complex vascular networks. This work marks the first successful
application of a part-based generative approach for 3D vessel modeling, setting
a new benchmark for vascular data generation. The code is available at:
https://github.com/CybercatChen/PartVessel.git.

</details>


### [175] [Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse Autoencoders](https://arxiv.org/abs/2507.15227)
*Krishna Kanth Nakka*

Main category: cs.CV

TL;DR: The paper introduces Sparse Autoencoder (SAE)-based interpretability to breast imaging using Mammo-CLIP, identifying clinically relevant features and confounding factors in model decisions.


<details>
  <summary>Details</summary>
Motivation: Interpretability is crucial in medical imaging for clinical adoption, especially in understanding model decisions.

Method: A patch-level Mammo-SAE is trained on Mammo-CLIP to probe latent features linked to breast concepts like mass and suspicious calcification.

Result: Top activated latent neurons align with ground truth regions, and confounding factors in decision-making are uncovered. The study also identifies neurons used in downstream finetuning.

Conclusion: SAE-based interpretability offers deeper insights into foundation models for breast imaging, aiding clinical understanding.

Abstract: Interpretability is critical in high-stakes domains such as medical imaging,
where understanding model decisions is essential for clinical adoption. In this
work, we introduce Sparse Autoencoder (SAE)-based interpretability to breast
imaging by analyzing {Mammo-CLIP}, a vision--language foundation model
pretrained on large-scale mammogram image--report pairs. We train a patch-level
\texttt{Mammo-SAE} on Mammo-CLIP to identify and probe latent features
associated with clinically relevant breast concepts such as \textit{mass} and
\textit{suspicious calcification}. Our findings reveal that top activated class
level latent neurons in the SAE latent space often tend to align with ground
truth regions, and also uncover several confounding factors influencing the
model's decision-making process. Additionally, we analyze which latent neurons
the model relies on during downstream finetuning for improving the breast
concept prediction. This study highlights the promise of interpretable SAE
latent representations in providing deeper insight into the internal workings
of foundation models at every layer for breast imaging.

</details>


### [176] [Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation](https://arxiv.org/abs/2507.15243)
*Naeem Paeedeh,Mahardhika Pratama,Wolfgang Mayer,Jimmy Cao,Ryszard Kowlczyk*

Main category: cs.CV

TL;DR: A new method, Coalescent Projection (CP), combined with pseudo-class generation and Self-Supervised Transformations (SSTs), outperforms SOTA in CD-FSL by addressing overfitting and domain shift.


<details>
  <summary>Details</summary>
Motivation: Overcoming overfitting in CD-FSL due to limited labeled samples and extreme domain shifts.

Method: Introduces Coalescent Projection (CP) and pseudo-class generation with SSTs, leveraging base domain data for unseen domains.

Result: Outperforms SOTA methods on the BSCD-FSL benchmark, especially in extreme domain shifts.

Conclusion: CP and SSTs effectively address CD-FSL challenges, offering a robust solution for domain adaptation.

Abstract: Despite the progress in Cross-Domain Few-Shot Learning (CD-FSL), a model
pre-trained with DINO combined with a prototypical classifier outperforms the
latest SOTA methods. A crucial limitation that needs to be overcome is that
updating too many parameters of the transformers leads to overfitting due to
the scarcity of labeled samples. To address this challenge, we propose a new
concept, Coalescent Projection (CP), as an effective successor to soft prompts.
Additionally, we propose a novel pseudo-class generation method combined with
Self-Supervised Transformations (SSTs) that relies solely on the base domain to
prepare the network for encountering unseen samples from different domains. The
proposed method exhibits its effectiveness in comprehensive experiments on the
extreme domain shift scenario of the BSCD-FSL benchmark. Our code is published
at https://github.com/Naeem-Paeedeh/CPLSR.

</details>


### [177] [FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers](https://arxiv.org/abs/2507.15249)
*Yanbing Zhang,Zhe Wang,Qin Zhou,Mengping Yang*

Main category: cs.CV

TL;DR: FreeCus is a training-free framework leveraging diffusion transformers for subject-driven synthesis, introducing attention sharing, dynamic shifting analysis, and MLLM integration to achieve zero-shot capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing methods for subject-driven synthesis rely on training procedures, limiting practicality and failing to utilize the zero-shot potential of diffusion transformers.

Method: FreeCus introduces attention sharing, an upgraded DiT variant for feature extraction, and integrates MLLMs for cross-modal semantics.

Result: The framework achieves state-of-the-art or comparable results without training, enabling consistent subject synthesis and compatibility with existing pipelines.

Conclusion: FreeCus successfully unlocks DiT's zero-shot potential for subject-driven synthesis, offering practical and flexible applications.

Abstract: In light of recent breakthroughs in text-to-image (T2I) generation,
particularly with diffusion transformers (DiT), subject-driven technologies are
increasingly being employed for high-fidelity customized production that
preserves subject identity from reference inputs, enabling thrilling design
workflows and engaging entertainment. Existing alternatives typically require
either per-subject optimization via trainable text embeddings or training
specialized encoders for subject feature extraction on large-scale datasets.
Such dependencies on training procedures fundamentally constrain their
practical applications. More importantly, current methodologies fail to fully
leverage the inherent zero-shot potential of modern diffusion transformers
(e.g., the Flux series) for authentic subject-driven synthesis. To bridge this
gap, we propose FreeCus, a genuinely training-free framework that activates
DiT's capabilities through three key innovations: 1) We introduce a pivotal
attention sharing mechanism that captures the subject's layout integrity while
preserving crucial editing flexibility. 2) Through a straightforward analysis
of DiT's dynamic shifting, we propose an upgraded variant that significantly
improves fine-grained feature extraction. 3) We further integrate advanced
Multimodal Large Language Models (MLLMs) to enrich cross-modal semantic
representations. Extensive experiments reflect that our method successfully
unlocks DiT's zero-shot ability for consistent subject synthesis across diverse
contexts, achieving state-of-the-art or comparable results compared to
approaches that require additional training. Notably, our framework
demonstrates seamless compatibility with existing inpainting pipelines and
control modules, facilitating more compelling experiences. Our code is
available at: https://github.com/Monalissaa/FreeCus.

</details>


### [178] [MinCD-PnP: Learning 2D-3D Correspondences with Approximate Blind PnP](https://arxiv.org/abs/2507.15257)
*Pei An,Jiaqi Yang,Muyao Peng,You Yang,Qiong Liu,Xiaolin Wu,Liangliang Nan*

Main category: cs.CV

TL;DR: The paper proposes MinCD-PnP, a robust method for image-to-point-cloud registration by simplifying blind PnP to minimize Chamfer distance, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Differential PnP is sensitive to noise and outliers, limiting correspondence learning. Blind PnP is robust but computationally expensive.

Method: Simplifies blind PnP to minimize Chamfer distance (MinCD-PnP) and introduces MinCD-Net, a lightweight multi-task learning module.

Result: MinCD-Net achieves higher inlier ratio and registration recall across datasets like 7-Scenes and ScanNet.

Conclusion: The proposed MinCD-PnP and MinCD-Net effectively address noise and outlier issues, improving I2P registration performance.

Abstract: Image-to-point-cloud (I2P) registration is a fundamental problem in computer
vision, focusing on establishing 2D-3D correspondences between an image and a
point cloud. The differential perspective-n-point (PnP) has been widely used to
supervise I2P registration networks by enforcing the projective constraints on
2D-3D correspondences. However, differential PnP is highly sensitive to noise
and outliers in the predicted correspondences. This issue hinders the
effectiveness of correspondence learning. Inspired by the robustness of blind
PnP against noise and outliers in correspondences, we propose an approximated
blind PnP based correspondence learning approach. To mitigate the high
computational cost of blind PnP, we simplify blind PnP to an amenable task of
minimizing Chamfer distance between learned 2D and 3D keypoints, called
MinCD-PnP. To effectively solve MinCD-PnP, we design a lightweight multi-task
learning module, named as MinCD-Net, which can be easily integrated into the
existing I2P registration architectures. Extensive experiments on 7-Scenes,
RGBD-V2, ScanNet, and self-collected datasets demonstrate that MinCD-Net
outperforms state-of-the-art methods and achieves a higher inlier ratio (IR)
and registration recall (RR) in both cross-scene and cross-dataset settings.

</details>


### [179] [Conditional Video Generation for High-Efficiency Video Compression](https://arxiv.org/abs/2507.15269)
*Fangqiu Yi,Jingyu Xu,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: A video compression framework using conditional diffusion models for perceptually optimized reconstruction, outperforming traditional and neural codecs.


<details>
  <summary>Details</summary>
Motivation: To leverage conditional diffusion models for video compression, aligning with human visual perception for better reconstruction quality.

Method: Reframes compression as conditional generation with multi-granular conditioning, compact representations, and multi-condition training.

Result: Significantly outperforms traditional and neural codecs on perceptual metrics like FVD and LPIPS, especially at high compression ratios.

Conclusion: The proposed framework effectively enhances video compression quality by integrating conditional diffusion models.

Abstract: Perceptual studies demonstrate that conditional diffusion models excel at
reconstructing video content aligned with human visual perception. Building on
this insight, we propose a video compression framework that leverages
conditional diffusion models for perceptually optimized reconstruction.
Specifically, we reframe video compression as a conditional generation task,
where a generative model synthesizes video from sparse, yet informative
signals. Our approach introduces three key modules: (1) Multi-granular
conditioning that captures both static scene structure and dynamic
spatio-temporal cues; (2) Compact representations designed for efficient
transmission without sacrificing semantic richness; (3) Multi-condition
training with modality dropout and role-aware embeddings, which prevent
over-reliance on any single modality and enhance robustness. Extensive
experiments show that our method significantly outperforms both traditional and
neural codecs on perceptual quality metrics such as Fr\'echet Video Distance
(FVD) and LPIPS, especially under high compression ratios.

</details>


### [180] [In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems](https://arxiv.org/abs/2507.15285)
*Lazaro Janier Gonzalez-Soler,Maciej Salwowski,Christoph Busch*

Main category: cs.CV

TL;DR: The paper proposes a Vision Language Model (VLM) framework for detecting biometric attacks, outperforming traditional CNNs without extensive training.


<details>
  <summary>Details</summary>
Motivation: Biometric systems face sophisticated attacks, and traditional deep learning models struggle with adaptability and data requirements.

Method: Uses in-context learning with VLMs to detect physical and digital attacks, evaluated on open-source models.

Result: Competitive performance in attack detection, surpassing some CNNs without intensive training.

Conclusion: VLMs offer a promising, resource-efficient solution for improving generalization in biometric attack detection.

Abstract: Recent advances in biometric systems have significantly improved the
detection and prevention of fraudulent activities. However, as detection
methods improve, attack techniques become increasingly sophisticated. Attacks
on face recognition systems can be broadly divided into physical and digital
approaches. Traditionally, deep learning models have been the primary defence
against such attacks. While these models perform exceptionally well in
scenarios for which they have been trained, they often struggle to adapt to
different types of attacks or varying environmental conditions. These
subsystems require substantial amounts of training data to achieve reliable
performance, yet biometric data collection faces significant challenges,
including privacy concerns and the logistical difficulties of capturing diverse
attack scenarios under controlled conditions. This work investigates the
application of Vision Language Models (VLM) and proposes an in-context learning
framework for detecting physical presentation attacks and digital morphing
attacks in biometric systems. Focusing on open-source models, the first
systematic framework for the quantitative evaluation of VLMs in
security-critical scenarios through in-context learning techniques is
established. The experimental evaluation conducted on freely available
databases demonstrates that the proposed subsystem achieves competitive
performance for physical and digital attack detection, outperforming some of
the traditional CNNs without resource-intensive training. The experimental
results validate the proposed framework as a promising tool for improving
generalisation in attack detection.

</details>


### [181] [Minutiae-Anchored Local Dense Representation for Fingerprint Matching](https://arxiv.org/abs/2507.15297)
*Zhiyu Pan,Xiongjun Guan,Yongjie Duan,Jianjiang Feng,Jie Zhou*

Main category: cs.CV

TL;DR: Proposes DMD, a minutiae-anchored dense representation for robust fingerprint matching under diverse conditions, achieving state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of fingerprint matching under varied capture conditions by combining minutiae and ridge textures.

Method: Extracts descriptors from minutia-centered patches, forming a 3D tensor for multi-level feature aggregation. Uses segmentation masks for efficient matching.

Result: Demonstrates superior accuracy on rolled, plain, partial, contactless, and latent fingerprint datasets.

Conclusion: DMD is effective, generalizable, and computationally efficient, suitable for large-scale fingerprint recognition.

Abstract: Fingerprint matching under diverse capture conditions remains a fundamental
challenge in biometric recognition. To achieve robust and accurate performance
in such scenarios, we propose DMD, a minutiae-anchored local dense
representation which captures both fine-grained ridge textures and
discriminative minutiae features in a spatially structured manner.
Specifically, descriptors are extracted from local patches centered and
oriented on each detected minutia, forming a three-dimensional tensor, where
two dimensions represent spatial locations on the fingerprint plane and the
third encodes semantic features. This representation explicitly captures
abstract features of local image patches, enabling a multi-level, fine-grained
description that aggregates information from multiple minutiae and their
surrounding ridge structures. Furthermore, thanks to its strong spatial
correspondence with the patch image, DMD allows for the use of foreground
segmentation masks to identify valid descriptor regions. During matching,
comparisons are then restricted to overlapping foreground areas, improving
efficiency and robustness. Extensive experiments on rolled, plain, parital,
contactless, and latent fingerprint datasets demonstrate the effectiveness and
generalizability of the proposed method. It achieves state-of-the-art accuracy
across multiple benchmarks while maintaining high computational efficiency,
showing strong potential for large-scale fingerprint recognition. Corresponding
code is available at https://github.com/Yu-Yy/DMD.

</details>


### [182] [Few-Shot Object Detection via Spatial-Channel State Space Model](https://arxiv.org/abs/2507.15308)
*Zhimeng Xin,Tianxu Wu,Yixiong Zou,Shiming Chen,Dingjie Fu,Xinge You*

Main category: cs.CV

TL;DR: The paper proposes a Spatial-Channel State Space Modeling (SCSM) module to address feature extraction challenges in few-shot object detection by leveraging inter-channel correlation, inspired by Mamba for temporal sequence modeling.


<details>
  <summary>Details</summary>
Motivation: Current few-shot object detection methods struggle with accurately extracting effective features due to limited training samples, leading to misalignment between channel weights and their actual effectiveness.

Method: The SCSM module includes a Spatial Feature Modeling (SFM) module for spatial-channel balance and a Channel State Modeling (CSM) module based on Mamba to model channel correlations.

Result: Experiments on VOC and COCO datasets demonstrate improved feature representation and state-of-the-art performance.

Conclusion: The SCSM module effectively enhances feature extraction in few-shot object detection by addressing channel correlation issues.

Abstract: Due to the limited training samples in few-shot object detection (FSOD), we
observe that current methods may struggle to accurately extract effective
features from each channel. Specifically, this issue manifests in two aspects:
i) channels with high weights may not necessarily be effective, and ii)
channels with low weights may still hold significant value. To handle this
problem, we consider utilizing the inter-channel correlation to facilitate the
novel model's adaptation process to novel conditions, ensuring the model can
correctly highlight effective channels and rectify those incorrect ones. Since
the channel sequence is also 1-dimensional, its similarity with the temporal
sequence inspires us to take Mamba for modeling the correlation in the channel
sequence. Based on this concept, we propose a Spatial-Channel State Space
Modeling (SCSM) module for spatial-channel state modeling, which highlights the
effective patterns and rectifies those ineffective ones in feature channels. In
SCSM, we design the Spatial Feature Modeling (SFM) module to balance the
learning of spatial relationships and channel relationships, and then introduce
the Channel State Modeling (CSM) module based on Mamba to learn correlation in
channels. Extensive experiments on the VOC and COCO datasets show that the SCSM
module enables the novel detector to improve the quality of focused feature
representation in channels and achieve state-of-the-art performance.

</details>


### [183] [BenchDepth: Are We on the Right Way to Evaluate Depth Foundation Models?](https://arxiv.org/abs/2507.15321)
*Zhenyu Li,Haotong Lin,Jiashi Feng,Peter Wonka,Bingyi Kang*

Main category: cs.CV

TL;DR: BenchDepth introduces a new benchmark for evaluating depth foundation models (DFMs) using five downstream tasks, avoiding biases in traditional alignment-based metrics.


<details>
  <summary>Details</summary>
Motivation: Existing depth evaluation protocols are inconsistent and biased, complicating fair comparisons of DFMs.

Method: Proposes BenchDepth, evaluating DFMs through five proxy tasks (e.g., depth completion, SLAM) to assess practical utility.

Result: Benchmarked eight DFMs, providing insights into their performance and limitations.

Conclusion: BenchDepth offers a fairer evaluation method, encouraging better practices and future research in depth estimation.

Abstract: Depth estimation is a fundamental task in computer vision with diverse
applications. Recent advancements in deep learning have led to powerful depth
foundation models (DFMs), yet their evaluation remains challenging due to
inconsistencies in existing protocols. Traditional benchmarks rely on
alignment-based metrics that introduce biases, favor certain depth
representations, and complicate fair comparisons. In this work, we propose
BenchDepth, a new benchmark that evaluates DFMs through five carefully selected
downstream proxy tasks: depth completion, stereo matching, monocular
feed-forward 3D scene reconstruction, SLAM, and vision-language spatial
understanding. Unlike conventional evaluation protocols, our approach assesses
DFMs based on their practical utility in real-world applications, bypassing
problematic alignment procedures. We benchmark eight state-of-the-art DFMs and
provide an in-depth analysis of key findings and observations. We hope our work
sparks further discussion in the community on best practices for depth model
evaluation and paves the way for future research and advancements in depth
estimation.

</details>


### [184] [ExDD: Explicit Dual Distribution Learning for Surface Defect Detection via Diffusion Synthesis](https://arxiv.org/abs/2507.15335)
*Muhammad Aqeel,Federico Leonardi,Francesco Setti*

Main category: cs.CV

TL;DR: ExDD is a novel framework for industrial defect detection that models dual feature distributions, uses synthetic defect generation, and achieves high performance metrics.


<details>
  <summary>Details</summary>
Motivation: Traditional one-class anomaly detection struggles with uniform outlier assumptions and data scarcity in real-world manufacturing.

Method: ExDD uses parallel memory banks for normality and anomaly patterns, latent diffusion models for synthetic defect generation, and a neighborhood-aware scoring mechanism.

Result: Achieves 94.2% I-AUROC and 97.7% P-AUROC on KSDD2, with optimal performance at 100 synthetic samples.

Conclusion: ExDD effectively addresses limitations of traditional methods, offering a robust solution for industrial defect detection.

Abstract: Industrial defect detection systems face critical limitations when confined
to one-class anomaly detection paradigms, which assume uniform outlier
distributions and struggle with data scarcity in realworld manufacturing
environments. We present ExDD (Explicit Dual Distribution), a novel framework
that transcends these limitations by explicitly modeling dual feature
distributions. Our approach leverages parallel memory banks that capture the
distinct statistical properties of both normality and anomalous patterns,
addressing the fundamental flaw of uniform outlier assumptions. To overcome
data scarcity, we employ latent diffusion models with domain-specific textual
conditioning, generating in-distribution synthetic defects that preserve
industrial context. Our neighborhood-aware ratio scoring mechanism elegantly
fuses complementary distance metrics, amplifying signals in regions exhibiting
both deviation from normality and similarity to known defect patterns.
Experimental validation on KSDD2 demonstrates superior performance (94.2%
I-AUROC, 97.7% P-AUROC), with optimal augmentation at 100 synthetic samples.

</details>


### [185] [RoadFusion: Latent Diffusion Model for Pavement Defect Detection](https://arxiv.org/abs/2507.15346)
*Muhammad Aqeel,Kidus Dagnaw Bellete,Francesco Setti*

Main category: cs.CV

TL;DR: RoadFusion uses synthetic anomaly generation and dual-path feature adaptation to improve pavement defect detection, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Address challenges like limited annotated data, domain shift, and variability in defect appearances for pavement defect detection.

Method: Uses a latent diffusion model for synthetic defect generation and dual-path feature adaptors for robust representation learning.

Result: Achieves strong performance on six benchmark datasets for classification and localization tasks.

Conclusion: RoadFusion sets new state-of-the-art metrics, proving effective for real-world road inspection.

Abstract: Pavement defect detection faces critical challenges including limited
annotated data, domain shift between training and deployment environments, and
high variability in defect appearances across different road conditions. We
propose RoadFusion, a framework that addresses these limitations through
synthetic anomaly generation with dual-path feature adaptation. A latent
diffusion model synthesizes diverse, realistic defects using text prompts and
spatial masks, enabling effective training under data scarcity. Two separate
feature adaptors specialize representations for normal and anomalous inputs,
improving robustness to domain shift and defect variability. A lightweight
discriminator learns to distinguish fine-grained defect patterns at the patch
level. Evaluated on six benchmark datasets, RoadFusion achieves consistently
strong performance across both classification and localization tasks, setting
new state-of-the-art in multiple metrics relevant to real-world road
inspection.

</details>


### [186] [DAViD: Data-efficient and Accurate Vision Models from Synthetic Data](https://arxiv.org/abs/2507.15365)
*Fatemeh Saleh,Sadegh Aliakbarian,Charlie Hewitt,Lohit Petikam,Xiao-Xian,Antonio Criminisi,Thomas J. Cashman,Tadas Baltrušaitis*

Main category: cs.CV

TL;DR: Training models on small, high-fidelity synthetic datasets achieves comparable accuracy to large-scale models while reducing costs and addressing fairness.


<details>
  <summary>Details</summary>
Motivation: To overcome the high computational and data requirements of large models in human-centric computer vision by leveraging synthetic datasets.

Method: Procedural data synthesis to create high-fidelity synthetic datasets with perfect labels and controlled diversity, used to train models for dense prediction tasks.

Result: Models trained on synthetic data match the accuracy of large-scale models while being more efficient and cost-effective.

Conclusion: Synthetic datasets offer a viable alternative to large-scale real datasets, providing accuracy, efficiency, and fairness benefits.

Abstract: The state of the art in human-centric computer vision achieves high accuracy
and robustness across a diverse range of tasks. The most effective models in
this domain have billions of parameters, thus requiring extremely large
datasets, expensive training regimes, and compute-intensive inference. In this
paper, we demonstrate that it is possible to train models on much smaller but
high-fidelity synthetic datasets, with no loss in accuracy and higher
efficiency. Using synthetic training data provides us with excellent levels of
detail and perfect labels, while providing strong guarantees for data
provenance, usage rights, and user consent. Procedural data synthesis also
provides us with explicit control on data diversity, that we can use to address
unfairness in the models we train. Extensive quantitative assessment on real
input images demonstrates accuracy of our models on three dense prediction
tasks: depth estimation, surface normal estimation, and soft foreground
segmentation. Our models require only a fraction of the cost of training and
inference when compared with foundational models of similar accuracy. Our
human-centric synthetic dataset and trained models are available at
https://aka.ms/DAViD.

</details>


### [187] [Rethinking Occlusion in FER: A Semantic-Aware Perspective and Go Beyond](https://arxiv.org/abs/2507.15401)
*Huiyu Zhai,Xingxing Yang,Yalan Ye,Chenyang Li,Bin Fan,Changze Li*

Main category: cs.CV

TL;DR: ORSANet improves facial expression recognition (FER) under occlusion by using multi-modal semantic guidance, a multi-scale fusion module, and a dynamic loss function, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing FER models struggle with occlusion and dataset biases, leading to inaccurate classifications.

Method: ORSANet uses semantic segmentation maps and facial landmarks as priors, a Multi-scale Cross-interaction Module (MCM) for fusion, and a Dynamic Adversarial Repulsion Enhancement Loss (DARELoss) for better class distinction.

Result: ORSANet achieves state-of-the-art performance on public benchmarks and the new Occlu-FER dataset.

Conclusion: ORSANet effectively addresses occlusion and bias challenges in FER, demonstrating superior performance.

Abstract: Facial expression recognition (FER) is a challenging task due to pervasive
occlusion and dataset biases. Especially when facial information is partially
occluded, existing FER models struggle to extract effective facial features,
leading to inaccurate classifications. In response, we present ORSANet, which
introduces the following three key contributions: First, we introduce auxiliary
multi-modal semantic guidance to disambiguate facial occlusion and learn
high-level semantic knowledge, which is two-fold: 1) we introduce semantic
segmentation maps as dense semantics prior to generate semantics-enhanced
facial representations; 2) we introduce facial landmarks as sparse geometric
prior to mitigate intrinsic noises in FER, such as identity and gender biases.
Second, to facilitate the effective incorporation of these two multi-modal
priors, we customize a Multi-scale Cross-interaction Module (MCM) to adaptively
fuse the landmark feature and semantics-enhanced representations within
different scales. Third, we design a Dynamic Adversarial Repulsion Enhancement
Loss (DARELoss) that dynamically adjusts the margins of ambiguous classes,
further enhancing the model's ability to distinguish similar expressions. We
further construct the first occlusion-oriented FER dataset to facilitate
specialized robustness analysis on various real-world occlusion conditions,
dubbed Occlu-FER. Extensive experiments on both public benchmarks and Occlu-FER
demonstrate that our proposed ORSANet achieves SOTA recognition performance.
Code is publicly available at https://github.com/Wenyuzhy/ORSANet-master.

</details>


### [188] [SurgX: Neuron-Concept Association for Explainable Surgical Phase Recognition](https://arxiv.org/abs/2507.15418)
*Ka Young Kim,Hyeon Bae Kim,Seong Tae Kim*

Main category: cs.CV

TL;DR: SurgX is a framework to improve interpretability in surgical phase recognition models by linking neurons to relevant concepts, validated on two models.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for surgical phase recognition lack interpretability, which reduces trust and complicates debugging.

Method: SurgX associates neurons with concepts, selects example sequences, constructs a concept set, and identifies key neurons for predictions.

Result: Experiments on two models validate SurgX's effectiveness in explaining surgical phase recognition.

Conclusion: SurgX enhances interpretability, aiding trust and debugging in surgical phase recognition models.

Abstract: Surgical phase recognition plays a crucial role in surgical workflow
analysis, enabling various applications such as surgical monitoring, skill
assessment, and workflow optimization. Despite significant advancements in deep
learning-based surgical phase recognition, these models remain inherently
opaque, making it difficult to understand how they make decisions. This lack of
interpretability hinders trust and makes it challenging to debug the model. To
address this challenge, we propose SurgX, a novel concept-based explanation
framework that enhances the interpretability of surgical phase recognition
models by associating neurons with relevant concepts. In this paper, we
introduce the process of selecting representative example sequences for
neurons, constructing a concept set tailored to the surgical video dataset,
associating neurons with concepts and identifying neurons crucial for
predictions. Through extensive experiments on two surgical phase recognition
models, we validate our method and analyze the explanation for prediction. This
highlights the potential of our method in explaining surgical phase
recognition. The code is available at https://github.com/ailab-kyunghee/SurgX

</details>


### [189] [EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent](https://arxiv.org/abs/2507.15428)
*Jiaao Li,Kaiyuan Li,Chen Gao,Yong Li,Xinlei Chen*

Main category: cs.CV

TL;DR: EgoPrune is a training-free token pruning method for egomotion video reasoning, improving efficiency by leveraging spatiotemporal continuity and motion constraints.


<details>
  <summary>Details</summary>
Motivation: Egomotion videos are crucial for embodied AI agents, but existing token pruning methods are inefficient for these videos due to their unique characteristics.

Method: EgoPrune includes a keyframe selector, Perspective-Aware Redundancy Filtering (PARF), and an MMR-based token selector to prune redundant tokens efficiently.

Result: EgoPrune outperforms prior methods, reducing FLOPs, memory usage, and latency, and is validated on an edge device.

Conclusion: EgoPrune is effective for on-device egomotion video reasoning, offering real-world efficiency.

Abstract: Egomotion videos are first-person recordings where the view changes
continuously due to the agent's movement. As they serve as the primary visual
input for embodied AI agents, making egomotion video reasoning more efficient
is therefore essential for real-world deployment. Recent advances in
vision-language models have enabled strong multimodal reasoning capabilities,
but their computational cost remains prohibitive for long, redundant video
inputs. Existing token pruning methods, typically designed for third-person
videos, fail to leverage the spatiotemporal continuity and motion constraints
inherent in egomotion settings. To address this, we propose EgoPrune, a
training-free token pruning method tailored for egomotion video reasoning.
EgoPrune comprises three components: a keyframe selector adapted from EmbodiedR
for temporally efficient sampling; Perspective-Aware Redundancy Filtering
(PARF), which aligns visual tokens using perspective transformations and
removes redundant tokens; and a Maximal Marginal Relevance (MMR)-based token
selector that jointly considers visual-text relevance and intra-frame
diversity. Experiments on two egomotion video benchmarks show that EgoPrune
consistently outperforms prior training-free methods across various pruning
ratios while significantly reducing FLOPs, memory usage, and latency. Moreover,
we deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB
edge device, demonstrating its real-world efficiency and suitability for
on-device egomotion video reasoning.

</details>


### [190] [One Last Attention for Your Vision-Language Model](https://arxiv.org/abs/2507.15480)
*Liang Chen,Ghazi Shazan Ahmad,Tianjun Yao,Lingqiao Liu,Zhiqiang Shen*

Main category: cs.CV

TL;DR: RAda is a method for fine-tuning VLMs by dynamically adjusting fused representations to improve cross-modal interactions without costly modifications.


<details>
  <summary>Details</summary>
Motivation: Existing methods neglect the role of fused representations in decision-making, limiting downstream performance.

Method: RAda uses a learned mask from a lightweight attention layer to calibrate the rational matrix, targeting cross-modal interactions.

Result: RAda improves baselines with minimal code and performs comparably to current methods in various settings.

Conclusion: RAda is a versatile and effective fine-tuning technique for VLMs.

Abstract: Pretrained vision-language models (VLMs), such as CLIP, achieve remarkable
zero-shot performance, yet their downstream potential hinges on effective
fine-tuning. Most adaptation methods typically focus on refining representation
from separate modalities (text or vision) but neglect the critical role of
their fused representations in the decision-making process, \emph{\ie} rational
matrix that drives the final prediction. To bridge the gap, we propose a simple
yet effective \textbf{R}ational \textbf{Ada}ptaion ({RAda}) to explicitly
exploit the final fused representation during fine-tuning. RAda employs a
learned mask, obtained from a lightweight attention layer attached at the end
of a VLM, to dynamically calibrate the contribution of each element in the
rational matrix, enabling targeted adjustments to the final cross-modal
interactions without incurring costly modifications to intermediate features.
Experiments in different settings (i.e., updating, or freezing pretrained
encoders in adaptation, and test-time training that can only access the
unlabeled test data) show that RAda serves as a versatile fine-tuning
technique, improving the baseline with minimal code and performing comparably
against current arts in most settings. Code is available at
\href{https://github.com/khufia/RAda/tree/main}{github.com/khufia/RAda}.

</details>


### [191] [An aerial color image anomaly dataset for search missions in complex forested terrain](https://arxiv.org/abs/2507.15492)
*Rakesh John Amala Arokia Nathan,Matthias Gessner,Nurullah Özkan,Marius Bock,Mohamed Youssef,Maximilian Mews,Björn Piltz,Ralf Berger,Oliver Bimber*

Main category: cs.CV

TL;DR: A crowd-search initiative created a dataset of hard-to-detect anomalies in dense forests, aiding future anomaly detection improvements for manhunts and rescues.


<details>
  <summary>Details</summary>
Motivation: Authorities struggled to locate a suspect in dense forest despite aerial imagery, revealing the need for better anomaly detection methods.

Method: High-resolution aerial imagery was analyzed via crowd-search due to ineffective automation, creating a labeled dataset of occluded anomalies.

Result: Existing anomaly detection methods performed poorly, emphasizing the need for context-aware approaches.

Conclusion: The openly accessible dataset and interactive web interface aim to advance anomaly detection in complex environments.

Abstract: After a family murder in rural Germany, authorities failed to locate the
suspect in a vast forest despite a massive search. To aid the search, a
research aircraft captured high-resolution aerial imagery. Due to dense
vegetation obscuring small clues, automated analysis was ineffective, prompting
a crowd-search initiative. This effort produced a unique dataset of labeled,
hard-to-detect anomalies under occluded, real-world conditions. It can serve as
a benchmark for improving anomaly detection approaches in complex forest
environments, supporting manhunts and rescue operations. Initial benchmark
tests showed existing methods performed poorly, highlighting the need for
context-aware approaches. The dataset is openly accessible for offline
processing. An additional interactive web interface supports online viewing and
dynamic growth by allowing users to annotate and submit new findings.

</details>


### [192] [Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images](https://arxiv.org/abs/2507.15496)
*JunYing Huang,Ao Xu,DongSun Yong,KeRen Li,YuanFeng Wang,Qi Qin*

Main category: cs.CV

TL;DR: A LiDAR-Visual odometry framework combining LiDAR and images for accurate pose estimation, using depth completion and attention mechanisms, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Odometry is crucial for autonomous systems, but existing methods may lack accuracy or robustness in dynamic environments. Integrating LiDAR and visual data can improve performance.

Method: The framework uses dense-depth maps from LiDAR and images, multi-scale feature extraction with attention, and hierarchical pose refinement for robust motion estimation.

Result: The method achieves similar or superior accuracy and robustness on the KITTI odometry benchmark compared to state-of-the-art techniques.

Conclusion: The proposed LiDAR-Visual odometry framework is effective for accurate and robust pose estimation in autonomous systems.

Abstract: Odometry is a critical task for autonomous systems for self-localization and
navigation. We propose a novel LiDAR-Visual odometry framework that integrates
LiDAR point clouds and images for accurate and robust pose estimation. Our
method utilizes a dense-depth map estimated from point clouds and images
through depth completion, and incorporates a multi-scale feature extraction
network with attention mechanisms, enabling adaptive depth-aware
representations. Furthermore, we leverage dense depth information to refine
flow estimation and mitigate errors in occlusion-prone regions. Our
hierarchical pose refinement module optimizes motion estimation progressively,
ensuring robust predictions against dynamic environments and scale ambiguities.
Comprehensive experiments on the KITTI odometry benchmark demonstrate that our
approach achieves similar or superior accuracy and robustness compared to
state-of-the-art visual and LiDAR odometry methods.

</details>


### [193] [Quantifying and Narrowing the Unknown: Interactive Text-to-Video Retrieval via Uncertainty Minimization](https://arxiv.org/abs/2507.15504)
*Bingqing Zhang,Zhuo Cao,Heming Du,Yang Li,Xue Li,Jiajun Liu,Sen Wang*

Main category: cs.CV

TL;DR: UMIVR is a framework for interactive text-to-video retrieval that quantifies uncertainties (text ambiguity, mapping uncertainty, frame uncertainty) to refine user queries, improving retrieval accuracy.


<details>
  <summary>Details</summary>
Motivation: Current interactive TVR systems lack explicit uncertainty quantification, limiting effectiveness. UMIVR addresses this gap by measuring uncertainties to guide query refinement.

Method: UMIVR uses training-free metrics: Text Ambiguity Score (TAS), Mapping Uncertainty Score (MUS), and Temporal Quality-based Frame Sampler (TQFS) to quantify uncertainties and generate clarifying questions.

Result: UMIVR achieves a Recall@1 of 69.2% after 10 rounds on MSR-VTT-1k, demonstrating significant improvement in retrieval accuracy.

Conclusion: UMIVR establishes a principled, uncertainty-minimizing approach for interactive TVR, validated by superior performance on benchmarks.

Abstract: Despite recent advances, Text-to-video retrieval (TVR) is still hindered by
multiple inherent uncertainties, such as ambiguous textual queries, indistinct
text-video mappings, and low-quality video frames. Although interactive systems
have emerged to address these challenges by refining user intent through
clarifying questions, current methods typically rely on heuristic or ad-hoc
strategies without explicitly quantifying these uncertainties, limiting their
effectiveness. Motivated by this gap, we propose UMIVR, an
Uncertainty-Minimizing Interactive Text-to-Video Retrieval framework that
explicitly quantifies three critical uncertainties-text ambiguity, mapping
uncertainty, and frame uncertainty-via principled, training-free metrics:
semantic entropy-based Text Ambiguity Score (TAS), Jensen-Shannon
divergence-based Mapping Uncertainty Score (MUS), and a Temporal Quality-based
Frame Sampler (TQFS). By adaptively generating targeted clarifying questions
guided by these uncertainty measures, UMIVR iteratively refines user queries,
significantly reducing retrieval ambiguity. Extensive experiments on multiple
benchmarks validate UMIVR's effectiveness, achieving notable gains in Recall@1
(69.2\% after 10 interactive rounds) on the MSR-VTT-1k dataset, thereby
establishing an uncertainty-minimizing foundation for interactive TVR.

</details>


### [194] [SAIGFormer: A Spatially-Adaptive Illumination-Guided Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.15520)
*Hanting Li,Fei Zhou,Xin Sun,Yang Hua,Jungong Han,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: SAIGFormer is a Transformer-based framework for low-light enhancement, addressing non-uniform lighting issues with spatially-adaptive illumination modeling and guided attention.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with non-uniform lighting scenarios like backlit and shadow, leading to over-exposure or inadequate brightness restoration.

Method: Proposes a dynamic integral image representation (SAI²E) and an Illumination-Guided Multi-head Self-Attention (IG-MSA) mechanism for accurate illumination restoration.

Result: Outperforms state-of-the-art methods on five datasets and a cross-domain benchmark, excelling in non-uniform illumination enhancement and generalization.

Conclusion: SAIGFormer effectively addresses non-uniform lighting challenges, offering superior performance and generalization in low-light enhancement.

Abstract: Recent Transformer-based low-light enhancement methods have made promising
progress in recovering global illumination. However, they still struggle with
non-uniform lighting scenarios, such as backlit and shadow, appearing as
over-exposure or inadequate brightness restoration. To address this challenge,
we present a Spatially-Adaptive Illumination-Guided Transformer (SAIGFormer)
framework that enables accurate illumination restoration. Specifically, we
propose a dynamic integral image representation to model the spatially-varying
illumination, and further construct a novel Spatially-Adaptive Integral
Illumination Estimator ($\text{SAI}^2\text{E}$). Moreover, we introduce an
Illumination-Guided Multi-head Self-Attention (IG-MSA) mechanism, which
leverages the illumination to calibrate the lightness-relevant features toward
visual-pleased illumination enhancement. Extensive experiments on five standard
low-light datasets and a cross-domain benchmark (LOL-Blur) demonstrate that our
SAIGFormer significantly outperforms state-of-the-art methods in both
quantitative and qualitative metrics. In particular, our method achieves
superior performance in non-uniform illumination enhancement while exhibiting
strong generalization capabilities across multiple datasets. Code is available
at https://github.com/LHTcode/SAIGFormer.git.

</details>


### [195] [Procedure Learning via Regularized Gromov-Wasserstein Optimal Transport](https://arxiv.org/abs/2507.15540)
*Syed Ahmed Mahmood,Ali Shah Ali,Umer Ahmed,Fawad Javed Fateh,M. Zeeshan Zia,Quoc-Huy Tran*

Main category: cs.CV

TL;DR: A self-supervised framework for learning procedures from unlabeled videos, using fused Gromov-Wasserstein optimal transport and contrastive regularization to avoid degenerate solutions.


<details>
  <summary>Details</summary>
Motivation: Address challenges like order variations, redundant frames, and repeated actions in procedure learning from unlabeled videos.

Method: Proposes a framework combining fused Gromov-Wasserstein optimal transport for temporal alignment and contrastive regularization to prevent embedding collapse.

Result: Outperforms previous methods on benchmarks like EgoProceL, ProceL, and CrossTask.

Conclusion: The approach effectively learns key steps and their order, overcoming limitations of prior methods.

Abstract: We study the problem of self-supervised procedure learning, which discovers
key steps and establishes their order from a set of unlabeled procedural
videos. Previous procedure learning methods typically learn frame-to-frame
correspondences between videos before determining key steps and their order.
However, their performance often suffers from order variations,
background/redundant frames, and repeated actions. To overcome these
challenges, we propose a self-supervised procedure learning framework, which
utilizes a fused Gromov-Wasserstein optimal transport formulation with a
structural prior for computing frame-to-frame mapping between videos. However,
optimizing exclusively for the above temporal alignment term may lead to
degenerate solutions, where all frames are mapped to a small cluster in the
embedding space and hence every video is associated with only one key step. To
address that limitation, we further integrate a contrastive regularization
term, which maps different frames to different points in the embedding space,
avoiding the collapse to trivial solutions. Finally, we conduct extensive
experiments on large-scale egocentric (i.e., EgoProceL) and third-person (i.e.,
ProceL and CrossTask) benchmarks to demonstrate superior performance by our
approach against previous methods, including OPEL which relies on a traditional
Kantorovich optimal transport formulation with an optimality prior.

</details>


### [196] [Towards Holistic Surgical Scene Graph](https://arxiv.org/abs/2507.15541)
*Jongmin Shin,Enki Cho,Ka Yong Kim,Jung Yong Kim,Seong Tae Kim,Namkee Oh*

Main category: cs.CV

TL;DR: The paper introduces Endoscapes-SG201 dataset and SSG-Com, a graph-based method, to enhance surgical scene understanding by modeling tool-action-target combinations and hand identity.


<details>
  <summary>Details</summary>
Motivation: Current graph-based representations of surgical scenes lack exploration of tool-action-target combinations and hand identity, which are crucial for comprehensive scene understanding.

Method: Proposes Endoscapes-SG201 dataset with annotations for tool-action-target and hand identity, and introduces SSG-Com, a graph-based method to model these elements.

Result: Experiments show improved performance in downstream tasks like critical view of safety assessment and action triplet recognition.

Conclusion: Incorporating tool-action-target and hand identity in graph representations significantly enhances surgical scene understanding.

Abstract: Surgical scene understanding is crucial for computer-assisted intervention
systems, requiring visual comprehension of surgical scenes that involves
diverse elements such as surgical tools, anatomical structures, and their
interactions. To effectively represent the complex information in surgical
scenes, graph-based approaches have been explored to structurally model
surgical entities and their relationships. Previous surgical scene graph
studies have demonstrated the feasibility of representing surgical scenes using
graphs. However, certain aspects of surgical scenes-such as diverse
combinations of tool-action-target and the identity of the hand operating the
tool-remain underexplored in graph-based representations, despite their
importance. To incorporate these aspects into graph representations, we propose
Endoscapes-SG201 dataset, which includes annotations for tool-action-target
combinations and hand identity. We also introduce SSG-Com, a graph-based method
designed to learn and represent these critical elements. Through experiments on
downstream tasks such as critical view of safety assessment and action triplet
recognition, we demonstrated the importance of integrating these essential
scene graph components, highlighting their significant contribution to surgical
scene understanding. The code and dataset are available at
https://github.com/ailab-kyunghee/SSG-Com

</details>


### [197] [HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation](https://arxiv.org/abs/2507.15542)
*Qinqian Lei,Bo Wang,Robby T. Tan*

Main category: cs.CV

TL;DR: HOLa improves zero-shot HOI detection by decomposing VLM text features and adapting weights, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generalizing to unseen actions in HOI detection by leveraging VLMs and enhancing action distinction.

Method: Decomposes VLM text features via low-rank factorization, adapts weights for each HOI class, and uses human-object tokens and LLM-derived action regularization.

Result: Achieves an unseen-class mAP of 27.91 on HICO-DET, setting a new state-of-the-art.

Conclusion: HOLa effectively enhances generalization and action distinction in zero-shot HOI detection.

Abstract: Zero-shot human-object interaction (HOI) detection remains a challenging
task, particularly in generalizing to unseen actions. Existing methods address
this challenge by tapping Vision-Language Models (VLMs) to access knowledge
beyond the training data. However, they either struggle to distinguish actions
involving the same object or demonstrate limited generalization to unseen
classes. In this paper, we introduce HOLa (Zero-Shot HOI Detection with
Low-Rank Decomposed VLM Feature Adaptation), a novel approach that both
enhances generalization to unseen classes and improves action distinction. In
training, HOLa decomposes VLM text features for given HOI classes via low-rank
factorization, producing class-shared basis features and adaptable weights.
These features and weights form a compact HOI representation that preserves
shared information across classes, enhancing generalization to unseen classes.
Subsequently, we refine action distinction by adapting weights for each HOI
class and introducing human-object tokens to enrich visual interaction
representations. To further distinguish unseen actions, we guide the weight
adaptation with LLM-derived action regularization. Experimental results show
that our method sets a new state-of-the-art across zero-shot HOI settings on
HICO-DET, achieving an unseen-class mAP of 27.91 in the unseen-verb setting.
Our code is available at https://github.com/ChelsieLei/HOLa.

</details>


### [198] [DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding](https://arxiv.org/abs/2507.15569)
*Xiaoyi Bao,Chenwei Xie,Hao Tang,Tingyu Weng,Xiaofeng Wang,Yun Zheng,Xingang Wang*

Main category: cs.CV

TL;DR: The paper introduces Dynamic-Image (DynImg), a novel video representation method using non-key frames as temporal prompts to improve spatial feature extraction for fast-moving objects, enhancing spatio-temporal interaction in video understanding.


<details>
  <summary>Details</summary>
Motivation: Traditional methods treat spatial and temporal information separately, often underemphasizing temporally important regions due to motion blur, hindering accurate video understanding.

Method: DynImg employs non-key frames as temporal prompts to highlight fast-moving objects and uses 4D Rotary Position Embedding to maintain spatio-temporal order.

Result: DynImg outperforms state-of-the-art methods by ~2% on multiple video understanding benchmarks.

Conclusion: DynImg effectively enhances video comprehension by integrating temporal prompts, improving spatio-temporal interaction.

Abstract: In recent years, the introduction of Multi-modal Large Language Models
(MLLMs) into video understanding tasks has become increasingly prevalent.
However, how to effectively integrate temporal information remains a critical
research focus. Traditional approaches treat spatial and temporal information
separately. Due to issues like motion blur, it is challenging to accurately
represent the spatial information of rapidly moving objects. This can lead to
temporally important regions being underemphasized during spatial feature
extraction, which in turn hinders accurate spatio-temporal interaction and
video understanding. To address this limitation, we propose an innovative video
representation method called Dynamic-Image (DynImg). Specifically, we introduce
a set of non-key frames as temporal prompts to highlight the spatial areas
containing fast-moving objects. During the process of visual feature
extraction, these prompts guide the model to pay additional attention to the
fine-grained spatial features corresponding to these regions. Moreover, to
maintain the correct sequence for DynImg, we employ a corresponding 4D video
Rotary Position Embedding. This retains both the temporal and spatial adjacency
of DynImg, helping MLLM understand the spatio-temporal order within this
combined format. Experimental evaluations reveal that DynImg surpasses the
state-of-the-art methods by approximately 2% across multiple video
understanding benchmarks, proving the effectiveness of our temporal prompts in
enhancing video comprehension.

</details>


### [199] [GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation](https://arxiv.org/abs/2507.15577)
*Hugo Carlesso,Maria Eliza Patulea,Moncef Garouani,Radu Tudor Ionescu,Josiane Mothe*

Main category: cs.CV

TL;DR: GeMix replaces naive pixel-wise mixup with a learned, label-aware interpolation using GANs, improving image realism and classification performance, especially in medical applications like COVID-19 detection.


<details>
  <summary>Details</summary>
Motivation: Naive pixel-wise mixup produces unrealistic images, hindering learning in high-stakes medical applications. GeMix aims to enhance realism and semantic fidelity.

Method: A two-stage framework: (1) Train a StyleGAN2-ADA generator on the target dataset. (2) Sample and blend label vectors from Dirichlet priors, then condition the generator to synthesize coherent images.

Result: GeMix outperforms traditional mixup, increasing macro-F1 scores and reducing false negatives in COVID-19 detection across multiple backbones.

Conclusion: GeMix is a drop-in replacement for mixup, offering better regularization and semantic fidelity without disrupting training pipelines.

Abstract: Mixup has become a popular augmentation strategy for image classification,
yet its naive pixel-wise interpolation often produces unrealistic images that
can hinder learning, particularly in high-stakes medical applications. We
propose GeMix, a two-stage framework that replaces heuristic blending with a
learned, label-aware interpolation powered by class-conditional GANs. First, a
StyleGAN2-ADA generator is trained on the target dataset. During augmentation,
we sample two label vectors from Dirichlet priors biased toward different
classes and blend them via a Beta-distributed coefficient. Then, we condition
the generator on this soft label to synthesize visually coherent images that
lie along a continuous class manifold. We benchmark GeMix on the large-scale
COVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101,
EfficientNet-B0). When combined with real data, our method increases macro-F1
over traditional mixup for all backbones, reducing the false negative rate for
COVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup,
delivering stronger regularization and greater semantic fidelity, without
disrupting existing training pipelines. We publicly release our code at
https://github.com/hugocarlesso/GeMix to foster reproducibility and further
research.

</details>


### [200] [Compress-Align-Detect: onboard change detection from unregistered images](https://arxiv.org/abs/2507.15578)
*Gabriele Inzerillo,Diego Valsesia,Aniello Fiengo,Enrico Magli*

Main category: cs.CV

TL;DR: Proposes an onboard satellite framework for real-time change detection using a deep neural network with three submodules: compression, co-registration, and change detection.


<details>
  <summary>Details</summary>
Motivation: Overcome delays in traditional satellite change detection by shifting the workflow onboard to enable real-time applications.

Method: Uses a deep neural network with three interlinked submodules: image compression, lightweight co-registration, and a temporally-invariant change detection model.

Result: Achieves compelling F1 scores and sustains 0.7 Mpixel/s throughput on low-power hardware.

Conclusion: The framework successfully addresses onboard processing constraints and outperforms state-of-the-art methods.

Abstract: Change detection from satellite images typically incurs a delay ranging from
several hours up to days because of latency in downlinking the acquired images
and generating orthorectified image products at the ground stations; this may
preclude real- or near real-time applications. To overcome this limitation, we
propose shifting the entire change detection workflow onboard satellites. This
requires to simultaneously solve challenges in data storage, image registration
and change detection with a strict complexity constraint. In this paper, we
present a novel and efficient framework for onboard change detection that
addresses the aforementioned challenges in an end-to-end fashion with a deep
neural network composed of three interlinked submodules: (1) image compression,
tailored to minimize onboard data storage resources; (2) lightweight
co-registration of non-orthorectified multi-temporal image pairs; and (3) a
novel temporally-invariant and computationally efficient change detection
model. This is the first approach in the literature combining all these tasks
in a single end-to-end framework with the constraints dictated by onboard
processing. Experimental results compare each submodule with the current
state-of-the-art, and evaluate the performance of the overall integrated system
in realistic setting on low-power hardware. Compelling change detection results
are obtained in terms of F1 score as a function of compression rate, sustaining
a throughput of 0.7 Mpixel/s on a 15W accelerator.

</details>


### [201] [SegDT: A Diffusion Transformer-Based Segmentation Model for Medical Imaging](https://arxiv.org/abs/2507.15595)
*Salah Eddine Bekhouche,Gaby Maroun,Fadi Dornaika,Abdenour Hadid*

Main category: cs.CV

TL;DR: SegDT, a diffusion transformer-based model for skin lesion segmentation, achieves state-of-the-art results on low-cost hardware with fast inference speeds.


<details>
  <summary>Details</summary>
Motivation: Improving medical image segmentation for skin lesions to aid in disease diagnosis and treatment planning.

Method: SegDT combines diffusion transformer (DiT) with Rectified Flow for efficient and high-quality segmentation.

Result: Achieves state-of-the-art performance on three datasets with fast inference speeds.

Conclusion: SegDT advances medical image analysis, offering a practical tool for healthcare professionals.

Abstract: Medical image segmentation is crucial for many healthcare tasks, including
disease diagnosis and treatment planning. One key area is the segmentation of
skin lesions, which is vital for diagnosing skin cancer and monitoring
patients. In this context, this paper introduces SegDT, a new segmentation
model based on diffusion transformer (DiT). SegDT is designed to work on
low-cost hardware and incorporates Rectified Flow, which improves the
generation quality at reduced inference steps and maintains the flexibility of
standard diffusion models. Our method is evaluated on three benchmarking
datasets and compared against several existing works, achieving
state-of-the-art results while maintaining fast inference speeds. This makes
the proposed model appealing for real-world medical applications. This work
advances the performance and capabilities of deep learning models in medical
image analysis, enabling faster, more accurate diagnostic tools for healthcare
professionals. The code is made publicly available at
\href{https://github.com/Bekhouche/SegDT}{GitHub}.

</details>


### [202] [Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos](https://arxiv.org/abs/2507.15597)
*Hao Luo,Yicheng Feng,Wanpeng Zhang,Sipeng Zheng,Ye Wang,Haoqi Yuan,Jiazheng Liu,Chaoyi Xu,Qin Jin,Zongqing Lu*

Main category: cs.CV

TL;DR: Being-H0 is a Vision-Language-Action model trained on human videos to improve dexterity and generalization in robotic tasks, using physical instruction tuning and part-level motion tokenization.


<details>
  <summary>Details</summary>
Motivation: Existing VLAs struggle with complex manipulation tasks due to synthetic data limitations or lack of diverse demonstrations. Being-H0 leverages human hand dexterity and scalable web data to address this.

Method: The approach combines VLA pretraining from human videos, physical space alignment, and post-training adaptation. It introduces part-level motion tokenization for precise hand trajectory modeling and a data curation pipeline integrating diverse sources.

Result: Being-H0 excels in hand motion generation and instruction following, scaling well with model and data sizes, and shows gains in real-world robotic manipulation.

Conclusion: Physical instruction tuning and leveraging human videos effectively enhance VLA performance in dexterous tasks, bridging the sim-to-real gap.

Abstract: We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained
on large-scale human videos. Existing VLAs struggle with complex manipulation
tasks requiring high dexterity and generalize poorly to novel scenarios and
tasks, primarily due to their reliance on synthetic data with significant
sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To
address this data bottleneck, we propose leveraging human hands as a foundation
manipulator, capitalizing on the rich dexterity and scalability present in web
data. Our approach centers on physical instruction tuning, a novel training
paradigm that combines large-scale VLA pretraining from human videos, physical
space alignment for 3D reasoning, and post-training adaptation for robotic
tasks. Additionally, we introduce a part-level motion tokenization method which
achieves millimeter-level reconstruction accuracy to model precise hand
trajectories for action learning. To support our proposed paradigm, we further
develop a comprehensive data curation pipeline that integrates heterogeneous
sources -- including motion capture, VR, and RGB-only videos -- into a
large-scale dataset with millions of motion-based instructional instances. We
empirically show the excellence of Being-H0 in hand motion generation and
instruction following, and it also scales well with model and data sizes.
Importantly, we observe the expected gains of Being-H0 in real-world robotic
manipulation as physical instruction tuning is applied. More details are
available at https://beingbeyond.github.io/Being-H0.

</details>


### [203] [SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting](https://arxiv.org/abs/2507.15602)
*Zihui Gao,Jia-Wang Bian,Guosheng Lin,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: A hybrid method combining SDF and 3DGS improves surface reconstruction and novel view rendering by leveraging coarse geometry and fine details.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of SDF (lack of fine details) and 3DGS (lack of global coherence) in sparse-view image tasks.

Method: Combines SDF for coarse geometry and 3DGS for detail refinement, using rendered images from 3DGS to enhance SDF accuracy.

Result: Outperforms state-of-the-art methods on DTU and MobileBrick datasets.

Conclusion: The hybrid approach effectively balances geometry coherence and detail accuracy, advancing sparse-view reconstruction and rendering.

Abstract: Surface reconstruction and novel view rendering from sparse-view images are
challenging. Signed Distance Function (SDF)-based methods struggle with fine
details, while 3D Gaussian Splatting (3DGS)-based approaches lack global
geometry coherence. We propose a novel hybrid method that combines the
strengths of both approaches: SDF captures coarse geometry to enhance
3DGS-based rendering, while newly rendered images from 3DGS refine the details
of SDF for accurate surface reconstruction. As a result, our method surpasses
state-of-the-art approaches in surface reconstruction and novel view synthesis
on the DTU and MobileBrick datasets. Code will be released at
https://github.com/Gaozihui/SurfaceSplat.

</details>


### [204] [CylinderPlane: Nested Cylinder Representation for 3D-aware Image Generation](https://arxiv.org/abs/2507.15606)
*Ru Jia,Xiaozhuang Ma,Jianji Wang,Nanning Zheng*

Main category: cs.CV

TL;DR: The paper introduces CylinderPlane, a cylindrical coordinate-based implicit representation to address feature ambiguity and multi-view consistency issues in 360° image synthesis, outperforming Tri-plane methods.


<details>
  <summary>Details</summary>
Motivation: The Tri-plane representation causes multi-face artifacts due to shared features in symmetric regions, limiting 360° view generation.

Method: Proposes CylinderPlane, a cylindrical coordinate system that separates features by angle, and introduces nested cylinders for multi-scale geometry handling.

Result: Achieves high-quality, artifact-free 360° image synthesis and adapts to complex geometry and varying resolutions.

Conclusion: CylinderPlane outperforms previous methods, is versatile for neural rendering, and demonstrates robustness in experiments.

Abstract: While the proposal of the Tri-plane representation has advanced the
development of the 3D-aware image generative models, problems rooted in its
inherent structure, such as multi-face artifacts caused by sharing the same
features in symmetric regions, limit its ability to generate 360$^\circ$ view
images. In this paper, we propose CylinderPlane, a novel implicit
representation based on Cylindrical Coordinate System, to eliminate the feature
ambiguity issue and ensure multi-view consistency in 360$^\circ$. Different
from the inevitable feature entanglement in Cartesian coordinate-based
Tri-plane representation, the cylindrical coordinate system explicitly
separates features at different angles, allowing our cylindrical representation
possible to achieve high-quality, artifacts-free 360$^\circ$ image synthesis.
We further introduce the nested cylinder representation that composites
multiple cylinders at different scales, thereby enabling the model more
adaptable to complex geometry and varying resolutions. The combination of
cylinders with different resolutions can effectively capture more critical
locations and multi-scale features, greatly facilitates fine detail learning
and robustness to different resolutions. Moreover, our representation is
agnostic to implicit rendering methods and can be easily integrated into any
neural rendering pipeline. Extensive experiments on both synthetic dataset and
unstructured in-the-wild images demonstrate that our proposed representation
achieves superior performance over previous methods.

</details>


### [205] [A Survey on Efficiency Optimization Techniques for DNN-based Video Analytics: Process Systems, Algorithms, and Applications](https://arxiv.org/abs/2507.15628)
*Shanjiang Tang,Rui Huang,Hsinyu Luo,Chunjiang Wang,Ce Yu,Yusen Li,Hao Fu,Chao Sun,and Jian Xiao*

Main category: cs.CV

TL;DR: A survey reviewing efficiency optimization techniques for DNNs in video analytics, covering hardware, data processing, and deployment, and discussing challenges.


<details>
  <summary>Details</summary>
Motivation: Address the gap in existing surveys by focusing on efficiency improvements for DNNs in video analytics, beyond just accuracy.

Method: Organizes methods bottom-up, examining hardware support, data processing, and operational deployment.

Result: Provides a comprehensive review of efficiency optimization techniques and identifies key challenges.

Conclusion: Highlights the need for further research to address efficiency challenges in DNN-based video analytics.

Abstract: The explosive growth of video data in recent years has brought higher demands
for video analytics, where accuracy and efficiency remain the two primary
concerns. Deep neural networks (DNNs) have been widely adopted to ensure
accuracy; however, improving their efficiency in video analytics remains an
open challenge. Different from existing surveys that make summaries of
DNN-based video mainly from the accuracy optimization aspect, in this survey,
we aim to provide a thorough review of optimization techniques focusing on the
improvement of the efficiency of DNNs in video analytics. We organize existing
methods in a bottom-up manner, covering multiple perspectives such as hardware
support, data processing, operational deployment, etc. Finally, based on the
optimization framework and existing works, we analyze and discuss the problems
and challenges in the performance optimization of DNN-based video analytics.

</details>


### [206] [Experimenting active and sequential learning in a medieval music manuscript](https://arxiv.org/abs/2507.15633)
*Sachin Sharma,Federico Simonetta,Michele Flammini*

Main category: cs.CV

TL;DR: The paper explores Active Learning (AL) and Sequential Learning (SL) for OMR in medieval manuscripts using YOLOv8, achieving near-fully supervised accuracy with fewer labels, but finds uncertainty-based AL ineffective in the tested dataset.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity of annotated data and complexity of historical manuscripts in Optical Music Recognition (OMR) for cultural heritage digitization.

Method: Uses YOLOv8 for object detection and layout recognition, selecting uncertain samples for iterative labeling and retraining, starting with one annotated image.

Result: Achieves comparable accuracy to fully supervised training with fewer labels, but uncertainty-based AL proves ineffective in the tested manuscript.

Conclusion: Highlights the need for more usable methods in data-scarcity scenarios, despite the success of the overall approach.

Abstract: Optical Music Recognition (OMR) is a cornerstone of music digitization
initiatives in cultural heritage, yet it remains limited by the scarcity of
annotated data and the complexity of historical manuscripts. In this paper, we
present a preliminary study of Active Learning (AL) and Sequential Learning
(SL) tailored for object detection and layout recognition in an old medieval
music manuscript. Leveraging YOLOv8, our system selects samples with the
highest uncertainty (lowest prediction confidence) for iterative labeling and
retraining. Our approach starts with a single annotated image and successfully
boosts performance while minimizing manual labeling. Experimental results
indicate that comparable accuracy to fully supervised training can be achieved
with significantly fewer labeled examples. We test the methodology as a
preliminary investigation on a novel dataset offered to the community by the
Anonymous project, which studies laude, a poetical-musical genre spread across
Italy during the 12th-16th Century. We show that in the manuscript at-hand,
uncertainty-based AL is not effective and advocates for more usable methods in
data-scarcity scenarios.

</details>


### [207] [Uncovering Critical Features for Deepfake Detection through the Lottery Ticket Hypothesis](https://arxiv.org/abs/2507.15636)
*Lisan Al Amin,Md. Ismail Hossain,Thanh Thi Nguyen,Tasnim Jahan,Mahbubul Islam,Faisal Quader*

Main category: cs.CV

TL;DR: The study applies the Lottery Ticket Hypothesis (LTH) to deepfake detection, identifying efficient subnetworks (winning tickets) that maintain accuracy even at high sparsity levels.


<details>
  <summary>Details</summary>
Motivation: Deepfake technology threatens information integrity, and current detection methods are resource-intensive and poorly understood.

Method: The study uses LTH to prune neural networks (MesoNet, CNN-5, ResNet-18) on OpenForensic and FaceForensics++ datasets, comparing iterative magnitude pruning to one-shot methods.

Result: Pruned networks retain high accuracy (e.g., MesoNet: 56.2% at 80% sparsity) with fewer parameters, and iterative pruning outperforms one-shot methods. Winning tickets transfer across datasets.

Conclusion: LTH enables efficient, deployable deepfake detection systems by identifying critical subnetworks, with potential for broader application.

Abstract: Recent advances in deepfake technology have created increasingly convincing
synthetic media that poses significant challenges to information integrity and
social trust. While current detection methods show promise, their underlying
mechanisms remain poorly understood, and the large sizes of their models make
them challenging to deploy in resource-limited environments. This study
investigates the application of the Lottery Ticket Hypothesis (LTH) to deepfake
detection, aiming to identify the key features crucial for recognizing
deepfakes. We examine how neural networks can be efficiently pruned while
maintaining high detection accuracy. Through extensive experiments with
MesoNet, CNN-5, and ResNet-18 architectures on the OpenForensic and
FaceForensics++ datasets, we find that deepfake detection networks contain
winning tickets, i.e., subnetworks, that preserve performance even at
substantial sparsity levels. Our results indicate that MesoNet retains 56.2%
accuracy at 80% sparsity on the OpenForensic dataset, with only 3,000
parameters, which is about 90% of its baseline accuracy (62.6%). The results
also show that our proposed LTH-based iterative magnitude pruning approach
consistently outperforms one-shot pruning methods. Using Grad-CAM
visualization, we analyze how pruned networks maintain their focus on critical
facial regions for deepfake detection. Additionally, we demonstrate the
transferability of winning tickets across datasets, suggesting potential for
efficient, deployable deepfake detection systems.

</details>


### [208] [Extracting Visual Facts from Intermediate Layers for Mitigating Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2507.15652)
*Haoran Zhou,Zihan Zhang,Hao Chen*

Main category: cs.CV

TL;DR: EVA is a training-free method to reduce object hallucinations in MLLMs by dynamically selecting intermediate layers with strong visual factual information and correcting output logits.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with object hallucinations due to suppressed visual information by prior knowledge in deep layers. The unclear suppression mechanism in intermediate layers motivates the study.

Method: EVA selects intermediate layers with significant visual factual information, contrasts their output distributions, and incorporates this knowledge into the final layer to correct logits.

Result: EVA significantly reduces hallucination rates on benchmarks, outperforming baseline methods.

Conclusion: EVA is an effective, model-agnostic solution to mitigate hallucinations in MLLMs, compatible with various decoding strategies.

Abstract: Multimodal Large Language Models (MLLMs) have made significant strides by
combining visual recognition and language understanding to generate content
that is both coherent and contextually accurate. However, MLLMs continue to
struggle with object hallucinations, where models produce seemingly plausible
but factually incorrect outputs, including objects that do not exist in the
image. Recent work has revealed that the prior knowledge in MLLMs significantly
suppresses visual information in deep layers, causing hallucinatory outputs.
However, how these priors suppress visual information at the intermediate layer
stage in MLLMs remains unclear. We observe that visual factual knowledge and
the differences between intermediate-layer prior/original probability
distributions show similar evolutionary trends in intermediate layers.
Motivated by this, we introduce Decoding by Extracting Visual Facts (EVA), a
simple, training-free method that dynamically selects intermediate layers with
the most significant visual factual information. By contrasting the output
distributions of the selected layer derived from the original input and
pure-text input, EVA extracts visual factual knowledge and proportionally
incorporates it into the final layer to correct the output logits. Importantly,
EVA is model-agnostic, seamlessly integrates with various classic decoding
strategies, and is applicable across different MLLMs. We validate EVA on
widely-used benchmarks, and the results show that it significantly reduces
hallucination rates compared to baseline methods, underscoring its
effectiveness in mitigating hallucinations.

</details>


### [209] [HW-MLVQA: Elucidating Multilingual Handwritten Document Understanding with a Comprehensive VQA Benchmark](https://arxiv.org/abs/2507.15655)
*Aniket Pal,Ajoy Mondal,Minesh Mathew,C. V. Jawahar*

Main category: cs.CV

TL;DR: HW-MLVQA is a new benchmark for multilingual handwritten document VQA, addressing gaps in current models with 1,600 pages and 2,400 Q&A pairs, evaluated across text, image, and combined modalities.


<details>
  <summary>Details</summary>
Motivation: Current MLVQA models underperform with handwritten documents, lacking benchmarks for multilingual handwritten comprehension.

Method: Introduces HW-MLVQA, a benchmark with 1,600 handwritten pages and 2,400 Q&A pairs, evaluated across text, image, and combined modalities, including OCR model assessment.

Result: Provides a robust framework for evaluating multilingual handwritten document interpretation, simulating real-world scenarios without ground truth transcriptions.

Conclusion: HW-MLVQA aims to drive advancements in multilingual handwritten document understanding, encouraging innovation in this niche field.

Abstract: The proliferation of MultiLingual Visual Question Answering (MLVQA)
benchmarks augments the capabilities of large language models (LLMs) and
multi-modal LLMs, thereby enabling them to adeptly capture the intricate
linguistic subtleties and visual complexities inherent across diverse
languages. Despite its potential, the current MLVQA model struggles to fully
utilize its capabilities when dealing with the extensive variety of handwritten
documents. This article delineates HW-MLVQA, an avant-garde VQA benchmark
meticulously crafted to mitigate the dearth of authentic Multilingual
Handwritten document comprehension. HW-MLVQA encompasses an extensive
collection of 1,600 handwritten Pages complemented by 2,400 question-answers.
Furthermore, it provides a robust benchmark evaluation framework spanning three
distinct modalities: text, image, and an integrated image & text modality. To
simulate authentic real-world contexts devoid of ground truth textual
transcriptions, we facilitates a rigorous assessment of proprietary and
open-source OCR models. The benchmark aspires to facilitate pivotal
advancements in multilingual handwritten document interpretation, fostering
innovation and scholarly inquiry within this specialized domain.

</details>


### [210] [Visual-Language Model Knowledge Distillation Method for Image Quality Assessment](https://arxiv.org/abs/2507.15680)
*Yongkang Hou,Jiarun Song*

Main category: cs.CV

TL;DR: A knowledge distillation method using CLIP improves IQA by reducing model complexity and enhancing local distortion identification.


<details>
  <summary>Details</summary>
Motivation: Address CLIP's limitations in IQA, such as high parameter burden and poor local distortion detection.

Method: Design quality-graded prompts, fine-tune CLIP, and use modality-adaptive distillation to transfer knowledge to a student model.

Result: Outperforms existing IQA methods with reduced complexity, validated on multiple datasets.

Conclusion: The method shows strong practical potential for efficient and effective IQA.

Abstract: Image Quality Assessment (IQA) is a core task in computer vision. Multimodal
methods based on vision-language models, such as CLIP, have demonstrated
exceptional generalization capabilities in IQA tasks. To address the issues of
excessive parameter burden and insufficient ability to identify local distorted
features in CLIP for IQA, this study proposes a visual-language model knowledge
distillation method aimed at guiding the training of models with architectural
advantages using CLIP's IQA knowledge. First, quality-graded prompt templates
were designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned
to enhance its capabilities in IQA tasks. Finally, a modality-adaptive
knowledge distillation strategy is proposed to achieve guidance from the CLIP
teacher model to the student model. Our experiments were conducted on multiple
IQA datasets, and the results show that the proposed method significantly
reduces model complexity while outperforming existing IQA methods,
demonstrating strong potential for practical deployment.

</details>


### [211] [Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization for Remote Sensing](https://arxiv.org/abs/2507.15683)
*Boni Hu,Zhenyu Xia,Lin Chen,Pengcheng Han,Shuhui Bu*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Visual relocalization, which estimates the 6-degree-of-freedom (6-DoF) camera
pose from query images, is fundamental to remote sensing and UAV applications.
Existing methods face inherent trade-offs: image-based retrieval and pose
regression approaches lack precision, while structure-based methods that
register queries to Structure-from-Motion (SfM) models suffer from
computational complexity and limited scalability. These challenges are
particularly pronounced in remote sensing scenarios due to large-scale scenes,
high altitude variations, and domain gaps of existing visual priors. To
overcome these limitations, we leverage 3D Gaussian Splatting (3DGS) as a novel
scene representation that compactly encodes both 3D geometry and appearance. We
introduce $\mathrm{Hi}^2$-GSLoc, a dual-hierarchical relocalization framework
that follows a sparse-to-dense and coarse-to-fine paradigm, fully exploiting
the rich semantic information and geometric constraints inherent in Gaussian
primitives. To handle large-scale remote sensing scenarios, we incorporate
partitioned Gaussian training, GPU-accelerated parallel matching, and dynamic
memory management strategies. Our approach consists of two stages: (1) a sparse
stage featuring a Gaussian-specific consistent render-aware sampling strategy
and landmark-guided detector for robust and accurate initial pose estimation,
and (2) a dense stage that iteratively refines poses through coarse-to-fine
dense rasterization matching while incorporating reliability verification.
Through comprehensive evaluation on simulation data, public datasets, and real
flight experiments, we demonstrate that our method delivers competitive
localization accuracy, recall rate, and computational efficiency while
effectively filtering unreliable pose estimates. The results confirm the
effectiveness of our approach for practical remote sensing applications.

</details>


### [212] [LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression](https://arxiv.org/abs/2507.15686)
*Wenjie Huang,Qi Yang,Shuting Xia,He Huang,Zhu Li,Yiling Xu*

Main category: cs.CV

TL;DR: Proposes LINR-PCGC, the first INR-based lossless point cloud geometry compression method, improving speed and efficiency over existing methods.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of AI-based and INR methods in point cloud compression, particularly dependency on training data and lossy-only results.

Method: Uses a group-level coding framework with network initialization for speed, and a lightweight multiscale SparseConv network for fast inference and compact size.

Result: Reduces bitstream by ~21.21% vs. G-PCC TMC13v23 and ~21.95% vs. SparsePCGC, with 60% faster encoding.

Conclusion: LINR-PCGC offers a superior, distribution-agnostic solution for lossless point cloud compression.

Abstract: Existing AI-based point cloud compression methods struggle with dependence on
specific training data distributions, which limits their real-world deployment.
Implicit Neural Representation (INR) methods solve the above problem by
encoding overfitted network parameters to the bitstream, resulting in more
distribution-agnostic results. However, due to the limitation of encoding time
and decoder size, current INR based methods only consider lossy geometry
compression. In this paper, we propose the first INR based lossless point cloud
geometry compression method called Lossless Implicit Neural Representations for
Point Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we
design a group of point clouds level coding framework with an effective network
initialization strategy, which can reduce around 60% encoding time. A
lightweight coding network based on multiscale SparseConv, consisting of scale
context extraction, child node prediction, and model compression modules, is
proposed to realize fast inference and compact decoder size. Experimental
results show that our method consistently outperforms traditional and AI-based
methods: for example, with the convergence time in the MVUB dataset, our method
reduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and
21.95% compared to SparsePCGC. Our project can be seen on
https://huangwenjie2023.github.io/LINR-PCGC/.

</details>


### [213] [DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting](https://arxiv.org/abs/2507.15690)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: DWTGS improves sparse-view 3D Gaussian Splatting by using wavelet-space losses for better generalization and reduced high-frequency artifacts.


<details>
  <summary>Details</summary>
Motivation: Overfitting to high-frequency details in sparse training views hinders quality reconstruction in 3DGS. Frequency regularization via Fourier transforms is hard to tune and biases learning.

Method: DWTGS uses wavelet-space losses, supervising low-frequency subbands and enforcing sparsity on high-frequency subbands self-supervised.

Result: DWTGS outperforms Fourier-based methods, improving generalization and reducing high-frequency hallucinations.

Conclusion: Wavelet-space losses offer a better alternative to Fourier transforms for frequency regularization in 3DGS.

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in
reconstructing high-quality novel views, as it often overfits to the
widely-varying high-frequency (HF) details of the sparse training views. While
frequency regularization can be a promising approach, its typical reliance on
Fourier transforms causes difficult parameter tuning and biases towards
detrimental HF learning. We propose DWTGS, a framework that rethinks frequency
regularization by leveraging wavelet-space losses that provide additional
spatial supervision. Specifically, we supervise only the low-frequency (LF) LL
subbands at multiple DWT levels, while enforcing sparsity on the HF HH subband
in a self-supervised manner. Experiments across benchmarks show that DWTGS
consistently outperforms Fourier-based counterparts, as this LF-centric
strategy improves generalization and reduces HF hallucinations.

</details>


### [214] [Efficient Face Image Quality Assessment via Self-training and Knowledge Distillation](https://arxiv.org/abs/2507.15709)
*Wei Sun,Weixia Zhang,Linhan Cao,Jun Jia,Xiangyang Zhu,Dandan Zhu,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: A computationally efficient FIQA method using teacher-student distillation and self-training achieves high performance with low overhead.


<details>
  <summary>Details</summary>
Motivation: To address the computational complexity of FIQA algorithms for real-world scalability and deployment.

Method: Two-stage approach: train a teacher model using labeled data and self-training, then distill a lightweight student model using pseudo-labels.

Result: Student model matches teacher performance with minimal computational cost; won ICCV 2025 VQualA FIQA Challenge.

Conclusion: The method successfully balances efficiency and performance, making it practical for real-world applications.

Abstract: Face image quality assessment (FIQA) is essential for various face-related
applications. Although FIQA has been extensively studied and achieved
significant progress, the computational complexity of FIQA algorithms remains a
key concern for ensuring scalability and practical deployment in real-world
systems. In this paper, we aim to develop a computationally efficient FIQA
method that can be easily deployed in real-world applications. Specifically,
our method consists of two stages: training a powerful teacher model and
distilling a lightweight student model from it. To build a strong teacher
model, we adopt a self-training strategy to improve its capacity. We first
train the teacher model using labeled face images, then use it to generate
pseudo-labels for a set of unlabeled images. These pseudo-labeled samples are
used in two ways: (1) to distill knowledge into the student model, and (2) to
combine with the original labeled images to further enhance the teacher model
through self-training. The enhanced teacher model is used to further
pseudo-label another set of unlabeled images for distilling the student models.
The student model is trained using a combination of labeled images,
pseudo-labeled images from the original teacher model, and pseudo-labeled
images from the enhanced teacher model. Experimental results demonstrate that
our student model achieves comparable performance to the teacher model with an
extremely low computational overhead. Moreover, our method achieved first place
in the ICCV 2025 VQualA FIQA Challenge. The code is available at
https://github.com/sunwei925/Efficient-FIQA.git.

</details>


### [215] [A Practical Investigation of Spatially-Controlled Image Generation with Transformers](https://arxiv.org/abs/2507.15724)
*Guoxuan Xia,Harleen Hanspal,Petru-Daniel Tudosiu,Shifeng Zhang,Sarah Parisot*

Main category: cs.CV

TL;DR: The paper evaluates transformer-based models for spatially-controlled image generation, comparing paradigms like diffusion, flow-based, and autoregressive models, and introduces control token prefilling and sampling enhancements.


<details>
  <summary>Details</summary>
Motivation: To address the lack of detailed and fair comparisons in spatially-controlled image generation research and clarify the literature for practitioners.

Method: Controlled experiments on ImageNet using diffusion-based, flow-based, and autoregressive models, focusing on control token prefilling, classifier-free guidance, and softmax truncation.

Result: Control token prefilling is a strong baseline; sampling enhancements like classifier-free guidance improve control-generation consistency; adapter-based approaches mitigate forgetting but underperform in consistency.

Conclusion: The study provides clear takeaways for transformer-based spatially-controlled generation, highlighting effective methods and addressing knowledge gaps.

Abstract: Enabling image generation models to be spatially controlled is an important
area of research, empowering users to better generate images according to their
own fine-grained specifications via e.g. edge maps, poses. Although this task
has seen impressive improvements in recent times, a focus on rapidly producing
stronger models has come at the cost of detailed and fair scientific
comparison. Differing training data, model architectures and generation
paradigms make it difficult to disentangle the factors contributing to
performance. Meanwhile, the motivations and nuances of certain approaches
become lost in the literature. In this work, we aim to provide clear takeaways
across generation paradigms for practitioners wishing to develop
transformer-based systems for spatially-controlled generation, clarifying the
literature and addressing knowledge gaps. We perform controlled experiments on
ImageNet across diffusion-based/flow-based and autoregressive (AR) models.
First, we establish control token prefilling as a simple, general and
performant baseline approach for transformers. We then investigate previously
underexplored sampling time enhancements, showing that extending
classifier-free guidance to control, as well as softmax truncation, have a
strong impact on control-generation consistency. Finally, we re-clarify the
motivation of adapter-based approaches, demonstrating that they mitigate
"forgetting" and maintain generation quality when trained on limited downstream
data, but underperform full training in terms of generation-control
consistency. Code will be released upon publication.

</details>


### [216] [TokensGen: Harnessing Condensed Tokens for Long Video Generation](https://arxiv.org/abs/2507.15728)
*Wenqi Ouyang,Zeqi Xiao,Danni Yang,Yifan Zhou,Shuai Yang,Lei Yang,Jianlou Si,Xingang Pan*

Main category: cs.CV

TL;DR: TokensGen is a two-stage framework using condensed tokens for long video generation, addressing memory and consistency issues with To2V and T2To models, and adaptive FIFO-Diffusion for smooth transitions.


<details>
  <summary>Details</summary>
Motivation: Extending diffusion-based models for long videos faces memory bottlenecks and long-term inconsistency. TokensGen aims to solve these challenges.

Method: Decomposes long video generation into three tasks: semantic control, consistency control, and smooth transitions. Uses To2V for short clips and T2To for global token generation.

Result: Enhances long-term coherence without excessive computational cost, scalable for storytelling and simulations.

Conclusion: TokensGen offers a modular, efficient solution for long video generation, enabling new applications in media and simulations.

Abstract: Generating consistent long videos is a complex challenge: while
diffusion-based generative models generate visually impressive short clips,
extending them to longer durations often leads to memory bottlenecks and
long-term inconsistency. In this paper, we propose TokensGen, a novel two-stage
framework that leverages condensed tokens to address these issues. Our method
decomposes long video generation into three core tasks: (1) inner-clip semantic
control, (2) long-term consistency control, and (3) inter-clip smooth
transition. First, we train To2V (Token-to-Video), a short video diffusion
model guided by text and video tokens, with a Video Tokenizer that condenses
short clips into semantically rich tokens. Second, we introduce T2To
(Text-to-Token), a video token diffusion transformer that generates all tokens
at once, ensuring global consistency across clips. Finally, during inference,
an adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips,
reducing boundary artifacts and enhancing smooth transitions. Experimental
results demonstrate that our approach significantly enhances long-term temporal
and content coherence without incurring prohibitive computational overhead. By
leveraging condensed tokens and pre-trained short video models, our method
provides a scalable, modular solution for long video generation, opening new
possibilities for storytelling, cinematic production, and immersive
simulations. Please see our project page at
https://vicky0522.github.io/tokensgen-webpage/ .

</details>


### [217] [Appearance Harmonization via Bilateral Grid Prediction with Transformers for 3DGS](https://arxiv.org/abs/2507.15748)
*Jisu Shin,Richard Shaw,Seunghyun Shin,Anton Pelykh,Zhensong Zhang,Hae-Gon Jeon,Eduardo Perez-Pellitero*

Main category: cs.CV

TL;DR: A transformer-based method predicts bilateral grids to correct photometric inconsistencies in multi-view scenes, improving novel view synthesis without scene-specific retraining.


<details>
  <summary>Details</summary>
Motivation: Photometric inconsistencies from camera pipelines degrade multi-view consistency and novel view synthesis quality. Existing methods increase computational complexity.

Method: Uses a transformer to predict spatially adaptive bilateral grids for photometric correction, integrated into the 3D Gaussian Splatting pipeline.

Result: Outperforms or matches scene-specific methods in reconstruction fidelity and convergence speed.

Conclusion: The proposed method enables robust cross-scene generalization and maintains high training efficiency.

Abstract: Modern camera pipelines apply extensive on-device processing, such as
exposure adjustment, white balance, and color correction, which, while
beneficial individually, often introduce photometric inconsistencies across
views. These appearance variations violate multi-view consistency and degrade
the quality of novel view synthesis. Joint optimization of scene
representations and per-image appearance embeddings has been proposed to
address this issue, but at the cost of increased computational complexity and
slower training. In this work, we propose a transformer-based method that
predicts spatially adaptive bilateral grids to correct photometric variations
in a multi-view consistent manner, enabling robust cross-scene generalization
without the need for scene-specific retraining. By incorporating the learned
grids into the 3D Gaussian Splatting pipeline, we improve reconstruction
quality while maintaining high training efficiency. Extensive experiments show
that our approach outperforms or matches existing scene-specific optimization
methods in reconstruction fidelity and convergence speed.

</details>


### [218] [Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization](https://arxiv.org/abs/2507.15765)
*Feng-Qi Cui,Anyang Tong,Jinyang Huang,Jie Zhang,Dan Guo,Zhi Liu,Meng Wang*

Main category: cs.CV

TL;DR: The paper introduces HDF, a framework for dynamic facial expression recognition, addressing sample heterogeneity with two modules: DAM for time-frequency modeling and DSM for optimization balance, achieving high accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing methods for DFER degrade under sample heterogeneity from multi-source data and individual variability.

Method: Proposes HDF with DAM (dual-branch attention for time-frequency) and DSM (adaptive optimization for loss balance).

Result: HDF improves recognition accuracy and robustness on DFEW and FERV39k datasets, achieving superior WAR and UAR.

Conclusion: HDF effectively handles heterogeneity and imbalance, offering stable and discriminative learning for DFER.

Abstract: Dynamic Facial Expression Recognition (DFER) plays a critical role in
affective computing and human-computer interaction. Although existing methods
achieve comparable performance, they inevitably suffer from performance
degradation under sample heterogeneity caused by multi-source data and
individual expression variability. To address these challenges, we propose a
novel framework, called Heterogeneity-aware Distributional Framework (HDF), and
design two plug-and-play modules to enhance time-frequency modeling and
mitigate optimization imbalance caused by hard samples. Specifically, the
Time-Frequency Distributional Attention Module (DAM) captures both temporal
consistency and frequency robustness through a dual-branch attention design,
improving tolerance to sequence inconsistency and visual style shifts. Then,
based on gradient sensitivity and information bottleneck principles, an
adaptive optimization module Distribution-aware Scaling Module (DSM) is
introduced to dynamically balance classification and contrastive losses,
enabling more stable and discriminative representation learning. Extensive
experiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF
significantly improves both recognition accuracy and robustness. Our method
achieves superior weighted average recall (WAR) and unweighted average recall
(UAR) while maintaining strong generalization across diverse and imbalanced
scenarios. Codes are released at https://github.com/QIcita/HDF_DFER.

</details>


### [219] [Label tree semantic losses for rich multi-class medical image segmentation](https://arxiv.org/abs/2507.15777)
*Junwen Wang,Oscar MacCormac,William Rochford,Aaron Kujawa,Jonathan Shapey,Tom Vercauteren*

Main category: cs.CV

TL;DR: The paper introduces tree-based semantic loss functions for medical image segmentation, improving accuracy by leveraging hierarchical label organization and sparse annotations.


<details>
  <summary>Details</summary>
Motivation: Current methods penalize all errors equally, ignoring inter-class semantics, which is problematic for rich and subtle label spaces.

Method: Proposes two tree-based semantic loss functions and integrates them with sparse, background-free annotation training.

Result: Achieves state-of-the-art performance in head MRI for whole brain parcellation and neurosurgical hyperspectral imaging tasks.

Conclusion: The proposed method effectively exploits hierarchical label semantics, enhancing segmentation accuracy in medical imaging.

Abstract: Rich and accurate medical image segmentation is poised to underpin the next
generation of AI-defined clinical practice by delineating critical anatomy for
pre-operative planning, guiding real-time intra-operative navigation, and
supporting precise post-operative assessment. However, commonly used learning
methods for medical and surgical imaging segmentation tasks penalise all errors
equivalently and thus fail to exploit any inter-class semantics in the labels
space. This becomes particularly problematic as the cardinality and richness of
labels increases to include subtly different classes. In this work, we propose
two tree-based semantic loss functions which take advantage of a hierarchical
organisation of the labels. We further incorporate our losses in a recently
proposed approach for training with sparse, background-free annotations to
extend the applicability of our proposed losses. Extensive experiments are
reported on two medical and surgical image segmentation tasks, namely head MRI
for whole brain parcellation (WBP) with full supervision and neurosurgical
hyperspectral imaging (HSI) for scene understanding with sparse annotations.
Results demonstrate that our proposed method reaches state-of-the-art
performance in both cases.

</details>


### [220] [Regularized Low-Rank Adaptation for Few-Shot Organ Segmentation](https://arxiv.org/abs/2507.15793)
*Ghassen Baklouti,Julio Silva-Rodríguez,Jose Dolz,Houda Bahig,Ismail Ben Ayed*

Main category: cs.CV

TL;DR: A novel PEFT method for medical image segmentation dynamically adjusts rank during adaptation, outperforming LoRA and other methods.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of selecting a fixed rank in LoRA for medical imaging tasks by introducing dynamic rank adjustment.

Method: Uses an l_1 sparsity regularizer on SVD-based low-rank representations, optimized proximally to automatically find task-adapted ranks.

Result: Significant performance improvements in few-shot fine-tuning, especially for base and novel organ segmentation tasks.

Conclusion: The method is efficient, robust against suboptimal rank initialization, and outperforms existing PEFT approaches.

Abstract: Parameter-efficient fine-tuning (PEFT) of pre-trained foundation models is
increasingly attracting interest in medical imaging due to its effectiveness
and computational efficiency. Among these methods, Low-Rank Adaptation (LoRA)
is a notable approach based on the assumption that the adaptation inherently
occurs in a low-dimensional subspace. While it has shown good performance, its
implementation requires a fixed and unalterable rank, which might be
challenging to select given the unique complexities and requirements of each
medical imaging downstream task. Inspired by advancements in natural image
processing, we introduce a novel approach for medical image segmentation that
dynamically adjusts the intrinsic rank during adaptation. Viewing the low-rank
representation of the trainable weight matrices as a singular value
decomposition, we introduce an l_1 sparsity regularizer to the loss function,
and tackle it with a proximal optimizer. The regularizer could be viewed as a
penalty on the decomposition rank. Hence, its minimization enables to find
task-adapted ranks automatically. Our method is evaluated in a realistic
few-shot fine-tuning setting, where we compare it first to the standard LoRA
and then to several other PEFT methods across two distinguishable tasks: base
organs and novel organs. Our extensive experiments demonstrate the significant
performance improvements driven by our method, highlighting its efficiency and
robustness against suboptimal rank initialization. Our code is publicly
available: https://github.com/ghassenbaklouti/ARENA

</details>


### [221] [Exploring Superposition and Interference in State-of-the-Art Low-Parameter Vision Models](https://arxiv.org/abs/2507.15798)
*Lilian Hollard,Lucas Mohimont,Nathalie Gaveau,Luiz-Angelo Steffenel*

Main category: cs.CV

TL;DR: The paper explores low-parameter deep neural networks for computer vision, focusing on bottleneck architectures and superlinear activation functions. It addresses interference in feature maps, proposing design elements to reduce it, and introduces the NoDepth Bottleneck architecture, which improves scaling and accuracy in small networks.


<details>
  <summary>Details</summary>
Motivation: To enhance the performance and scalability of low-parameter deep neural networks by addressing interference in feature maps, a challenge in bottleneck architectures.

Method: Examines bottleneck architectures and superlinear activation functions, identifies design elements to reduce interference, and proposes the NoDepth Bottleneck architecture.

Result: Demonstrates improved scaling and accuracy in networks under 1.5M parameters, validated on the ImageNet dataset.

Conclusion: The findings contribute to more efficient and scalable neural networks for low-parameter ranges and advance understanding of bottlenecks in computer vision.

Abstract: The paper investigates the performance of state-of-the-art low-parameter deep
neural networks for computer vision, focusing on bottleneck architectures and
their behavior using superlinear activation functions. We address interference
in feature maps, a phenomenon associated with superposition, where neurons
simultaneously encode multiple characteristics. Our research suggests that
limiting interference can enhance scaling and accuracy in very low-scaled
networks (under 1.5M parameters). We identify key design elements that reduce
interference by examining various bottleneck architectures, leading to a more
efficient neural network. Consequently, we propose a proof-of-concept
architecture named NoDepth Bottleneck built on mechanistic insights from our
experiments, demonstrating robust scaling accuracy on the ImageNet dataset.
These findings contribute to more efficient and scalable neural networks for
the low-parameter range and advance the understanding of bottlenecks in
computer vision. https://caiac.pubpub.org/pub/3dh6rsel

</details>


### [222] [ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction](https://arxiv.org/abs/2507.15803)
*Danhui Chen,Ziquan Liu,Chuxi Yang,Dan Wang,Yan Yan,Yi Xu,Xiangyang Ji*

Main category: cs.CV

TL;DR: ConformalSAM leverages a foundational segmentation model (SEEM) to address label scarcity in semi-supervised semantic segmentation, using conformal prediction for uncertainty calibration and filtering unreliable labels.


<details>
  <summary>Details</summary>
Motivation: Labeling pixel-level data is costly, and foundational models like SEEM offer potential for generalization. This work explores using SEEM as an annotator and improving its reliability.

Method: Proposes ConformalSAM, which calibrates SEEM using labeled target data, filters unreliable labels via conformal prediction, and employs self-reliance training to avoid overfitting.

Result: Outperforms recent SSSS methods on benchmarks and enhances other methods as a plug-in.

Conclusion: ConformalSAM effectively leverages foundational models for SSSS, improving performance and reliability.

Abstract: Pixel-level vision tasks, such as semantic segmentation, require extensive
and high-quality annotated data, which is costly to obtain. Semi-supervised
semantic segmentation (SSSS) has emerged as a solution to alleviate the
labeling burden by leveraging both labeled and unlabeled data through
self-training techniques. Meanwhile, the advent of foundational segmentation
models pre-trained on massive data, has shown the potential to generalize
across domains effectively. This work explores whether a foundational
segmentation model can address label scarcity in the pixel-level vision task as
an annotator for unlabeled images. Specifically, we investigate the efficacy of
using SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual
input, to generate predictive masks for unlabeled data. To address the
shortcomings of using SEEM-generated masks as supervision, we propose
ConformalSAM, a novel SSSS framework which first calibrates the foundation
model using the target domain's labeled data and then filters out unreliable
pixel labels of unlabeled data so that only high-confidence labels are used as
supervision. By leveraging conformal prediction (CP) to adapt foundation models
to target data through uncertainty calibration, ConformalSAM exploits the
strong capability of the foundational segmentation model reliably which
benefits the early-stage learning, while a subsequent self-reliance training
strategy mitigates overfitting to SEEM-generated masks in the later training
stage. Our experiment demonstrates that, on three standard benchmarks of SSSS,
ConformalSAM achieves superior performance compared to recent SSSS methods and
helps boost the performance of those methods as a plug-in.

</details>


### [223] [True Multimodal In-Context Learning Needs Attention to the Visual Context](https://arxiv.org/abs/2507.15807)
*Shuo Chen,Jianzhe Liu,Zhen Han,Yan Xia,Daniel Cremers,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CV

TL;DR: The paper addresses the limitations of Multimodal Large Language Models (MLLMs) in effectively leveraging visual information during Multimodal In-Context Learning (MICL). It introduces Dynamic Attention Reallocation (DARA) and TrueMICL dataset to enhance and evaluate true multimodal adaptation.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle to integrate visual cues in demonstrations, relying too heavily on textual patterns, which limits the practical utility of MICL. This issue is often masked by performance on tasks not requiring visual context.

Method: The authors propose DARA, a fine-tuning strategy to rebalance attention between visual and textual tokens, and introduce TrueMICL, a dataset explicitly requiring multimodal integration for task completion.

Result: Experiments show significant improvements in true multimodal in-context learning capabilities, validating the effectiveness of DARA and TrueMICL.

Conclusion: The proposed solutions enhance MICL by addressing visual neglect in MLLMs, providing a reliable framework for evaluating and improving multimodal adaptation.

Abstract: Multimodal Large Language Models (MLLMs), built on powerful language
backbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new
tasks from a few multimodal demonstrations consisting of images, questions, and
answers. Despite showing noticeable improvement on standard vision-language
datasets, current MLLMs struggle to leverage visual information in the
demonstrations. Specifically, they tend to neglect visual cues and over-rely on
textual patterns, leading to mere text imitation rather than genuine multimodal
adaptation. This behavior makes MICL still unimodal and largely restricts its
practical utility. More importantly, this limitation is often concealed by the
improved performance on tasks that do not require understanding the visual
context. As a result, how to effectively enhance MICL ability and reliably
evaluate the MICL performance remains underexplored. To address these issues,
we first introduce Dynamic Attention Reallocation (DARA), an efficient
fine-tuning strategy that encourages models to attend to the visual context by
rebalancing attention across visual and textual tokens. In addition, we present
TrueMICL, an MICL-dedicated dataset with both support and test sets that
explicitly requires the integration of multimodal information-particularly
visual content-for correct task completion. Extensive experiments demonstrate
the effectiveness of our holistic solution, showcasing substantial improvements
in the true multimodal in-context learning capabilities. Code and datasets are
available at https://chenxshuo.github.io/true-micl-colm .

</details>


### [224] [Diffusion models for multivariate subsurface generation and efficient probabilistic inversion](https://arxiv.org/abs/2507.15809)
*Roberto Miele,Niklas Linde*

Main category: cs.CV

TL;DR: Diffusion models improve multivariate subsurface modeling and probabilistic inversion, outperforming VAEs and GANs, with enhanced robustness and reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: To enhance multivariate modeling and probabilistic inversion in subsurface scenarios using diffusion models, addressing limitations of existing methods like VAEs and GANs.

Method: Proposes corrections to Diffusion Posterior Sampling, including a noise-contamination-aware likelihood approximation, and tests in geological scenarios with facies and acoustic impedance.

Result: Shows improved statistical robustness, better posterior sampling, and lower computational costs compared to original methods, handling both hard and indirect data.

Conclusion: Diffusion models offer efficient, robust inversion within the generative process, outperforming outer-loop methods like MCMC.

Abstract: Diffusion models offer stable training and state-of-the-art performance for
deep generative modeling tasks. Here, we consider their use in the context of
multivariate subsurface modeling and probabilistic inversion. We first
demonstrate that diffusion models enhance multivariate modeling capabilities
compared to variational autoencoders and generative adversarial networks. In
diffusion modeling, the generative process involves a comparatively large
number of time steps with update rules that can be modified to account for
conditioning data. We propose different corrections to the popular Diffusion
Posterior Sampling approach by Chung et al. (2023). In particular, we introduce
a likelihood approximation accounting for the noise-contamination that is
inherent in diffusion modeling. We assess performance in a multivariate
geological scenario involving facies and correlated acoustic impedance.
Conditional modeling is demonstrated using both local hard data (well logs) and
nonlinear geophysics (fullstack seismic data). Our tests show significantly
improved statistical robustness, enhanced sampling of the posterior probability
density function and reduced computational costs, compared to the original
approach. The method can be used with both hard and indirect conditioning data,
individually or simultaneously. As the inversion is included within the
diffusion process, it is faster than other methods requiring an outer-loop
around the generative model, such as Markov chain Monte Carlo.

</details>


### [225] [Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models](https://arxiv.org/abs/2507.15824)
*Enes Sanli,Baris Sarper Tezcan,Aykut Erdem,Erkut Erdem*

Main category: cs.CV

TL;DR: PhysVidBench is a benchmark to evaluate physical commonsense in text-to-video models, using 383 prompts and a three-stage evaluation pipeline.


<details>
  <summary>Details</summary>
Motivation: Current T2V models lack physical commonsense, producing videos that violate intuitive physics.

Method: A three-stage pipeline: formulate physics questions, caption videos, and answer questions using captions.

Result: Provides a structured framework to assess physical plausibility in T2V models.

Conclusion: PhysVidBench addresses gaps in T2V evaluations by focusing on physical reasoning.

Abstract: Recent progress in text-to-video (T2V) generation has enabled the synthesis
of visually compelling and temporally coherent videos from natural language.
However, these models often fall short in basic physical commonsense, producing
outputs that violate intuitive expectations around causality, object behavior,
and tool use. Addressing this gap, we present PhysVidBench, a benchmark
designed to evaluate the physical reasoning capabilities of T2V systems. The
benchmark includes 383 carefully curated prompts, emphasizing tool use,
material properties, and procedural interactions, and domains where physical
plausibility is crucial. For each prompt, we generate videos using diverse
state-of-the-art models and adopt a three-stage evaluation pipeline: (1)
formulate grounded physics questions from the prompt, (2) caption the generated
video with a vision-language model, and (3) task a language model to answer
several physics-involved questions using only the caption. This indirect
strategy circumvents common hallucination issues in direct video-based
evaluation. By highlighting affordances and tool-mediated actions, areas
overlooked in current T2V evaluations, PhysVidBench provides a structured,
interpretable framework for assessing physical commonsense in generative video
models.

</details>


### [226] [SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction](https://arxiv.org/abs/2507.15852)
*Zhixiong Zhang,Shuangrui Ding,Xiaoyi Dong,Songxin He,Jianfan Lin,Junsong Tang,Yuhang Zang,Yuhang Cao,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: SeC is a concept-driven VOS framework using LVLMs for robust object segmentation, outperforming SAM 2.1 by 11.8 points on the new SeCVOS benchmark.


<details>
  <summary>Details</summary>
Motivation: Current VOS methods rely on appearance matching, lacking human-like conceptual understanding, which limits robustness in complex scenarios.

Method: SeC integrates LVLMs to build high-level object representations, balancing semantic reasoning and feature matching dynamically.

Result: SeC achieves an 11.8-point improvement over SAM 2.1 on the SeCVOS benchmark.

Conclusion: SeC sets a new state-of-the-art in concept-aware VOS, demonstrating the value of semantic reasoning in handling complex scenarios.

Abstract: Video Object Segmentation (VOS) is a core task in computer vision, requiring
models to track and segment target objects across video frames. Despite notable
advances with recent efforts, current techniques still lag behind human
capabilities in handling drastic visual variations, occlusions, and complex
scene changes. This limitation arises from their reliance on appearance
matching, neglecting the human-like conceptual understanding of objects that
enables robust identification across temporal dynamics. Motivated by this gap,
we propose Segment Concept (SeC), a concept-driven segmentation framework that
shifts from conventional feature matching to the progressive construction and
utilization of high-level, object-centric representations. SeC employs Large
Vision-Language Models (LVLMs) to integrate visual cues across diverse frames,
constructing robust conceptual priors. During inference, SeC forms a
comprehensive semantic representation of the target based on processed frames,
realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively
balances LVLM-based semantic reasoning with enhanced feature matching,
dynamically adjusting computational efforts based on scene complexity. To
rigorously assess VOS methods in scenarios demanding high-level conceptual
reasoning and robust semantic understanding, we introduce the Semantic Complex
Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160
manually annotated multi-scenario videos designed to challenge models with
substantial appearance variations and dynamic scene transformations. In
particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,
establishing a new state-of-the-art in concept-aware video object segmentation.

</details>


### [227] [Latent Denoising Makes Good Visual Tokenizers](https://arxiv.org/abs/2507.15856)
*Jiawei Yang,Tianhong Li,Lijie Fan,Yonglong Tian,Yue Wang*

Main category: cs.CV

TL;DR: The paper proposes aligning tokenizer embeddings with the denoising objective to improve generative modeling, introducing the Latent Denoising Tokenizer (l-DeTok), which outperforms standard tokenizers.


<details>
  <summary>Details</summary>
Motivation: Modern generative models share a denoising objective, but it's unclear what makes visual tokenizers effective for this. The paper aims to align tokenizer embeddings with denoising to improve reconstruction.

Method: Introduces l-DeTok, a tokenizer trained to reconstruct clean images from corrupted latent embeddings using interpolative noise and random masking.

Result: l-DeTok consistently outperforms standard tokenizers across six generative models on ImageNet 256x256.

Conclusion: Denoising is a key design principle for tokenizers, and l-DeTok's success could inspire future tokenizer designs.

Abstract: Despite their fundamental role, it remains unclear what properties could make
visual tokenizers more effective for generative modeling. We observe that
modern generative models share a conceptually similar training objective --
reconstructing clean signals from corrupted inputs such as Gaussian noise or
masking -- a process we term denoising. Motivated by this insight, we propose
aligning tokenizer embeddings directly with the downstream denoising objective,
encouraging latent embeddings to be more easily reconstructed even when heavily
corrupted. To achieve this, we introduce the Latent Denoising Tokenizer
(l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images
from latent embeddings corrupted by interpolative noise and random masking.
Extensive experiments on ImageNet 256x256 demonstrate that our tokenizer
consistently outperforms standard tokenizers across six representative
generative models. Our findings highlight denoising as a fundamental design
principle for tokenizer development, and we hope it could motivate new
perspectives for future tokenizer design.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [228] [Real-Time Scene Reconstruction using Light Field Probes](https://arxiv.org/abs/2507.14624)
*Yaru Liu,Derek Nowrouzezahri,Morgan Mcguire*

Main category: cs.GR

TL;DR: A novel method for reconstructing large-scale scenes without explicit geometry, using probe data for efficient rendering and scalability.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiency of current neural rendering and geometry-based methods in handling large-scale, complex scenes.

Method: Uses sparse images to create multi-scale implicit representations with probe data, avoiding explicit geometry.

Result: Efficient reconstruction and rendering of complex scenes, scalable for VR/AR applications.

Conclusion: Probe-based neural representation offers a scalable, efficient alternative for large-scale scene reconstruction and rendering.

Abstract: Reconstructing photo-realistic large-scale scenes from images, for example at
city scale, is a long-standing problem in computer graphics. Neural rendering
is an emerging technique that enables photo-realistic image synthesis from
previously unobserved viewpoints; however, state-of-the-art neural rendering
methods have difficulty efficiently rendering a high complex large-scale scene
because these methods typically trade scene size, fidelity, and rendering speed
for quality. The other stream of techniques utilizes scene geometries for
reconstruction. But the cost of building and maintaining a large set of
geometry data increases as scene size grows. Our work explores novel view
synthesis methods that efficiently reconstruct complex scenes without explicit
use of scene geometries. Specifically, given sparse images of the scene
(captured from the real world), we reconstruct intermediate, multi-scale,
implicit representations of scene geometries. In this way, our method avoids
explicitly relying on scene geometry, significantly reducing the computational
cost of maintaining large 3D data. Unlike current methods, we reconstruct the
scene using a probe data structure. Probe data hold highly accurate depth
information of dense data points, enabling the reconstruction of highly complex
scenes. By reconstructing the scene using probe data, the rendering cost is
independent of the complexity of the scene. As such, our approach combines
geometry reconstruction and novel view synthesis. Moreover, when rendering
large-scale scenes, compressing and streaming probe data is more efficient than
using explicit scene geometry. Therefore, our neural representation approach
can potentially be applied to virtual reality (VR) and augmented reality (AR)
applications.

</details>


### [229] [Towards Geometric and Textural Consistency 3D Scene Generation via Single Image-guided Model Generation and Layout Optimization](https://arxiv.org/abs/2507.14841)
*Xiang Tang,Ruotong Li,Xiaopeng Fan*

Main category: cs.GR

TL;DR: A novel three-stage framework for 3D scene generation from a single RGB image, ensuring object quality and scene coherence via image-guided generation and layout optimization.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with object generation quality and scene coherence in multi-object scenarios from single images.

Method: Three-stage process: image instance segmentation/inpainting, pseudo-stereo viewpoint construction for geometry capture, and layout optimization via Chamfer distance minimization.

Result: Outperforms state-of-the-art in geometric accuracy, texture fidelity, and scene layout synthesis.

Conclusion: The proposed framework effectively addresses challenges in 3D scene generation from single images, achieving superior results.

Abstract: In recent years, 3D generation has made great strides in both academia and
industry. However, generating 3D scenes from a single RGB image remains a
significant challenge, as current approaches often struggle to ensure both
object generation quality and scene coherence in multi-object scenarios. To
overcome these limitations, we propose a novel three-stage framework for 3D
scene generation with explicit geometric representations and high-quality
textural details via single image-guided model generation and spatial layout
optimization. Our method begins with an image instance segmentation and
inpainting phase, which recovers missing details of occluded objects in the
input images, thereby achieving complete generation of foreground 3D assets.
Subsequently, our approach captures the spatial geometry of reference image by
constructing pseudo-stereo viewpoint for camera parameter estimation and scene
depth inference, while employing a model selection strategy to ensure optimal
alignment between the 3D assets generated in the previous step and the input.
Finally, through model parameterization and minimization of the Chamfer
distance between point clouds in 3D and 2D space, our approach optimizes layout
parameters to produce an explicit 3D scene representation that maintains
precise alignment with input guidance image. Extensive experiments on
multi-object scene image sets have demonstrated that our approach not only
outperforms state-of-the-art methods in terms of geometric accuracy and texture
fidelity of individual generated 3D models, but also has significant advantages
in scene layout synthesis.

</details>


### [230] [Blended Point Cloud Diffusion for Localized Text-guided Shape Editing](https://arxiv.org/abs/2507.15399)
*Etai Sella,Noam Atia,Ron Mokady,Hadar Averbuch-Elor*

Main category: cs.GR

TL;DR: An inpainting-based framework for editing 3D point cloud shapes using natural language, ensuring global coherence and local fidelity via a coordinate blending algorithm.


<details>
  <summary>Details</summary>
Motivation: Prior methods struggle with preserving global coherence while making localized edits to 3D shapes.

Method: Uses foundation 3D diffusion models and a partial conditional shape for guidance, along with an inference-time coordinate blending algorithm.

Result: Outperforms alternatives in fidelity to the original shape and adherence to textual descriptions.

Conclusion: The proposed method enables fine-grained 3D shape editing while preserving identity and coherence.

Abstract: Natural language offers a highly intuitive interface for enabling localized
fine-grained edits of 3D shapes. However, prior works face challenges in
preserving global coherence while locally modifying the input 3D shape. In this
work, we introduce an inpainting-based framework for editing shapes represented
as point clouds. Our approach leverages foundation 3D diffusion models for
achieving localized shape edits, adding structural guidance in the form of a
partial conditional shape, ensuring that other regions correctly preserve the
shape's identity. Furthermore, to encourage identity preservation also within
the local edited region, we propose an inference-time coordinate blending
algorithm which balances reconstruction of the full shape with inpainting at a
progression of noise levels during the inference process. Our coordinate
blending algorithm seamlessly blends the original shape with its edited
version, enabling a fine-grained editing of 3D shapes, all while circumventing
the need for computationally expensive and often inaccurate inversion.
Extensive experiments show that our method outperforms alternative techniques
across a wide range of metrics that evaluate both fidelity to the original
shape and also adherence to the textual description.

</details>


### [231] [ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting](https://arxiv.org/abs/2507.15454)
*Ruijie Zhu,Mulin Yu,Linning Xu,Lihan Jiang,Yixuan Li,Tianzhu Zhang,Jiangmiao Pang,Bo Dai*

Main category: cs.GR

TL;DR: ObjectGS enhances 3D Gaussian Splatting by adding semantic understanding, enabling object-level reconstruction and outperforming state-of-the-art methods in segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting lacks semantic understanding, limiting object-level perception. ObjectGS addresses this by unifying 3D reconstruction with semantic awareness.

Method: ObjectGS models objects as local anchors with neural Gaussians and shared IDs, dynamically adjusting anchors during training and using ID encoding for semantic constraints.

Result: ObjectGS excels in open-vocabulary and panoptic segmentation and integrates well with applications like mesh extraction and scene editing.

Conclusion: ObjectGS successfully combines 3D reconstruction with semantic understanding, offering improved performance and practical applications.

Abstract: 3D Gaussian Splatting is renowned for its high-fidelity reconstructions and
real-time novel view synthesis, yet its lack of semantic understanding limits
object-level perception. In this work, we propose ObjectGS, an object-aware
framework that unifies 3D scene reconstruction with semantic understanding.
Instead of treating the scene as a unified whole, ObjectGS models individual
objects as local anchors that generate neural Gaussians and share object IDs,
enabling precise object-level reconstruction. During training, we dynamically
grow or prune these anchors and optimize their features, while a one-hot ID
encoding with a classification loss enforces clear semantic constraints. We
show through extensive experiments that ObjectGS not only outperforms
state-of-the-art methods on open-vocabulary and panoptic segmentation tasks,
but also integrates seamlessly with applications like mesh extraction and scene
editing. Project page: https://ruijiezhu94.github.io/ObjectGS_page

</details>


### [232] [Gaussian Splatting with Discretized SDF for Relightable Assets](https://arxiv.org/abs/2507.15629)
*Zuo-Liang Zhu,Jian Yang,Beibei Wang*

Main category: cs.GR

TL;DR: The paper introduces a discretized SDF method for 3D Gaussian splatting to improve inverse rendering quality without extra memory or complex training.


<details>
  <summary>Details</summary>
Motivation: The discrete nature of Gaussian primitives in 3DGS complicates geometry constraints for inverse rendering, prompting the need for a simpler, memory-efficient solution.

Method: A discretized SDF is encoded within each Gaussian, linking SDF to opacity via transformation, and a projection-based consistency loss regularizes the discrete samples.

Result: The method achieves higher relighting quality, outperforms existing Gaussian-based inverse rendering methods, and avoids extra memory or complex optimization.

Conclusion: Discretized SDF within 3DGS offers a practical solution for inverse rendering, balancing quality and efficiency.

Abstract: 3D Gaussian splatting (3DGS) has shown its detailed expressive ability and
highly efficient rendering speed in the novel view synthesis (NVS) task. The
application to inverse rendering still faces several challenges, as the
discrete nature of Gaussian primitives makes it difficult to apply geometry
constraints. Recent works introduce the signed distance field (SDF) as an extra
continuous representation to regularize the geometry defined by Gaussian
primitives. It improves the decomposition quality, at the cost of increasing
memory usage and complicating training. Unlike these works, we introduce a
discretized SDF to represent the continuous SDF in a discrete manner by
encoding it within each Gaussian using a sampled value. This approach allows us
to link the SDF with the Gaussian opacity through an SDF-to-opacity
transformation, enabling rendering the SDF via splatting and avoiding the
computational cost of ray marching.The key challenge is to regularize the
discrete samples to be consistent with the underlying SDF, as the discrete
representation can hardly apply the gradient-based constraints (\eg Eikonal
loss). For this, we project Gaussians onto the zero-level set of SDF and
enforce alignment with the surface from splatting, namely a projection-based
consistency loss. Thanks to the discretized SDF, our method achieves higher
relighting quality, while requiring no extra memory beyond GS and avoiding
complex manually designed optimization. The experiments reveal that our method
outperforms existing Gaussian-based inverse rendering methods. Our code is
available at https://github.com/NK-CS-ZZL/DiscretizedSDF.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [233] [Prompt-aware of Frame Sampling for Efficient Text-Video Retrieval](https://arxiv.org/abs/2507.15491)
*Deyu Zhang,Tingting Long,Jinrui Zhang,Ligeng Chen,Ju Ren,Yaoxue Zhang*

Main category: cs.MM

TL;DR: ProCLIP is a user-centric framework for efficient text-video retrieval, balancing accuracy and computational efficiency via prompt-aware frame sampling and two-stage pruning.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to balance accuracy and efficiency in text-video retrieval, with uniform frame sampling being computationally costly and salient-frame sampling being query-agnostic.

Method: ProCLIP uses prompt-aware frame sampling to dynamically select relevant frames and a two-stage pruning strategy (coarse filtering + CLIP-powered re-ranking) for efficiency.

Result: ProCLIP reduces latency by 75.3% while maintaining competitive accuracy (R@1=49.0 on MSR-VTT).

Conclusion: ProCLIP offers a practical solution for efficient text-video retrieval on edge devices, outperforming existing methods in both speed and accuracy.

Abstract: Enabling efficient text-video retrieval on edge-end devices is critical for
real-world applications. Yet, existing methods face a critical challenge in
balancing accuracy and computational efficiency: uniform frame sampling methods
ensure content coverage but incur prohibitive computational costs, while
salient-frame sampling methods reduce overhead but suffer from query-agnostic
frame selection that biases retrieval results. To address this, we propose
ProCLIP, a user-centric framework that achieves state-of-the-art accuracy with
significantly improved efficiency. We design a prompt-aware frame sampling
strategy that dynamically guides lightweight feature extractors using textual
prompts to select semantically relevant frames, overcoming the limitations of
existing salient-frame sampling methods which rely on static, query-agnostic
selection criteria. Moreover, we adopt a two-stage candidate pruning strategy
that combines rapid coarse filtering via a lightweight module with CLIP-powered
fine-grained re-ranking, enhancing retrieval efficiency while preserving
accuracy. Experiments across benchmarks show ProCLIP achieves 75.3% latency
reduction versus baselines while maintaining competitive accuracy, i.e.,
R@1=49.0 in MSR-VTT dataset. Code is available at
https://github.com/tiffylong/ProCLIP.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [234] [Uncertainty-aware Probabilistic 3D Human Motion Forecasting via Invertible Networks](https://arxiv.org/abs/2507.14694)
*Yue Ma,Kanglei Zhou,Fuyang Yu,Frederick W. B. Li,Xiaohui Liang*

Main category: cs.RO

TL;DR: ProbHMI introduces invertible networks for 3D human motion forecasting, enabling explicit uncertainty quantification for safer human-robot collaboration.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack effective uncertainty quantification, which is critical for safety in human-robot collaboration.

Method: ProbHMI uses invertible networks to parameterize poses in a disentangled latent space and predicts future latent distributions explicitly.

Result: ProbHMI performs well on benchmarks for deterministic and diverse predictions while validating uncertainty calibration.

Conclusion: ProbHMI addresses uncertainty quantification effectively, supporting risk-aware decision-making in safety-critical applications.

Abstract: 3D human motion forecasting aims to enable autonomous applications.
Estimating uncertainty for each prediction (i.e., confidence based on
probability density or quantile) is essential for safety-critical contexts like
human-robot collaboration to minimize risks. However, existing diverse motion
forecasting approaches struggle with uncertainty quantification due to implicit
probabilistic representations hindering uncertainty modeling. We propose
ProbHMI, which introduces invertible networks to parameterize poses in a
disentangled latent space, enabling probabilistic dynamics modeling. A
forecasting module then explicitly predicts future latent distributions,
allowing effective uncertainty quantification. Evaluated on benchmarks, ProbHMI
achieves strong performance for both deterministic and diverse prediction while
validating uncertainty calibration, critical for risk-aware decision making.

</details>


### [235] [Low-Latency Event-Based Velocimetry for Quadrotor Control in a Narrow Pipe](https://arxiv.org/abs/2507.15444)
*Leonard Bauersfeld,Davide Scaramuzza*

Main category: cs.RO

TL;DR: A closed-loop control system for quadrotors hovering in narrow pipes uses real-time airflow measurements and a learning-based controller to counteract aerodynamic disturbances, enabling stable flight.


<details>
  <summary>Details</summary>
Motivation: Autonomous quadrotor flight in confined spaces like pipes is challenging due to unsteady airflow. Existing methods either require constant motion or lack stability during hovering.

Method: Developed a low-latency, event-based smoke velocimetry for airflow measurement, a disturbance estimator using a recurrent CNN, and a reinforcement learning-trained controller.

Result: The system effectively counters aerodynamic disturbances during lateral maneuvers, preventing collisions with pipe walls.

Conclusion: This is the first demonstration of real-time flow-feedback control for aerial robots, advancing research in aerodynamically complex environments and providing insights into fluid dynamics.

Abstract: Autonomous quadrotor flight in confined spaces such as pipes and tunnels
presents significant challenges due to unsteady, self-induced aerodynamic
disturbances. Very recent advances have enabled flight in such conditions, but
they either rely on constant motion through the pipe to mitigate airflow
recirculation effects or suffer from limited stability during hovering. In this
work, we present the first closed-loop control system for quadrotors for
hovering in narrow pipes that leverages real-time flow field measurements. We
develop a low-latency, event-based smoke velocimetry method that estimates
local airflow at high temporal resolution. This flow information is used by a
disturbance estimator based on a recurrent convolutional neural network, which
infers force and torque disturbances in real time. The estimated disturbances
are integrated into a learning-based controller trained via reinforcement
learning. The flow-feedback control proves particularly effective during
lateral translation maneuvers in the pipe cross-section. There, the real-time
disturbance information enables the controller to effectively counteract
transient aerodynamic effects, thereby preventing collisions with the pipe
wall. To the best of our knowledge, this work represents the first
demonstration of an aerial robot with closed-loop control informed by real-time
flow field measurements. This opens new directions for research on flight in
aerodynamically complex environments. In addition, our work also sheds light on
the characteristic flow structures that emerge during flight in narrow,
circular pipes, providing new insights at the intersection of robotics and
fluid dynamics.

</details>


### [236] [GR-3 Technical Report](https://arxiv.org/abs/2507.15493)
*Chilam Cheang,Sijin Chen,Zhongren Cui,Yingdong Hu,Liqun Huang,Tao Kong,Hang Li,Yifeng Li,Yuxiao Liu,Xiao Ma,Hao Niu,Wenxuan Ou,Wanli Peng,Zeyu Ren,Haixin Shi,Jiawen Tian,Hongtao Wu,Xin Xiao,Yuyang Xiao,Jiafeng Xu,Yichu Yang*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We report our recent progress towards building generalist robot policies, the
development of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.
It showcases exceptional capabilities in generalizing to novel objects,
environments, and instructions involving abstract concepts. Furthermore, it can
be efficiently fine-tuned with minimal human trajectory data, enabling rapid
and cost-effective adaptation to new settings. GR-3 also excels in handling
long-horizon and dexterous tasks, including those requiring bi-manual
manipulation and mobile movement, showcasing robust and reliable performance.
These capabilities are achieved through a multi-faceted training recipe that
includes co-training with web-scale vision-language data, efficient fine-tuning
from human trajectory data collected via VR devices, and effective imitation
learning with robot trajectory data. In addition, we introduce ByteMini, a
versatile bi-manual mobile robot designed with exceptional flexibility and
reliability, capable of accomplishing a wide range of tasks when integrated
with GR-3. Through extensive real-world experiments, we show GR-3 surpasses the
state-of-the-art baseline method, $\pi_0$, on a wide variety of challenging
tasks. We hope GR-3 can serve as a step towards building generalist robots
capable of assisting humans in daily life.

</details>


### [237] [Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers](https://arxiv.org/abs/2507.15833)
*Ian Chuang,Andrew Lee,Dechen Gao,Jinyu Zou,Iman Soltani*

Main category: cs.RO

TL;DR: The paper explores integrating human-like active gaze into robotic policies to improve efficiency and performance, using foveated image processing and Vision Transformers (ViTs).


<details>
  <summary>Details</summary>
Motivation: Human vision is task-driven and efficient, while robotic systems often process images uniformly. The work aims to bridge this gap by emulating human gaze for better robotic vision.

Method: The authors introduce a framework for collecting eye-tracking and robot demonstration data, integrate gaze into ViTs via foveated patch tokenization, and explore two gaze imitation/prediction approaches.

Result: Foveated robot vision reduces computational overhead and enhances performance in precision tasks and robustness to distractors.

Conclusion: Human-inspired visual processing provides a valuable inductive bias for robotic vision systems.

Abstract: Human vision is a highly active process driven by gaze, which directs
attention and fixation to task-relevant regions and dramatically reduces visual
processing. In contrast, robot learning systems typically rely on passive,
uniform processing of raw camera images. In this work, we explore how
incorporating human-like active gaze into robotic policies can enhance both
efficiency and performance. We build on recent advances in foveated image
processing and apply them to an Active Vision robot system that emulates both
human head movement and eye tracking. Extending prior work on the AV-ALOHA
robot simulation platform, we introduce a framework for simultaneously
collecting eye-tracking data and robot demonstrations from a human operator as
well as a simulation benchmark and dataset for training robot policies that
incorporate human gaze. Given the widespread use of Vision Transformers (ViTs)
in robot learning, we integrate gaze information into ViTs using a foveated
patch tokenization scheme inspired by recent work in image segmentation.
Compared to uniform patch tokenization, this significantly reduces the number
of tokens-and thus computation-without sacrificing visual fidelity near regions
of interest. We also explore two approaches to gaze imitation and prediction
from human data. The first is a two-stage model that predicts gaze to guide
foveation and action; the second integrates gaze into the action space,
allowing the policy to jointly predict gaze and actions end-to-end. Our results
show that our method for foveated robot vision not only drastically reduces
computational overhead, but also improves performance for high precision tasks
and robustness to unseen distractors. Together, these findings suggest that
human-inspired visual processing offers a useful inductive bias for robotic
vision systems. https://ian-chuang.github.io/gaze-av-aloha/

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [238] [Self-Supervised Distillation of Legacy Rule-Based Methods for Enhanced EEG-Based Decision-Making](https://arxiv.org/abs/2507.14542)
*Yipeng Zhang,Yuanyi Ding,Chenda Duan,Atsuro Daida,Hiroki Nariai,Vwani Roychowdhury*

Main category: cs.CE

TL;DR: The paper introduces SS2LD, a self-supervised framework to refine HFO detection in epilepsy treatment by leveraging legacy detectors and a VAE for improved precision without heavy reliance on labeled data.


<details>
  <summary>Details</summary>
Motivation: Traditional HFO detectors have high false positives, and supervised methods require scarce labeled data. The lack of consensus on pathological HFOs further complicates refinement.

Method: SS2LD uses a VAE for morphological pre-training, clusters latent representations for weak supervision, and trains a classifier on real and augmented data.

Result: SS2LD outperforms state-of-the-art methods on multi-institutional datasets, offering scalable and label-efficient HFO detection.

Conclusion: SS2LD provides a clinically effective solution for refining HFO detection, reducing reliance on labeled data and improving precision.

Abstract: High-frequency oscillations (HFOs) in intracranial Electroencephalography
(iEEG) are critical biomarkers for localizing the epileptogenic zone in
epilepsy treatment. However, traditional rule-based detectors for HFOs suffer
from unsatisfactory precision, producing false positives that require
time-consuming manual review. Supervised machine learning approaches have been
used to classify the detection results, yet they typically depend on labeled
datasets, which are difficult to acquire due to the need for specialized
expertise. Moreover, accurate labeling of HFOs is challenging due to low
inter-rater reliability and inconsistent annotation practices across
institutions. The lack of a clear consensus on what constitutes a pathological
HFO further challenges supervised refinement approaches. To address this, we
leverage the insight that legacy detectors reliably capture clinically relevant
signals despite their relatively high false positive rates. We thus propose the
Self-Supervised to Label Discovery (SS2LD) framework to refine the large set of
candidate events generated by legacy detectors into a precise set of
pathological HFOs. SS2LD employs a variational autoencoder (VAE) for
morphological pre-training to learn meaningful latent representation of the
detected events. These representations are clustered to derive weak supervision
for pathological events. A classifier then uses this supervision to refine
detection boundaries, trained on real and VAE-augmented data. Evaluated on
large multi-institutional interictal iEEG datasets, SS2LD outperforms
state-of-the-art methods. SS2LD offers a scalable, label-efficient, and
clinically effective strategy to identify pathological HFOs using legacy
detectors.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [239] [On Splitting Lightweight Semantic Image Segmentation for Wireless Communications](https://arxiv.org/abs/2507.14199)
*Ebrahim Abu-Helalah,Jordi Serra,Jordi Perez-Romero*

Main category: cs.NI

TL;DR: A novel approach splits semantic image segmentation between transmitter and receiver, reducing bandwidth and computational load while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between computational efficiency, bandwidth, and accuracy in semantic communication for image segmentation, especially in resource-limited environments.

Method: Proposes splitting the semantic image segmentation process between a constrained transmitter and receiver to save bandwidth and reduce computational load.

Result: Simulations show up to 72% reduction in bit rate and over 19% decrease in transmitter computational load without sacrificing accuracy.

Conclusion: The technique is promising for communication systems, particularly 6G, due to its efficiency and resource-saving benefits.

Abstract: Semantic communication represents a promising technique towards reducing
communication costs, especially when dealing with image segmentation, but it
still lacks a balance between computational efficiency and bandwidth
requirements while maintaining high image segmentation accuracy, particularly
in resource-limited environments and changing channel conditions. On the other
hand, the more complex and larger semantic image segmentation models become,
the more stressed the devices are when processing data. This paper proposes a
novel approach to implementing semantic communication based on splitting the
semantic image segmentation process between a resource constrained transmitter
and the receiver. This allows saving bandwidth by reducing the transmitted data
while maintaining the accuracy of the semantic image segmentation.
Additionally, it reduces the computational requirements at the resource
constrained transmitter compared to doing all the semantic image segmentation
in the transmitter. The proposed approach is evaluated by means of
simulation-based experiments in terms of different metrics such as
computational resource usage, required bit rate and segmentation accuracy. The
results when comparing the proposal with the full semantic image segmentation
in the transmitter show that up to 72% of the bit rate was reduced in the
transmission process. In addition, the computational load of the transmitter is
reduced by more than 19%. This reflects the interest of this technique for its
application in communication systems, particularly in the upcoming 6G systems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [240] [ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation](https://arxiv.org/abs/2507.14201)
*Yiran Wu,Mauricio Velazco,Andrew Zhao,Manuel Raúl Meléndez Luján,Srisuma Movva,Yogesh K Roy,Quang Nguyen,Roberto Rodriguez,Qingyun Wu,Michael Albada,Julia Kiseleva,Anand Mudgerikar*

Main category: cs.CR

TL;DR: ExCyTIn-Bench is the first benchmark for evaluating LLM agents in cyber threat investigation using security questions derived from investigation graphs. It includes a dataset of 8 simulated attacks, 57 log tables, and 589 questions, with explainable ground truth answers. Initial experiments show low model performance, indicating room for improvement.


<details>
  <summary>Details</summary>
Motivation: Real-world cyber threat investigation is complex, involving heterogeneous alerts and multi-hop evidence chains. LLM-based agents offer a promising solution, but lack benchmarks for development and evaluation.

Method: Constructed a dataset from a controlled Azure tenant with simulated attacks, log tables, and LLM-generated questions anchored to investigation graphs. Questions use start nodes as context and end nodes as answers.

Result: Experiments show low average reward (0.249) across models, with the best at 0.368, highlighting task difficulty.

Conclusion: ExCyTIn-Bench provides a reusable, extensible benchmark for LLM agents in cyber threat investigation, with significant potential for future research.

Abstract: We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x on
the task of Cyber Threat Investigation through security questions derived from
investigation graphs. Real-world security analysts must sift through a large
number of heterogeneous alert signals and security logs, follow multi-hop
chains of evidence, and compile an incident report. With the developments of
LLMs, building LLM-based agents for automatic thread investigation is a
promising direction. To assist the development and evaluation of LLM agents, we
construct a dataset from a controlled Azure tenant that covers 8 simulated
real-world multi-step attacks, 57 log tables from Microsoft Sentinel and
related services, and 589 automatically generated questions. We leverage
security logs extracted with expert-crafted detection logic to build threat
investigation graphs, and then generate questions with LLMs using paired nodes
on the graph, taking the start node as background context and the end node as
answer. Anchoring each question to these explicit nodes and edges not only
provides automatic, explainable ground truth answers but also makes the
pipeline reusable and readily extensible to new logs. This also enables the
automatic generation of procedural tasks with verifiable rewards, which can be
naturally extended to training agents via reinforcement learning. Our
comprehensive experiments with different models confirm the difficulty of the
task: with the base setting, the average reward across all evaluated models is
0.249, and the best achieved is 0.368, leaving substantial headroom for future
research. Code and data are coming soon!

</details>


### [241] [Breaking the Illusion of Security via Interpretation: Interpretable Vision Transformer Systems under Attack](https://arxiv.org/abs/2507.14248)
*Eldor Abdukhamidov,Mohammed Abuhamad,Simon S. Woo,Hyoungshick Kim,Tamer Abuhmed*

Main category: cs.CR

TL;DR: AdViT is an adversarial attack method targeting Vision Transformer (ViT) models and their interpretation models, achieving high success rates in misleading both while maintaining accurate interpretations.


<details>
  <summary>Details</summary>
Motivation: Despite ViT models being considered secure, adversarial attacks can have severe consequences. Existing research overlooks the impact on model interpretations, which can aid in detecting adversarial examples.

Method: Proposes AdViT, an attack method that generates adversarial examples to deceive both ViT models and their interpretation models. Tests on various models and interpreters in white-box and black-box scenarios.

Result: AdViT achieves 100% attack success rate, with up to 98% misclassification confidence in white-box and 76% in black-box scenarios, while maintaining accurate interpretations.

Conclusion: AdViT demonstrates the vulnerability of ViT models even with interpretation models, highlighting the need for improved defenses against such attacks.

Abstract: Vision transformer (ViT) models, when coupled with interpretation models, are
regarded as secure and challenging to deceive, making them well-suited for
security-critical domains such as medical applications, autonomous vehicles,
drones, and robotics. However, successful attacks on these systems can lead to
severe consequences. Recent research on threats targeting ViT models primarily
focuses on generating the smallest adversarial perturbations that can deceive
the models with high confidence, without considering their impact on model
interpretations. Nevertheless, the use of interpretation models can effectively
assist in detecting adversarial examples. This study investigates the
vulnerability of transformer models to adversarial attacks, even when combined
with interpretation models. We propose an attack called "AdViT" that generates
adversarial examples capable of misleading both a given transformer model and
its coupled interpretation model. Through extensive experiments on various
transformer models and two transformer-based interpreters, we demonstrate that
AdViT achieves a 100% attack success rate in both white-box and black-box
scenarios. In white-box scenarios, it reaches up to 98% misclassification
confidence, while in black-box scenarios, it reaches up to 76%
misclassification confidence. Remarkably, AdViT consistently generates accurate
interpretations in both scenarios, making the adversarial examples more
difficult to detect.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [242] [Assessing the Reliability of Large Language Models for Deductive Qualitative Coding: A Comparative Study of ChatGPT Interventions](https://arxiv.org/abs/2507.14384)
*Angjelin Hila,Elliott Hauser*

Main category: cs.HC

TL;DR: The study explores using ChatGPT for deductive qualitative coding, testing four methods, with Step-by-Step Task Decomposition showing the highest reliability.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored potential of LLMs in deductive classification tasks aligned with human-coded schemes.

Method: Tested four intervention methods (zero-shot, few-shot, definition-based, Step-by-Step Task Decomposition) on U.S. Supreme Court case summaries using the CAP Master Codebook. Evaluated performance with classification metrics and construct validity tests.

Result: Step-by-Step Task Decomposition achieved the highest reliability (accuracy = 0.775, kappa = 0.744, alpha = 0.746). Intervention strategies significantly influenced classification behavior.

Conclusion: Targeted interventions enable LLMs to achieve reliability suitable for rigorous qualitative coding workflows.

Abstract: In this study, we investigate the use of large language models (LLMs),
specifically ChatGPT, for structured deductive qualitative coding. While most
current research emphasizes inductive coding applications, we address the
underexplored potential of LLMs to perform deductive classification tasks
aligned with established human-coded schemes. Using the Comparative Agendas
Project (CAP) Master Codebook, we classified U.S. Supreme Court case summaries
into 21 major policy domains. We tested four intervention methods: zero-shot,
few-shot, definition-based, and a novel Step-by-Step Task Decomposition
strategy, across repeated samples. Performance was evaluated using standard
classification metrics (accuracy, F1-score, Cohen's kappa, Krippendorff's
alpha), and construct validity was assessed using chi-squared tests and
Cramer's V. Chi-squared and effect size analyses confirmed that intervention
strategies significantly influenced classification behavior, with Cramer's V
values ranging from 0.359 to 0.613, indicating moderate to strong shifts in
classification patterns. The Step-by-Step Task Decomposition strategy achieved
the strongest reliability (accuracy = 0.775, kappa = 0.744, alpha = 0.746),
achieving thresholds for substantial agreement. Despite the semantic ambiguity
within case summaries, ChatGPT displayed stable agreement across samples,
including high F1 scores in low-support subclasses. These findings demonstrate
that with targeted, custom-tailored interventions, LLMs can achieve reliability
levels suitable for integration into rigorous qualitative coding workflows.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [243] [Hear Your Code Fail, Voice-Assisted Debugging for Python](https://arxiv.org/abs/2507.15007)
*Sayed Mahbub Hasan Amiri,Md. Mainul Islam,Mohammad Shakhawat Hossen,Sayed Majhab Hasan Amiri,Mohammad Shawkat Ali Mamun,Sk. Humaun Kabir,Naznin Akter*

Main category: cs.PL

TL;DR: A voice-assisted Python debugging plugin converts silent errors into audible diagnostics, reducing cognitive load by 37% and speeding up error identification by 78%. It uses text-to-speech and GUI visualization, achieving low latency and broad compatibility.


<details>
  <summary>Details</summary>
Motivation: To improve debugging accessibility and efficiency, especially for visually impaired users and novice programmers, by transforming silent errors into audible and visual feedback.

Method: Implemented a global exception hook with pyttsx3 for text-to-speech and Tkinter for GUI visualization, providing multimodal error feedback.

Result: Empirical results show 37% reduced cognitive load, 78% faster error identification, sub-1.2s voice latency, and under 18% CPU overhead. Compatible with Python 3.7+ on major platforms.

Conclusion: The plugin enhances debugging accessibility and efficiency, with potential for educational use and future integration of GPT-based repairs and multilingual support.

Abstract: This research introduces an innovative voice-assisted debugging plugin for
Python that transforms silent runtime errors into actionable audible
diagnostics. By implementing a global exception hook architecture with pyttsx3
text-to-speech conversion and Tkinter-based GUI visualization, the solution
delivers multimodal error feedback through parallel auditory and visual
channels. Empirical evaluation demonstrates 37% reduced cognitive load (p<0.01,
n=50) compared to traditional stack-trace debugging, while enabling 78% faster
error identification through vocalized exception classification and
contextualization. The system achieves sub-1.2 second voice latency with under
18% CPU overhead during exception handling, vocalizing error types and
consequences while displaying interactive tracebacks with documentation deep
links. Criteria validate compatibility across Python 3.7+ environments on
Windows, macOS, and Linux platforms. Needing only two lines of integration
code, the plugin significantly boosts availability for aesthetically impaired
designers and supports multitasking workflows through hands-free error medical
diagnosis. Educational applications show particular promise, with pilot studies
indicating 45% faster debugging skill acquisition among novice programmers.
Future development will incorporate GPT-based repair suggestions and real-time
multilingual translation to further advance auditory debugging paradigms. The
solution represents a fundamental shift toward human-centric error diagnostics,
bridging critical gaps in programming accessibility while establishing new
standards for cognitive efficiency in software development workflows.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [244] [APTx Neuron: A Unified Trainable Neuron Architecture Integrating Activation and Computation](https://arxiv.org/abs/2507.14270)
*Ravin Kumar*

Main category: cs.NE

TL;DR: The APTx Neuron integrates activation and transformation into a single trainable unit, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: To simplify neural architectures by unifying activation and linear transformation, reducing computational overhead.

Method: Derives from the APTx activation function, using a trainable expression combining non-linear and linear operations.

Result: Achieves 96.69% test accuracy on MNIST with 332K parameters in 20 epochs.

Conclusion: The APTx Neuron offers superior expressiveness and efficiency, suggesting a new direction in neuron design.

Abstract: We propose the APTx Neuron, a novel, unified neural computation unit that
integrates non-linear activation and linear transformation into a single
trainable expression. The APTx Neuron is derived from the APTx activation
function, thereby eliminating the need for separate activation layers and
making the architecture both computationally efficient and elegant. The
proposed neuron follows the functional form $y = \sum_{i=1}^{n} ((\alpha_i +
\tanh(\beta_i x_i)) \cdot \gamma_i x_i) + \delta$, where all parameters
$\alpha_i$, $\beta_i$, $\gamma_i$, and $\delta$ are trainable. We validate our
APTx Neuron-based architecture on the MNIST dataset, achieving up to 96.69\%
test accuracy in just 20 epochs using approximately 332K trainable parameters.
The results highlight the superior expressiveness and computational efficiency
of the APTx Neuron compared to traditional neurons, pointing toward a new
paradigm in unified neuron design and the architectures built upon it.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [245] [A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering](https://arxiv.org/abs/2507.14179)
*Nobel Dhar,Bobin Deng,Md Romyull Islam,Xinyue Zhang,Kazi Fahim Ahmad Nasif,Kun Suo*

Main category: cs.LG

TL;DR: A clustering-based framework is proposed to efficiently predict and utilize activation sparsity in LLMs, reducing computational costs while maintaining model quality.


<details>
  <summary>Details</summary>
Motivation: Activation sparsity in LLMs offers computational savings, but predicting neuron-level activation is impractical due to scale.

Method: Clusters similar activation patterns into representative groups, reducing prediction complexity.

Result: Achieves 79.34% clustering precision and a low PPL score of 12.49, balancing efficiency and quality.

Conclusion: The framework enables scalable activation prediction, improving sparse computation efficiency for future LLMs.

Abstract: Large Language Models (LLMs) exhibit significant activation sparsity, where
only a subset of neurons are active for a given input. Although this sparsity
presents opportunities to reduce computational cost, efficiently utilizing it
requires predicting activation patterns in a scalable manner. However, direct
prediction at the neuron level is computationally expensive due to the vast
number of neurons in modern LLMs. To enable efficient prediction and
utilization of activation sparsity, we propose a clustering-based activation
pattern compression framework. Instead of treating each neuron independently,
we group similar activation patterns into a small set of representative
clusters. Our method achieves up to 79.34% clustering precision, outperforming
standard binary clustering approaches while maintaining minimal degradation in
perplexity (PPL) scores. With a sufficiently large number of clusters, our
approach attains a PPL score as low as 12.49, demonstrating its effectiveness
in preserving model quality while reducing computational overhead. By
predicting cluster assignments rather than individual neuron states, future
models can efficiently infer activation patterns from pre-computed centroids.
We detail the clustering algorithm, analyze its effectiveness in capturing
meaningful activation structures, and demonstrate its potential to improve
sparse computation efficiency. This clustering-based formulation serves as a
foundation for future work on activation pattern prediction, paving the way for
efficient inference in large-scale language models.

</details>


### [246] [LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models](https://arxiv.org/abs/2507.14204)
*Dachuan Shi,Yonggan Fu,Xiangchi Yuan,Zhongzhi Yu,Haoran You,Sixu Li,Xin Dong,Jan Kautz,Pavlo Molchanov,Yingyan,Lin*

Main category: cs.LG

TL;DR: LaCache is a training-free KV cache optimization method for LLMs, enhancing long-range capabilities and continuous generation without OOM by using a ladder-shaped KV cache pattern and iterative compaction.


<details>
  <summary>Details</summary>
Motivation: Addressing the efficiency bottleneck caused by increasing KV pairs in LLMs as sequence lengths grow, while maintaining robust long-range capabilities and avoiding OOM errors.

Method: LaCache integrates a ladder-shaped KV cache pattern for extended dependency capture and an iterative compaction mechanism for dynamic cache compression.

Result: Experiments show LaCache effectively boosts LLMs' long-range capabilities across tasks and benchmarks.

Conclusion: LaCache provides an efficient and accurate solution for long-range modeling in LLMs, validated by consistent experimental results.

Abstract: Recent advancements in Large Language Models (LLMs) have spurred interest in
numerous applications requiring robust long-range capabilities, essential for
processing extensive input contexts and continuously generating extended
outputs. As sequence lengths increase, the number of Key-Value (KV) pairs in
LLMs escalates, creating a significant efficiency bottleneck. In this paper, we
propose a new KV cache optimization paradigm called LaCache, a training-free
method for efficient and accurate generative inference of LLMs. LaCache enables
LLMs to simultaneously address both of the critical challenges in long-range
modeling: robust long-range capabilities and continuous generation without
running out-of-memory (OOM). Specifically, LaCache integrates two key
innovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only
sequentially (left-to-right within each layer) but also across layers (from
shallow to deep), providing an extended span for capturing long-range
dependencies under a fixed storage budget, thereby boosting long-range
capabilities; and (2) an iterative compaction mechanism that progressively
compresses older caches, freeing up space for new tokens within a fixed cache
size. This token distance-based dynamic compression enables more effective
continuous generation under constrained cache budgets. Experiments across
various tasks, benchmarks, and LLM models consistently validate LaCache's
effectiveness in enhancing LLMs' long-range capabilities. Our code is available
at https://github.com/GATECH-EIC/LaCache.

</details>


### [247] [Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers](https://arxiv.org/abs/2507.14353)
*Harsh Nilesh Pathak,Randy Paffenroth*

Main category: cs.LG

TL;DR: Solo Connection is a PEFT method adapting decoder-block representations, outperforming LoRA with fewer parameters and inspired by homotopy theory.


<details>
  <summary>Details</summary>
Motivation: To improve fine-tuning efficiency and stability by adapting decoder-block representations rather than individual weight matrices, inspired by homotopy theory.

Method: Introduces Solo Connection, a trainable linear transformation interpolating between zero and task-specific representations, focusing on long skip connections between decoder blocks.

Result: Outperforms LoRA on E2E benchmarks, reduces trainable parameters by 59% vs. LoRA and >99% vs. full fine-tuning.

Conclusion: Solo Connection offers a more efficient and stable fine-tuning approach, especially beneficial for larger models with many decoder blocks.

Abstract: Parameter efficient fine tuning (PEFT) is a versatile and extensible approach
for adapting a Large Language Model (LLM) for newer tasks. One of the most
prominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on
adjusting the attention weight matrices within individual decoder blocks of a
Generative Pre trained Transformer (GPT2). In contrast, we introduce Solo
Connection a novel method that adapts the representation at the decoder-block
level rather than modifying individual weight matrices. Not only does Solo
Connection outperform LoRA on E2E natural language generation benchmarks, but
it also reduces the number of trainable parameters by 59% relative to LoRA and
by more than 99% compared to full fine-tuning of GPT2, an early version of
Large Language Models (LLMs). Solo Connection is also motivated by homotopy
theory: we introduce a trainable linear transformation that gradually
interpolates between a zero vector and the task-specific representation,
enabling smooth and stable adaptation over time. While skip connections in the
original 12 layer GPT2 are typically confined to individual decoder blocks,
subsequent GPT2 variants scale up to 48 layers, and even larger language models
can include 128 or more decoder blocks. These expanded architectures underscore
the need to revisit how skip connections are employed during fine-tuning. This
paper focuses on long skip connections that link outputs of different decoder
blocks, potentially enhancing the model's ability to adapt to new tasks while
leveraging pre-trained knowledge.

</details>


### [248] [It's Not That Simple. An Analysis of Simple Test-Time Scaling](https://arxiv.org/abs/2507.14419)
*Guojun Wu*

Main category: cs.LG

TL;DR: Simple test-time scaling is mainly effective when scaling down by enforcing a maximum length, while scaling up via 'Wait' appending causes inconsistencies. Fine-tuning on long CoT data has little impact. o1-like models naturally scale up compute during RL, outperforming simple scaling methods.


<details>
  <summary>Details</summary>
Motivation: To analyze the effectiveness of simple test-time scaling methods and compare them with the natural scaling behavior of o1-like models.

Method: Examined scaling down (enforcing max length) and scaling up (appending 'Wait') in test-time compute, and compared with o1-like models' natural scaling during RL.

Result: Scaling down works, but scaling up leads to inconsistencies. Fine-tuning on long CoT data is ineffective. o1-like models outperform when scaling up naturally.

Conclusion: Simple test-time scaling can replicate scaling behavior but limits performance. The goal should be unlocking higher performance, not just mimicking scaling.

Abstract: Prior work proposed simple test-time scaling, a method for replicating this
scaling behavior with models distilled from o1-like models by manually
controlling test-time compute: either scaling down by enforcing a maximum
length or scaling up by iteratively appending "Wait" when the model is about to
terminate its generation. This paper presents an analysis of simple test-time
scaling and finds that the scaling behavior is largely attributed to scaling
down by enforcing a maximum length. In contrast, fine-tuning on long CoT data
distilled from o1-like models has no significant impact on scaling behavior,
and scaling up by appending "Wait" leads to inconsistencies, as the model may
oscillate between solutions. A key distinction exists between scaling down by
enforcing a maximum length and scaling up test-time compute in o1-like models,
such as DeepSeek-R1\@. These models are typically allowed to utilize as much
compute as needed, with the only constraint being the model's maximum supported
length. By learning to naturally scale up test-time compute during
reinforcement learning, o1-like models surpass their peak performance when
scaling up. In contrast, simple test-time scaling progressively imposes a lower
upper limit on model performance as it scales down. While replicating the
test-time scaling behavior of o1 models can be straightforward by scaling down,
it is crucial to recognize that the goal of scaling test-time compute is to
unlock higher performance -- beyond what the model could originally achieve --
rather than merely reproducing the appearance of scaling behavior.

</details>


### [249] [GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks](https://arxiv.org/abs/2507.14679)
*Zixin Xu,Zhijie Wang,Zhiyuan Pan*

Main category: cs.LG

TL;DR: GCC-Spam is a novel spam-text detection framework combining character similarity networks, contrastive learning, and GANs to address adversarial spam strategies and data scarcity, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: The rise of spam text poses risks like information leakage and social instability, requiring robust detection methods despite adversarial tactics and limited labeled data.

Method: GCC-Spam integrates a character similarity network for obfuscation resistance, contrastive learning for better discrimination, and GANs to generate pseudo-spam samples, addressing data scarcity.

Result: The framework achieves higher detection rates with fewer labeled examples, outperforming baseline methods in real-world experiments.

Conclusion: GCC-Spam effectively tackles spam detection challenges, offering improved accuracy and robustness against adversarial strategies and data limitations.

Abstract: The exponential growth of spam text on the Internet necessitates robust
detection mechanisms to mitigate risks such as information leakage and social
instability. This work addresses two principal challenges: adversarial
strategies employed by spammers and the scarcity of labeled data. We propose a
novel spam-text detection framework GCC-Spam, which integrates three core
innovations. First, a character similarity network captures orthographic and
phonetic features to counter character-obfuscation attacks and furthermore
produces sentence embeddings for downstream classification. Second, contrastive
learning enhances discriminability by optimizing the latent-space distance
between spam and normal texts. Third, a Generative Adversarial Network (GAN)
generates realistic pseudo-spam samples to alleviate data scarcity while
improving model robustness and classification accuracy. Extensive experiments
on real-world datasets demonstrate that our model outperforms baseline
approaches, achieving higher detection rates with significantly fewer labeled
examples.

</details>


### [250] [The Invisible Leash: Why RLVR May Not Escape Its Origin](https://arxiv.org/abs/2507.14843)
*Fang Wu,Weihao Xuan,Ximing Lu,Zaid Harchaoui,Yejin Choi*

Main category: cs.LG

TL;DR: RLVR improves precision but may limit exploration and original solutions, constrained by the base model's support.


<details>
  <summary>Details</summary>
Motivation: To investigate whether RLVR expands reasoning boundaries or just amplifies high-reward outputs.

Method: Theoretical analysis and empirical experiments on RLVR's constraints and tradeoffs.

Result: RLVR improves pass@1 but shrinks empirical support, reducing access to correct answers.

Conclusion: RLVR has limits in extending reasoning; future innovations like exploration mechanisms are needed.

Abstract: Recent advances in large reasoning models highlight Reinforcement Learning
with Verifiable Rewards (RLVR) as a promising method for enhancing AI's
capabilities, particularly in solving complex logical tasks. However, it
remains unclear whether RLVR truly expands a model's reasoning boundary or
merely amplifies high-reward outputs that the base model already knows for
improved precision. This study presents a theoretical and empirical
investigation that provides fresh insights into the potential limits of RLVR.
First, we offer a new theoretical perspective that RLVR is constrained by the
base model's support-unable to sample solutions with zero initial
probability-and operates as a conservative reweighting mechanism that may
restrict the discovery of entirely original solutions. We also identify an
entropy-reward tradeoff: while RLVR reliably enhances precision, it may
progressively narrow exploration and potentially overlook correct yet
underrepresented solutions. Extensive empirical experiments validate that while
RLVR consistently improves pass@1, the shrinkage of empirical support generally
outweighs the expansion of empirical support under larger sampling budgets,
failing to recover correct answers that were previously accessible to the base
model. Interestingly, we also observe that while RLVR sometimes increases
token-level entropy, resulting in greater uncertainty at each generation step,
answer-level entropy declines, indicating that these seemingly more uncertain
paths ultimately converge onto a smaller set of distinct answers. Taken
together, these findings reveal potential limits of RLVR in extending reasoning
horizons. Breaking this invisible leash may require future algorithmic
innovations such as explicit exploration mechanisms or hybrid strategies that
seed probability mass into underrepresented solution regions.

</details>


### [251] [Long-Short Distance Graph Neural Networks and Improved Curriculum Learning for Emotion Recognition in Conversation](https://arxiv.org/abs/2507.15205)
*Xinran Li,Xiujuan Xu,Jiaqi Qiao*

Main category: cs.LG

TL;DR: The paper introduces LSDGNN, a multimodal approach for Emotion Recognition in Conversation (ERC), using long- and short-distance graph neural networks with a Differential Regularizer and BiAffine Module for feature interaction. It also proposes Improved Curriculum Learning (ICL) to handle data imbalance, achieving superior results on IEMOCAP and MELD datasets.


<details>
  <summary>Details</summary>
Motivation: ERC is challenging due to the complexity of multimodal interactions and data imbalance. The paper aims to improve performance by capturing distinct long- and short-distance utterance features and addressing imbalance issues.

Method: LSDGNN combines long- and short-distance graph neural networks (DAG-based) with a Differential Regularizer and BiAffine Module for feature interaction. ICL uses a 'weighted emotional shift' metric and difficulty measurer for balanced training.

Result: The model outperforms benchmarks on IEMOCAP and MELD datasets, demonstrating effectiveness in ERC.

Conclusion: LSDGNN and ICL provide a robust solution for ERC, improving feature representation and handling data imbalance.

Abstract: Emotion Recognition in Conversation (ERC) is a practical and challenging
task. This paper proposes a novel multimodal approach, the Long-Short Distance
Graph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it
constructs a long-distance graph neural network and a short-distance graph
neural network to obtain multimodal features of distant and nearby utterances,
respectively. To ensure that long- and short-distance features are as distinct
as possible in representation while enabling mutual influence between the two
modules, we employ a Differential Regularizer and incorporate a BiAffine Module
to facilitate feature interaction. In addition, we propose an Improved
Curriculum Learning (ICL) to address the challenge of data imbalance. By
computing the similarity between different emotions to emphasize the shifts in
similar emotions, we design a "weighted emotional shift" metric and develop a
difficulty measurer, enabling a training process that prioritizes learning easy
samples before harder ones. Experimental results on the IEMOCAP and MELD
datasets demonstrate that our model outperforms existing benchmarks.

</details>


### [252] [Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2507.15507)
*Johannes Ackermann,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: The paper addresses overoptimization in RLHF by proposing OCRM, an off-policy correction method for reward models, improving policy alignment with human preferences.


<details>
  <summary>Details</summary>
Motivation: Overoptimization in RLHF causes reward models to become inaccurate as the LM's responses diverge from training data, misaligning with human preferences.

Method: Proposes Off-Policy Corrected Reward Modeling (OCRM), which iteratively corrects the reward model using importance weighting without new labels.

Result: OCRM outperforms standard RLHF methods, yielding a more accurate reward model and improved policy in summarization and chatbot tasks.

Conclusion: OCRM effectively mitigates overoptimization by addressing distribution shift, enhancing the alignment of learned behavior with human preferences.

Abstract: Reinforcement Learning from Human Feedback (RLHF) allows us to train models,
such as language models (LMs), to follow complex human preferences. In RLHF for
LMs, we first train an LM using supervised fine-tuning, sample pairs of
responses, obtain human feedback, and use the resulting data to train a reward
model (RM). RL methods are then used to train the LM to maximize the reward
given by the RM. As training progresses, the responses generated by the LM no
longer resemble the responses seen by the RM during training, leading to the RM
becoming inaccurate. The score given by the RM keeps increasing, but the
learned behavior no longer matches the human preferences. This issue is known
as overoptimization. We investigate overoptimization from the point of view of
distribution shift and show that the shift results in an inconsistent estimate
of the RM parameters, leading to an inconsistent estimate of the policy
gradient. We propose Off-Policy Corrected Reward Modeling (OCRM), which
iteratively off-policy corrects the RM using importance weighting, without
requiring new labels or samples. This results in a more accurate RM, which
empirically leads to an improved final policy. We validate our approach in
experiments with summarization and chatbot datasets and show that it performs
significantly better than standard RLHF methods and baselines. Our
implementation is available at
https://github.com/JohannesAck/OffPolicyCorrectedRewardModeling

</details>


### [253] [Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training](https://arxiv.org/abs/2507.15640)
*Kailai Yang,Xiao Liu,Lei Ji,Hao Li,Yeyun Gong,Peng Cheng,Mao Yang*

Main category: cs.LG

TL;DR: Proposes Data Mixing Agent, a model-based framework using reinforcement learning to re-weight domains for balanced performance in continual pre-training, outperforming manual heuristics.


<details>
  <summary>Details</summary>
Motivation: Addresses catastrophic forgetting in continual pre-training by automating domain reweighting, moving beyond manual heuristics.

Method: Uses reinforcement learning to train a Data Mixing Agent on data mixing trajectories, learning generalizable heuristics.

Result: Outperforms baselines in math reasoning, generalizes to unseen fields/models, and adapts to code generation.

Conclusion: Demonstrates effectiveness, adaptability, and alignment with human intuition, requiring less source data for superior performance.

Abstract: Continual pre-training on small-scale task-specific data is an effective
method for improving large language models in new target fields, yet it risks
catastrophic forgetting of their original capabilities. A common solution is to
re-weight training data mixtures from source and target fields on a domain
space to achieve balanced performance. Previous domain reweighting strategies
rely on manual designation with certain heuristics based on human intuition or
empirical results. In this work, we prove that more general heuristics can be
parameterized by proposing Data Mixing Agent, the first model-based, end-to-end
framework that learns to re-weight domains. The agent learns generalizable
heuristics through reinforcement learning on large quantities of data mixing
trajectories with corresponding feedback from an evaluation environment.
Experiments in continual pre-training on math reasoning show that Data Mixing
Agent outperforms strong baselines in achieving balanced performance across
source and target field benchmarks. Furthermore, it generalizes well across
unseen source fields, target models, and domain spaces without retraining.
Direct application to the code generation field also indicates its adaptability
across target domains. Further analysis showcases the agents' well-aligned
heuristics with human intuitions and their efficiency in achieving superior
model performance with less source-field data.

</details>


### [254] [Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning](https://arxiv.org/abs/2507.15788)
*Sneheel Sarangi,Hanan Salam*

Main category: cs.LG

TL;DR: Small LLMs trained with RLVR struggle to develop generalizable Theory of Mind (ToM) capabilities, showing narrow overfitting instead of abstract understanding.


<details>
  <summary>Details</summary>
Motivation: To explore if RL techniques can instill nuanced social intelligence (ToM) in small LLMs.

Method: Train small LLMs on ToM datasets (HiToM, ExploreToM, FANToM) using RL with verifiable rewards (RLVR) and test generalization on held-out datasets (e.g., OpenToM).

Result: Small LLMs improve on in-distribution tasks but fail to generalize to unseen ToM tasks, showing narrow overfitting.

Conclusion: RLVR leads to narrow overfitting in small LLMs, not true ToM capability.

Abstract: Recent advancements in large language models (LLMs) have demonstrated
emergent capabilities in complex reasoning, largely spurred by rule-based
Reinforcement Learning (RL) techniques applied during the post-training. This
has raised the question of whether similar methods can instill more nuanced,
human-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This
paper investigates whether small-scale LLMs can acquire a robust and
generalizable ToM capability through RL with verifiable rewards (RLVR). We
conduct a systematic evaluation by training models on various combinations of
prominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for
generalization on held-out datasets (e.g., OpenToM). Our findings indicate that
small LLMs struggle to develop a generic ToM capability. While performance on
in-distribution tasks improves, this capability fails to transfer to unseen ToM
tasks with different characteristics. Furthermore, we demonstrate that
prolonged RL training leads to models ``hacking'' the statistical patterns of
the training datasets, resulting in significant performance gains on in-domain
data but no change, or degradation of performance on out-of-distribution tasks.
This suggests the learned behavior is a form of narrow overfitting rather than
the acquisition of a true, abstract ToM capability.

</details>


### [255] [GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding](https://arxiv.org/abs/2507.15846)
*Fei Tang,Zhangxuan Gu,Zhengxi Lu,Xuyang Liu,Shuheng Shen,Changhua Meng,Wen Wang,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: GUI-G² introduces Gaussian rewards for GUI grounding, outperforming UI-TARS-72B by 24.7% on ScreenSpot-Pro by modeling spatial interactions continuously.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement learning uses sparse binary rewards, ignoring the continuous nature of spatial interactions. Human clicking behavior, which forms Gaussian distributions, inspired the solution.

Method: GUI-G² models GUI elements as Gaussian distributions with two mechanisms: Gaussian point rewards for precise localization and coverage rewards for spatial alignment. An adaptive variance mechanism handles diverse element scales.

Result: GUI-G² outperforms UI-TARS-72B by 24.7% on ScreenSpot-Pro, showing robustness to interface variations and better generalization to unseen layouts.

Conclusion: GUI-G² transforms GUI grounding into dense continuous optimization, setting a new paradigm for spatial reasoning in GUI interaction tasks.

Abstract: Graphical User Interface (GUI) grounding maps natural language instructions
to precise interface locations for autonomous interaction. Current
reinforcement learning approaches use binary rewards that treat elements as
hit-or-miss targets, creating sparse signals that ignore the continuous nature
of spatial interactions. Motivated by human clicking behavior that naturally
forms Gaussian distributions centered on target elements, we introduce GUI
Gaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that
models GUI elements as continuous Gaussian distributions across the interface
plane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point
rewards model precise localization through exponentially decaying distributions
centered on element centroids, while coverage rewards assess spatial alignment
by measuring the overlap between predicted Gaussian distributions and target
regions. To handle diverse element scales, we develop an adaptive variance
mechanism that calibrates reward distributions based on element dimensions.
This framework transforms GUI grounding from sparse binary classification to
dense continuous optimization, where Gaussian distributions generate rich
gradient signals that guide models toward optimal interaction positions.
Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro
benchmarks demonstrate that GUI-G$^2$, substantially outperforms
state-of-the-art method UI-TARS-72B, with the most significant improvement of
24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides
superior robustness to interface variations and enhanced generalization to
unseen layouts, establishing a new paradigm for spatial reasoning in GUI
interaction tasks.

</details>


### [256] [Generative Distribution Distillation](https://arxiv.org/abs/2507.14503)
*Jiequan Cui,Beier Zhu,Qingshan Xu,Xiaogang Xu,Pengguang Chen,Xiaojuan Qi,Bei Yu,Hanwang Zhang,Richang Hong*

Main category: cs.LG

TL;DR: The paper proposes Generative Distribution Distillation (GenDD) for knowledge distillation, addressing high-dimensional optimization and lack of label supervision with Split Tokenization and Distribution Contraction. It achieves competitive results, outperforming baselines by 16.29% in unsupervised settings and setting a new SOTA (82.28% accuracy) with supervised training.


<details>
  <summary>Details</summary>
Motivation: To address challenges in knowledge distillation (KD) as a conditional generative problem, specifically high-dimensional optimization and lack of semantic supervision from labels.

Method: Introduces Split Tokenization for stable unsupervised KD and Distribution Contraction to integrate label supervision into reconstruction. Theoretical proof links GenDD to multi-task learning.

Result: GenDD outperforms KL baseline by 16.29% on ImageNet in unsupervised settings. With supervision, ResNet-50 achieves 82.28% top-1 accuracy in 600 epochs.

Conclusion: GenDD is a robust framework for KD, excelling in both unsupervised and supervised settings, with theoretical and empirical validation.

Abstract: In this paper, we formulate the knowledge distillation (KD) as a conditional
generative problem and propose the \textit{Generative Distribution Distillation
(GenDD)} framework. A naive \textit{GenDD} baseline encounters two major
challenges: the curse of high-dimensional optimization and the lack of semantic
supervision from labels. To address these issues, we introduce a \textit{Split
Tokenization} strategy, achieving stable and effective unsupervised KD.
Additionally, we develop the \textit{Distribution Contraction} technique to
integrate label supervision into the reconstruction objective. Our theoretical
proof demonstrates that \textit{GenDD} with \textit{Distribution Contraction}
serves as a gradient-level surrogate for multi-task learning, realizing
efficient supervised training without explicit classification loss on
multi-step sampling image representations. To evaluate the effectiveness of our
method, we conduct experiments on balanced, imbalanced, and unlabeled data.
Experimental results show that \textit{GenDD} performs competitively in the
unsupervised setting, significantly surpassing KL baseline by \textbf{16.29\%}
on ImageNet validation set. With label supervision, our ResNet-50 achieves
\textbf{82.28\%} top-1 accuracy on ImageNet in 600 epochs training,
establishing a new state-of-the-art.

</details>


### [257] [The Origin of Self-Attention: From Pairwise Affinity Matrices to Transformers](https://arxiv.org/abs/2507.14560)
*Giorgio Roffo*

Main category: cs.LG

TL;DR: The paper connects self-attention in Transformers to the broader concept of affinity-based computation, highlighting Infinite Feature Selection (Inf-FS) as a foundational approach. It shows self-attention as a special case of Inf-FS and unifies diverse ML models under this paradigm.


<details>
  <summary>Details</summary>
Motivation: To trace the origins of self-attention and situate it within the broader framework of affinity-based computation, revealing common principles across domains like vision, NLP, and graph learning.

Method: Comparative analysis of self-attention and Inf-FS, focusing on how affinity matrices (A) are defined and used. Self-attention uses dynamic token similarities, while Inf-FS employs domain knowledge or learned affinities with multi-hop propagation.

Result: Self-attention is a single-hop variant of Inf-FS, sharing the core idea of reasoning over pairwise relationships. The key differences lie in affinity matrix construction and application.

Conclusion: The paper unifies self-attention and Inf-FS under affinity-based computation, emphasizing their shared mathematical foundation and potential for cross-domain insights.

Abstract: The self-attention mechanism, now central to deep learning architectures such
as Transformers, is a modern instance of a more general computational
principle: learning and using pairwise affinity matrices to control how
information flows through a model. This paper traces the conceptual origins of
self-attention across multiple domains, including computer vision, natural
language processing, and graph learning, through their shared reliance on an
affinity matrix, denoted as A. We highlight Infinite Feature Selection (Inf-FS)
as a foundational approach that generalizes the idea of affinity-based
weighting. Unlike the fixed dot-product structure used in Transformers, Inf-FS
defines A either through domain knowledge or by learning, and computes feature
relevance through multi-hop propagation over the affinity graph. From this
perspective, self-attention can be seen as a special case of Inf-FS: it uses a
single-hop affinity computation where A is dynamically built from token
similarities. We argue that the underlying structure, reasoning over pairwise
relationships, is preserved across both approaches, and the key differences lie
in how the affinity matrix is defined and applied. By situating self-attention
within the broader paradigm of affinity-based computation, we unify several
strands of machine learning research and highlight a common mathematical
foundation that underpins diverse models and tasks.

</details>


### [258] [CXR-TFT: Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories](https://arxiv.org/abs/2507.14766)
*Mehak Arora,Ayman Ali,Kaiyuan Wu,Carolyn Davis,Takashi Shimazui,Mahmoud Alwakeel,Victor Moas,Philip Yang,Annette Esper,Rishikesan Kamaleswaran*

Main category: cs.LG

TL;DR: CXR-TFT is a multi-modal framework integrating sparse CXR data and high-frequency clinical metrics to predict abnormal CXR findings in ICU patients up to 12 hours early, improving time-sensitive condition management.


<details>
  <summary>Details</summary>
Motivation: Existing CXR tools lack temporal dynamics, limiting their utility in ICU settings where early intervention is critical.

Method: CXR-TFT combines sparse CXR imaging, radiology reports, and hourly clinical data (vital signs, lab values) using a vision encoder and transformer model to predict CXR trajectories.

Result: In a study of 20,000 ICU patients, CXR-TFT accurately predicted abnormal CXR findings 12 hours before radiographic evidence.

Conclusion: CXR-TFT enhances early intervention for conditions like acute respiratory distress syndrome, improving clinical outcomes through timely insights.

Abstract: In intensive care units (ICUs), patients with complex clinical conditions
require vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a
vital diagnostic tool, providing insights into clinical trajectories, but their
irregular acquisition limits their utility. Existing tools for CXR
interpretation are constrained by cross-sectional analysis, failing to capture
temporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal
framework that integrates temporally sparse CXR imaging and radiology reports
with high-frequency clinical data, such as vital signs, laboratory values, and
respiratory flow sheets, to predict the trajectory of CXR findings in
critically ill patients. CXR-TFT leverages latent embeddings from a vision
encoder that are temporally aligned with hourly clinical data through
interpolation. A transformer model is then trained to predict CXR embeddings at
each hour, conditioned on previous embeddings and clinical measurements. In a
retrospective study of 20,000 ICU patients, CXR-TFT demonstrated high accuracy
in forecasting abnormal CXR findings up to 12 hours before they became
radiographically evident. This predictive capability in clinical data holds
significant potential for enhancing the management of time-sensitive conditions
like acute respiratory distress syndrome, where early intervention is crucial
and diagnoses are often delayed. By providing distinctive temporal resolution
in prognostic CXR analysis, CXR-TFT offers actionable 'whole patient' insights
that can directly improve clinical outcomes.

</details>


### [259] [Flow Equivariant Recurrent Neural Networks](https://arxiv.org/abs/2507.14793)
*T. Anderson Keller*

Main category: cs.LG

TL;DR: The paper extends equivariant network theory to time-parameterized transformations (flows) in sequence models, showing improved performance in RNNs through flow equivariance.


<details>
  <summary>Details</summary>
Motivation: Current equivariant networks are limited to static transformations and feed-forward architectures, missing the continuous symmetries in real-world data streams.

Method: The authors introduce flow equivariance in RNNs, ensuring hidden states transform geometrically for moving stimuli.

Result: Flow-equivariant RNNs outperform non-equivariant models in training speed, length generalization, and velocity generalization.

Conclusion: This work advances sequence models by respecting time-parameterized symmetries, with broader implications for real-world applications.

Abstract: Data arrives at our senses as a continuous stream, smoothly transforming from
one instant to the next. These smooth transformations can be viewed as
continuous symmetries of the environment that we inhabit, defining equivalence
relations between stimuli over time. In machine learning, neural network
architectures that respect symmetries of their data are called equivariant and
have provable benefits in terms of generalization ability and sample
efficiency. To date, however, equivariance has been considered only for static
transformations and feed-forward networks, limiting its applicability to
sequence models, such as recurrent neural networks (RNNs), and corresponding
time-parameterized sequence transformations. In this work, we extend
equivariant network theory to this regime of `flows' -- one-parameter Lie
subgroups capturing natural transformations over time, such as visual motion.
We begin by showing that standard RNNs are generally not flow equivariant:
their hidden states fail to transform in a geometrically structured manner for
moving stimuli. We then show how flow equivariance can be introduced, and
demonstrate that these models significantly outperform their non-equivariant
counterparts in terms of training speed, length generalization, and velocity
generalization, on both next step prediction and sequence classification. We
present this work as a first step towards building sequence models that respect
the time-parameterized symmetries which govern the world around us.

</details>


### [260] [To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models](https://arxiv.org/abs/2507.15381)
*Julia Machnio,Mads Nielsen,Mostafa Mehdipour Ghazi*

Main category: cs.LG

TL;DR: PALM is a unified model for analyzing active learning (AL) trajectories, predicting performance, and comparing strategies using four key parameters.


<details>
  <summary>Details</summary>
Motivation: Traditional AL evaluations focus only on final accuracy, missing the dynamics of the learning process. PALM addresses this gap.

Method: PALM uses four parameters (achievable accuracy, coverage efficiency, early-stage performance, scalability) to model AL behavior and predict future performance.

Result: Validated on CIFAR and ImageNet datasets, PALM generalizes well, predicts learning curves, and provides insights into AL efficiency and scalability.

Conclusion: PALM enables systematic, reproducible, and cost-effective AL evaluation, aiding research and real-world applications.

Abstract: Active learning (AL) seeks to reduce annotation costs by selecting the most
informative samples for labeling, making it particularly valuable in
resource-constrained settings. However, traditional evaluation methods, which
focus solely on final accuracy, fail to capture the full dynamics of the
learning process. To address this gap, we propose PALM (Performance Analysis of
Active Learning Models), a unified and interpretable mathematical model that
characterizes AL trajectories through four key parameters: achievable accuracy,
coverage efficiency, early-stage performance, and scalability. PALM provides a
predictive description of AL behavior from partial observations, enabling the
estimation of future performance and facilitating principled comparisons across
different strategies. We validate PALM through extensive experiments on
CIFAR-10/100 and ImageNet-50/100/200, covering a wide range of AL methods and
self-supervised embeddings. Our results demonstrate that PALM generalizes
effectively across datasets, budgets, and strategies, accurately predicting
full learning curves from limited labeled data. Importantly, PALM reveals
crucial insights into learning efficiency, data space coverage, and the
scalability of AL methods. By enabling the selection of cost-effective
strategies and predicting performance under tight budget constraints, PALM lays
the basis for more systematic, reproducible, and data-efficient evaluation of
AL in both research and real-world applications. The code is available at:
https://github.com/juliamachnio/PALM.

</details>


### [261] [Diffusion Beats Autoregressive in Data-Constrained Settings](https://arxiv.org/abs/2507.15857)
*Mihir Prabhudesai,Menging Wu,Amir Zadeh,Katerina Fragkiadaki,Deepak Pathak*

Main category: cs.LG

TL;DR: Diffusion models outperform autoregressive (AR) models in data-scarce settings due to better utilization of repeated data and implicit data augmentation.


<details>
  <summary>Details</summary>
Motivation: To explore the advantages of diffusion-based language models over AR models, especially in data-constrained scenarios.

Method: Systematic study of masked diffusion models in data-constrained settings, comparing their performance with AR models.

Result: Diffusion models achieve lower validation loss and superior downstream performance when compute is abundant but data is scarce.

Conclusion: Diffusion models are a compelling alternative to AR models when data is the bottleneck, not compute.

Abstract: Autoregressive (AR) models have long dominated the landscape of large
language models, driving progress across a wide range of tasks. Recently,
diffusion-based language models have emerged as a promising alternative, though
their advantages over AR models remain underexplored. In this paper, we
systematically study masked diffusion models in data-constrained settings-where
training involves repeated passes over limited data-and find that they
significantly outperform AR models when compute is abundant but data is scarce.
Diffusion models make better use of repeated data, achieving lower validation
loss and superior downstream performance. We interpret this advantage as
implicit data augmentation: masked diffusion exposes the model to a diverse
distribution of token orderings and prediction tasks, unlike AR's fixed
left-to-right factorization. We find new scaling laws for diffusion models and
derive a closed-form expression for the critical compute threshold at which
diffusion begins to outperform AR. These results suggest that when data, not
compute, is the bottleneck, diffusion models offer a compelling alternative to
the standard AR paradigm. Our code is available at:
https://diffusion-scaling.github.io.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [262] [WebGuard: Building a Generalizable Guardrail for Web Agents](https://arxiv.org/abs/2507.14293)
*Boyuan Zheng,Zeyi Liao,Scott Salisbury,Zeyuan Liu,Michael Lin,Qinyuan Zheng,Zifan Wang,Xiang Deng,Dawn Song,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: WebGuard is a dataset for assessing web agent action risks, revealing LLMs' poor performance in predicting outcomes and highlighting the need for specialized guardrail models.


<details>
  <summary>Details</summary>
Motivation: The rise of autonomous web agents powered by LLMs poses risks of unintended or harmful actions, necessitating safety measures like WebGuard.

Method: WebGuard includes 4,939 human-annotated actions from 193 websites, categorized into SAFE, LOW, and HIGH risk. It supports fine-tuning guardrail models.

Result: Frontier LLMs perform poorly (accuracy <60%, recall <60% for HIGH-risk actions). Fine-tuning improves accuracy (37% to 80%) and recall (20% to 76%).

Conclusion: Despite improvements, guardrail models still lack the reliability for high-stakes deployment, requiring near-perfect accuracy and recall.

Abstract: The rapid development of autonomous web agents powered by Large Language
Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of
taking unintended or harmful actions. This situation underscores an urgent need
for effective safety measures, akin to access controls for human users. To
address this critical challenge, we introduce WebGuard, the first comprehensive
dataset designed to support the assessment of web agent action risks and
facilitate the development of guardrails for real-world online environments. In
doing so, WebGuard specifically focuses on predicting the outcome of
state-changing actions and contains 4,939 human-annotated actions from 193
websites across 22 diverse domains, including often-overlooked long-tail
websites. These actions are categorized using a novel three-tier risk schema:
SAFE, LOW, and HIGH. The dataset includes designated training and test splits
to support evaluation under diverse generalization settings. Our initial
evaluations reveal a concerning deficiency: even frontier LLMs achieve less
than 60% accuracy in predicting action outcomes and less than 60% recall in
lagging HIGH-risk actions, highlighting the risks of deploying
current-generation agents without dedicated safeguards. We therefore
investigate fine-tuning specialized guardrail models using WebGuard. We conduct
comprehensive evaluations across multiple generalization settings and find that
a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in
performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from
20% to 76%. Despite these improvements, the performance still falls short of
the reliability required for high-stakes deployment, where guardrails must
approach near-perfect accuracy and recall.

</details>


### [263] [Inverse Scaling in Test-Time Compute](https://arxiv.org/abs/2507.14417)
*Aryo Pradipta Gema,Alexander Hägele,Runjin Chen,Andy Arditi,Jacob Goldman-Wetzler,Kit Fraser-Taliente,Henry Sleight,Linda Petrini,Julian Michael,Beatrice Alex,Pasquale Minervini,Yanda Chen,Joe Benton,Ethan Perez*

Main category: cs.AI

TL;DR: Extending reasoning length in Large Reasoning Models (LRMs) can degrade performance, revealing inverse scaling between compute and accuracy. Five failure modes are identified, highlighting risks of prolonged reasoning.


<details>
  <summary>Details</summary>
Motivation: To investigate how extended reasoning affects LRM performance and identify potential failure modes, ensuring safe and effective model scaling.

Method: Constructed evaluation tasks across four categories (counting, regression, deduction, AI risks) to test models under varying reasoning lengths.

Result: Five failure modes emerged, including distraction, overfitting, spurious correlations, focus loss, and amplified concerning behaviors.

Conclusion: While test-time compute scaling is promising, it risks reinforcing problematic reasoning. Diverse reasoning-length evaluations are crucial for LRM safety.

Abstract: We construct evaluation tasks where extending the reasoning length of Large
Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling
relationship between test-time compute and accuracy. Our evaluation tasks span
four categories: simple counting tasks with distractors, regression tasks with
spurious features, deduction tasks with constraint tracking, and advanced AI
risks. We identify five distinct failure modes when models reason for longer:
1) Claude models become increasingly distracted by irrelevant information; 2)
OpenAI o-series models resist distractors but overfit to problem framings; 3)
models shift from reasonable priors to spurious correlations; 4) all models
show difficulties in maintaining focus on complex deductive tasks; and 5)
extended reasoning may amplify concerning behaviors, with Claude Sonnet 4
showing increased expressions of self-preservation. These findings suggest that
while test-time compute scaling remains promising for improving model
capabilities, it may inadvertently reinforce problematic reasoning patterns.
Our results demonstrate the importance of evaluating models across diverse
reasoning lengths to identify and address these failure modes in LRMs.

</details>


### [264] [Routine: A Structural Planning Framework for LLM Agent System in Enterprise](https://arxiv.org/abs/2507.14447)
*Guancheng Zeng,Xueyi Chen,Jiawang Hu,Shaohua Qi,Yaxuan Mao,Zhantao Wang,Yifan Nie,Shuang Li,Qiuyang Feng,Pengxu Qiu,Yujia Wang,Wenqiang Han,Linyan Huang,Gang Li,Jingjing Mo,Haowen Hu*

Main category: cs.AI

TL;DR: Routine is a multi-step agent planning framework that improves execution stability and accuracy in enterprise environments, significantly boosting model performance.


<details>
  <summary>Details</summary>
Motivation: Address challenges like disorganized plans and poor execution stability in agent systems by introducing a structured framework.

Method: Introduces Routine, a framework with clear structure, explicit instructions, and seamless parameter passing for multi-step tool-calling tasks. Evaluated in real-world enterprise scenarios.

Result: Increased execution accuracy: GPT-4o from 41.1% to 96.3%, Qwen3-14B from 32.6% to 83.3%. Fine-tuning further improved Qwen3-14B to 88.2% and 95.5% with distilled data.

Conclusion: Routine effectively enhances agent system stability and adaptability, accelerating deployment in enterprise environments.

Abstract: The deployment of agent systems in an enterprise environment is often
hindered by several challenges: common models lack domain-specific process
knowledge, leading to disorganized plans, missing key tools, and poor execution
stability. To address this, this paper introduces Routine, a multi-step agent
planning framework designed with a clear structure, explicit instructions, and
seamless parameter passing to guide the agent's execution module in performing
multi-step tool-calling tasks with high stability. In evaluations conducted
within a real-world enterprise scenario, Routine significantly increases the
execution accuracy in model tool calls, increasing the performance of GPT-4o
from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed
a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an
accuracy increase to 88.2% on scenario-specific evaluations, indicating
improved adherence to execution plans. In addition, we employed Routine-based
distillation to create a scenario-specific, multi-step tool-calling dataset.
Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%,
approaching GPT-4o's performance. These results highlight Routine's
effectiveness in distilling domain-specific tool-usage patterns and enhancing
model adaptability to new scenarios. Our experimental results demonstrate that
Routine provides a practical and accessible approach to building stable agent
workflows, accelerating the deployment and adoption of agent systems in
enterprise environments, and advancing the technical vision of AI for Process.

</details>


### [265] [When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems](https://arxiv.org/abs/2507.14660)
*Qibing Ren,Sitao Xie,Longxuan Wei,Zhenfei Yin,Junchi Yan,Lizhuang Ma,Jing Shao*

Main category: cs.AI

TL;DR: The paper explores risks of AI-driven multi-agent systems (MAS) collusion, simulating malicious actions in misinformation and e-commerce fraud, finding decentralized systems more harmful and adaptable.


<details>
  <summary>Details</summary>
Motivation: Concerns about AI-driven groups causing harm, similar to human-coordinated fraud or scams, with MAS risks underexplored.

Method: A proof-of-concept framework simulates MAS collusion, testing centralized vs. decentralized coordination in misinformation and fraud.

Result: Decentralized MAS are more effective and adaptable in malicious actions, evading traditional interventions like content flagging.

Conclusion: Highlights the need for better detection and countermeasures against malicious MAS, especially decentralized ones.

Abstract: Recent large-scale events like election fraud and financial scams have shown
how harmful coordinated efforts by human groups can be. With the rise of
autonomous AI systems, there is growing concern that AI-driven groups could
also cause similar harm. While most AI safety research focuses on individual AI
systems, the risks posed by multi-agent systems (MAS) in complex real-world
situations are still underexplored. In this paper, we introduce a
proof-of-concept to simulate the risks of malicious MAS collusion, using a
flexible framework that supports both centralized and decentralized
coordination structures. We apply this framework to two high-risk fields:
misinformation spread and e-commerce fraud. Our findings show that
decentralized systems are more effective at carrying out malicious actions than
centralized ones. The increased autonomy of decentralized systems allows them
to adapt their strategies and cause more damage. Even when traditional
interventions, like content flagging, are applied, decentralized groups can
adjust their tactics to avoid detection. We present key insights into how these
malicious groups operate and the need for better detection systems and
countermeasures. Code is available at https://github.com/renqibing/RogueAgent.

</details>


### [266] [Towards physician-centered oversight of conversational diagnostic AI](https://arxiv.org/abs/2507.15743)
*Elahe Vedadi,David Barrett,Natalie Harris,Ellery Wulczyn,Shashir Reddy,Roma Ruparel,Mike Schaekermann,Tim Strother,Ryutaro Tanno,Yash Sharma,Jihyeon Lee,Cían Hughes,Dylan Slack,Anil Palepu,Jan Freyberg,Khaled Saab,Valentin Liévin,Wei-Hung Weng,Tao Tu,Yun Liu,Nenad Tomasev,Kavita Kulkarni,S. Sara Mahdavi,Kelvin Guu,Joëlle Barral,Dale R. Webster,James Manyika,Avinatan Hassidim,Katherine Chou,Yossi Matias,Pushmeet Kohli,Adam Rodman,Vivek Natarajan,Alan Karthikesalingam,David Stutz*

Main category: cs.AI

TL;DR: The paper proposes g-AMIE, a multi-agent AI system for medical history taking under guardrails, with asynchronous oversight by physicians, showing improved efficiency and decision quality.


<details>
  <summary>Details</summary>
Motivation: To address the need for regulated, safe diagnostic dialogue systems while leveraging AI for efficiency, inspired by physician oversight of NPs/PAs.

Method: Developed g-AMIE, a system abstaining from direct advice, conveying assessments to physicians via a cockpit interface, tested in a virtual OSCE with 60 scenarios.

Result: g-AMIE outperformed NPs/PAs and PCPs in intake quality, case summaries, and proposed plans, leading to better composite decisions and time efficiency.

Conclusion: Asynchronous oversight by physicians is a feasible paradigm for AI diagnostic systems, enhancing care quality and efficiency, though clinical replication is needed.

Abstract: Recent work has demonstrated the promise of conversational AI systems for
diagnostic dialogue. However, real-world assurance of patient safety means that
providing individual diagnoses and treatment plans is considered a regulated
activity by licensed professionals. Furthermore, physicians commonly oversee
other team members in such activities, including nurse practitioners (NPs) or
physician assistants/associates (PAs). Inspired by this, we propose a framework
for effective, asynchronous oversight of the Articulate Medical Intelligence
Explorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent
system that performs history taking within guardrails, abstaining from
individualized medical advice. Afterwards, g-AMIE conveys assessments to an
overseeing primary care physician (PCP) in a clinician cockpit interface. The
PCP provides oversight and retains accountability of the clinical decision.
This effectively decouples oversight from intake and can thus happen
asynchronously. In a randomized, blinded virtual Objective Structured Clinical
Examination (OSCE) of text consultations with asynchronous oversight, we
compared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across
60 scenarios, g-AMIE outperformed both groups in performing high-quality
intake, summarizing cases, and proposing diagnoses and management plans for the
overseeing PCP to review. This resulted in higher quality composite decisions.
PCP oversight of g-AMIE was also more time-efficient than standalone PCP
consultations in prior work. While our study does not replicate existing
clinical practices and likely underestimates clinicians' capabilities, our
results demonstrate the promise of asynchronous oversight as a feasible
paradigm for diagnostic AI systems to operate under expert human oversight for
enhancing real-world care.

</details>


### [267] [LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization](https://arxiv.org/abs/2507.15758)
*Xingyu Wu,Yuchen Yan,Shangke Lyu,Linjuan Wu,Yiwen Qiu,Yongliang Shen,Weiming Lu,Jian Shao,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: LAPO is a framework that optimizes reasoning length in models, reducing token usage by 40.9% while improving accuracy by 2.3%.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models generate excessive tokens for simple problems, needing a solution to internalize reasoning depth control.

Method: LAPO uses a two-stage reinforcement learning process: first learning natural reasoning patterns, then embedding them for inference-time flexibility.

Result: Reduces token usage by 40.9% and improves accuracy by 2.3% on mathematical reasoning benchmarks.

Conclusion: LAPO enables efficient reasoning by adapting computational resources to problem complexity without quality loss.

Abstract: Large reasoning models have achieved remarkable performance through extended
chain-of-thought sequences, yet this computational freedom leads to excessive
token generation even for simple problems. We present Length-Adaptive Policy
Optimization (LAPO), a novel framework that transforms reasoning length control
from an external constraint into an intrinsic model capability. Unlike existing
approaches that impose rigid limits or rely on post-hoc interventions, LAPO
enables models to internalize an understanding of appropriate reasoning depth
through a two-stage reinforcement learning process. In the first stage, models
learn natural reasoning patterns by discovering the statistical distribution of
successful solution lengths. The second stage leverages these patterns as
meta-cognitive guidance, embedding them directly within the model's reasoning
context to ensure inference-time flexibility. Experiments on mathematical
reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\%
while improving accuracy by 2.3\%. Our analysis reveals that models trained
with LAPO develop emergent abilities to allocate computational resources based
on problem complexity, achieving efficient reasoning without sacrificing
quality.

</details>


### [268] [Hierarchical Budget Policy Optimization for Adaptive Reasoning](https://arxiv.org/abs/2507.15844)
*Shangke Lyu,Linjuan Wu,Yuchen Yan,Xingyu Wu,Hao Li,Yongliang Shen,Peisheng Jiang,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: HBPO is a reinforcement learning framework that optimizes reasoning efficiency in large models by learning problem-specific reasoning depths, reducing token usage by 60.6% while improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Current reasoning models use uniform strategies regardless of problem complexity, leading to computational inefficiency. HBPO aims to address this by enabling adaptive reasoning depths.

Method: HBPO uses hierarchical budget exploration, partitioning samples into subgroups with distinct token budgets, and employs differentiated reward mechanisms to align effort with problem complexity.

Result: HBPO reduces token usage by up to 60.6% and improves accuracy by 3.14% across benchmarks, showing emergent adaptive behavior without external constraints.

Conclusion: HBPO demonstrates that reasoning efficiency and capability can coexist through hierarchical training, preserving exploration diversity and enabling adaptive behavior.

Abstract: Large reasoning models achieve remarkable performance through extensive
chain-of-thought generation, yet exhibit significant computational inefficiency
by applying uniform reasoning strategies regardless of problem complexity. We
present Hierarchical Budget Policy Optimization (HBPO), a reinforcement
learning framework that enables models to learn problem-specific reasoning
depths without sacrificing capability. HBPO addresses the fundamental challenge
of exploration space collapse in efficiency-oriented training, where penalties
on long output length systematically bias models away from necessary long
reasoning paths. Through hierarchical budget exploration, our approach
partitions rollout samples into multiple subgroups with distinct token budgets,
aiming to enable efficient resource allocation while preventing degradation of
capability. We introduce differentiated reward mechanisms that create
budget-aware incentives aligned with the complexity of the problem, allowing
models to discover natural correspondences between task requirements and
computational effort. Extensive experiments demonstrate that HBPO reduces
average token usage by up to 60.6% while improving accuracy by 3.14% across
four reasoning benchmarks. Unlike existing methods that impose external
constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive
behavior where models automatically adjust reasoning depth based on problem
complexity. Our results suggest that reasoning efficiency and capability are
not inherently conflicting, and can be simultaneously optimized through
appropriately structured hierarchical training that preserves exploration
diversity.

</details>


### [269] [InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis](https://arxiv.org/abs/2507.14899)
*Jiale Liu,Huan Wang,Yue Zhang,Xiaoyu Luo,Jiaxiang Hu,Zhiliang Liu,Min Xie*

Main category: cs.AI

TL;DR: InsightX Agent is an LMM-based framework for X-ray NDT, improving reliability, interpretability, and interactivity by combining SDMSD for defect detection and EGR for validation.


<details>
  <summary>Details</summary>
Motivation: Existing deep-learning NDT methods lack interactivity, interpretability, and self-assessment, limiting reliability and trust.

Method: Uses a Large Multimodal Model (LMM) to coordinate SDMSD for defect detection and EGR for validation through a review process.

Result: Achieves 96.35% F1-score on GDXray+ dataset, with enhanced interpretability and trustworthiness.

Conclusion: InsightX Agent demonstrates the potential of agentic LLM frameworks for industrial inspection.

Abstract: Non-destructive testing (NDT), particularly X-ray inspection, is vital for
industrial quality assurance, yet existing deep-learning-based approaches often
lack interactivity, interpretability, and the capacity for critical
self-assessment, limiting their reliability and operator trust. To address
these shortcomings, this paper proposes InsightX Agent, a novel LMM-based
agentic framework designed to deliver reliable, interpretable, and interactive
X-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent
positions a Large Multimodal Model (LMM) as a central orchestrator,
coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the
Evidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect
region proposals for multi-scale feature maps and sparsifies them through
Non-Maximum Suppression (NMS), optimizing detection of small, dense targets in
X-ray images while maintaining computational efficiency. The EGR tool guides
the LMM agent through a chain-of-thought-inspired review process, incorporating
context assessment, individual defect analysis, false positive elimination,
confidence recalibration and quality assurance to validate and refine the
SDMSD's initial proposals. By strategically employing and intelligently using
tools, InsightX Agent moves beyond passive data processing to active reasoning,
enhancing diagnostic reliability and providing interpretations that integrate
diverse information sources. Experimental evaluations on the GDXray+ dataset
demonstrate that InsightX Agent not only achieves a high object detection
F1-score of 96.35% but also offers significantly improved interpretability and
trustworthiness in its analyses, highlighting the transformative potential of
agentic LLM frameworks for industrial inspection tasks.

</details>


### [270] [Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner](https://arxiv.org/abs/2507.15509)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: Chart-R1 introduces a chart-domain vision-language model with reinforcement learning fine-tuning for complex reasoning, supported by programmatic data synthesis and a two-stage training strategy.


<details>
  <summary>Details</summary>
Motivation: To extend R1-Style methods beyond mathematical reasoning and code intelligence to multimodal data like charts, addressing the lack of reasoning data in this domain.

Method: Uses programmatic data synthesis for high-quality reasoning data and a two-stage training strategy: Chart-COT for step-by-step supervision and Chart-RFT for numerically sensitive reinforcement fine-tuning.

Result: Chart-R1 outperforms chart-domain methods and competes with large-scale models like GPT-4o and Claude-3.5.

Conclusion: Chart-R1 successfully advances chart reasoning, demonstrating the potential of reinforcement learning fine-tuning in multimodal domains.

Abstract: Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based
on reinforcement learning fine-tuning has received widespread attention from
the community. Previous R1-Style methods mainly focus on mathematical reasoning
and code intelligence. It is of great research significance to verify their
advantages on more general multimodal data. Chart is an important multimodal
data type with rich information, which brings important research challenges in
complex reasoning. In this work, we introduce Chart-R1, a chart-domain
vision-language model with reinforcement learning fine-tuning to enable complex
chart reasoning. To support Chart-R1, we first propose a novel programmatic
data synthesis technology to generate high-quality step-by-step chart reasoning
data covering single- and multi-subcharts, which makes up for the lack of
reasoning data in the chart domain. Then we develop a two-stage training
strategy: Chart-COT with step-by-step chain-of-thought supervision, and
Chart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims
to decompose complex chart reasoning tasks into fine-grained, understandable
subtasks through step-by-step supervision, which lays a good foundation for
improving the reasoning level of reinforcement learning. Chart-RFT utilize the
typical group relative policy optimization strategy, in which a relatively soft
reward is adopted for numerical response to emphasize the numerical sensitivity
in the chart domain. We conduct extensive experiments on open-source benchmarks
and self-built chart reasoning dataset (\emph{i.e., ChartRQA}). Experimental
results show that Chart-R1 has significant advantages compared to chart-domain
methods, even comparable to open/closed source large-scale models (\emph{e.g.,
GPT-4o, Claude-3.5}).

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [271] [Dissociating model architectures from inference computations](https://arxiv.org/abs/2507.15776)
*Noor Sajid,Johan Medrano*

Main category: q-bio.NC

TL;DR: Auto-regressive and deep temporal models differ in non-Markovian sequence modeling. The paper shows how deep temporal computations can be mimicked by auto-regressive models through structured context access, maintaining predictive capacity with fewer computations.


<details>
  <summary>Details</summary>
Motivation: To dissociate model architectures from inference computations and explore how auto-regressive models can mimic deep temporal computations.

Method: Using a transformer trained on next-token prediction, hierarchical temporal factorization is induced during iterative inference.

Result: Auto-regressive models can mimic deep temporal computations while maintaining predictive capacity and reducing computations.

Conclusion: Prediction construction and refinement processes are not strictly tied to model architectures.

Abstract: Parr et al., 2025 examines how auto-regressive and deep temporal models
differ in their treatment of non-Markovian sequence modelling. Building on
this, we highlight the need for dissociating model architectures, i.e., how the
predictive distribution factorises, from the computations invoked at inference.
We demonstrate that deep temporal computations are mimicked by autoregressive
models by structuring context access during iterative inference. Using a
transformer trained on next-token prediction, we show that inducing
hierarchical temporal factorisation during iterative inference maintains
predictive capacity while instantiating fewer computations. This emphasises
that processes for constructing and refining predictions are not necessarily
bound to their underlying model architectures.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [272] [Identifying Algorithmic and Domain-Specific Bias in Parliamentary Debate Summarisation](https://arxiv.org/abs/2507.14221)
*Eoghan Cunningham,James Cross,Derek Greene*

Main category: cs.CY

TL;DR: The paper investigates biases in LLM-generated summaries of European Parliament debates, proposing a multi-stage framework to improve fairness and analyzing positional and partisan biases.


<details>
  <summary>Details</summary>
Motivation: To make parliamentary debates more accessible while ensuring equitable representation of all speakers in summaries.

Method: A structured, multi-stage summarisation framework using LLMs, tested on proprietary and open-weight models to analyze biases.

Result: Found consistent positional and partisan biases, with hierarchical summarisation strategies showing potential to reduce disparities.

Conclusion: Highlights the need for domain-sensitive metrics and ethical oversight in LLM applications for democracy.

Abstract: The automated summarisation of parliamentary debates using large language
models (LLMs) offers a promising way to make complex legislative discourse more
accessible to the public. However, such summaries must not only be accurate and
concise but also equitably represent the views and contributions of all
speakers. This paper explores the use of LLMs to summarise plenary debates from
the European Parliament and investigates the algorithmic and representational
biases that emerge in this context. We propose a structured, multi-stage
summarisation framework that improves textual coherence and content fidelity,
while enabling the systematic analysis of how speaker attributes -- such as
speaking order or political affiliation -- influence the visibility and
accuracy of their contributions in the final summaries. Through our experiments
using both proprietary and open-weight LLMs, we find evidence of consistent
positional and partisan biases, with certain speakers systematically
under-represented or misattributed. Our analysis shows that these biases vary
by model and summarisation strategy, with hierarchical approaches offering the
greatest potential to reduce disparity. These findings underscore the need for
domain-sensitive evaluation metrics and ethical oversight in the deployment of
LLMs for democratic applications.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [273] [Exposing and Mitigating Calibration Biases and Demographic Unfairness in MLLM Few-Shot In-Context Learning for Medical Image Classification](https://arxiv.org/abs/2506.23298)
*Xing Shen,Justin Szeto,Mingyang Li,Hengguan Huang,Tal Arbel*

Main category: eess.IV

TL;DR: The paper investigates calibration biases and demographic unfairness in multimodal large language models (MLLMs) for medical image classification, introducing CALIN, an inference-time calibration method to mitigate these biases.


<details>
  <summary>Details</summary>
Motivation: Safe deployment of MLLMs in clinical practice requires analyzing prediction accuracies and calibration errors across demographic subgroups.

Method: CALIN uses a bi-level procedure to estimate calibration needs (via calibration matrices) from population to subgroup levels, applying this during inference.

Result: Experiments on three datasets show CALIN improves fairness in confidence calibration, prediction accuracy, and minimizes fairness-utility trade-offs.

Conclusion: CALIN effectively addresses calibration biases and unfairness in MLLMs for medical image classification, enhancing clinical deployment safety.

Abstract: Multimodal large language models (MLLMs) have enormous potential to perform
few-shot in-context learning in the context of medical image analysis. However,
safe deployment of these models into real-world clinical practice requires an
in-depth analysis of the accuracies of their predictions, and their associated
calibration errors, particularly across different demographic subgroups. In
this work, we present the first investigation into the calibration biases and
demographic unfairness of MLLMs' predictions and confidence scores in few-shot
in-context learning for medical image classification. We introduce CALIN, an
inference-time calibration method designed to mitigate the associated biases.
Specifically, CALIN estimates the amount of calibration needed, represented by
calibration matrices, using a bi-level procedure: progressing from the
population level to the subgroup level prior to inference. It then applies this
estimation to calibrate the predicted confidence scores during inference.
Experimental results on three medical imaging datasets: PAPILA for fundus image
classification, HAM10000 for skin cancer classification, and MIMIC-CXR for
chest X-ray classification demonstrate CALIN's effectiveness at ensuring fair
confidence calibration in its prediction, while improving its overall
prediction accuracies and exhibiting minimum fairness-utility trade-off. Our
codebase can be found at
https://github.com/xingbpshen/medical-calibration-fairness-mllm.

</details>


### [274] [MiDeSeC: A Dataset for Mitosis Detection and Segmentation in Breast Cancer Histopathology Images](https://arxiv.org/abs/2507.14271)
*Refik Samet,Nooshin Nemati,Emrah Hancer,Serpil Sak,Bilge Ayca Kirmizi,Zeynep Yildirim*

Main category: eess.IV

TL;DR: The MiDeSeC dataset comprises H&E stained breast carcinoma slides from 25 patients, with 50 regions (1024x1024 pixels) containing over 500 mitoses, split into training and testing sets.


<details>
  <summary>Details</summary>
Motivation: To address the variability in mitosis shapes, a large and diverse dataset is needed for accurate detection and analysis in breast carcinoma.

Method: Slides were scanned using 3D Histech Panoramic p250 Flash-3 scanner and Olympus BX50 microscope, with 50 regions selected for analysis.

Result: The dataset includes over 500 mitoses, with two-thirds of regions for training and one-third for testing.

Conclusion: MiDeSeC provides a robust dataset for mitosis detection in breast carcinoma, aiding research and diagnostic accuracy.

Abstract: The MiDeSeC dataset is created through H&E stained invasive breast carcinoma,
no special type (NST) slides of 25 different patients captured at 40x
magnification from the Department of Medical Pathology at Ankara University.
The slides have been scanned by 3D Histech Panoramic p250 Flash-3 scanner and
Olympus BX50 microscope. As several possible mitosis shapes exist, it is
crucial to have a large dataset to cover all the cases. Accordingly, a total of
50 regions is selected from glass slides for 25 patients, each of regions with
a size of 1024*1024 pixels. There are more than 500 mitoses in total in these
50 regions. Two-thirds of the regions are reserved for training, the other
third for testing.

</details>


### [275] [NuSeC: A Dataset for Nuclei Segmentation in Breast Cancer Histopathology Images](https://arxiv.org/abs/2507.14272)
*Refik Samet,Nooshin Nemati,Emrah Hancer,Serpil Sak,Bilge Ayca Kirmizi*

Main category: eess.IV

TL;DR: The NuSeC dataset consists of 100 images (1024*1024 pixels) from 25 patients, split into 75 training and 25 testing images for consistent analysis.


<details>
  <summary>Details</summary>
Motivation: To enable consistent comparative analysis of future methods using the NuSeC dataset.

Method: Dataset creation: 4 images per patient (25 patients). Split: 75% training (75 images, ~30k nuclei), 25% testing (25 images, ~6k nuclei). Random selection for testing.

Result: Training set: 75 images (~30k nuclei). Testing set: 25 images (~6k nuclei).

Conclusion: The NuSeC dataset is structured to support reproducible and comparative research in nuclei analysis.

Abstract: The NuSeC dataset is created by selecting 4 images with the size of 1024*1024
pixels from the slides of each patient among 25 patients. Therefore, there are
a total of 100 images in the NuSeC dataset. To carry out a consistent
comparative analysis between the methods that will be developed using the NuSeC
dataset by the researchers in the future, we divide the NuSeC dataset 75% as
the training set and 25% as the testing set. In detail, an image is randomly
selected from 4 images of each patient among 25 patients to build the testing
set, and then the remaining images are reserved for the training set. While the
training set includes 75 images with around 30000 nuclei structures, the
testing set includes 25 images with around 6000 nuclei structures.

</details>


### [276] [Self-Supervised Joint Reconstruction and Denoising of T2-Weighted PROPELLER MRI of the Lungs at 0.55T](https://arxiv.org/abs/2507.14308)
*Jingjia Chen,Haoyang Pei,Christoph Maier,Mary Bruno,Qiuting Wen,Seon-Hi Shin,William Moore,Hersh Chandarana,Li Feng*

Main category: eess.IV

TL;DR: A self-supervised joint reconstruction and denoising model improves 0.55T T2-weighted PROPELLER lung MRI, outperforming traditional methods and enabling scan time reduction.


<details>
  <summary>Details</summary>
Motivation: To enhance the clarity and structural integrity of 0.55T T2-weighted lung MRI by leveraging self-supervised learning without requiring clean targets.

Method: A self-supervised framework splits PROPELLER acquisition blades into partitions for training and loss calculation, using matched noise statistics for denoising. Compared with MPPCA-denoised images.

Result: Improved image clarity and alignment with CT scans, reduced scan time by half, and outperformed MPPCA-denoised images in reader evaluations (p<0.001).

Conclusion: The self-supervised model effectively reconstructs and denoises 0.55T T2-weighted lung MRI by utilizing intrinsic structural redundancies in k-space subsets.

Abstract: Purpose: This study aims to improve 0.55T T2-weighted PROPELLER lung MRI
through a self-supervised joint reconstruction and denoising model.
  Methods: T2-weighted 0.55T lung MRI dataset including 44 patients with
previous covid infection were used. A self-supervised learning framework was
developed, where each blade of the PROPELLER acquisition was split along the
readout direction into two partitions. One subset trains the unrolled
reconstruction network, while the other subset is used for loss calculation,
enabling self-supervised training without clean targets and leveraging matched
noise statistics for denoising. For comparison, Marchenko-Pastur Principal
Component Analysis (MPPCA) was performed along the coil dimension, followed by
conventional parallel imaging reconstruction. The quality of the reconstructed
lung MRI was assessed visually by two experienced radiologists independently.
  Results: The proposed self-supervised model improved the clarity and
structural integrity of the lung images. For cases with available CT scans, the
reconstructed images demonstrated strong alignment with corresponding CT
images. Additionally, the proposed model enables further scan time reduction by
requiring only half the number of blades. Reader evaluations confirmed that the
proposed method outperformed MPPCA-denoised images across all categories
(Wilcoxon signed-rank test, p<0.001), with moderate inter-reader agreement
(weighted Cohen's kappa=0.55; percentage of exact and within +/-1 point
agreement=91%).
  Conclusion: By leveraging intrinsic structural redundancies between two
disjoint splits of k-space subsets, the proposed self-supervised learning model
effectively reconstructs the image while suppressing the noise for 0.55T
T2-weighted lung MRI with PROPELLER sampling.

</details>


### [277] [Classification of Histopathology Slides with Persistence Homology Convolutions](https://arxiv.org/abs/2507.14378)
*Shrunal Pothagoni,Benjamin Schweinhart*

Main category: eess.IV

TL;DR: A novel method, Persistent Homology Convolutions, improves CNN performance in histopathology by capturing local topological features, outperforming conventional models.


<details>
  <summary>Details</summary>
Motivation: Typical CNNs lose topological information, crucial in domains like histopathology where topology distinguishes disease-indicating tissue.

Method: Introduces Persistent Homology Convolutions, a modified convolution operator, to generate local persistent homology-based data, preserving locality and translation invariance of topological features.

Result: Models with persistent homology convolutions outperform conventional models and are less sensitive to hyperparameters.

Conclusion: Persistent homology convolutions effectively extract meaningful geometric information from histopathology slides, enhancing diagnostics.

Abstract: Convolutional neural networks (CNNs) are a standard tool for computer vision
tasks such as image classification. However, typical model architectures may
result in the loss of topological information. In specific domains such as
histopathology, topology is an important descriptor that can be used to
distinguish between disease-indicating tissue by analyzing the shape
characteristics of cells. Current literature suggests that reintroducing
topological information using persistent homology can improve medical
diagnostics; however, previous methods utilize global topological summaries
which do not contain information about the locality of topological features. To
address this gap, we present a novel method that generates local persistent
homology-based data using a modified version of the convolution operator called
Persistent Homology Convolutions. This method captures information about the
locality and translation invariance of topological features. We perform a
comparative study using various representations of histopathology slides and
find that models trained with persistent homology convolutions outperform
conventionally trained models and are less sensitive to hyperparameters. These
results indicate that persistent homology convolutions extract meaningful
geometric information from the histopathology slides.

</details>


### [278] [QUTCC: Quantile Uncertainty Training and Conformal Calibration for Imaging Inverse Problems](https://arxiv.org/abs/2507.14760)
*Cassandra Tong Ye,Shamus Li,Tyler King,Kristina Monakhova*

Main category: eess.IV

TL;DR: QUTCC introduces a nonlinear, non-uniform scaling method for quantile predictions to tighten uncertainty bounds in deep learning models, improving reliability in medical imaging tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the issue of hallucinations in deep learning models for medical imaging, where accuracy is critical, by improving uncertainty quantification.

Method: QUTCC uses a U-Net with quantile embedding to predict full conditional quantile distributions and iteratively refines bounds during calibration.

Result: QUTCC achieves tighter uncertainty intervals and better identifies hallucinations compared to prior methods, maintaining statistical coverage.

Conclusion: QUTCC enhances reliability in medical imaging by providing tighter, more informative uncertainty bounds.

Abstract: Deep learning models often hallucinate, producing realistic artifacts that
are not truly present in the sample. This can have dire consequences for
scientific and medical inverse problems, such as MRI and microscopy denoising,
where accuracy is more important than perceptual quality. Uncertainty
quantification techniques, such as conformal prediction, can pinpoint outliers
and provide guarantees for image regression tasks, improving reliability.
However, existing methods utilize a linear constant scaling factor to calibrate
uncertainty bounds, resulting in larger, less informative bounds. We propose
QUTCC, a quantile uncertainty training and calibration technique that enables
nonlinear, non-uniform scaling of quantile predictions to enable tighter
uncertainty estimates. Using a U-Net architecture with a quantile embedding,
QUTCC enables the prediction of the full conditional distribution of quantiles
for the imaging task. During calibration, QUTCC generates uncertainty bounds by
iteratively querying the network for upper and lower quantiles, progressively
refining the bounds to obtain a tighter interval that captures the desired
coverage. We evaluate our method on several denoising tasks as well as
compressive MRI reconstruction. Our method successfully pinpoints
hallucinations in image estimates and consistently achieves tighter uncertainty
intervals than prior methods while maintaining the same statistical coverage.

</details>


### [279] [PET Image Reconstruction Using Deep Diffusion Image Prior](https://arxiv.org/abs/2507.15078)
*Fumio Hashimoto,Kuang Gong*

Main category: eess.IV

TL;DR: A diffusion model-based PET image reconstruction method using anatomical priors and HQS for efficiency, showing robust performance across tracers and scanners.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of diffusion models in PET imaging, such as tracer-specific contrast variability and high computational demands.

Method: Combines diffusion sampling and model fine-tuning guided by PET sinogram, with HQS for decoupling optimization from reconstruction.

Result: Demonstrated robust generalization across tracer distributions and scanner types in simulation and clinical datasets.

Conclusion: Provides an efficient, versatile framework for low-dose PET imaging with cross-tracer applicability.

Abstract: Diffusion models have shown great promise in medical image denoising and
reconstruction, but their application to Positron Emission Tomography (PET)
imaging remains limited by tracer-specific contrast variability and high
computational demands. In this work, we proposed an anatomical prior-guided PET
image reconstruction method based on diffusion models, inspired by the deep
diffusion image prior (DDIP) framework. The proposed method alternated between
diffusion sampling and model fine-tuning guided by the PET sinogram, enabling
the reconstruction of high-quality images from various PET tracers using a
score function pretrained on a dataset of another tracer. To improve
computational efficiency, the half-quadratic splitting (HQS) algorithm was
adopted to decouple network optimization from iterative PET reconstruction. The
proposed method was evaluated using one simulation and two clinical datasets.
For the simulation study, a model pretrained on [$^{18}$F]FDG data was tested
on amyloid-negative PET data to assess out-of-distribution (OOD) performance.
For the clinical-data validation, ten low-dose [$^{18}$F]FDG datasets and one
[$^{18}$F]Florbetapir dataset were tested on a model pretrained on data from
another tracer. Experiment results show that the proposed PET reconstruction
method can generalize robustly across tracer distributions and scanner types,
providing an efficient and versatile reconstruction framework for low-dose PET
imaging.

</details>


### [280] [Performance Analysis of Post-Training Quantization for CNN-based Conjunctival Pallor Anemia Detection](https://arxiv.org/abs/2507.15151)
*Sebastian A. Cruz Romero,Wilfredo E. Lugo Beauchamp*

Main category: eess.IV

TL;DR: A deep learning model using MobileNet detects anemia via conjunctival pallor with high accuracy, precision, and F1 score, while exploring quantization for edge deployment.


<details>
  <summary>Details</summary>
Motivation: Traditional anemia detection methods are costly and require expertise, limiting accessibility in low-resource settings.

Method: MobileNet architecture fine-tuned with data augmentation and cross-validation on the CP-AnemiC dataset (710 images).

Result: Achieved accuracy of 0.9313, precision of 0.9374, and F1 score of 0.9773. FP16 quantization maintained performance, while INT8/INT4 degraded it.

Conclusion: Supports further optimization of quantization for mobile healthcare, balancing model size, speed, and accuracy.

Abstract: Anemia is a widespread global health issue, particularly among young children
in low-resource settings. Traditional methods for anemia detection often
require expensive equipment and expert knowledge, creating barriers to early
and accurate diagnosis. To address these challenges, we explore the use of deep
learning models for detecting anemia through conjunctival pallor, focusing on
the CP-AnemiC dataset, which includes 710 images from children aged 6-59
months. The dataset is annotated with hemoglobin levels, gender, age and other
demographic data, enabling the development of machine learning models for
accurate anemia detection. We use the MobileNet architecture as a backbone,
known for its efficiency in mobile and embedded vision applications, and
fine-tune our model end-to-end using data augmentation techniques and a
cross-validation strategy. Our model implementation achieved an accuracy of
0.9313, a precision of 0.9374, and an F1 score of 0.9773 demonstrating strong
performance on the dataset. To optimize the model for deployment on edge
devices, we performed post-training quantization, evaluating the impact of
different bit-widths (FP32, FP16, INT8, and INT4) on model performance.
Preliminary results suggest that while FP16 quantization maintains high
accuracy (0.9250), precision (0.9370), and F1 Score (0.9377), more aggressive
quantization (INT8 and INT4) leads to significant performance degradation.
Overall, our study supports further exploration of quantization schemes and
hardware optimizations to assess trade-offs between model size, inference time,
and diagnostic accuracy in mobile healthcare applications.

</details>


### [281] [A Study of Anatomical Priors for Deep Learning-Based Segmentation of Pheochromocytoma in Abdominal CT](https://arxiv.org/abs/2507.15193)
*Tanjin Taher Toma,Tejas Sudharshan Mathai,Bikash Santra,Pritam Mukherjee,Jianfei Liu,Wesley Jong,Darwish Alabyad,Vivek Batheja,Abhishek Jha,Mayank Patel,Darko Pucar,Jayadira del Rivero,Karel Pacak,Ronald M. Summers*

Main category: eess.IV

TL;DR: The study evaluates anatomical priors for improving deep learning-based PCC segmentation in CT scans, finding the Tumor + Kidney + Aorta (TKA) strategy most effective.


<details>
  <summary>Details</summary>
Motivation: Accurate PCC segmentation aids tumor burden estimation, prognosis, and treatment planning, while reducing reliance on expensive genetic testing.

Method: The nnU-Net framework was used to test 11 annotation strategies, including novel multi-class schemes based on organ-specific anatomical priors, on 105 CT scans.

Result: TKA outperformed the Tumor + Body (TB) strategy in segmentation accuracy (DSC, NSD, F1 score) and tumor burden quantification (R^2 = 0.968).

Conclusion: Incorporating relevant anatomical context enhances PCC segmentation, supporting clinical assessment and monitoring.

Abstract: Accurate segmentation of pheochromocytoma (PCC) in abdominal CT scans is
essential for tumor burden estimation, prognosis, and treatment planning. It
may also help infer genetic clusters, reducing reliance on expensive testing.
This study systematically evaluates anatomical priors to identify
configurations that improve deep learning-based PCC segmentation. We employed
the nnU-Net framework to evaluate eleven annotation strategies for accurate 3D
segmentation of pheochromocytoma, introducing a set of novel multi-class
schemes based on organ-specific anatomical priors. These priors were derived
from adjacent organs commonly surrounding adrenal tumors (e.g., liver, spleen,
kidney, aorta, adrenal gland, and pancreas), and were compared against a broad
body-region prior used in previous work. The framework was trained and tested
on 105 contrast-enhanced CT scans from 91 patients at the NIH Clinical Center.
Performance was measured using Dice Similarity Coefficient (DSC), Normalized
Surface Distance (NSD), and instance-wise F1 score. Among all strategies, the
Tumor + Kidney + Aorta (TKA) annotation achieved the highest segmentation
accuracy, significantly outperforming the previously used Tumor + Body (TB)
annotation across DSC (p = 0.0097), NSD (p = 0.0110), and F1 score (25.84%
improvement at an IoU threshold of 0.5), measured on a 70-30 train-test split.
The TKA model also showed superior tumor burden quantification (R^2 = 0.968)
and strong segmentation across all genetic subtypes. In five-fold
cross-validation, TKA consistently outperformed TB across IoU thresholds (0.1
to 0.5), reinforcing its robustness and generalizability. These findings
highlight the value of incorporating relevant anatomical context in deep
learning models to achieve precise PCC segmentation, supporting clinical
assessment and longitudinal monitoring.

</details>


### [282] [Personalized 3D Myocardial Infarct Geometry Reconstruction from Cine MRI with Explicit Cardiac Motion Modeling](https://arxiv.org/abs/2507.15194)
*Yilin Lyu,Fan Yang,Xiaoyue Liu,Zichen Jiang,Joshua Dillon,Debbie Zhao,Martyn Nash,Charlene Mauger,Alistair Young,Ching-Hui Sia,Mark YY Chan,Lei Li*

Main category: eess.IV

TL;DR: A novel framework for 3D myocardial infarct reconstruction from 2D cine MRI, eliminating contrast agents by leveraging motion patterns.


<details>
  <summary>Details</summary>
Motivation: LGE MRI, the gold standard for infarct detection, requires contrast agents and suffers from limited spatial resolution due to sparse 2D slices.

Method: Reconstructs 4D biventricular mesh from cine MRI using biv-me, then uses CMotion2Infarct-Net to localize infarcts via motion patterns.

Result: Shows reasonable agreement with manual delineation on 205 cine MRI scans from 126 MI patients.

Conclusion: Demonstrates feasibility of contrast-free, motion-driven 3D infarct reconstruction for digital twin applications.

Abstract: Accurate representation of myocardial infarct geometry is crucial for
patient-specific cardiac modeling in MI patients. While Late gadolinium
enhancement (LGE) MRI is the clinical gold standard for infarct detection, it
requires contrast agents, introducing side effects and patient discomfort.
Moreover, infarct reconstruction from LGE often relies on sparsely sampled 2D
slices, limiting spatial resolution and accuracy. In this work, we propose a
novel framework for automatically reconstructing high-fidelity 3D myocardial
infarct geometry from 2D clinically standard cine MRI, eliminating the need for
contrast agents. Specifically, we first reconstruct the 4D biventricular mesh
from multi-view cine MRIs via an automatic deep shape fitting model, biv-me.
Then, we design a infarction reconstruction model, CMotion2Infarct-Net, to
explicitly utilize the motion patterns within this dynamic geometry to localize
infarct regions. Evaluated on 205 cine MRI scans from 126 MI patients, our
method shows reasonable agreement with manual delineation. This study
demonstrates the feasibility of contrast-free, cardiac motion-driven 3D infarct
reconstruction, paving the way for efficient digital twin of MI.

</details>


### [283] [Personalized 4D Whole Heart Geometry Reconstruction from Cine MRI for Cardiac Digital Twins](https://arxiv.org/abs/2507.15203)
*Xiaoyue Liu,Xicheng Sheng,Xiahai Zhuang,Vicente Grau,Mark YY Chan,Ching-Hui Sia,Lei Li*

Main category: eess.IV

TL;DR: A weakly supervised learning model is proposed to reconstruct 4D heart meshes from multi-view 2D cardiac cine MRIs, enabling personalized cardiac digital twins for precision medicine.


<details>
  <summary>Details</summary>
Motivation: Whole-heart cardiac digital twin models simulating full organ-scale electromechanics are limited. This work aims to bridge this gap by creating personalized 4D heart models from 2D MRIs.

Method: A weakly supervised learning model maps multi-view 2D cardiac cine MRIs to 4D heart meshes, leveraging self-supervised learning for accurate reconstruction.

Result: The model successfully generates 4D heart meshes, enabling automatic extraction of key cardiac variables like ejection fraction and dynamic chamber volume changes.

Conclusion: The study demonstrates the feasibility of inferring personalized 4D heart models from MRIs, advancing efficient cardiac digital twin platforms for precision medicine.

Abstract: Cardiac digital twins (CDTs) provide personalized in-silico cardiac
representations and hold great potential for precision medicine in cardiology.
However, whole-heart CDT models that simulate the full organ-scale
electromechanics of all four heart chambers remain limited. In this work, we
propose a weakly supervised learning model to reconstruct 4D (3D+t) heart mesh
directly from multi-view 2D cardiac cine MRIs. This is achieved by learning a
self-supervised mapping between cine MRIs and 4D cardiac meshes, enabling the
generation of personalized heart models that closely correspond to input cine
MRIs. The resulting 4D heart meshes can facilitate the automatic extraction of
key cardiac variables, including ejection fraction and dynamic chamber volume
changes with high temporal resolution. It demonstrates the feasibility of
inferring personalized 4D heart models from cardiac MRIs, paving the way for an
efficient CDT platform for precision medicine. The code will be publicly
released once the manuscript is accepted.

</details>


### [284] [EndoControlMag: Robust Endoscopic Vascular Motion Magnification with Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Contro](https://arxiv.org/abs/2507.15292)
*An Wanga,Rulin Zhou,Mengya Xu,Yiru Ye,Longfei Gou,Yiting Chang,Hao Chen,Chwee Ming Lim,Jiankun Wang,Hongliang Ren*

Main category: eess.IV

TL;DR: EndoControlMag is a training-free, Lagrangian-based framework for magnifying subtle vascular motions in endoscopic surgery, featuring a PRR scheme and HTM framework for robust performance.


<details>
  <summary>Details</summary>
Motivation: Visualizing subtle vascular motions is critical for surgical precision but challenging due to dynamic surgical scenes.

Method: Uses PRR for error-free temporal coherence and HTM with dual-mode mask dilation for adaptive magnification.

Result: Outperforms existing methods in accuracy and visual quality, validated on the EndoVMM24 dataset.

Conclusion: EndoControlMag offers a robust solution for vascular motion magnification in endoscopic surgery.

Abstract: Visualizing subtle vascular motions in endoscopic surgery is crucial for
surgical precision and decision-making, yet remains challenging due to the
complex and dynamic nature of surgical scenes. To address this, we introduce
EndoControlMag, a training-free, Lagrangian-based framework with
mask-conditioned vascular motion magnification tailored to endoscopic
environments. Our approach features two key modules: a Periodic Reference
Resetting (PRR) scheme that divides videos into short overlapping clips with
dynamically updated reference frames to prevent error accumulation while
maintaining temporal coherence, and a Hierarchical Tissue-aware Magnification
(HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores
using a pretrained visual tracking model to maintain accurate localization
despite occlusions and view changes. It then applies one of two adaptive
softening strategies to surrounding tissues: motion-based softening that
modulates magnification strength proportional to observed tissue displacement,
or distance-based exponential decay that simulates biomechanical force
attenuation. This dual-mode approach accommodates diverse surgical
scenarios-motion-based softening excels with complex tissue deformations while
distance-based softening provides stability during unreliable optical flow
conditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four
different surgery types and various challenging scenarios, including
occlusions, instrument disturbance, view changes, and vessel deformations.
Quantitative metrics, visual assessments, and expert surgeon evaluations
demonstrate that EndoControlMag significantly outperforms existing methods in
both magnification accuracy and visual quality while maintaining robustness
across challenging surgical conditions. The code, dataset, and video results
are available at https://szupc.github.io/EndoControlMag/.

</details>


### [285] [MedSR-Impact: Transformer-Based Super-Resolution for Lung CT Segmentation, Radiomics, Classification, and Prognosis](https://arxiv.org/abs/2507.15340)
*Marc Boubnovski Martell,Kristofer Linton-Reid,Mitchell Chen,Sumeet Hindocha,Benjamin Hunter,Marco A. Calzado,Richard Lee,Joram M. Posma,Eric O. Aboagye*

Main category: eess.IV

TL;DR: TVSRN-V2, a transformer-based super-resolution network, improves lung CT analysis by enhancing resolution in low-dose scans, boosting segmentation, radiomics, and prognosis accuracy.


<details>
  <summary>Details</summary>
Motivation: High-resolution CT is crucial for thoracic disease diagnosis but limited by radiation dose and costs. TVSRN-V2 aims to address this by enhancing low-dose CT scans.

Method: Uses Through-Plane Attention Blocks (TAB) and Swin Transformer V2 for super-resolution, with pseudo-low-resolution augmentation for robustness across protocols.

Result: Improves segmentation accuracy (+4% Dice), radiomic feature reproducibility, and predictive performance (+0.06 C-index and AUC).

Conclusion: TVSRN-V2 is a clinically viable solution for dose-efficient CT imaging, enhancing clinical decision support.

Abstract: High-resolution volumetric computed tomography (CT) is essential for accurate
diagnosis and treatment planning in thoracic diseases; however, it is limited
by radiation dose and hardware costs. We present the Transformer Volumetric
Super-Resolution Network (\textbf{TVSRN-V2}), a transformer-based
super-resolution (SR) framework designed for practical deployment in clinical
lung CT analysis. Built from scalable components, including Through-Plane
Attention Blocks (TAB) and Swin Transformer V2 -- our model effectively
reconstructs fine anatomical details in low-dose CT volumes and integrates
seamlessly with downstream analysis pipelines. We evaluate its effectiveness on
three critical lung cancer tasks -- lobe segmentation, radiomics, and prognosis
-- across multiple clinical cohorts. To enhance robustness across variable
acquisition protocols, we introduce pseudo-low-resolution augmentation,
simulating scanner diversity without requiring private data. TVSRN-V2
demonstrates a significant improvement in segmentation accuracy (+4\% Dice),
higher radiomic feature reproducibility, and enhanced predictive performance
(+0.06 C-index and AUC). These results indicate that SR-driven recovery of
structural detail significantly enhances clinical decision support, positioning
TVSRN-V2 as a well-engineered, clinically viable system for dose-efficient
imaging and quantitative analysis in real-world CT workflows.

</details>


### [286] [Latent Space Synergy: Text-Guided Data Augmentation for Direct Diffusion Biomedical Segmentation](https://arxiv.org/abs/2507.15361)
*Muhammad Aqeel,Maham Nazir,Zanxi Ruan,Francesco Setti*

Main category: eess.IV

TL;DR: SynDiff combines text-guided synthetic data generation and efficient diffusion-based segmentation to address data scarcity in medical image segmentation, achieving high accuracy and real-time performance.


<details>
  <summary>Details</summary>
Motivation: Medical image segmentation, especially polyp detection, faces data scarcity due to the need for specialized annotation expertise. SynDiff aims to bridge this gap by generating realistic synthetic data.

Method: The framework uses latent diffusion models for text-conditioned inpainting to create diverse synthetic polyps. It introduces direct latent estimation for single-step inference, reducing computational overhead.

Result: SynDiff achieves 96.0% Dice and 92.9% IoU on CVC-ClinicDB, with real-time capability suitable for clinical use.

Conclusion: SynDiff effectively improves segmentation robustness through controlled synthetic augmentation, offering a practical solution for resource-limited medical settings.

Abstract: Medical image segmentation suffers from data scarcity, particularly in polyp
detection where annotation requires specialized expertise. We present SynDiff,
a framework combining text-guided synthetic data generation with efficient
diffusion-based segmentation. Our approach employs latent diffusion models to
generate clinically realistic synthetic polyps through text-conditioned
inpainting, augmenting limited training data with semantically diverse samples.
Unlike traditional diffusion methods requiring iterative denoising, we
introduce direct latent estimation enabling single-step inference with T x
computational speedup. On CVC-ClinicDB, SynDiff achieves 96.0% Dice and 92.9%
IoU while maintaining real-time capability suitable for clinical deployment.
The framework demonstrates that controlled synthetic augmentation improves
segmentation robustness without distribution shift. SynDiff bridges the gap
between data-hungry deep learning models and clinical constraints, offering an
efficient solution for deployment in resourcelimited medical settings.

</details>


### [287] [A Steel Surface Defect Detection Method Based on Lightweight Convolution Optimization](https://arxiv.org/abs/2507.15476)
*Cong Chen,Ming Chen,Hoileong Lee,Yan Li,Jiyang Yu*

Main category: eess.IV

TL;DR: A deep learning framework (YOLOv9s with C3Ghost, SCConv, and CARAFE) improves steel surface defect detection accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with multi-scale defect detection due to insufficient accuracy and high miss-detection rates.

Method: Combines YOLOv9s with SCConv (reduces feature redundancy), C3Ghost (enhances feature extraction), and CARAFE (optimizes upsampling).

Result: Higher accuracy and robustness in defect detection compared to other methods.

Conclusion: The proposed framework effectively addresses challenges in steel surface defect detection.

Abstract: Surface defect detection of steel, especially the recognition of multi-scale
defects, has always been a major challenge in industrial manufacturing. Steel
surfaces not only have defects of various sizes and shapes, which limit the
accuracy of traditional image processing and detection methods in complex
environments. However, traditional defect detection methods face issues of
insufficient accuracy and high miss-detection rates when dealing with small
target defects. To address this issue, this study proposes a detection
framework based on deep learning, specifically YOLOv9s, combined with the
C3Ghost module, SCConv module, and CARAFE upsampling operator, to improve
detection accuracy and model performance. First, the SCConv module is used to
reduce feature redundancy and optimize feature representation by reconstructing
the spatial and channel dimensions. Second, the C3Ghost module is introduced to
enhance the model's feature extraction ability by reducing redundant
computations and parameter volume, thereby improving model efficiency. Finally,
the CARAFE upsampling operator, which can more finely reorganize feature maps
in a content-aware manner, optimizes the upsampling process and ensures
detailed restoration of high-resolution defect regions. Experimental results
demonstrate that the proposed model achieves higher accuracy and robustness in
steel surface defect detection tasks compared to other methods, effectively
addressing defect detection problems.

</details>


### [288] [DeSamba: Decoupled Spectral Adaptive Framework for 3D Multi-Sequence MRI Lesion Classification](https://arxiv.org/abs/2507.15487)
*Dezhen Wang,Sheng Miao,Rongxin Chai,Jiufa Cui*

Main category: eess.IV

TL;DR: DeSamba is a novel framework for 3D lesion classification in multi-sequence MRI, combining decoupled representation learning and adaptive spectral-spatial feature fusion, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Effective integration of multi-sequence MRI data for robust 3D lesion classification is challenging.

Method: DeSamba uses a Decoupled Representation Learning Module (DRLM) and Spectral Adaptive Modulation Block (SAMB) for feature decoupling and adaptive fusion.

Result: Achieves 62.10% Top-1 accuracy on spinal metastasis and 70.00%/64.52% accuracy on spondylitis datasets, outperforming baselines.

Conclusion: DeSamba is a generalizable and effective solution for 3D lesion classification in medical imaging.

Abstract: Magnetic Resonance Imaging (MRI) sequences provide rich spatial and frequency
domain information, which is crucial for accurate lesion classification in
medical imaging. However, effectively integrating multi-sequence MRI data for
robust 3D lesion classification remains a challenge. In this paper, we propose
DeSamba (Decoupled Spectral Adaptive Network and Mamba-Based Model), a novel
framework designed to extract decoupled representations and adaptively fuse
spatial and spectral features for lesion classification. DeSamba introduces a
Decoupled Representation Learning Module (DRLM) that decouples features from
different MRI sequences through self-reconstruction and cross-reconstruction,
and a Spectral Adaptive Modulation Block (SAMB) within the proposed SAMNet,
enabling dynamic fusion of spectral and spatial information based on lesion
characteristics. We evaluate DeSamba on two clinically relevant 3D datasets. On
a six-class spinal metastasis dataset (n=1,448), DeSamba achieves 62.10% Top-1
accuracy, 63.62% F1-score, 87.71% AUC, and 93.55% Top-3 accuracy on an external
validation set (n=372), outperforming all state-of-the-art (SOTA) baselines. On
a spondylitis dataset (n=251) involving a challenging binary classification
task, DeSamba achieves 70.00%/64.52% accuracy and 74.75/73.88 AUC on internal
and external validation sets, respectively. Ablation studies demonstrate that
both DRLM and SAMB significantly contribute to overall performance, with over
10% relative improvement compared to the baseline. Our results highlight the
potential of DeSamba as a generalizable and effective solution for 3D lesion
classification in multi-sequence medical imaging.

</details>


### [289] [RARE-UNet: Resolution-Aligned Routing Entry for Adaptive Medical Image Segmentation](https://arxiv.org/abs/2507.15524)
*Simon Winther Albertsen,Hjalte Svaneborg Bjørnstrup,Mostafa Mehdipour Ghazi*

Main category: eess.IV

TL;DR: RARE-UNet is a resolution-aware multi-scale segmentation model that dynamically adapts to input resolution, outperforming standard models in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing segmentation models degrade with lower-resolution inputs, limiting real-world clinical applications.

Method: Proposes RARE-UNet with multi-scale blocks, resolution-aware routing, and consistency-driven training for dynamic adaptation.

Result: Achieves highest Dice scores (0.84, 0.65) for hippocampus and tumor segmentation, with reduced inference time at lower resolutions.

Conclusion: RARE-UNet is effective and scalable for resolution-robust segmentation, with code available for public use.

Abstract: Accurate segmentation is crucial for clinical applications, but existing
models often assume fixed, high-resolution inputs and degrade significantly
when faced with lower-resolution data in real-world scenarios. To address this
limitation, we propose RARE-UNet, a resolution-aware multi-scale segmentation
architecture that dynamically adapts its inference path to the spatial
resolution of the input. Central to our design are multi-scale blocks
integrated at multiple encoder depths, a resolution-aware routing mechanism,
and consistency-driven training that aligns multi-resolution features with
full-resolution representations. We evaluate RARE-UNet on two benchmark brain
imaging tasks for hippocampus and tumor segmentation. Compared to standard
UNet, its multi-resolution augmented variant, and nnUNet, our model achieves
the highest average Dice scores of 0.84 and 0.65 across resolution, while
maintaining consistent performance and significantly reduced inference time at
lower resolutions. These results highlight the effectiveness and scalability of
our architecture in achieving resolution-robust segmentation. The codes are
available at: https://github.com/simonsejse/RARE-UNet.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [290] [Hyper-spectral Unmixing algorithms for remote compositional surface mapping: a review of the state of the art](https://arxiv.org/abs/2507.14260)
*Alfredo Gimenez Zapiola,Andrea Boselli,Alessandra Menafoglio,Simone Vantini*

Main category: astro-ph.IM

TL;DR: A review of hyper-spectral unmixing methods for analyzing remotely sensed images of Earth and other astronomical objects, including material inference, abundance estimation, and spatial distribution.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurately inferring surface materials and their distributions from hyper-spectral images of large areas.

Method: Review and comparison of successful hyper-spectral unmixing methods, analysis of recent methodologies, and exploration of public datasets.

Result: Identification of key methods and datasets, along with insights into their effectiveness.

Conclusion: Highlights open problems and provides recommendations for future research in hyper-spectral unmixing.

Abstract: This work concerns a detailed review of data analysis methods used for
remotely sensed images of large areas of the Earth and of other solid
astronomical objects. In detail, it focuses on the problem of inferring the
materials that cover the surfaces captured by hyper-spectral images and
estimating their abundances and spatial distributions within the region. The
most successful and relevant hyper-spectral unmixing methods are reported as
well as compared, as an addition to analysing the most recent methodologies.
The most important public data-sets in this setting, which are vastly used in
the testing and validation of the former, are also systematically explored.
Finally, open problems are spotlighted and concrete recommendations for future
research are provided.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [291] [Design of an Edge-based Portable EHR System for Anemia Screening in Remote Health Applications](https://arxiv.org/abs/2507.15146)
*Sebastian A. Cruz Romero,Misael J. Mercado Hernandez,Samir Y. Ali Rivera,Jorge A. Santiago Fernandez,Wilfredo E. Lugo Beauchamp*

Main category: cs.ET

TL;DR: A portable, edge-enabled Electronic Health Record platform is designed for offline-first operation, secure data management, and modular diagnostics, addressing challenges in remote healthcare.


<details>
  <summary>Details</summary>
Motivation: To overcome poor interoperability, lack of offline support, and high infrastructure costs in medical systems for resource-limited areas.

Method: Developed a platform with AES-256 encrypted local storage, cloud sync, and integrated anemia screening using fingernail pallor analysis with a Random Forest model and YOLOv8n-based detector.

Result: Achieved test RMSE of 1.969 g/dL, MAE of 1.490 g/dL, and 79.2% sensitivity for anemia screening. Optimized detector latency reduced from 46.96 ms to 21.50 ms.

Conclusion: The system offers a scalable, low-cost solution for digital health in underserved regions, emphasizing modularity, privacy, and offline functionality.

Abstract: The design of medical systems for remote, resource-limited environments faces
persistent challenges due to poor interoperability, lack of offline support,
and dependency on costly infrastructure. Many existing digital health solutions
neglect these constraints, limiting their effectiveness for frontline health
workers in underserved regions. This paper presents a portable, edge-enabled
Electronic Health Record platform optimized for offline-first operation, secure
patient data management, and modular diagnostic integration. Running on
small-form factor embedded devices, it provides AES-256 encrypted local storage
with optional cloud synchronization for interoperability. As a use case, we
integrated a non-invasive anemia screening module leveraging fingernail pallor
analysis. Trained on 250 patient cases (27\% anemia prevalence) with
KDE-balanced data, the Random Forest model achieved a test RMSE of 1.969 g/dL
and MAE of 1.490 g/dL. A severity-based model reached 79.2\% sensitivity. To
optimize performance, a YOLOv8n-based nail bed detector was quantized to INT8,
reducing inference latency from 46.96 ms to 21.50 ms while maintaining mAP@0.5
at 0.995. The system emphasizes low-cost deployment, modularity, and data
privacy compliance (HIPAA/GDPR), addressing critical barriers to digital health
adoption in disconnected settings. Our work demonstrates a scalable approach to
enhance portable health information systems and support frontline healthcare in
underserved regions.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [292] [Optimizing Legal Document Retrieval in Vietnamese with Semi-Hard Negative Mining](https://arxiv.org/abs/2507.14619)
*Van-Hoang Le,Duc-Vu Nguyen,Kiet Van Nguyen,Ngan Luu-Thuy Nguyen*

Main category: cs.IR

TL;DR: A two-stage framework (Retrieval and Re-ranking) improves legal document retrieval for LLMs, using fine-tuned Bi-Encoder and Cross-Encoder with strategic negative mining. Achieved top-three in SoICT Hackathon 2024.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle in specialized domains like law due to precision and domain-specific knowledge requirements.

Method: Fine-tuned Bi-Encoder for retrieval, Cross-Encoder for re-ranking, optimized with negative example mining and Exist@m metric.

Result: Top-three performance in SoICT Hackathon 2024, competitive with lightweight single-pass approach.

Conclusion: Optimized data processing, tailored loss functions, and balanced negative sampling are key for robust legal retrieval systems.

Abstract: Large Language Models (LLMs) face significant challenges in specialized
domains like law, where precision and domain-specific knowledge are critical.
This paper presents a streamlined two-stage framework consisting of Retrieval
and Re-ranking to enhance legal document retrieval efficiency and accuracy. Our
approach employs a fine-tuned Bi-Encoder for rapid candidate retrieval,
followed by a Cross-Encoder for precise re-ranking, both optimized through
strategic negative example mining. Key innovations include the introduction of
the Exist@m metric to evaluate retrieval effectiveness and the use of semi-hard
negatives to mitigate training bias, which significantly improved re-ranking
performance. Evaluated on the SoICT Hackathon 2024 for Legal Document
Retrieval, our team, 4Huiter, achieved a top-three position. While
top-performing teams employed ensemble models and iterative self-training on
large bge-m3 architectures, our lightweight, single-pass approach offered a
competitive alternative with far fewer parameters. The framework demonstrates
that optimized data processing, tailored loss functions, and balanced negative
sampling are pivotal for building robust retrieval-augmented systems in legal
contexts.

</details>


### [293] [GREAT: Guiding Query Generation with a Trie for Recommending Related Search about Video at Kuaishou](https://arxiv.org/abs/2507.15267)
*Ninglu Shao,Jinshan Wang,Chenxu Wang,Qingbiao Li,Xiaoxue Zang,Han Li*

Main category: cs.IR

TL;DR: The paper introduces a novel LLM-based framework, GREAT, for query recommendation in video-related search, addressing the lack of research and datasets in this domain.


<details>
  <summary>Details</summary>
Motivation: Short video platforms lack academic research and datasets for query recommendation (I2Q), prompting the need for a systematic solution.

Method: Proposes GREAT, an LLM-based framework using a query-based trie to guide query generation and post-processing for relevance.

Result: Offline and online experiments confirm the effectiveness of GREAT in improving query recommendation.

Conclusion: GREAT successfully addresses the I2Q recommendation challenge, offering a scalable and effective solution.

Abstract: Currently, short video platforms have become the primary place for
individuals to share experiences and obtain information. To better meet users'
needs for acquiring information while browsing short videos, some apps have
introduced a search entry at the bottom of videos, accompanied with recommended
relevant queries. This scenario is known as query recommendation in
video-related search, where core task is item-to-query (I2Q) recommendation. As
this scenario has only emerged in recent years, there is a notable scarcity of
academic research and publicly available datasets in this domain. To address
this gap, we systematically examine the challenges associated with this
scenario for the first time. Subsequently, we release a large-scale dataset
derived from real-world data pertaining to the query recommendation in
video-\textit{\textbf{r}}elated \textit{\textbf{s}}earch on the
\textit{\textbf{Kuai}}shou app (\textbf{KuaiRS}). Presently, existing methods
rely on embeddings to calculate similarity for matching short videos with
queries, lacking deep interaction between the semantic content and the query.
In this paper, we introduce a novel LLM-based framework named \textbf{GREAT},
which \textit{\textbf{g}}uides que\textit{\textbf{r}}y
g\textit{\textbf{e}}ner\textit{\textbf{a}}tion with a \textit{\textbf{t}}rie to
address I2Q recommendation in related search. Specifically, we initially gather
high-quality queries with high exposure and click-through rate to construct a
query-based trie. During training, we enhance the LLM's capability to generate
high-quality queries using the query-based trie. In the inference phase, the
query-based trie serves as a guide for the token generation. Finally, we
further refine the relevance and literal quality between items and queries via
a post-processing module. Extensive offline and online experiments demonstrate
the effectiveness of our proposed method.

</details>


### [294] [LOVO: Efficient Complex Object Query in Large-Scale Video Datasets](https://arxiv.org/abs/2507.14301)
*Yuxin Liu,Yuezhang Peng,Hefeng Zhou,Hongze Liu,Xinyu Lu,Jiong Lou,Chentao Wu,Wei Zhao,Jie Li*

Main category: cs.IR

TL;DR: LOVO is a system for efficient object queries in large-scale video datasets, using pre-trained visual encoders and an inverted multi-index structure for fast, accurate searches with low latency.


<details>
  <summary>Details</summary>
Motivation: The exponential growth of video data demands efficient querying systems to handle massive volumes, complex queries, and low-latency execution, which existing methods fail to address adequately.

Method: LOVO performs one-time feature extraction, organizes embeddings in an inverted multi-index, and uses approximate nearest-neighbor searches with cross-modal reranking for refined results.

Result: LOVO achieves near-optimal query accuracy, up to 85x lower search latency, and reduced index construction costs compared to existing methods.

Conclusion: LOVO sets a new benchmark for complex object queries in video analysis, offering scalability, efficiency, and adaptability to dynamic environments.

Abstract: The widespread deployment of cameras has led to an exponential increase in
video data, creating vast opportunities for applications such as traffic
management and crime surveillance. However, querying specific objects from
large-scale video datasets presents challenges, including (1) processing
massive and continuously growing data volumes, (2) supporting complex query
requirements, and (3) ensuring low-latency execution. Existing video analysis
methods struggle with either limited adaptability to unseen object classes or
suffer from high query latency. In this paper, we present LOVO, a novel system
designed to efficiently handle comp$\underline{L}$ex $\underline{O}$bject
queries in large-scale $\underline{V}$ide$\underline{O}$ datasets. Agnostic to
user queries, LOVO performs one-time feature extraction using pre-trained
visual encoders, generating compact visual embeddings for key frames to build
an efficient index. These visual embeddings, along with associated bounding
boxes, are organized in an inverted multi-index structure within a vector
database, which supports queries for any objects. During the query phase, LOVO
transforms object queries to query embeddings and conducts fast approximate
nearest-neighbor searches on the visual embeddings. Finally, a cross-modal
rerank is performed to refine the results by fusing visual features with
detailed textual features. Evaluation on real-world video datasets demonstrates
that LOVO outperforms existing methods in handling complex queries, with
near-optimal query accuracy and up to 85x lower search latency, while
significantly reducing index construction costs. This system redefines the
state-of-the-art object query approaches in video analysis, setting a new
benchmark for complex object queries with a novel, scalable, and efficient
approach that excels in dynamic environments.

</details>


### [295] [U-MARVEL: Unveiling Key Factors for Universal Multimodal Retrieval via Embedding Learning with MLLMs](https://arxiv.org/abs/2507.14902)
*Xiaojie Li,Chu Li,Shi-Zhe Chen,Xi Chen*

Main category: cs.IR

TL;DR: The paper introduces U-MARVEL, a unified framework for universal multimodal retrieval (UMR), analyzing key factors for effective embedding learning with MLLMs and outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM-based UMR methods lack understanding of their retrieval mechanisms, leading to suboptimal performance and limited generalization.

Method: The study implements a general MLLM-based pipeline, analyzes embedding generation and training strategies (e.g., progressive transition, hard negative mining), and introduces U-MARVEL.

Result: U-MARVEL significantly outperforms competitors on the M-BEIR benchmark and shows strong zero-shot performance in tasks like composed image retrieval.

Conclusion: The framework demonstrates strong generalization across embedding-based retrieval tasks, highlighting the impact of overlooked factors.

Abstract: Universal multimodal retrieval (UMR), which aims to address complex retrieval
tasks where both queries and candidates span diverse modalities, has been
significantly advanced by the emergence of MLLMs. While state-of-the-art
MLLM-based methods in the literature predominantly adopt contrastive learning
principles, they often differ in their specific training recipes. Despite their
success, the mechanisms underlying their retrieval capabilities remain largely
unexplored, potentially resulting in suboptimal performance and limited
generalization ability. To address these issues, we present a comprehensive
study aimed at uncovering the key factors that drive effective embedding
learning for UMR using MLLMs. We begin by implementing a general MLLM-based
embedding learning pipeline, and systematically analyze the primary
contributors to high-performing universal retrieval systems. Based on this, we
explore various aspects of the details in embedding generation and training
strategies, including progressive transition, hard negative mining and
re-ranker distillation. Notably, our findings reveal that often-overlooked
factors can have a substantial impact on model performance. Building on these
discoveries, we introduce a unified framework termed U-MARVEL
(\textbf{U}niversal \textbf{M}ultimod\textbf{A}l \textbf{R}etrie\textbf{V}al
via \textbf{E}mbedding \textbf{L}earning), which outperforms state-of-the-art
competitors on the M-BEIR benchmark by a large margin in supervised settings,
and also exihibits strong zero-shot performance on several tasks such as
composed image retrieval and text-to-video retrieval. These results underscore
the generalization potential of our framework across various embedding-based
retrieval tasks. Code is available at https://github.com/chaxjli/U-MARVEL

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [296] [Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion](https://arxiv.org/abs/2507.14534)
*Yu Zhang,Baotong Tian,Zhiyao Duan*

Main category: eess.AS

TL;DR: Conan is a zero-shot online voice conversion model addressing real-time constraints, semantic fidelity, and unseen speaker adaptation with three core components: Stream Content Extractor, Adaptive Style Encoder, and Causal Shuffle Vocoder.


<details>
  <summary>Details</summary>
Motivation: Current VC models struggle with real-time constraints, semantic fidelity, and adapting to unseen speaker characteristics.

Method: Conan uses a Stream Content Extractor (Emformer-based), Adaptive Style Encoder, and Causal Shuffle Vocoder (HiFiGAN with pixel-shuffle).

Result: Conan outperforms baseline models in subjective and objective metrics.

Conclusion: Conan effectively addresses challenges in zero-shot online voice conversion, offering improved performance and adaptability.

Abstract: Zero-shot online voice conversion (VC) holds significant promise for
real-time communications and entertainment. However, current VC models struggle
to preserve semantic fidelity under real-time constraints, deliver
natural-sounding conversions, and adapt effectively to unseen speaker
characteristics. To address these challenges, we introduce Conan, a chunkwise
online zero-shot voice conversion model that preserves the content of the
source while matching the voice timbre and styles of reference speech. Conan
comprises three core components: 1) a Stream Content Extractor that leverages
Emformer for low-latency streaming content encoding; 2) an Adaptive Style
Encoder that extracts fine-grained stylistic features from reference speech for
enhanced style adaptation; 3) a Causal Shuffle Vocoder that implements a fully
causal HiFiGAN using a pixel-shuffle mechanism. Experimental evaluations
demonstrate that Conan outperforms baseline models in subjective and objective
metrics. Audio samples can be found at https://aaronz345.github.io/ConanDemo.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [297] [What do Large Language Models know about materials?](https://arxiv.org/abs/2507.14586)
*Adrian Ehrenhofer,Thomas Wallmersperger,Gianaurelio Cuniberti*

Main category: physics.app-ph

TL;DR: The paper explores the application of LLMs in mechanical engineering and materials science, focusing on their ability to generate accurate material knowledge and proposing a benchmark for their use in the PSPP chain.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used in engineering, but their training on non-scientific internet content raises concerns about their accuracy for scientific tasks, necessitating an evaluation of their material knowledge capabilities.

Method: The study evaluates LLMs' performance in generating correct material information, using the Periodic Table of Elements as an example to analyze vocabulary, tokenization, and model accuracy.

Result: The findings highlight the importance of vocabulary and tokenization for material uniqueness and assess the factual correctness of various LLMs, leading to a benchmark for their applicability in the PSPP chain.

Conclusion: The study provides a benchmark to determine where LLMs can be effectively applied in materials science and where specialized models are needed, emphasizing the need for tailored training for scientific accuracy.

Abstract: Large Language Models (LLMs) are increasingly applied in the fields of
mechanical engineering and materials science. As models that establish
connections through the interface of language, LLMs can be applied for
step-wise reasoning through the Processing-Structure-Property-Performance chain
of material science and engineering. Current LLMs are built for adequately
representing a dataset, which is the most part of the accessible internet.
However, the internet mostly contains non-scientific content. If LLMs should be
applied for engineering purposes, it is valuable to investigate models for
their intrinsic knowledge -- here: the capacity to generate correct information
about materials. In the current work, for the example of the Periodic Table of
Elements, we highlight the role of vocabulary and tokenization for the
uniqueness of material fingerprints, and the LLMs' capabilities of generating
factually correct output of different state-of-the-art open models. This leads
to a material knowledge benchmark for an informed choice, for which steps in
the PSPP chain LLMs are applicable, and where specialized models are required.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [298] [Towards a Proactive Autoscaling Framework for Data Stream Processing at the Edge using GRU and Transfer Learning](https://arxiv.org/abs/2507.14597)
*Eugene Armah,Linda Amoako Bannning*

Main category: cs.DC

TL;DR: The paper proposes a three-step solution for proactive edge stream processing autoscaling, combining GRU-based load forecasting, transfer learning, and dynamic resource scaling to address workload fluctuations.


<details>
  <summary>Details</summary>
Motivation: High-speed data processing is critical, but edge stream processing faces challenges like rapid workload fluctuations and inefficient resource provisioning, leading to bottlenecks or wastage. Existing reactive methods and RL approaches have limitations.

Method: A GRU neural network forecasts upstream load, transfer learning integrates the model into an online system using DTW and joint distribution adaptation, and a horizontal autoscaling module dynamically adjusts operator parallelism.

Result: The GRU model achieved up to 1.3% SMAPE on real-world data, outperforming CNN, ARIMA, and Prophet in accuracy and training time.

Conclusion: The proposed solution effectively addresses edge stream processing challenges with accurate load forecasting and dynamic scaling, outperforming existing methods.

Abstract: Processing data at high speeds is becoming increasingly critical as digital
economies generate enormous data. The current paradigms for timely data
processing are edge computing and data stream processing (DSP). Edge computing
places resources closer to where data is generated, while stream processing
analyzes the unbounded high-speed data in motion. However, edge stream
processing faces rapid workload fluctuations, complicating resource
provisioning. Inadequate resource allocation leads to bottlenecks, whereas
excess allocation results in wastage. Existing reactive methods, such as
threshold-based policies and queuing theory scale only after performance
degrades, potentially violating SLAs. Although reinforcement learning (RL)
offers a proactive approach through agents that learn optimal runtime
adaptation policies, it requires extensive simulation. Furthermore, predictive
machine learning models face online distribution and concept drift that
minimize their accuracy. We propose a three-step solution to the proactive edge
stream processing autoscaling problem. Firstly, a GRU neural network forecasts
the upstream load using real-world and synthetic DSP datasets. Secondly, a
transfer learning framework integrates the predictive model into an online
stream processing system using the DTW algorithm and joint distribution
adaptation to handle the disparities between offline and online domains.
Finally, a horizontal autoscaling module dynamically adjusts the degree of
operator parallelism, based on predicted load while considering edge resource
constraints. The lightweight GRU model for load predictions recorded up to
1.3\% SMAPE value on a real-world data set. It outperformed CNN, ARIMA, and
Prophet on the SMAPE and RMSE evaluation metrics, with lower training time than
the computationally intensive RL models.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [299] [Exploiting Context-dependent Duration Features for Voice Anonymization Attack Systems](https://arxiv.org/abs/2507.15214)
*Natalia Tomashenko,Emmanuel Vincent,Marc Tommasi*

Main category: cs.SD

TL;DR: A new method for speaker verification uses context-dependent duration embeddings from speech temporal dynamics, improving performance over simpler methods.


<details>
  <summary>Details</summary>
Motivation: To leverage temporal dynamics (rhythm, intonation, speaking rate) for better speaker identity representation and analyze vulnerabilities in verification and anonymization systems.

Method: Extracts context-dependent duration embeddings from speech temporal dynamics and develops novel attack models.

Result: Attack models significantly improve speaker verification performance for original and anonymized data.

Conclusion: The proposed method outperforms simpler representations, highlighting its effectiveness and potential vulnerabilities in speaker verification systems.

Abstract: The temporal dynamics of speech, encompassing variations in rhythm,
intonation, and speaking rate, contain important and unique information about
speaker identity. This paper proposes a new method for representing speaker
characteristics by extracting context-dependent duration embeddings from speech
temporal dynamics. We develop novel attack models using these representations
and analyze the potential vulnerabilities in speaker verification and voice
anonymization systems.The experimental results show that the developed attack
models provide a significant improvement in speaker verification performance
for both original and anonymized data in comparison with simpler
representations of speech temporal dynamics reported in the literature.

</details>


### [300] [A2TTS: TTS for Low Resource Indian Languages](https://arxiv.org/abs/2507.15272)
*Ayush Singh Bhadoriya,Abhishek Nikunj Shinde,Isha Pandey,Ganesh Ramakrishnan*

Main category: cs.SD

TL;DR: A diffusion-based TTS system for unseen speakers and Indian languages, using speaker embeddings and cross-attention for improved prosody and zero-shot generation.


<details>
  <summary>Details</summary>
Motivation: Address challenges in generating speech for unseen speakers and support diverse Indian languages.

Method: Uses a diffusion-based TTS architecture with speaker embeddings and cross-attention for duration prediction. Classifier-free guidance enhances zero-shot generation.

Result: Speech closely resembles target speakers with improved duration modeling and expressiveness.

Conclusion: The system effectively generates natural speech for diverse Indian languages and unseen speakers.

Abstract: We present a speaker conditioned text-to-speech (TTS) system aimed at
addressing challenges in generating speech for unseen speakers and supporting
diverse Indian languages. Our method leverages a diffusion-based TTS
architecture, where a speaker encoder extracts embeddings from short reference
audio samples to condition the DDPM decoder for multispeaker generation. To
further enhance prosody and naturalness, we employ a cross-attention based
duration prediction mechanism that utilizes reference audio, enabling more
accurate and speaker consistent timing. This results in speech that closely
resembles the target speaker while improving duration modeling and overall
expressiveness. Additionally, to improve zero-shot generation, we employed
classifier free guidance, allowing the system to generate speech more near
speech for unknown speakers. Using this approach, we trained language-specific
speaker-conditioned models. Using the IndicSUPERB dataset for multiple Indian
languages such as Bengali, Gujarati, Hindi, Marathi, Malayalam, Punjabi and
Tamil.

</details>

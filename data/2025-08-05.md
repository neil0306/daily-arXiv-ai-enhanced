<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 97]
- [cs.CV](#cs.CV) [Total: 227]
- [cs.RO](#cs.RO) [Total: 9]
- [cs.CR](#cs.CR) [Total: 7]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.AI](#cs.AI) [Total: 13]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.LG](#cs.LG) [Total: 20]
- [cs.SE](#cs.SE) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [eess.IV](#eess.IV) [Total: 20]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 4]
- [cs.HC](#cs.HC) [Total: 7]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Rethinking Graph-Based Document Classification: Learning Data-Driven Structures Beyond Heuristic Approaches](https://arxiv.org/abs/2508.00864)
*Margarita Bugueño,Gerard de Melo*

Main category: cs.CL

TL;DR: The paper proposes a data-driven method for learning graph structures in document classification, outperforming heuristic-based approaches with higher accuracy and F1 scores.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based document representations rely on manual heuristics or domain knowledge, limiting their adaptability and effectiveness.

Method: The approach constructs homogeneous weighted graphs with sentences as nodes, learning edges via self-attention and applying statistical filtering to retain strongly correlated sentences.

Result: Experiments show the learned graphs outperform heuristic-based ones in accuracy and F1 score, with statistical filtering enhancing robustness.

Conclusion: Automatic graph generation is more effective than heuristic approaches, offering potential for broader NLP applications.

Abstract: In document classification, graph-based models effectively capture document
structure, overcoming sequence length limitations and enhancing contextual
understanding. However, most existing graph document representations rely on
heuristics, domain-specific rules, or expert knowledge. Unlike previous
approaches, we propose a method to learn data-driven graph structures,
eliminating the need for manual design and reducing domain dependence. Our
approach constructs homogeneous weighted graphs with sentences as nodes, while
edges are learned via a self-attention model that identifies dependencies
between sentence pairs. A statistical filtering strategy aims to retain only
strongly correlated sentences, improving graph quality while reducing the graph
size. Experiments on three document classification datasets demonstrate that
learned graphs consistently outperform heuristic-based graphs, achieving higher
accuracy and $F_1$ score. Furthermore, our study demonstrates the effectiveness
of the statistical filtering in improving classification robustness. These
results highlight the potential of automatic graph generation over traditional
heuristic approaches and open new directions for broader applications in NLP.

</details>


### [2] [FECT: Factuality Evaluation of Interpretive AI-Generated Claims in Contact Center Conversation Transcripts](https://arxiv.org/abs/2508.00889)
*Hagyeong Shin,Binoy Robin Dalal,Iwona Bialynicka-Birula,Navjot Matharu,Ryan Muir,Xingwei Yang,Samuel W. K. Wong*

Main category: cs.CL

TL;DR: The paper addresses hallucinations in LLMs for enterprise applications, introduces a 3D paradigm for factuality evaluation, and presents the FECT benchmark dataset for contact center conversations.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs can harm business decisions, especially in contact center analysis where ground-truth labels for interpretations are lacking.

Method: Proposes a 3D (Decompose, Decouple, Detach) paradigm for human annotation and LLM-judges' prompts, and introduces the FECT dataset.

Result: The 3D paradigm and FECT dataset enable better factuality evaluation of AI-generated claims in contact center transcripts.

Conclusion: The work provides a novel approach to automatically evaluate factuality in AI systems for contact center conversation analysis.

Abstract: Large language models (LLMs) are known to hallucinate, producing natural
language outputs that are not grounded in the input, reference materials, or
real-world knowledge. In enterprise applications where AI features support
business decisions, such hallucinations can be particularly detrimental. LLMs
that analyze and summarize contact center conversations introduce a unique set
of challenges for factuality evaluation, because ground-truth labels often do
not exist for analytical interpretations about sentiments captured in the
conversation and root causes of the business problems. To remedy this, we first
introduce a \textbf{3D} -- \textbf{Decompose, Decouple, Detach} -- paradigm in
the human annotation guideline and the LLM-judges' prompt to ground the
factuality labels in linguistically-informed evaluation criteria. We then
introduce \textbf{FECT}, a novel benchmark dataset for \textbf{F}actuality
\textbf{E}valuation of Interpretive AI-Generated \textbf{C}laims in Contact
Center Conversation \textbf{T}ranscripts, labeled under our 3D paradigm.
Lastly, we report our findings from aligning LLM-judges on the 3D paradigm.
Overall, our findings contribute a new approach for automatically evaluating
the factuality of outputs generated by an AI system for analyzing contact
center conversations.

</details>


### [3] [XAutoLM: Efficient Fine-Tuning of Language Models via Meta-Learning and AutoML](https://arxiv.org/abs/2508.00924)
*Ernesto L. Estevanell-Valladares,Suilan Estevez-Velarde,Yoan Gutiérrez,Andrés Montoyo,Ruslan Mitkov*

Main category: cs.CL

TL;DR: XAutoLM is a meta-learning-augmented AutoML framework for efficient LM fine-tuning, outperforming zero-shot optimizers and reducing resource usage.


<details>
  <summary>Details</summary>
Motivation: Experts rely on domain knowledge for LM fine-tuning, but automated frameworks lack efficiency. XAutoLM addresses this gap by reusing past experiences to optimize pipelines.

Method: XAutoLM uses meta-features from past successes and failures to bias sampling, improving efficiency in model selection and hyperparameter optimization.

Result: XAutoLM outperforms zero-shot optimizers, reduces evaluation time by 4.5x, lowers error ratios, and discovers more efficient pipelines.

Conclusion: XAutoLM enables resource-efficient LM fine-tuning, promoting Green AI in NLP.

Abstract: Experts in machine learning leverage domain knowledge to navigate decisions
in model selection, hyperparameter optimisation, and resource allocation. This
is particularly critical for fine-tuning language models (LMs), where repeated
trials incur substantial computational overhead and environmental impact.
However, no existing automated framework simultaneously tackles the entire
model selection and HPO task for resource-efficient LM fine-tuning. We
introduce XAutoLM, a meta-learning-augmented AutoML framework that reuses past
experiences to optimise discriminative and generative LM fine-tuning pipelines
efficiently. XAutoLM learns from stored successes and failures by extracting
task- and system-level meta-features to bias its sampling toward fruitful
configurations and away from costly dead ends. On four text classification and
two question-answering benchmarks, XAutoLM surpasses zero-shot optimiser's peak
F1 on five of six tasks, cuts mean evaluation time by up to 4.5x, reduces error
ratios by up to sevenfold, and uncovers up to 50% more pipelines above the
zero-shot Pareto front. In contrast, simpler memory-based baselines suffer
negative transfer. We release XAutoLM and our experience store to catalyse
resource-efficient, Green AI fine-tuning in the NLP community.

</details>


### [4] [MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented Generation](https://arxiv.org/abs/2508.01005)
*Yiqun Chen,Erhan Zhang,Lingyong Yan,Shuaiqiang Wang,Jizhou Huang,Dawei Yin,Jiaxin Mao*

Main category: cs.CL

TL;DR: The paper introduces MAO-ARAG, an adaptive RAG framework using multi-agent orchestration to dynamically tailor workflows for QA queries, balancing performance and cost efficiency.


<details>
  <summary>Details</summary>
Motivation: Fixed RAG pipelines struggle with varying query complexities, leading to inefficiencies in performance and cost.

Method: MAO-ARAG employs a multi-turn framework with executor agents (e.g., query reformulation, document selection) and a planner agent trained via reinforcement learning to optimize workflows.

Result: Experiments show MAO-ARAG achieves high answer quality while maintaining acceptable cost and latency.

Conclusion: The adaptive MAO-ARAG framework effectively addresses the limitations of fixed RAG systems, offering a scalable solution for diverse QA queries.

Abstract: In question-answering (QA) systems, Retrieval-Augmented Generation (RAG) has
become pivotal in enhancing response accuracy and reducing hallucination
issues. The architecture of RAG systems varies significantly, encompassing
single-round RAG, iterative RAG, and reasoning RAG, each tailored to address
different types of queries. Due to the varying complexity of real-world
queries, a fixed RAG pipeline often struggles to balance performance and cost
efficiency across different queries. To address this challenge, we propose an
adaptive RAG framework called MAO-ARAG, which leverages multi-agent
orchestration. Our adaptive RAG is conceived as a multi-turn framework.
Specifically, we define multiple executor agents, representing typical RAG
modules such as query reformulation agents, document selection agent, and
generation agents. A planner agent intelligently selects and integrates the
appropriate agents from these executors into a suitable workflow tailored for
each query, striving for high-quality answers while maintaining reasonable
costs. During each turn, the planner agent is trained using reinforcement
learning, guided by an outcome-based reward (F1 score) and a cost-based
penalty, continuously improving answer quality while keeping costs within a
reasonable range. Experiments conducted on multiple QA datasets demonstrate
that our approach, which dynamically plans workflows for each query, not only
achieves high answer quality but also maintains both cost and latency within
acceptable limits.The code of MAO-ARAG is on
https://github.com/chenyiqun/Agentic-RAG.

</details>


### [5] [UrBLiMP: A Benchmark for Evaluating the Linguistic Competence of Large Language Models in Urdu](https://arxiv.org/abs/2508.01006)
*Farah Adeeba,Brian Dillon,Hassan Sajjad,Rajesh Bhatt*

Main category: cs.CL

TL;DR: The paper introduces UrBLiMP, a benchmark for evaluating multilingual LLMs' syntactic knowledge in Urdu, revealing performance variations and limitations despite high accuracy in some models.


<details>
  <summary>Details</summary>
Motivation: Assess the linguistic knowledge of multilingual LLMs in Urdu, a low-resource language, due to data disparity compared to high-resource languages.

Method: Created UrBLiMP, a dataset of 5,696 minimal pairs targeting ten syntactic phenomena, evaluated using human annotations and tested on twenty multilingual LLMs.

Result: LLaMA-3-70B achieved the highest accuracy (94.73%), but performance varied across linguistic phenomena, with comparable results to models like Gemma-3-27B-PT.

Conclusion: Current multilingual LLMs show potential but have limitations in capturing fine-grained syntactic knowledge in low-resource languages like Urdu.

Abstract: Multilingual Large Language Models (LLMs) have shown remarkable performance
across various languages; however, they often include significantly less data
for low-resource languages such as Urdu compared to high-resource languages
like English. To assess the linguistic knowledge of LLMs in Urdu, we present
the Urdu Benchmark of Linguistic Minimal Pairs (UrBLiMP) i.e. pairs of
minimally different sentences that contrast in grammatical acceptability.
UrBLiMP comprises 5,696 minimal pairs targeting ten core syntactic phenomena,
carefully curated using the Urdu Treebank and diverse Urdu text corpora. A
human evaluation of UrBLiMP annotations yielded a 96.10% inter-annotator
agreement, confirming the reliability of the dataset. We evaluate twenty
multilingual LLMs on UrBLiMP, revealing significant variation in performance
across linguistic phenomena. While LLaMA-3-70B achieves the highest average
accuracy (94.73%), its performance is statistically comparable to other top
models such as Gemma-3-27B-PT. These findings highlight both the potential and
the limitations of current multilingual LLMs in capturing fine-grained
syntactic knowledge in low-resource languages.

</details>


### [6] [Cross-Domain Web Information Extraction at Pinterest](https://arxiv.org/abs/2508.01096)
*Michael Farag,Patrick Halina,Andrey Zaytsev,Alekhya Munagala,Imtihan Ahmed,Junhao Wang*

Main category: cs.CL

TL;DR: Pinterest's system for extracting structured product data from e-commerce sites uses a novel webpage representation combining structural, visual, and text modalities, enabling simple models like XGBoost to outperform complex LLMs like GPT in accuracy and cost-efficiency.


<details>
  <summary>Details</summary>
Motivation: The internet's unstructured data poses challenges for converting it into structured formats, which is crucial for enhancing user experiences and content distribution at Pinterest.

Method: The system employs a compact webpage representation that integrates structural, visual, and text data for each HTML node, optimized for small model learning like XGBoost.

Result: The system achieves high accuracy and scalability, processing over 1,000 URLs per second, and is 1,000 times more cost-effective than GPT alternatives.

Conclusion: The approach demonstrates that simpler models with optimized representations can outperform complex LLMs in attribute extraction, offering scalability and cost-efficiency.

Abstract: The internet offers a massive repository of unstructured information, but
it's a significant challenge to convert this into a structured format. At
Pinterest, the ability to accurately extract structured product data from
e-commerce websites is essential to enhance user experiences and improve
content distribution. In this paper, we present Pinterest's system for
attribute extraction, which achieves remarkable accuracy and scalability at a
manageable cost. Our approach leverages a novel webpage representation that
combines structural, visual, and text modalities into a compact form,
optimizing it for small model learning. This representation captures each
visible HTML node with its text, style and layout information. We show how this
allows simple models such as eXtreme Gradient Boosting (XGBoost) to extract
attributes more accurately than much more complex Large Language Models (LLMs)
such as Generative Pre-trained Transformer (GPT). Our results demonstrate a
system that is highly scalable, processing over 1,000 URLs per second, while
being 1000 times more cost-effective than the cheapest GPT alternatives.

</details>


### [7] [Asking the Right Questions: Benchmarking Large Language Models in the Development of Clinical Consultation Templates](https://arxiv.org/abs/2508.01159)
*Liam G. McCoy,Fateme Nateghi Haredasht,Kanav Chopra,David Wu,David JH Wu,Abass Conteh,Sarita Khemani,Saloni Kumar Maharaj,Vishnu Ravi,Arth Pahwa,Yingjie Weng,Leah Rosengaus,Lena Giang,Kelvin Zhenghao Li,Olivia Jee,Daniel Shirvani,Ethan Goh,Jonathan H. Chen*

Main category: cs.CL

TL;DR: The study evaluates LLMs' ability to generate structured clinical consultation templates, finding high comprehensiveness but issues with length and prioritization, especially in narrative-driven fields.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' potential in improving structured clinical information exchange between physicians.

Method: Used 145 expert-crafted templates to evaluate frontier models (e.g., GPT-4o, Claude 4 Sonnet) via a multi-agent pipeline with prompt optimization, semantic autograding, and prioritization analysis.

Result: Models achieved high comprehensiveness (up to 92.2%) but produced overly long templates and struggled with prioritization under length constraints, with performance varying by specialty.

Conclusion: LLMs can enhance clinical information exchange but require better evaluation methods for prioritization in real-world physician communication.

Abstract: This study evaluates the capacity of large language models (LLMs) to generate
structured clinical consultation templates for electronic consultation. Using
145 expert-crafted templates developed and routinely used by Stanford's
eConsult team, we assess frontier models -- including o3, GPT-4o, Kimi K2,
Claude 4 Sonnet, Llama 3 70B, and Gemini 2.5 Pro -- for their ability to
produce clinically coherent, concise, and prioritized clinical question
schemas. Through a multi-agent pipeline combining prompt optimization, semantic
autograding, and prioritization analysis, we show that while models like o3
achieve high comprehensiveness (up to 92.2\%), they consistently generate
excessively long templates and fail to correctly prioritize the most clinically
important questions under length constraints. Performance varies across
specialties, with significant degradation in narrative-driven fields such as
psychiatry and pain medicine. Our findings demonstrate that LLMs can enhance
structured clinical information exchange between physicians, while highlighting
the need for more robust evaluation methods that capture a model's ability to
prioritize clinically salient information within the time constraints of
real-world physician communication.

</details>


### [8] [CSIRO-LT at SemEval-2025 Task 11: Adapting LLMs for Emotion Recognition for Multiple Languages](https://arxiv.org/abs/2508.01161)
*Jiyu Chen,Necva Bölücü,Sarvnaz Karimi,Diego Mollá,Cécile L. Paris*

Main category: cs.CL

TL;DR: Fine-tuning a multilingual LLM with LoRA per language is the most effective method for cross-lingual emotion recognition.


<details>
  <summary>Details</summary>
Motivation: Challenges in detecting emotions across languages due to cultural nuances.

Method: Fine-tune pre-trained multilingual LLMs with LoRA separately for each language.

Result: Most effective strategy for emotion recognition across languages.

Conclusion: Task-specific adaptation of LLMs improves cross-lingual emotion detection.

Abstract: Detecting emotions across different languages is challenging due to the
varied and culturally nuanced ways of emotional expressions. The
\textit{Semeval 2025 Task 11: Bridging the Gap in Text-Based emotion} shared
task was organised to investigate emotion recognition across different
languages. The goal of the task is to implement an emotion recogniser that can
identify the basic emotional states that general third-party observers would
attribute to an author based on their written text snippet, along with the
intensity of those emotions. We report our investigation of various
task-adaptation strategies for LLMs in emotion recognition. We show that the
most effective method for this task is to fine-tune a pre-trained multilingual
LLM with LoRA setting separately for each language.

</details>


### [9] [Adaptive Content Restriction for Large Language Models via Suffix Optimization](https://arxiv.org/abs/2508.01198)
*Yige Li,Peihai Jiang,Jun Sun,Peng Shu,Tianming Liu,Zhen Xiang*

Main category: cs.CL

TL;DR: The paper introduces Adaptive Content Restriction (AdaCoRe), a lightweight method to prevent LLMs from generating restricted terms without fine-tuning, using Suffix Optimization (SOP).


<details>
  <summary>Details</summary>
Motivation: Content restriction needs vary across users and time, making fine-tuning impractical. AdaCoRe addresses this with lightweight strategies.

Method: Proposes Suffix Optimization (SOP), which appends an optimized suffix to prompts to block restricted terms while maintaining output quality.

Result: SOP outperforms baselines by 6-17% in restriction rates across multiple LLMs and is effective on real-world platforms like POE.

Conclusion: AdaCoRe and SOP offer a practical, efficient solution for dynamic content restriction in LLMs.

Abstract: Large Language Models (LLMs) have demonstrated significant success across
diverse applications. However, enforcing content restrictions remains a
significant challenge due to their expansive output space. One aspect of
content restriction is preventing LLMs from generating harmful content via
model alignment approaches such as supervised fine-tuning (SFT). Yet, the need
for content restriction may vary significantly across user groups, change
rapidly over time, and not always align with general definitions of
harmfulness. Applying SFT to each of these specific use cases is impractical
due to the high computational, data, and storage demands. Motivated by this
need, we propose a new task called \textit{Adaptive Content Restriction}
(AdaCoRe), which focuses on lightweight strategies -- methods without model
fine-tuning -- to prevent deployed LLMs from generating restricted terms for
specific use cases. We propose the first method for AdaCoRe, named
\textit{Suffix Optimization (SOP)}, which appends a short, optimized suffix to
any prompt to a) prevent a target LLM from generating a set of restricted
terms, while b) preserving the output quality. To evaluate AdaCoRe approaches,
including our SOP, we create a new \textit{Content Restriction Benchmark}
(CoReBench), which contains 400 prompts for 80 restricted terms across 8
carefully selected categories. We demonstrate the effectiveness of SOP on
CoReBench, which outperforms the system-level baselines such as system suffix
by 15\%, 17\%, 10\%, 9\%, and 6\% on average restriction rates for Gemma2-2B,
Mistral-7B, Vicuna-7B, Llama3-8B, and Llama3.1-8B, respectively. We also
demonstrate that SOP is effective on POE, an online platform hosting various
commercial LLMs, highlighting its practicality in real-world scenarios.

</details>


### [10] [Show or Tell? Modeling the evolution of request-making in Human-LLM conversations](https://arxiv.org/abs/2508.01213)
*Shengqi Zhu,Jeffrey M. Rzeszotarski,David Mimno*

Main category: cs.CL

TL;DR: The paper introduces a task to segment chat queries into components like requests, roles, and context, revealing differences between LLM and human-human interactions. It also explores how user behavior evolves over time and is influenced by model capabilities.


<details>
  <summary>Details</summary>
Motivation: To understand user behavior in LLM interactions by analyzing chat logs, which are often obscured by query variability.

Method: Segmentation of chat queries into categories (requests, roles, context, expressions) and diachronic analysis of user behavior patterns.

Result: User behavior differs from human-human interactions, evolves with experience, and is impacted by model capabilities, especially with new model introductions.

Conclusion: The study provides insights into LLM user behavior, highlighting the importance of query segmentation and the impact of model updates on user patterns.

Abstract: Chat logs provide a rich source of information about LLM users, but patterns
of user behavior are often masked by the variability of queries. We present a
new task, segmenting chat queries into contents of requests, roles,
query-specific context, and additional expressions. We find that, despite the
familiarity of chat-based interaction, request-making in LLM queries remains
significantly different from comparable human-human interactions. With the data
resource, we introduce an important perspective of diachronic analyses with
user expressions. We find that query patterns vary between early ones
emphasizing requests, and individual users explore patterns but tend to
converge with experience. Finally, we show that model capabilities affect user
behavior, particularly with the introduction of new models, which are traceable
at the community level.

</details>


### [11] [WebDS: An End-to-End Benchmark for Web-based Data Science](https://arxiv.org/abs/2508.01222)
*Ethan Hsu,Hong Meng Yam,Ines Bouissou,Aaron Murali John,Raj Thota,Josh Koe,Vivek Sarath Putta,G K Dharesan,Alexander Spangher,Shikhar Murty,Tenghao Huang,Christopher D. Manning*

Main category: cs.CL

TL;DR: WebDS is a new benchmark for web-based data science tasks, addressing gaps in existing benchmarks by focusing on complex, multi-step operations and diverse data formats. Current LLM agents struggle with these tasks, highlighting new failure modes.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack realism and complexity for modern data science workflows, focusing on simplistic interactions or static datasets. WebDS aims to bridge this gap.

Method: WebDS includes 870 tasks across 29 diverse websites, requiring multi-step operations, tool use, and handling of heterogeneous data formats.

Result: Current LLM agents perform poorly on WebDS (15% success rate vs. 80% on simpler benchmarks), revealing issues like poor information grounding and repetitive behavior.

Conclusion: WebDS provides a realistic testing ground for advancing LLM-based data science, addressing practical challenges in the field.

Abstract: A large portion of real-world data science tasks are complex and require
multi-hop web-based interactions: finding appropriate data available on the
internet, synthesizing real-time data of various modalities from different
locations, and producing summarized analyses. Existing web benchmarks often
focus on simplistic interactions, such as form submissions or e-commerce
transactions, and often do not require diverse tool-using capabilities required
for web based data science. Conversely, traditional data science benchmarks
typically concentrate on static, often textually bound datasets and do not
assess end-to-end workflows that encompass data acquisition, cleaning,
analysis, and insight generation. In response, we introduce WebDS, the first
end-to-end web-based data science benchmark. It comprises 870 web-based data
science tasks across 29 diverse websites from structured government data
portals to unstructured news media, challenging agents to perform complex,
multi-step operations requiring the use of tools and heterogeneous data formats
that better reflect the realities of modern data analytics. Evaluations of
current SOTA LLM agents indicate significant performance gaps in accomplishing
these tasks. For instance, Browser Use, which accomplishes 80% of tasks on Web
Voyager, successfully completes only 15% of tasks in WebDS, which our analysis
suggests is due to new failure modes like poor information grounding,
repetitive behavior and shortcut-taking that agents performing WebDS' tasks
display. By providing a more robust and realistic testing ground, WebDS sets
the stage for significant advances in the development of practically useful
LLM-based data science.

</details>


### [12] [WarriorMath: Enhancing the Mathematical Ability of Large Language Models with a Defect-aware Framework](https://arxiv.org/abs/2508.01245)
*Yue Chen,Minghua He,Fangkai Yang,Pu Zhao,Lu Wang,Yu Kang,Yifei Dong,Yuefeng Zhan,Hao Sun,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.CL

TL;DR: WarriorMath is a defect-aware framework for improving LLMs' mathematical problem-solving by generating targeted training data and using progressive learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods for augmenting training data overlook LLMs' specific failure modes, limiting performance gains.

Method: WarriorMath uses expert LLMs to generate, critique, and refine problems, then employs progressive training with tailored data.

Result: WarriorMath outperforms baselines by 12.57% on six benchmarks, achieving state-of-the-art results.

Conclusion: The defect-aware, multi-expert framework effectively enhances LLMs' mathematical abilities.

Abstract: Large Language Models (LLMs) excel in solving mathematical problems, yet
their performance is often limited by the availability of high-quality, diverse
training data. Existing methods focus on augmenting datasets through rephrasing
or difficulty progression but overlook the specific failure modes of LLMs. This
results in synthetic questions that the model can already solve, providing
minimal performance gains. To address this, we propose WarriorMath, a
defect-aware framework for mathematical problem solving that integrates both
targeted data synthesis and progressive training. In the synthesis stage, we
employ multiple expert LLMs in a collaborative process to generate, critique,
and refine problems. Questions that base LLMs fail to solve are identified and
iteratively improved through expert-level feedback, producing high-quality,
defect-aware training data. In the training stage, we introduce a progressive
learning framework that iteratively fine-tunes the model using increasingly
challenging data tailored to its weaknesses. Experiments on six mathematical
benchmarks show that WarriorMath outperforms strong baselines by 12.57% on
average, setting a new state-of-the-art. Our results demonstrate the
effectiveness of a defect-aware, multi-expert framework for improving
mathematical ability.

</details>


### [13] [Bridging LLMs and Symbolic Reasoning in Educational QA Systems: Insights from the XAI Challenge at IJCNN 2025](https://arxiv.org/abs/2508.01263)
*Long S. T. Nguyen,Khang H. N. Vo,Thu H. A. Nguyen,Tuan C. Bui,Duc Q. Nguyen,Thanh-Tung Tran,Anh D. Nguyen,Minh L. Nguyen,Fabien Baldacci,Thang H. Bui,Emanuel Di Nardo,Angelo Ciaramella,Son H. Le,Ihsan Ullah,Lorenzo Di Rocco,Tho T. Quan*

Main category: cs.CL

TL;DR: The paper analyzes the XAI Challenge 2025, a hackathon focused on creating explainable AI QA systems for educational contexts, using lightweight LLMs or hybrid systems.


<details>
  <summary>Details</summary>
Motivation: Address the lack of XAI-focused hackathons in real-world education, promoting transparency and interpretability in AI systems for student queries.

Method: Participants built QA systems with logic-based explanations, using a high-quality dataset validated by Z3 and refined by expert students.

Result: The challenge successfully bridged LLMs and symbolic reasoning, offering insights for future XAI educational systems and research.

Conclusion: The XAI Challenge 2025 represents a novel approach to integrating explainability in AI for education, setting a precedent for future initiatives.

Abstract: The growing integration of Artificial Intelligence (AI) into education has
intensified the need for transparency and interpretability. While hackathons
have long served as agile environments for rapid AI prototyping, few have
directly addressed eXplainable AI (XAI) in real-world educational contexts.
This paper presents a comprehensive analysis of the XAI Challenge 2025, a
hackathon-style competition jointly organized by Ho Chi Minh City University of
Technology (HCMUT) and the International Workshop on Trustworthiness and
Reliability in Neurosymbolic AI (TRNS-AI), held as part of the International
Joint Conference on Neural Networks (IJCNN 2025). The challenge tasked
participants with building Question-Answering (QA) systems capable of answering
student queries about university policies while generating clear, logic-based
natural language explanations. To promote transparency and trustworthiness,
solutions were required to use lightweight Large Language Models (LLMs) or
hybrid LLM-symbolic systems. A high-quality dataset was provided, constructed
via logic-based templates with Z3 validation and refined through expert student
review to ensure alignment with real-world academic scenarios. We describe the
challenge's motivation, structure, dataset construction, and evaluation
protocol. Situating the competition within the broader evolution of AI
hackathons, we argue that it represents a novel effort to bridge LLMs and
symbolic reasoning in service of explainability. Our findings offer actionable
insights for future XAI-centered educational systems and competitive research
initiatives.

</details>


### [14] [Prompting Large Language Models with Partial Knowledge for Answering Questions with Unseen Entities](https://arxiv.org/abs/2508.01290)
*Zhichao Yan,Jiapu Wang,Jiaoyan Chen,Yanyan Wang,Hongye Tan,Jiye Liang,Xiaoli Li,Ru Li,Jeff Z. Pan*

Main category: cs.CL

TL;DR: The paper explores how partially relevant knowledge can 'awaken' LLMs in RAG systems, improving performance in incomplete knowledge scenarios, and introduces a new task, Unseen Entity KGQA.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of effectively utilizing partially relevant knowledge in RAG systems, especially in incomplete knowledge bases, and to propose a novel perspective on LLM awakening.

Method: Uses triplets from gold reasoning paths (and variants) to construct partially relevant knowledge, analyzes the awakening effect theoretically, and validates it on KG QA datasets. Introduces Unseen Entity KGQA for real-world challenges.

Result: The awakening-based approach outperforms traditional methods, especially in noisy or incomplete knowledge scenarios.

Conclusion: Partially relevant knowledge can effectively awaken LLMs, offering a practical solution for incomplete knowledge retrieval and outperforming similarity-based methods.

Abstract: Retrieval-Augmented Generation (RAG) shows impressive performance by
supplementing and substituting parametric knowledge in Large Language Models
(LLMs). Retrieved knowledge can be divided into three types: explicit answer
evidence, implicit answer clue, and insufficient answer context which can be
further categorized into totally irrelevant and partially relevant information.
Effectively utilizing partially relevant knowledge remains a key challenge for
RAG systems, especially in incomplete knowledge base retrieval. Contrary to the
conventional view, we propose a new perspective: LLMs can be awakened via
partially relevant knowledge already embedded in LLMs. To comprehensively
investigate this phenomenon, the triplets located in the gold reasoning path
and their variants are used to construct partially relevant knowledge by
removing the path that contains the answer. We provide theoretical analysis of
the awakening effect in LLMs and support our hypothesis with experiments on two
Knowledge Graphs (KGs) Question Answering (QA) datasets. Furthermore, we
present a new task, Unseen Entity KGQA, simulating real-world challenges where
entity linking fails due to KG incompleteness. Our awakening-based approach
demonstrates greater efficacy in practical applications, outperforms
traditional methods that rely on embedding-based similarity which are prone to
returning noisy information.

</details>


### [15] [KEDAS: Knowledge Editing Alignment with Diverse Augmentation and Self-adaptive Inference](https://arxiv.org/abs/2508.01302)
*Chenming Tang,Yutong Yang,Yunfang Wu*

Main category: cs.CL

TL;DR: KEDAS improves knowledge editing in LLMs via diverse augmentation and self-adaptive inference, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Efficiently update outdated knowledge in LLMs while retaining their capabilities.

Method: Uses low-rank adaptation, diverse edit augmentation, and self-adaptive inference with a smart retriever.

Result: Achieves highest performance in 35/36 cases, surpassing baselines by 19.8 harmonic mean scores.

Conclusion: KEDAS is robust and efficient, offering an ideal paradigm for knowledge editing alignment.

Abstract: Knowledge editing aims to modify outdated knowledge in large language models
(LLMs) efficiently while retaining their powerful capabilities. Most existing
methods rely on either parameter-level editing or retrieval-based approaches.
In this work, we propose Knowledge Editing alignment with Diverse Augmentation
and Self-adaptive inference (KEDAS) to better align LLMs with knowledge
editing. In the alignment phase, LLMs learn to apply in-context edited
knowledge via low-rank adaptation. During editing, we design a diverse edit
augmentation technique to improve the recall of edits. After that, a
self-adaptive post-alignment inference mechanism is proposed, in which a
filter-based smart retriever is employed to perform a dynamic selection of
inference routing. Specifically, irrelevant queries will go through the
original pre-alignment model directly, while relevant ones, together with their
related edits, go through the model with aligned adapters activated. In
experiments, KEDAS secures the highest overall performance scores in 35 out of
36 cases across four datasets with three LLMs on three settings, surpassing its
strong knowledge editing alignment counterpart by about 19.8 harmonic mean
scores of edit success, locality and portability and outperforming both
parameter editing and retrieval-based baselines significantly. Analysis of
computational cost and performance on general tasks further validates the
robustness and efficiency of KEDAS, indicating that it presents an ideal
paradigm of knowledge editing alignment.

</details>


### [16] [D-SCoRE: Document-Centric Segmentation and CoT Reasoning with Structured Export for QA-CoT Data Generation](https://arxiv.org/abs/2508.01309)
*Weibo Zhou,Lingbo Li,Shangsong Liang*

Main category: cs.CL

TL;DR: D-SCoRE is a training-free pipeline for generating diverse, high-quality QA datasets from text, outperforming human-annotated datasets in domain-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: High-quality QA datasets are scarce and costly, limiting supervised fine-tuning for domain-specific LLMs.

Method: Uses document-centric processing, segmentation, CoT reasoning, and structured export to create QA-CoT datasets with multi-dimensional controls for diversity.

Result: D-SCoRE outperforms human-annotated datasets (SQuAD, Covid-QA) in evaluations, generating QA pairs efficiently on consumer hardware.

Conclusion: D-SCoRE offers a scalable, efficient solution for domain-aware QA dataset generation and high-performance fine-tuning.

Abstract: The scarcity and high cost of high-quality question-answering (QA) datasets
hinder supervised fine-tuning (SFT) for domain-specific large language models
(LLMs). To address this, we introduce D-SCoRE, a training-free pipeline that
utilizes LLMs and prompt engineering to produce diverse, high-quality QA
datasets from arbitrary textual sources. D-SCoRE integrates
$\textbf{D}$ocument-centric processing, $\textbf{S}$egmentation, $\textbf{Co}$T
$\textbf{R}$easoning, and structured $\textbf{E}$xport to generate QA-COT
datasets tailored for domain-aware SFT. Multi-dimensional control mechanisms,
such as semantic role transformation, question type balancing, and
counterfactual materials, enhance diversity and relevance, overcoming
limitations of existing QA generation. LLMs fine-tuned on D-SCoRE-generated QA
datasets, and human-annotated QA datasets (SQuAD, Covid-QA) are evaluated on
SQuADShifts and Covid-QA test sets, with D-SCoRE outperforming across most
domains. D-SCoRE generates six QA-CoT pairs with four-option counterfactual
materials per 100-200-word text in 90 seconds using an 8B LLM on consumer-grade
hardware. Its simplicity and scalability enable efficient QA generation and
high-performance fine-tuning across domains.

</details>


### [17] [LinkQA: Synthesizing Diverse QA from Multiple Seeds Strongly Linked by Knowledge Points](https://arxiv.org/abs/2508.01317)
*Xuemiao Zhang,Can Ren,Chengying Tu,Rongxiang Weng,Hongfei Yan,Jingang Wang,Xunliang Cai*

Main category: cs.CL

TL;DR: LinkSyn is a KP graph-based framework for synthesizing diverse QA data, improving LLM training by balancing KP coverage and popularity.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity of high-quality, diverse training data for LLMs.

Method: Constructs a KP graph from QA seeds, uses knowledge distribution and diffusion-based synthesis, and enhances high-difficulty QA.

Result: Synthesized LinkQA dataset (50B tokens) improves Llama-3 8B performance by 11.51% on MMLU and CMMLU.

Conclusion: LinkSyn effectively enhances LLM training with diverse, high-quality data, achieving SOTA results.

Abstract: The advancement of large language models (LLMs) struggles with the scarcity
of high-quality, diverse training data. To address this limitation, we propose
LinkSyn, a novel knowledge point (KP) graph-based synthesis framework that
enables flexible control over discipline and difficulty distributions while
balancing KP coverage and popularity. LinkSyn extracts KPs from
question-answering (QA) seed data and constructs a KP graph to synthesize
diverse QA data from multiple seeds strongly linked by KPs and sampled from
graph walks. Specifically, LinkSyn incorporates (1) a knowledge distribution
value function to guide the adjustment of path sampling probability and balance
KP coverage and popularity during graph walks; (2) diffusion-based synthesis
via DeepSeek-R1 by leveraging multiple seeds with dense logical associations
along each path; and (3) high-difficulty QA enhancement within given
disciplines by flexible difficulty adjustments. By executing LinkSyn, we
synthesize LinkQA, a diverse multi-disciplinary QA dataset with 50B tokens.
Extensive experiments on Llama-3 8B demonstrate that continual pre-training
with LinkQA yields an average improvement of $\mathbf{11.51\%}$ on MMLU and
CMMLU, establishing new SOTA results. LinkQA consistently enhances performance
across model size and initial FLOPs scales.

</details>


### [18] [Large-Scale Diverse Synthesis for Mid-Training](https://arxiv.org/abs/2508.01326)
*Xuemiao Zhang,Chengying Tu,Can Ren,Rongxiang Weng,Hongfei Yan,Jingang Wang,Xunliang Cai*

Main category: cs.CL

TL;DR: The paper introduces BoostQA, a 100B-token QA dataset, to address limitations in knowledge diversity and scalability in LLM training. It improves model performance, especially in STEM and high-difficulty contexts.


<details>
  <summary>Details</summary>
Motivation: High-quality, knowledge-intensive training data is scarce, and traditional corpora lack diversity and scalability, particularly in cross-domain and high-difficulty scenarios.

Method: A diversified pipeline synthesizes BoostQA by curating seed data, using DeepSeek-R1 for STEM-focused synthesis, and refining answers with DeepSeek-V3. Mid-training optimizes domain-specific knowledge.

Result: Llama-3 8B, mid-trained on BoostQA, shows a 12.74% average improvement on MMLU and CMMLU, achieving SOTA performance across 12 benchmarks.

Conclusion: BoostQA effectively enhances LLM performance and scalability, demonstrating its utility in addressing data diversity and quality challenges.

Abstract: The scarcity of high-quality, knowledge-intensive training data hinders the
development of large language models (LLMs), as traditional corpora provide
limited information. Previous studies have synthesized and integrated
corpora-dependent question-answering (QA) data to improve model performance but
face challenges in QA data scalability and knowledge diversity, particularly in
cross-domain contexts. Furthermore, leveraging our designed discipline and
difficulty annotation system, we probe model deficiencies in STEM disciplines
and high-difficulty data. To overcome these limitations, we propose a novel
diversified pipeline to synthesize BoostQA, a 100B-token large-scale QA
dataset. Our synthesis framework: (1) curates seed data from heterogeneous
sources; (2) utilizes DeepSeek-R1 to implement STEM-focused multi-grade
synthesis to boost data diversity and high-difficulty synthesis to mitigate
difficulty degradation; (3) refines answers via DeepSeek-V3 to improve output
quality. We utilize BoostQA in mid-training, a mid-stage between pre-training
and post-training, to optimize domain-specific knowledge acquisition and
enhance data quality. Our method enables Llama-3 8B, mid-trained on a 40B-token
dataset, to achieve an average improvement of $\mathbf{12.74\%}$ on MMLU and
CMMLU and establish SOTA average performance across 12 benchmarks. BoostQA also
demonstrates robust scalability, with performance consistently improving as
model size, data volume, and initial FLOPs scale.

</details>


### [19] [MaRGen: Multi-Agent LLM Approach for Self-Directed Market Research and Analysis](https://arxiv.org/abs/2508.01370)
*Roman Koshkin,Pengyu Dai,Nozomi Fujikawa,Masahito Togami,Marco Visentini-Scarzanella*

Main category: cs.CL

TL;DR: An autonomous framework using LLMs automates business analysis and market report generation, improving quality through iterative reviews and leveraging consultants' knowledge.


<details>
  <summary>Details</summary>
Motivation: To automate the creation of professional-quality market reports affordably and efficiently, replicating expert methodologies.

Method: Specialized agents (Researcher, Reviewer, Writer, Retriever) collaborate, using in-context learning from Amazon consultants. The process includes data querying, analysis, visualization, and report generation, with an LLM-based evaluation system for quality assessment.

Result: Generates detailed 6-page reports in 7 minutes at ~$1, with quality improved by automated reviews and consultants' knowledge.

Conclusion: The framework is a significant step toward affordable, automated market insights.

Abstract: We present an autonomous framework that leverages Large Language Models
(LLMs) to automate end-to-end business analysis and market report generation.
At its core, the system employs specialized agents - Researcher, Reviewer,
Writer, and Retriever - that collaborate to analyze data and produce
comprehensive reports. These agents learn from real professional consultants'
presentation materials at Amazon through in-context learning to replicate
professional analytical methodologies. The framework executes a multi-step
process: querying databases, analyzing data, generating insights, creating
visualizations, and composing market reports. We also introduce a novel
LLM-based evaluation system for assessing report quality, which shows alignment
with expert human evaluations. Building on these evaluations, we implement an
iterative improvement mechanism that optimizes report quality through automated
review cycles. Experimental results show that report quality can be improved by
both automated review cycles and consultants' unstructured knowledge. In
experimental validation, our framework generates detailed 6-page reports in 7
minutes at a cost of approximately \$1. Our work could be an important step to
automatically create affordable market insights.

</details>


### [20] [MedSynth: Realistic, Synthetic Medical Dialogue-Note Pairs](https://arxiv.org/abs/2508.01401)
*Ahmad Rezaie Mianroodi,Amirali Rezaie,Niko Grisel Todorov,Cyril Rakovski,Frank Rudzicz*

Main category: cs.CL

TL;DR: MedSynth introduces a synthetic dataset for medical dialogue-note tasks, improving model performance in generating notes from dialogues and vice versa.


<details>
  <summary>Details</summary>
Motivation: Reduce physician burnout by automating medical documentation through robust tools.

Method: Created MedSynth, a dataset of 10,000+ dialogue-note pairs covering 2000+ ICD-10 codes, analyzed for disease distributions.

Result: Enhanced model performance in Dialogue-to-Note and Note-to-Dialogue tasks.

Conclusion: MedSynth provides a valuable, privacy-compliant resource for medical documentation automation.

Abstract: Physicians spend significant time documenting clinical encounters, a burden
that contributes to professional burnout. To address this, robust automation
tools for medical documentation are crucial. We introduce MedSynth -- a novel
dataset of synthetic medical dialogues and notes designed to advance the
Dialogue-to-Note (Dial-2-Note) and Note-to-Dialogue (Note-2-Dial) tasks.
Informed by an extensive analysis of disease distributions, this dataset
includes over 10,000 dialogue-note pairs covering over 2000 ICD-10 codes. We
demonstrate that our dataset markedly enhances the performance of models in
generating medical notes from dialogues, and dialogues from medical notes. The
dataset provides a valuable resource in a field where open-access,
privacy-compliant, and diverse training data are scarce. Code is available at
https://github.com/ahmadrezarm/MedSynth/tree/main and the dataset is available
at https://huggingface.co/datasets/Ahmad0067/MedSynth.

</details>


### [21] [ArzEn-MultiGenre: An aligned parallel dataset of Egyptian Arabic song lyrics, novels, and subtitles, with English translations](https://arxiv.org/abs/2508.01411)
*Rania Al-Sabbagh*

Main category: cs.CL

TL;DR: ArzEn-MultiGenre is a parallel dataset of Egyptian Arabic and English texts (song lyrics, novels, TV subtitles) for machine translation benchmarking, research, and pedagogy.


<details>
  <summary>Details</summary>
Motivation: To provide a resource for benchmarking machine translation models, supporting research in translation studies, and aiding translation education and professionals.

Method: Manual translation and alignment of 25,557 segment pairs across diverse genres (song lyrics, novels, TV subtitles).

Result: A gold-standard dataset with unique genres not found in existing Egyptian Arabic-English parallel datasets.

Conclusion: ArzEn-MultiGenre fills a gap in resources for Egyptian Arabic-English translation and supports diverse applications in research and industry.

Abstract: ArzEn-MultiGenre is a parallel dataset of Egyptian Arabic song lyrics,
novels, and TV show subtitles that are manually translated and aligned with
their English counterparts. The dataset contains 25,557 segment pairs that can
be used to benchmark new machine translation models, fine-tune large language
models in few-shot settings, and adapt commercial machine translation
applications such as Google Translate. Additionally, the dataset is a valuable
resource for research in various disciplines, including translation studies,
cross-linguistic analysis, and lexical semantics. The dataset can also serve
pedagogical purposes by training translation students and aid professional
translators as a translation memory. The contributions are twofold: first, the
dataset features textual genres not found in existing parallel Egyptian Arabic
and English datasets, and second, it is a gold-standard dataset that has been
translated and aligned by human experts.

</details>


### [22] [Discovering Bias Associations through Open-Ended LLM Generations](https://arxiv.org/abs/2508.01412)
*Jinhao Pan,Chahat Raj,Ziwei Zhu*

Main category: cs.CL

TL;DR: BADF is a framework for discovering known and new biases in LLMs by analyzing open-ended outputs, advancing bias understanding and providing a scalable tool.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing bias evaluation methods in LLMs, which rely on predefined associations and miss unexpected biases.

Method: BADF systematically extracts associations between demographic identities and concepts from open-ended LLM outputs, tested across models and contexts.

Result: BADF successfully maps diverse bias associations, revealing both known and unrecognized biases in LLMs.

Conclusion: BADF enhances bias analysis in LLMs, offering a scalable solution for identifying and understanding biases in open-ended generation.

Abstract: Social biases embedded in Large Language Models (LLMs) raise critical
concerns, resulting in representational harms -- unfair or distorted portrayals
of demographic groups -- that may be expressed in subtle ways through generated
language. Existing evaluation methods often depend on predefined
identity-concept associations, limiting their ability to surface new or
unexpected forms of bias. In this work, we present the Bias Association
Discovery Framework (BADF), a systematic approach for extracting both known and
previously unrecognized associations between demographic identities and
descriptive concepts from open-ended LLM outputs. Through comprehensive
experiments spanning multiple models and diverse real-world contexts, BADF
enables robust mapping and analysis of the varied concepts that characterize
demographic identities. Our findings advance the understanding of biases in
open-ended generation and provide a scalable tool for identifying and analyzing
bias associations in LLMs. Data, code, and results are available at
https://github.com/JP-25/Discover-Open-Ended-Generation

</details>


### [23] [From Query to Logic: Ontology-Driven Multi-Hop Reasoning in LLMs](https://arxiv.org/abs/2508.01424)
*Haonan Bian,Yutao Qi,Rui Yang,Yuanxi Che,Jiaqian Wang,Heming Xia,Ranran Zhen*

Main category: cs.CL

TL;DR: ORACLE enhances LLMs for multi-hop QA by integrating knowledge graphs and logic-based reasoning, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex multi-hop QA due to poor capture of deep conceptual relationships.

Method: ORACLE constructs question-specific ontologies, converts them to logic chains, and decomposes queries into sub-questions.

Result: Achieves competitive performance on MQA benchmarks, rivaling models like DeepSeek-R1.

Conclusion: ORACLE improves logical reasoning and interpretability in multi-hop QA.

Abstract: Large Language Models (LLMs), despite their success in question answering,
exhibit limitations in complex multi-hop question answering (MQA) tasks that
necessitate non-linear, structured reasoning. This limitation stems from their
inability to adequately capture deep conceptual relationships between entities.
To overcome this challenge, we present **ORACLE** (**O**ntology-driven
**R**easoning **A**nd **C**hain for **L**ogical **E**ucidation), a
training-free framework that combines LLMs' generative capabilities with the
structural benefits of knowledge graphs. Our approach operates through three
stages: (1) dynamic construction of question-specific knowledge ontologies
using LLMs, (2) transformation of these ontologies into First-Order Logic
reasoning chains, and (3) systematic decomposition of the original query into
logically coherent sub-questions. Experimental results on several standard MQA
benchmarks show that our framework achieves highly competitive performance,
rivaling current state-of-the-art models like DeepSeek-R1. Detailed analyses
further confirm the effectiveness of each component, while demonstrating that
our method generates more logical and interpretable reasoning chains than
existing approaches.

</details>


### [24] [Towards Efficient Medical Reasoning with Minimal Fine-Tuning Data](https://arxiv.org/abs/2508.01450)
*Xinlin Zhuang,Feilong Tang,Haolin Yang,Ming Hu,Huifa Li,Haochen Xue,Yichen Li,Junjun He,Zongyuan Ge,Ying Qian,Imran Razzak*

Main category: cs.CL

TL;DR: DIQ is a data selection strategy for SFT that balances sample difficulty and gradient influence, improving medical reasoning with minimal data.


<details>
  <summary>Details</summary>
Motivation: Existing SFT practices use unfiltered datasets with redundant/low-quality samples, leading to inefficiency and poor performance.

Method: Proposes DIQ, prioritizing high-difficulty-high-influence samples to balance reasoning complexity and optimization utility.

Result: DIQ-selected subsets (1-10% of data) match or outperform full-dataset performance, enhancing clinical reasoning alignment.

Conclusion: DIQ demonstrates the superiority of principled data selection over brute-force scaling in SFT for medical reasoning.

Abstract: Supervised Fine-Tuning (SFT) plays a pivotal role in adapting Large Language
Models (LLMs) to specialized domains such as medical reasoning. However,
existing SFT practices often rely on unfiltered datasets that contain redundant
and low-quality samples, leading to substantial computational costs and
suboptimal performance. Although existing methods attempt to alleviate this
problem by selecting data based on sample difficulty, defined by knowledge and
reasoning complexity, they overlook each sample's optimization utility
reflected in its gradient. Interestingly, we find that gradient-based influence
alone favors easy-to-optimize samples that cause large parameter shifts but
lack deep reasoning chains, while difficulty alone selects noisy or overly
complex cases that fail to guide stable optimization. Based on this
observation, we propose a data selection strategy, Difficulty-Influence
Quadrant (DIQ), which prioritizes samples in the high-difficulty-high-influence
quadrant to balance complex clinical reasoning with substantial gradient
influence, enabling efficient medical reasoning with minimal fine-tuning data.
Furthermore, Human and LLM-as-a-judge evaluations show that DIQ-selected
subsets demonstrate higher data quality and generate clinical reasoning that is
more aligned with expert practices in differential diagnosis, safety check, and
evidence citation, as DIQ emphasizes samples that foster expert-like reasoning
patterns. Extensive experiments on medical reasoning benchmarks demonstrate
that DIQ enables models fine-tuned on only 1% of selected data to match
full-dataset performance, while using 10% consistently outperforms the
baseline, highlighting the superiority of principled data selection over
brute-force scaling. The code and data are available at
https://github.com/mihara-bot/DIQ.

</details>


### [25] [TreeDiff: AST-Guided Code Generation with Diffusion LLMs](https://arxiv.org/abs/2508.01473)
*Yiming Zeng,Jinghan Cao,Zexin Li,Yiming Chen,Tao Ren,Dawei Xiang,Xidong Wu,Shangqian Gao,Tingting Yu*

Main category: cs.CL

TL;DR: A syntax-aware diffusion framework improves code generation by incorporating structural priors from ASTs, enhancing syntactic correctness and generalization.


<details>
  <summary>Details</summary>
Motivation: Diffusion models struggle with structured domains like source code due to strict syntactic and semantic rules, which standard token-level corruption ignores.

Method: Proposes a syntax-aware diffusion framework that corrupts syntactically meaningful code spans from AST subtrees, preserving grammatical boundaries.

Result: Syntax-aware corruption improves syntactic correctness, reconstruction accuracy, and generalization to unseen code patterns.

Conclusion: Incorporating structural information into diffusion models is promising for advancing code generation tasks.

Abstract: Recent advances in diffusion-based language models have opened new
possibilities for controllable and bidirectional sequence generation. These
models provide an alternative to traditional autoregressive approaches by
framing text generation as an iterative denoising process. However, applying
diffusion models to structured domains such as source code remains a
significant challenge. Programming languages differ from natural language in
that they follow strict syntactic and semantic rules, with hierarchical
organization that must be preserved for correctness. Standard token-level
corruption techniques used during training often ignore this structure, which
may hinder the model's ability to learn meaningful representations of code. To
address this limitation, we propose a syntax-aware diffusion framework that
incorporates structural priors from Abstract Syntax Trees (ASTs) into the
denoising process. Instead of masking individual tokens at random, we
selectively corrupt syntactically meaningful code spans derived from AST
subtrees. This enables the model to reconstruct programs in a way that respects
grammatical boundaries and captures long-range dependencies. Experimental
results demonstrate that syntax-aware corruption significantly improves
syntactic correctness, reconstruction accuracy, and generalization to unseen
code patterns. These findings highlight the potential of incorporating
structural information into diffusion-based training and suggest that
syntax-guided denoising is a promising direction for advancing diffusion-based
language models in code generation tasks.

</details>


### [26] [Harnessing Collective Intelligence of LLMs for Robust Biomedical QA: A Multi-Model Approach](https://arxiv.org/abs/2508.01480)
*Dimitra Panou,Alexandros C. Dimopoulos,Manolis Koubarakis,Martin Reczko*

Main category: cs.CL

TL;DR: The paper discusses using open-source large language models (LLMs) for biomedical question-answering in the BioASQ challenge, achieving top results.


<details>
  <summary>Details</summary>
Motivation: The exponential growth of biomedical literature makes text mining and question-answering tasks highly demanding, necessitating efficient solutions.

Method: Deployed retrieval-augmented LLMs, using majority voting for Yes/No questions and answer unions for list/factoid questions, evaluating 13 LLMs for optimal combinations.

Result: Achieved 1st place for ideal answers and 2nd for exact answers in Synergy task rounds, with tailored LLM pipelines for question types.

Conclusion: Combining LLMs effectively improves biomedical question-answering, with specific model combinations excelling for different question types.

Abstract: Biomedical text mining and question-answering are essential yet highly
demanding tasks, particularly in the face of the exponential growth of
biomedical literature. In this work, we present our participation in the 13th
edition of the BioASQ challenge, which involves biomedical semantic
question-answering for Task 13b and biomedical question-answering for
developing topics for the Synergy task. We deploy a selection of open-source
large language models (LLMs) as retrieval-augmented generators to answer
biomedical questions. Various models are used to process the questions. A
majority voting system combines their output to determine the final answer for
Yes/No questions, while for list and factoid type questions, the union of their
answers in used. We evaluated 13 state-of-the-art open source LLMs, exploring
all possible model combinations to contribute to the final answer, resulting in
tailored LLM pipelines for each question type. Our findings provide valuable
insight into which combinations of LLMs consistently produce superior results
for specific question types. In the four rounds of the 2025 BioASQ challenge,
our system achieved notable results: in the Synergy task, we secured 1st place
for ideal answers and 2nd place for exact answers in round 2, as well as two
shared 1st places for exact answers in round 3 and 4.

</details>


### [27] [TeSent: A Benchmark Dataset for Fairness-aware Explainable Sentiment Classification in Telugu](https://arxiv.org/abs/2508.01486)
*Vallabhaneni Raj Kumar,Ashwin S,Supriya Manna,Niladri Sett,Cheedella V S N M S Hema Harshitha,Kurakula Harshitha,Anand Kumar Sharma,Basina Deepakraj,Tanuj Sarkar,Bondada Navaneeth Krishna,Samanthapudi Shakeer*

Main category: cs.CL

TL;DR: The paper introduces TeSent, a benchmark dataset for Telugu sentiment classification, addressing resource scarcity. It includes explainability and fairness evaluations, and shows that training with rationales improves model accuracy and reduces bias.


<details>
  <summary>Details</summary>
Motivation: Telugu is underrepresented in NLP due to lack of annotated resources. The work aims to fill this gap with a high-quality dataset and evaluations for sentiment classification.

Method: Scraped Telugu texts from diverse sources, annotated 26,150 sentences, and fine-tuned SOTA models with/without rationales. Evaluated plausibility, faithfulness, and fairness.

Result: Training with rationales improved model accuracy, reduced bias, and aligned explainers' outputs with human reasoning.

Conclusion: TeSent and TeEEC provide valuable resources for Telugu NLP, demonstrating the benefits of incorporating rationales in model training for better performance and fairness.

Abstract: In the Indian subcontinent, Telugu, one of India's six classical languages,
is the most widely spoken Dravidian Language. Despite its 96 million speaker
base worldwide, Telugu remains underrepresented in the global NLP and Machine
Learning landscape, mainly due to lack of high-quality annotated resources.
This work introduces TeSent, a comprehensive benchmark dataset for sentiment
classification, a key text classification problem, in Telugu. TeSent not only
provides ground truth labels for the sentences, but also supplements with
provisions for evaluating explainability and fairness, two critical
requirements in modern-day machine learning tasks. We scraped Telugu texts
covering multiple domains from various social media platforms, news websites
and web-blogs to preprocess and generate 26,150 sentences, and developed a
custom-built annotation platform and a carefully crafted annotation protocol
for collecting the ground truth labels along with their human-annotated
rationales. We then fine-tuned several SOTA pre-trained models in two ways:
with rationales, and without rationales. Further, we provide a detailed
plausibility and faithfulness evaluation suite, which exploits the rationales,
for six widely used post-hoc explainers applied on the trained models. Lastly,
we curate TeEEC, Equity Evaluation Corpus in Telugu, a corpus to evaluate
fairness of Telugu sentiment and emotion related NLP tasks, and provide a
fairness evaluation suite for the trained classifier models. Our experimental
results suggest that training with rationales may improve model accuracy,
reduce bias in models, and make the explainers' output more aligned to human
reasoning.

</details>


### [28] [The Homogenizing Effect of Large Language Models on Human Expression and Thought](https://arxiv.org/abs/2508.01491)
*Zhivar Sourati,Alireza S. Ziabari,Morteza Dehghani*

Main category: cs.CL

TL;DR: LLMs risk standardizing language and reasoning, marginalizing diverse voices and cognitive styles, which threatens collective intelligence.


<details>
  <summary>Details</summary>
Motivation: To highlight how LLMs may homogenize cognitive diversity by reinforcing dominant language and reasoning styles, potentially undermining creativity and adaptability.

Method: Synthesizes evidence from linguistics, cognitive, and computer science to analyze LLMs' impact on language and reasoning diversity.

Result: LLMs reflect and amplify dominant patterns in training data, marginalizing alternative voices and reasoning strategies.

Conclusion: Unchecked homogenization by LLMs could flatten cognitive diversity, harming collective intelligence and adaptability.

Abstract: Cognitive diversity, reflected in variations of language, perspective, and
reasoning, is essential to creativity and collective intelligence. This
diversity is rich and grounded in culture, history, and individual experience.
Yet as large language models (LLMs) become deeply embedded in people's lives,
they risk standardizing language and reasoning. This Review synthesizes
evidence across linguistics, cognitive, and computer science to show how LLMs
reflect and reinforce dominant styles while marginalizing alternative voices
and reasoning strategies. We examine how their design and widespread use
contribute to this effect by mirroring patterns in their training data and
amplifying convergence as all people increasingly rely on the same models
across contexts. Unchecked, this homogenization risks flattening the cognitive
landscapes that drive collective intelligence and adaptability.

</details>


### [29] [A Theory of Adaptive Scaffolding for LLM-Based Pedagogical Agents](https://arxiv.org/abs/2508.01503)
*Clayton Cohn,Surya Rayala,Namrata Srivastava,Joyce Horn Fonteles,Shruti Jain,Xinying Luo,Divya Mereddy,Naveeduddin Mohammed,Gautam Biswas*

Main category: cs.CL

TL;DR: A framework combining Evidence-Centered Design and Social Cognitive Theory for LLM-based pedagogical agents in STEM+C learning, demonstrated by Inquizzitor, shows effective adaptive scaffolding and feedback.


<details>
  <summary>Details</summary>
Motivation: Current LLM systems in classrooms lack theoretical grounding, unlike earlier intelligent tutoring systems.

Method: Proposed framework integrates Evidence-Centered Design and Social Cognitive Theory, implemented in Inquizzitor, an LLM-based formative assessment agent.

Result: Inquizzitor provides high-quality, theory-aligned assessment and interaction, valued by students and teachers.

Conclusion: Theory-driven LLM integration in education can offer adaptive, principled instruction, as shown by Inquizzitor.

Abstract: Large language models (LLMs) present new opportunities for creating
pedagogical agents that engage in meaningful dialogue to support student
learning. However, the current use of LLM systems like ChatGPT in classrooms
often lacks the solid theoretical foundation found in earlier intelligent
tutoring systems. To bridge this gap, we propose a framework that combines
Evidence-Centered Design with Social Cognitive Theory for adaptive scaffolding
in LLM-based agents focused on STEM+C learning. We illustrate this framework
with Inquizzitor, an LLM-based formative assessment agent that integrates
human-AI hybrid intelligence and provides feedback grounded in cognitive
science principles. Our findings show that Inquizzitor delivers high-quality
assessment and interaction aligned with core learning theories, offering
teachers effective guidance that students value. This research underscores the
potential for theory-driven LLM integration in education, highlighting the
ability of these systems to provide adaptive and principled instruction.

</details>


### [30] [MOPrompt: Multi-objective Semantic Evolution for Prompt Optimization](https://arxiv.org/abs/2508.01541)
*Sara Câmara,Eduardo Luz,Valéria Carvalho,Ivan Meneghini,Gladston Moreira*

Main category: cs.CL

TL;DR: MOPrompt is a multi-objective evolutionary framework optimizing prompts for both accuracy and context size in LLMs, outperforming baselines with a 31% token reduction for the same accuracy.


<details>
  <summary>Details</summary>
Motivation: Manual prompt design is complex and time-consuming, and existing automated methods often ignore the trade-off between performance and context size.

Method: MOPrompt uses Multi-objective Evolutionary Optimization to map Pareto fronts of prompt solutions, balancing accuracy and token length.

Result: MOPrompt achieves the same peak accuracy (0.97) as baselines but with a 31% reduction in token length for the Sabiazinho model.

Conclusion: MOPrompt effectively addresses the trade-off between efficiency and effectiveness in prompt optimization, offering practical solutions for LLM deployment.

Abstract: Prompt engineering is crucial for unlocking the potential of Large Language
Models (LLMs). Still, since manual prompt design is often complex,
non-intuitive, and time-consuming, automatic prompt optimization has emerged as
a research area. However, a significant challenge in prompt optimization is
managing the inherent trade-off between task performance, such as accuracy, and
context size. Most existing automated methods focus on a single objective,
typically performance, thereby failing to explore the critical spectrum of
efficiency and effectiveness. This paper introduces the MOPrompt, a novel
Multi-objective Evolutionary Optimization (EMO) framework designed to optimize
prompts for both accuracy and context size (measured in tokens) simultaneously.
Our framework maps the Pareto front of prompt solutions, presenting
practitioners with a set of trade-offs between context size and performance, a
crucial tool for deploying Large Language Models (LLMs) in real-world
applications. We evaluate MOPrompt on a sentiment analysis task in Portuguese,
using Gemma-2B and Sabiazinho-3 as evaluation models. Our findings show that
MOPrompt substantially outperforms the baseline framework. For the Sabiazinho
model, MOPrompt identifies a prompt that achieves the same peak accuracy (0.97)
as the best baseline solution, but with a 31% reduction in token length.

</details>


### [31] [Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models](https://arxiv.org/abs/2508.01554)
*Yujia Zheng,Tianhao Li,Haotian Huang,Tianyu Zeng,Jingyu Lu,Chuangxin Chu,Yuekai Huang,Ziyou Jiang,Qian Xiong,Yuyao Ge,Mingyang Li*

Main category: cs.CL

TL;DR: PromptAnatomy dissects prompts into components for targeted adversarial attacks, improving robustness evaluation for LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial attack methods overlook structural heterogeneity in prompts, leading to unreliable robustness assessments.

Method: Introduces PromptAnatomy, dissecting prompts into functional components and using ComPerturb for selective perturbations, with PPL-based filtering for plausibility.

Result: Achieves state-of-the-art attack success rates across datasets and LLMs, validated by ablation studies.

Conclusion: Highlighting prompt structure awareness and controlled perturbation enhances adversarial robustness evaluation in LLMs.

Abstract: Prompt-based adversarial attacks have become an effective means to assess the
robustness of large language models (LLMs). However, existing approaches often
treat prompts as monolithic text, overlooking their structural
heterogeneity-different prompt components contribute unequally to adversarial
robustness. Prior works like PromptRobust assume prompts are value-neutral, but
our analysis reveals that complex, domain-specific prompts with rich structures
have components with differing vulnerabilities. To address this gap, we
introduce PromptAnatomy, an automated framework that dissects prompts into
functional components and generates diverse, interpretable adversarial examples
by selectively perturbing each component using our proposed method, ComPerturb.
To ensure linguistic plausibility and mitigate distribution shifts, we further
incorporate a perplexity (PPL)-based filtering mechanism. As a complementary
resource, we annotate four public instruction-tuning datasets using the
PromptAnatomy framework, verified through human review. Extensive experiments
across these datasets and five advanced LLMs demonstrate that ComPerturb
achieves state-of-the-art attack success rates. Ablation studies validate the
complementary benefits of prompt dissection and PPL filtering. Our results
underscore the importance of prompt structure awareness and controlled
perturbation for reliable adversarial robustness evaluation in LLMs. Code and
data are available at https://github.com/Yujiaaaaa/PACP.

</details>


### [32] [OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets](https://arxiv.org/abs/2508.01630)
*Maziyar Panahi*

Main category: cs.CL

TL;DR: OpenMed NER introduces a suite of open-source, domain-adapted transformer models for biomedical NER, achieving state-of-the-art performance efficiently.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of extracting structured information from unstructured healthcare data while maintaining computational efficiency.

Method: Combines lightweight domain-adaptive pre-training (DAPT) with parameter-efficient Low-Rank Adaptation (LoRA) on ethically sourced data, using DeBERTa-v3, PubMedBERT, and BioELECTRA backbones.

Result: Achieves new state-of-the-art micro-F1 scores on 10 out of 12 biomedical NER benchmarks, with significant gains across diverse entity types.

Conclusion: Strategically adapted open-source models can outperform closed-source solutions efficiently, with low computational cost and compliance benefits.

Abstract: Named-entity recognition (NER) is fundamental to extracting structured
information from the >80% of healthcare data that resides in unstructured
clinical notes and biomedical literature. Despite recent advances with large
language models, achieving state-of-the-art performance across diverse entity
types while maintaining computational efficiency remains a significant
challenge. We introduce OpenMed NER, a suite of open-source, domain-adapted
transformer models that combine lightweight domain-adaptive pre-training (DAPT)
with parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs
cost-effective DAPT on a 350k-passage corpus compiled from ethically sourced,
publicly available research repositories and de-identified clinical notes
(PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA
backbones. This is followed by task-specific fine-tuning with LoRA, which
updates less than 1.5% of model parameters. We evaluate our models on 12
established biomedical NER benchmarks spanning chemicals, diseases, genes, and
species. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of
these 12 datasets, with substantial gains across diverse entity types. Our
models advance the state-of-the-art on foundational disease and chemical
benchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger
improvements of over 5.3 and 9.7 percentage points on more specialized gene and
clinical cell line corpora. This work demonstrates that strategically adapted
open-source models can surpass closed-source solutions. This performance is
achieved with remarkable efficiency: training completes in under 12 hours on a
single GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively
licensed, open-source checkpoints designed to help practitioners facilitate
compliance with emerging data protection and AI regulations, such as the EU AI
Act.

</details>


### [33] [Authorship Attribution in Multilingual Machine-Generated Texts](https://arxiv.org/abs/2508.01656)
*Lucio La Cava,Dominik Macko,Róbert Móro,Ivan Srba,Andrea Tagarelli*

Main category: cs.CL

TL;DR: The paper introduces Multilingual Authorship Attribution (AA), addressing the challenge of attributing texts to human or LLM authors across diverse languages, revealing limitations in monolingual AA methods for multilingual settings.


<details>
  <summary>Details</summary>
Motivation: The difficulty in distinguishing machine-generated text (MGT) from human-written content, especially in multilingual contexts, motivates the need for fine-grained authorship attribution beyond binary classification.

Method: The study evaluates monolingual AA methods' suitability and cross-lingual transferability across 18 languages and 8 generators (7 LLMs and human-authored texts).

Result: Monolingual AA methods show adaptability but face significant limitations in cross-lingual transfer, especially across diverse language families.

Conclusion: The complexity of multilingual AA highlights the need for more robust approaches to match real-world scenarios.

Abstract: As Large Language Models (LLMs) have reached human-like fluency and
coherence, distinguishing machine-generated text (MGT) from human-written
content becomes increasingly difficult. While early efforts in MGT detection
have focused on binary classification, the growing landscape and diversity of
LLMs require a more fine-grained yet challenging authorship attribution (AA),
i.e., being able to identify the precise generator (LLM or human) behind a
text. However, AA remains nowadays confined to a monolingual setting, with
English being the most investigated one, overlooking the multilingual nature
and usage of modern LLMs. In this work, we introduce the problem of
Multilingual Authorship Attribution, which involves attributing texts to human
or multiple LLM generators across diverse languages. Focusing on 18 languages
-- covering multiple families and writing scripts -- and 8 generators (7 LLMs
and the human-authored class), we investigate the multilingual suitability of
monolingual AA methods, their cross-lingual transferability, and the impact of
generators on attribution performance. Our results reveal that while certain
monolingual AA methods can be adapted to multilingual settings, significant
limitations and challenges remain, particularly in transferring across diverse
language families, underscoring the complexity of multilingual AA and the need
for more robust approaches to better match real-world scenarios.

</details>


### [34] [CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions](https://arxiv.org/abs/2508.01674)
*Tae Soo Kim,Yoonjoo Lee,Yoonah Park,Jiho Kim,Young-Ho Kim,Juho Kim*

Main category: cs.CL

TL;DR: The paper introduces CUPID, a benchmark for evaluating LLMs' ability to infer and apply dynamic user preferences from multi-turn interactions, revealing current models' limitations in contextual personalization.


<details>
  <summary>Details</summary>
Motivation: Human preferences are dynamic and context-dependent, but LLMs often assume static preferences. The study aims to address this gap by assessing LLMs' capability to infer and apply contextual preferences.

Method: The authors created CUPID, a benchmark of 756 human-curated interaction sessions with LLM-based assistants, where users express preferences through multi-turn feedback. The benchmark evaluates LLMs' ability to infer relevant preferences for new requests.

Result: Evaluation of 10 LLMs showed poor performance (under 50% precision and 65% recall) in inferring preferences and discerning relevant context, highlighting limitations in contextual personalization.

Conclusion: The study underscores the need for advancements in LLMs to handle dynamic preferences and proposes CUPID as a tool to drive such improvements.

Abstract: Personalization of Large Language Models (LLMs) often assumes users hold
static preferences that reflect globally in all tasks. In reality, humans hold
dynamic preferences that change depending on the context. As users interact
with an LLM in various contexts, they naturally reveal their contextual
preferences, which a model must infer and apply in future contexts to ensure
alignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated
interaction session histories between users and LLM-based chat assistants. In
each interaction session, the user provides a request in a specific context and
expresses their preference through multi-turn feedback. Given a new user
request and prior interaction sessions, our benchmark assesses whether LLMs can
infer the preference relevant to this request and generate a response that
satisfies this preference. With CUPID, we evaluated 10 open and proprietary
LLMs, revealing that state-of-the-art LLMs struggle to infer preferences from
multi-turn interactions and fail to discern what previous context is relevant
to a new request -- under 50% precision and 65% recall. Our work highlights the
need to advance LLM capabilities for more contextually personalized
interactions and proposes CUPID as a resource to drive these improvements.

</details>


### [35] [The Bidirectional Process Reward Model](https://arxiv.org/abs/2508.01682)
*Lingyin Zhang,Jun Gao,Xiaoxue Ren,Ziqiang Cao*

Main category: cs.CL

TL;DR: BiPRM introduces a bidirectional evaluation paradigm for Process Reward Models (PRMs), improving reasoning quality in LLMs by leveraging both left-to-right and right-to-left evaluations without added complexity.


<details>
  <summary>Details</summary>
Motivation: Existing PRMs use unidirectional (L2R) evaluation, limiting global context and consistency verification. BiPRM addresses this by incorporating bidirectional assessment.

Method: BiPRM adds a right-to-left (R2L) evaluation stream alongside L2R, implemented via prompt modifications, requiring no extra parameters or latency.

Result: BiPRM outperforms unidirectional baselines, achieving up to 31.9% improvement in stepwise reward evaluation across multiple benchmarks and models.

Conclusion: BiPRM is effective, robust, and broadly applicable, advancing process-based reward modeling.

Abstract: Process Reward Models (PRMs) have emerged as a promising approach to enhance
the reasoning quality of Large Language Models (LLMs) by assigning fine-grained
scores to intermediate reasoning steps within a solution trajectory. However,
existing PRMs predominantly adopt a unidirectional left-to-right (L2R)
evaluation paradigm, which limits their ability to leverage global context,
making it challenging to verify the consistency of earlier steps based on later
ones. In light of these challenges, we propose a novel bidirectional evaluation
paradigm, named Bidirectional Process Reward Model (BiPRM). BiPRM seamlessly
incorporates a parallel right-to-left (R2L) evaluation stream alongside the
conventional L2R flow, enabling later reasoning steps to help assess earlier
ones in real time. Notably, the built-in R2L evaluation is implemented solely
through prompt modifications that reverse the original reasoning trajectory,
without any additional parameters or inference latency introduced. This ensures
BiPRM remains both efficient and broadly compatible with existing PRM studies.
We conduct extensive experiments on two mathematical reasoning benchmarks using
samples generated by three different policy models. Our method, BiPRM, is
evaluated across three backbones and three distinct PRM objectives. Across all
settings, BiPRM consistently outperforms unidirectional baselines, achieving up
to a 31.9% improvement in stepwise reward evaluation. Generally, our results
highlight BiPRM's effectiveness, robustness, and general applicability,
offering a promising new direction for process-based reward modeling.

</details>


### [36] [Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy](https://arxiv.org/abs/2508.01696)
*Yi Jiang,Sendong Zhao,Jianbo Li,Haochun Wang,Lizhe Zhang,Yan Liu,Bin Qin*

Main category: cs.CL

TL;DR: The paper introduces Collaborative Chain-of-Agents (CoCoA), a framework to improve synergy between parametric and retrieved knowledge in RAG, enhancing LLM performance in knowledge-intensive tasks.


<details>
  <summary>Details</summary>
Motivation: Current RAG methods struggle to fully exploit knowledge during generation, limiting synergy between internal and external knowledge.

Method: Proposes CoCoA-zero (multi-agent RAG framework) and CoCoA (long-chain training strategy) to integrate and leverage both knowledge types.

Result: CoCoA-zero and CoCoA achieve superior performance on open-domain and multi-hop QA tasks.

Conclusion: The framework effectively enhances knowledge integration and generation accuracy in LLMs.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising framework for
enhancing the capabilities of Large Language Models (LLMs), especially in
knowledge-intensive tasks. Despite its advantages, current RAG methods often
struggle to *fully exploit knowledge during generation*. In particular, the
synergy between the model's internal parametric knowledge and external
retrieved knowledge remains limited. Retrieved contents may sometimes mislead
generation, while certain generated content can guide the model toward more
accurate outputs. In this work, we propose Collaborative Chain-of-Agents, a
framework designed to enhance explicitly synergy over both parametric and
retrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent
RAG framework that first performs conditional knowledge induction and then
reasons answers. Building on this, we develop CoCoA, a long-chain training
strategy that synthesizes extended multi-agent reasoning trajectories from
CoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability
to explicitly integrate and jointly leverage parametric and retrieved
knowledge. Experiments results show that CoCoA-zero and CoCoA achieve superior
performance on open-domain and multi-hop QA tasks.

</details>


### [37] [Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large Language Models as a Symptom of Irrelevancy Disruption](https://arxiv.org/abs/2508.01708)
*Berkay Köprü,Mehrzad Mashal,Yigit Gurses,Akos Kadar,Maximilian Schmitt,Ditty Mathew,Felix Burkhardt,Florian Eyben,Björn W. Schuller*

Main category: cs.CL

TL;DR: The paper introduces 'expression leakage' in LLMs, where models generate sentimentally charged expressions unrelated to input context. It provides a dataset, evaluation pipeline, and shows leakage reduces with model scale but requires specific mitigation efforts.


<details>
  <summary>Details</summary>
Motivation: To address the issue of LLMs generating sentimentally charged expressions ('expression leakage') that are semantically unrelated to input context, which prior work on semantic leakage did not cover.

Method: The authors collect a benchmark dataset, propose an automatic evaluation pipeline, and analyze expression leakage across LLM scales and prompt conditions.

Result: Larger models reduce expression leakage within the same family, but mitigation requires model-building care and cannot be fixed by prompting. Negative sentiment prompts cause higher leakage.

Conclusion: Expression leakage is a distinct issue in LLMs, requiring targeted mitigation strategies during model development, as scaling alone is insufficient.

Abstract: Large language models (LLMs) have advanced natural language processing (NLP)
skills such as through next-token prediction and self-attention, but their
ability to integrate broad context also makes them prone to incorporating
irrelevant information. Prior work has focused on semantic leakage, bias
introduced by semantically irrelevant context. In this paper, we introduce
expression leakage, a novel phenomenon where LLMs systematically generate
sentimentally charged expressions that are semantically unrelated to the input
context. To analyse the expression leakage, we collect a benchmark dataset
along with a scheme to automatically generate a dataset from free-form text
from common-crawl. In addition, we propose an automatic evaluation pipeline
that correlates well with human judgment, which accelerates the benchmarking by
decoupling from the need of annotation for each analysed model. Our experiments
show that, as the model scales in the parameter space, the expression leakage
reduces within the same LLM family. On the other hand, we demonstrate that
expression leakage mitigation requires specific care during the model building
process, and cannot be mitigated by prompting. In addition, our experiments
indicate that, when negative sentiment is injected in the prompt, it disrupts
the generation process more than the positive sentiment, causing a higher
expression leakage rate.

</details>


### [38] [CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications](https://arxiv.org/abs/2508.01710)
*Raviraj Joshi,Rakesh Paul,Kanishk Singla,Anusha Kamath,Michael Evans,Katherine Luna,Shaona Ghosh,Utkarsh Vaidya,Eileen Long,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

TL;DR: CultureGuard introduces a pipeline to create multilingual safety datasets, enabling training of a state-of-the-art safety guard model for LLMs in non-English languages.


<details>
  <summary>Details</summary>
Motivation: Address the lack of culturally aligned safety datasets for non-English languages in LLM applications.

Method: A four-stage pipeline: cultural data segregation, adaptation, machine translation, and quality filtering to expand an English dataset into eight languages.

Result: Creation of a 386,661-sample multilingual dataset and a model achieving top performance on safety benchmarks.

Conclusion: This work bridges the safety gap in multilingual LLMs by providing culturally aware safety guard models.

Abstract: The increasing use of Large Language Models (LLMs) in agentic applications
highlights the need for robust safety guard models. While content safety in
English is well-studied, non-English languages lack similar advancements due to
the high cost of collecting culturally aligned labeled datasets. We present
CultureGuard, a novel solution for curating culturally aligned, high-quality
safety datasets across multiple languages. Our approach introduces a four-stage
synthetic data generation and filtering pipeline: cultural data segregation,
cultural data adaptation, machine translation, and quality filtering. This
pipeline enables the conversion and expansion of the
Nemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct
languages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese.
The resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1,
comprises 386,661 samples in 9 languages and facilitates the training of
Llama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning.
The final model achieves state-of-the-art performance on several multilingual
content safety benchmarks. We also benchmark the latest open LLMs on
multilingual safety and observe that these LLMs are more prone to give unsafe
responses when prompted in non-English languages. This work represents a
significant step toward closing the safety gap in multilingual LLMs by enabling
the development of culturally aware safety guard models.

</details>


### [39] [Enhancing the Preference Extractor in Multi-turn Dialogues: From Annotating Disasters to Accurate Preference Extraction](https://arxiv.org/abs/2508.01739)
*Cheng Wang,ziru Liu,Pengcheng Tang,Mingyu Zhang,Quanyu Dai,Yue Zhu*

Main category: cs.CL

TL;DR: The paper proposes IterChat, a framework for generating high-quality dialogue data to improve user preference extraction in multi-turn dialogues, addressing annotation challenges and enhancing model performance.


<details>
  <summary>Details</summary>
Motivation: The difficulty in obtaining high-quality labeled multi-turn dialogue data and the challenges in tracking user preference transitions across turns (termed 'Annotating Disaster') motivate the need for a better data generation method.

Method: The IterChat framework decomposes multi-turn preference extraction into iterative one-turn processes, using GPT4 to pre-define preference slots and generate diverse dialogue datasets.

Result: Fine-tuning or few-shot prompting with the new dialogue format outperforms original multi-turn dialogues, and annotation efficiency improves by 28.4%.

Conclusion: IterChat effectively addresses annotation and training challenges, improving both performance and efficiency in user preference extraction for dialogue systems.

Abstract: Identifying user preferences in dialogue systems is a pivotal aspect of
providing satisfying services. Current research shows that using large language
models (LLMs) to fine-tune a task-specific preference extractor yields
excellent results in terms of accuracy and generalization. However, the primary
challenge stems from the inherent difficulty in obtaining high-quality labeled
multi-turn dialogue data. Accurately tracking user preference transitions
across turns not only demands intensive domain expertise and contextual
consistency maintenance for annotators (termed \textbf{``Annotating
Disaster''}) but also complicates model training due to error propagation in
sequential dependency learning. Inspired by the observation that multi-turn
preference extraction can be decomposed into iterative executions of one-turn
extraction processes. We propose a novel dialogue data generation framework
named \textbf{IterChat}. First, we construct a new data format that categorizes
the dialogue data into attributed historical preferences and one-turn
dialogues. This reduces the probability of annotation errors and improves
annotation efficiency. Then, to generate a high-quality and diverse dialogue
dataset, we adopt GPT4 to pre-define the preference slots in the target
preference extractor task and then randomly sample the subset of the slots and
their corresponding schema values to create the dialogue datasets. Experimental
results indicate that fine-tuning or only few-shot prompting with the new
dialogue format yields superior performance compared to the original multi-turn
dialogues. Additionally, the new data format improves annotator efficiency with
a win rate of 28.4\% higher than the original multi-turn dialogues.

</details>


### [40] [AI-Generated Text is Non-Stationary: Detection via Temporal Tomography](https://arxiv.org/abs/2508.01754)
*Alva West,Yixuan Weng,Minjun Zhu,Luodan Zhang,Zhen Lin,Guangsheng Bao,Yue Zhang*

Main category: cs.CL

TL;DR: TDT introduces a novel AI-generated text detection method using signal processing to preserve positional information, outperforming baselines and showing robustness against adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Existing AI-generated text detectors discard positional information, failing against localized adversarial perturbations due to non-stationarity in AI text.

Method: Temporal Discrepancy Tomography (TDT) treats token-level discrepancies as a time-series signal, applying Continuous Wavelet Transform for a time-scale representation.

Result: TDT achieves 0.855 AUROC (7.1% improvement) on RAID and 14.1% AUROC improvement on adversarial tasks, with only 13% computational overhead.

Conclusion: Non-stationarity is a key characteristic of AI-generated text; preserving temporal dynamics is crucial for robust detection.

Abstract: The field of AI-generated text detection has evolved from supervised
classification to zero-shot statistical analysis. However, current approaches
share a fundamental limitation: they aggregate token-level measurements into
scalar scores, discarding positional information about where anomalies occur.
Our empirical analysis reveals that AI-generated text exhibits significant
non-stationarity, statistical properties vary by 73.8\% more between text
segments compared to human writing. This discovery explains why existing
detectors fail against localized adversarial perturbations that exploit this
overlooked characteristic. We introduce Temporal Discrepancy Tomography (TDT),
a novel detection paradigm that preserves positional information by
reformulating detection as a signal processing task. TDT treats token-level
discrepancies as a time-series signal and applies Continuous Wavelet Transform
to generate a two-dimensional time-scale representation, capturing both the
location and linguistic scale of statistical anomalies. On the RAID benchmark,
TDT achieves 0.855 AUROC (7.1\% improvement over the best baseline). More
importantly, TDT demonstrates robust performance on adversarial tasks, with
14.1\% AUROC improvement on HART Level 2 paraphrasing attacks. Despite its
sophisticated analysis, TDT maintains practical efficiency with only 13\%
computational overhead. Our work establishes non-stationarity as a fundamental
characteristic of AI-generated text and demonstrates that preserving temporal
dynamics is essential for robust detection.

</details>


### [41] [A comprehensive taxonomy of hallucinations in Large Language Models](https://arxiv.org/abs/2508.01781)
*Manuel Cossio*

Main category: cs.CL

TL;DR: The paper provides a taxonomy of LLM hallucinations, defining types, causes, and mitigation strategies, emphasizing their inevitability and the need for human oversight.


<details>
  <summary>Details</summary>
Motivation: Address the critical challenge of LLM hallucinations, which generate plausible but incorrect content, to improve reliability in NLP applications.

Method: Develops a theoretical framework, categorizes hallucinations (intrinsic/extrinsic, factuality/faithfulness), and analyzes causes (data, model, prompt-related).

Result: Identifies specific manifestations (factual errors, inconsistencies, ethical violations) and proposes mitigation strategies and evaluation benchmarks.

Conclusion: LLM hallucinations are inevitable; future efforts should focus on detection, mitigation, and human oversight for responsible deployment.

Abstract: Large language models (LLMs) have revolutionized natural language processing,
yet their propensity for hallucination, generating plausible but factually
incorrect or fabricated content, remains a critical challenge. This report
provides a comprehensive taxonomy of LLM hallucinations, beginning with a
formal definition and a theoretical framework that posits its inherent
inevitability in computable LLMs, irrespective of architecture or training. It
explores core distinctions, differentiating between intrinsic (contradicting
input context) and extrinsic (inconsistent with training data or reality), as
well as factuality (absolute correctness) and faithfulness (adherence to
input). The report then details specific manifestations, including factual
errors, contextual and logical inconsistencies, temporal disorientation,
ethical violations, and task-specific hallucinations across domains like code
generation and multimodal applications. It analyzes the underlying causes,
categorizing them into data-related issues, model-related factors, and
prompt-related influences. Furthermore, the report examines cognitive and human
factors influencing hallucination perception, surveys evaluation benchmarks and
metrics for detection, and outlines architectural and systemic mitigation
strategies. Finally, it introduces web-based resources for monitoring LLM
releases and performance. This report underscores the complex, multifaceted
nature of LLM hallucinations and emphasizes that, given their theoretical
inevitability, future efforts must focus on robust detection, mitigation, and
continuous human oversight for responsible and reliable deployment in critical
applications.

</details>


### [42] [HeQ: a Large and Diverse Hebrew Reading Comprehension Benchmark](https://arxiv.org/abs/2508.01812)
*Amir DN Cohen,Hilla Merhav,Yoav Goldberg,Reut Tsarfaty*

Main category: cs.CL

TL;DR: A Hebrew Machine Reading Comprehension (MRC) dataset, HeQ, is introduced to address the lack of semantic benchmarks in Hebrew NLP, with improved guidelines and metrics for morphologically rich languages.


<details>
  <summary>Details</summary>
Motivation: Current Hebrew NLP benchmarks focus on morpho-syntactic tasks, ignoring semantic understanding. This paper aims to fill that gap with a Hebrew MRC dataset.

Method: Developed a Hebrew MRC dataset (HeQ) with 30,147 question-answer pairs, using novel guidelines, crowdsourcing, and revised evaluation metrics tailored for Hebrew's morphological complexity.

Result: Standard metrics like F1 and EM are inadequate for Hebrew; proposed enhancements. Low correlation found between morpho-syntactic and MRC task performance.

Conclusion: HeQ highlights challenges in Hebrew NLU and advances better models for Hebrew and other morphologically rich languages.

Abstract: Current benchmarks for Hebrew Natural Language Processing (NLP) focus mainly
on morpho-syntactic tasks, neglecting the semantic dimension of language
understanding. To bridge this gap, we set out to deliver a Hebrew Machine
Reading Comprehension (MRC) dataset, where MRC is to be realized as extractive
Question Answering. The morphologically rich nature of Hebrew poses a challenge
to this endeavor: the indeterminacy and non-transparency of span boundaries in
morphologically complex forms lead to annotation inconsistencies,
disagreements, and flaws in standard evaluation metrics.
  To remedy this, we devise a novel set of guidelines, a controlled
crowdsourcing protocol, and revised evaluation metrics that are suitable for
the morphologically rich nature of the language. Our resulting benchmark, HeQ
(Hebrew QA), features 30,147 diverse question-answer pairs derived from both
Hebrew Wikipedia articles and Israeli tech news. Our empirical investigation
reveals that standard evaluation metrics such as F1 scores and Exact Match (EM)
are not appropriate for Hebrew (and other MRLs), and we propose a relevant
enhancement.
  In addition, our experiments show low correlation between models' performance
on morpho-syntactic tasks and on MRC, which suggests that models designed for
the former might underperform on semantics-heavy tasks. The development and
exploration of HeQ illustrate some of the challenges MRLs pose in natural
language understanding (NLU), fostering progression towards more and better NLU
models for Hebrew and other MRLs.

</details>


### [43] [AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning over Heterogeneous Knowledge Graphs for the Circular Economy](https://arxiv.org/abs/2508.01815)
*Yang Zhao,Chengxiao Dai,Wei Zhuo,Tan Chuan Fu,Yue Xiu,Dusit Niyato,Jonathan Z. Low,Eugene Ho Hong Zhuang,Daren Zong Loong Tan*

Main category: cs.CL

TL;DR: AgenticT$^2$S is a modular framework for KGQA that uses specialized agents for retrieval, query generation, and verification, improving accuracy and efficiency in cross-graph reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing KGQA methods struggle with generalizability in low-resource domains and handling queries across multiple graphs, especially in sustainability domains like the circular economy.

Method: AgenticT$^2$S decomposes KGQA into subtasks managed by agents, employs weak-to-strong alignment strategies, and uses a two-stage verifier for query validation.

Result: The framework improves execution accuracy by 17.3%, triple level F$_1$ by 25.4%, and reduces prompt length by 46.4% over baselines.

Conclusion: AgenticT$^2$S demonstrates the effectiveness of agent-based, schema-aware reasoning for scalable KGQA and supports robust cross-graph reasoning in sustainability domains.

Abstract: Question answering over heterogeneous knowledge graphs (KGQA) involves
reasoning across diverse schemas, incomplete alignments, and distributed data
sources. Existing text-to-SPARQL approaches rely on large-scale domain-specific
fine-tuning or operate within single-graph settings, limiting their
generalizability in low-resource domains and their ability to handle queries
spanning multiple graphs. These challenges are particularly relevant in domains
such as the circular economy, where information about classifications,
processes, and emissions is distributed across independently curated knowledge
graphs (KGs). We present AgenticT$^2$S, a modular framework that decomposes
KGQA into subtasks managed by specialized agents responsible for retrieval,
query generation, and verification. A scheduler assigns subgoals to different
graphs using weak-to-strong alignment strategies. A two-stage verifier detects
structurally invalid and semantically underspecified queries through symbolic
validation and counterfactual consistency checks. Experiments on real-world
circular economy KGs demonstrate that AgenticT$^2$S improves execution accuracy
by 17.3% and triple level F$_1$ by 25.4% over the best baseline, while reducing
the average prompt length by 46.4%. These results demonstrate the benefits of
agent-based schema-aware reasoning for scalable KGQA and support
decision-making in sustainability domains through robust cross-graph reasoning.

</details>


### [44] [MLP Memory: Language Modeling with Retriever-pretrained External Memory](https://arxiv.org/abs/2508.01832)
*Rubin Wei,Jiaqi Cao,Jiarui Wang,Jushi Kai,Qipeng Guo,Bowen Zhou,Zhouhan Lin*

Main category: cs.CL

TL;DR: The paper proposes a decoupled memory architecture for LLMs to reduce hallucinations, combining a transformer decoder with a pretrained MLP memory, achieving better performance and speed.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in decoder-only LLMs hinder their use in knowledge-intensive tasks, and retriever-augmented generation lacks deep interaction with LLMs.

Method: Decouples memorization from the LLM decoder using a pretrained, differentiable external MLP memory, trained to imitate a retriever.

Result: Achieves 17.5% and 24.1% improvement on WikiText-103 and Web datasets, with faster inference and better performance on hallucination benchmarks.

Conclusion: The proposed architecture outperforms decoder-only models and kNN-LM, offering faster inference and improved reasoning without overfitting.

Abstract: While modern decoder-only LLMs achieve superior performance across various
domains, hallucinations have risen to be a common problem in their generated
text, hindering their application in knowledge-intensive tasks.
Retriever-augmented generation (RAG) offers a solution, but the non-parametric
nature of the retriever hinders its deep interaction with LLM. In this work, we
propose to decouple memorization from the LLM decoder using a pretrained,
differentiable external memory. The external memory is an MLP pretrained by
imitating the behavior of a retriever on the entire pretraining dataset. Our
resulting architecture, which comprises a transformer decoder and an external
MLP memory pretrained on language modeling and retriever imitation
respectively, demonstrates strong perplexity and performance on downstream
tasks. Experiments show our architecture exhibits steeper power-law scaling
with model size, achieving 17.5% and 24.1% improvement on WikiText-103 and Web
datasets compared to decoder-only models while benefiting from added training
without overfitting. We demonstrate superior performance on three hallucination
benchmarks and nine memory-intensive tasks. Additionally, our approach delivers
$80\times$ speedup over $k$NN-LM (500M tokens) and $1.3\times$ faster inference
than decoder-only models. Unlike $k$NN-LM, which impairs reasoning, our MLP
memory improves StrategyQA performance. We will open-source our code and models
in the future.

</details>


### [45] [Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents](https://arxiv.org/abs/2508.01858)
*Yuhan Guo,Cong Guo,Aiwen Sun,Hongliang He,Xinyu Yang,Yue Lu,Yingji Zhang,Xuntao Guo,Dong Zhang,Jianzhuang Liu,Jiang Duan,Yijia Xiao,Liangjian Wen,Hai-Ming Xu,Yong Dai*

Main category: cs.CL

TL;DR: The paper introduces the Web-CogKnowledge Framework and Web-CogReasoner, a knowledge-driven agent for web tasks, emphasizing knowledge acquisition (Factual, Conceptual, Procedural) and cognitive reasoning (Memorizing, Understanding, Exploring).


<details>
  <summary>Details</summary>
Motivation: Web agents need sufficient knowledge for cognitive reasoning, decomposed into knowledge content learning and cognitive processes.

Method: Proposes the Web-CogKnowledge Framework, constructs Web-CogDataset, and develops Web-CogReasoner using a knowledge-driven Chain-of-Thought reasoning framework.

Result: Web-CogReasoner outperforms existing models, especially in generalizing to unseen tasks.

Conclusion: The framework and dataset enable effective web agent development, with open-sourced code and data for broader use.

Abstract: Multimodal large-scale models have significantly advanced the development of
web agents, enabling perception and interaction with digital environments akin
to human cognition. In this paper, we argue that web agents must first acquire
sufficient knowledge to effectively engage in cognitive reasoning. Therefore,
we decompose a web agent's capabilities into two essential stages: knowledge
content learning and cognitive processes. To formalize this, we propose
Web-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and
Procedural. In this framework, knowledge content learning corresponds to the
agent's processes of Memorizing and Understanding, which rely on the first two
knowledge types, representing the "what" of learning. Conversely, cognitive
processes correspond to Exploring, grounded in Procedural knowledge, defining
the "how" of reasoning and action. To facilitate knowledge acquisition, we
construct the Web-CogDataset, a structured resource curated from 14 real-world
websites, designed to systematically instill core knowledge necessary for web
agent. This dataset serves as the agent's conceptual grounding-the "nouns" upon
which comprehension is built-as well as the basis for learning how to reason
and act. Building on this foundation, we operationalize these processes through
a novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing
and training our proposed agent, the Web-CogReasoner. Extensive experimentation
reveals its significant superiority over existing models, especially in
generalizing to unseen tasks where structured knowledge is decisive. To enable
rigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation
suite designed to assess and compare agent performance across the delineated
knowledge domains and cognitive capabilities. Our code and data is open sourced
at https://github.com/Gnonymous/Web-CogReasoner

</details>


### [46] [Counterfactual Probing for Hallucination Detection and Mitigation in Large Language Models](https://arxiv.org/abs/2508.01862)
*Yijun Feng*

Main category: cs.CL

TL;DR: Counterfactual Probing detects and mitigates hallucinations in LLM outputs by generating plausible but factually incorrect statements and evaluating model sensitivity.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce fluent but factually incorrect outputs (hallucinations), necessitating a method to detect and mitigate such errors.

Method: Generates counterfactual statements with subtle errors, evaluates model sensitivity, and uses adaptive mitigation strategies.

Result: Superior detection performance on benchmarks like TruthfulQA, reducing hallucination scores by 24.5%.

Conclusion: Counterfactual probing is effective, requires no retraining, and integrates into existing LLM pipelines for real-time verification.

Abstract: Large Language Models have demonstrated remarkable capabilities across
diverse tasks, yet they frequently generate hallucinations outputs that are
fluent but factually incorrect or unsupported. We propose Counterfactual
Probing, a novel approach for detecting and mitigating hallucinations in LLM
outputs. Our method dynamically generates counterfactual statements that appear
plausible but contain subtle factual errors, then evaluates the model's
sensitivity to these perturbations. We hypothesize that genuine knowledge
exhibits robustness to counterfactual variations, while hallucinated content
shows inconsistent confidence patterns when confronted with plausible
alternatives. Our comprehensive evaluation on TruthfulQA, factual statement
datasets, and curated hallucination examples demonstrates that counterfactual
probing achieves superior detection performance compared to baseline methods,
while our adaptive mitigation strategies reduce hallucination scores by an
average of 24.5%. The approach requires no model retraining and can be
integrated into existing LLM pipelines as a realtime verification mechanism.

</details>


### [47] [Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language](https://arxiv.org/abs/2508.01918)
*Jaskaranjeet Singh,Rakesh Thakur*

Main category: cs.CL

TL;DR: PunGPT2 is the first open-source Punjabi LLM suite, trained on a diverse 35GB corpus. It includes Pun-RAG for retrieval-augmented generation and Pun-Instruct for efficient instruction tuning. Quantum-RAG introduces quantum-inspired retrieval, outperforming multilingual baselines.


<details>
  <summary>Details</summary>
Motivation: Low-resource languages like Punjabi are often excluded from NLP advancements. This work aims to bridge that gap with scalable, innovative solutions.

Method: Developed PunGPT2 with a Punjabi-optimized tokenizer and pretraining. Introduced Pun-RAG for retrieval-augmented generation and Pun-Instruct for efficient tuning. Proposed Quantum-RAG, a hybrid quantum-inspired retrieval system.

Result: Outperformed multilingual baselines (mBERT, mT5, MuRIL) in perplexity, factuality, and fluency. Demonstrated practical quantum integration in low-resource NLP.

Conclusion: Provides a scalable blueprint for underrepresented languages and pioneers quantum-aware retrieval in low-resource NLP.

Abstract: Despite the rapid advancement of large language models (LLMs), low-resource
languages remain largely excluded from the NLP landscape. We present PunGPT2,
the first fully open-source suite of Punjabi large language models, trained
from scratch on a 35GB domain-diverse corpus encompassing literature, religious
texts, news, and social discourse. Unlike prior multilingual approaches,
PunGPT2 captures rich syntactic and morphological features unique to Punjabi
through a tokenizer optimised with byte pair encoding and linguistically
aligned pretraining objectives. To improve factual grounding and domain recall,
we introduce Pun-RAG, a retrieval-augmented generation framework combining
PunGPT2 with a dense FAISS retriever over a curated Punjabi knowledge base. We
further develop Pun-Instruct, a parameter-efficient, instruction-tuned variant
using QLoRA, enabling robust zero-shot and instruction-following performance
with significantly reduced compute needs.
  As a key innovation, we propose Quantum-RAG, a novel hybrid retrieval system
that fuses sparse (BM25) and dense methods with quantum-inspired semantic
matching. By encoding queries using amplitude-based embeddings and retrieving
via quantum kernel similarity, Quantum-RAG achieves improved contextual
relevance with minimal memory overhead marking the first practical integration
of quantum representations in low-resource language generation. Our models
significantly outperform strong multilingual baselines (mBERT, mT5, MuRIL) in
perplexity, factuality, and fluency. This work provides a scalable,
reproducible blueprint for extending LLM capabilities to underrepresented
languages and pioneers quantum-aware retrieval in low-resource NLP

</details>


### [48] [Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback](https://arxiv.org/abs/2508.01930)
*Tom S. Juzek,Zina B. Ward*

Main category: cs.CL

TL;DR: The study investigates why LLMs overuse certain terms like 'delve' and 'intricate,' linking it to Learning from Human Feedback (LHF). It proposes a method to detect LHF-induced lexical preferences and shows experimental evidence of human preference for such terms, highlighting potential misalignment between LHF workers and LLM users.


<details>
  <summary>Details</summary>
Motivation: To understand the reasons behind LLMs' lexical overuse of specific terms and explore the role of LHF in this phenomenon.

Method: Uses Meta's Llama model to detect LHF-induced lexical preferences and experimentally emulates LHF to show human preference for certain words.

Result: Demonstrates that LHF contributes to lexical overuse, revealing a misalignment between LHF workers' preferences and LLM users' expectations.

Conclusion: Highlights the need for transparency in alignment research and contributes to explainable AI by linking LHF to lexical overuse.

Abstract: Large Language Models (LLMs) are known to overuse certain terms like "delve"
and "intricate." The exact reasons for these lexical choices, however, have
been unclear. Using Meta's Llama model, this study investigates the
contribution of Learning from Human Feedback (LHF), under which we subsume
Reinforcement Learning from Human Feedback and Direct Preference Optimization.
We present a straightforward procedure for detecting the lexical preferences of
LLMs that are potentially LHF-induced. Next, we more conclusively link LHF to
lexical overuse by experimentally emulating the LHF procedure and demonstrating
that participants systematically prefer text variants that include certain
words. This lexical overuse can be seen as a sort of misalignment, though our
study highlights the potential divergence between the lexical expectations of
different populations -- namely LHF workers versus LLM users. Our work
contributes to the growing body of research on explainable artificial
intelligence and emphasizes the importance of both data and procedural
transparency in alignment research.

</details>


### [49] [ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks](https://arxiv.org/abs/2508.01943)
*Philip Schroeder,Ondrej Biza,Thomas Weng,Hongyin Luo,James Glass*

Main category: cs.CL

TL;DR: ROVER is a framework for VLMs to recursively decompose long video trajectories into shorter segments, improving reasoning over video sequences.


<details>
  <summary>Details</summary>
Motivation: VLMs struggle with reasoning over long video sequences, limiting their use in embodied settings.

Method: ROVER recursively decomposes videos into shorter subtask segments, using in-context learning for focused reasoning.

Result: ROVER outperforms baselines in task progress estimation, frame-level reasoning, and video QA, reducing hallucinations.

Conclusion: ROVER improves video reasoning efficiency and accuracy, scaling linearly with video length.

Abstract: Vision-language models (VLMs) have exhibited impressive capabilities across
diverse image understanding tasks, but still struggle in settings that require
reasoning over extended sequences of camera frames from a video. This limits
their utility in embodied settings, which require reasoning over long frame
sequences from a continuous stream of visual input at each moment of a task
attempt. To address this limitation, we propose ROVER (Reasoning Over VidEo
Recursively), a framework that enables the model to recursively decompose
long-horizon video trajectories into segments corresponding to shorter subtasks
within the trajectory. In doing so, ROVER facilitates more focused and accurate
reasoning over temporally localized frame sequences without losing global
context. We evaluate ROVER, implemented using an in-context learning approach,
on diverse OpenX Embodiment videos and on a new dataset derived from RoboCasa
that consists of 543 videos showing both expert and perturbed non-expert
trajectories across 27 robotic manipulation tasks. ROVER outperforms strong
baselines across three video reasoning tasks: task progress estimation,
frame-level natural language reasoning, and video question answering. We
observe that, by reducing the number of frames the model reasons over at each
timestep, ROVER mitigates hallucinations, especially during unexpected or
non-optimal moments of a trajectory. In addition, by enabling the
implementation of a subtask-specific sliding context window, ROVER's time
complexity scales linearly with video length, an asymptotic improvement over
baselines. Demos, code, and data available at: https://rover-vlm.github.io

</details>


### [50] [SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension](https://arxiv.org/abs/2508.01959)
*Junjie Wu,Jiangnan Li,Yuqing Li,Lemao Liu,Liyan Xu,Jiwei Li,Dit-Yan Yeung,Jie Zhou,Mo Yu*

Main category: cs.CL

TL;DR: The paper proposes situated embedding models (SitEmb) to enhance retrieval performance by conditioning short chunks on broader context, outperforming larger models.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of current retrieval-augmented generation (RAG) methods, which struggle with contextual dependencies and embedding capacity for longer chunks.

Method: Introduces a new training paradigm for embedding models (SitEmb) to encode situated context, and evaluates it on a book-plot retrieval dataset.

Result: SitEmb models (1B and 8B parameters) outperform larger state-of-the-art models, with the 8B variant improving performance by over 10%.

Conclusion: SitEmb effectively enhances retrieval by leveraging situated context, demonstrating strong performance across languages and applications.

Abstract: Retrieval-augmented generation (RAG) over long documents typically involves
splitting the text into smaller chunks, which serve as the basic units for
retrieval. However, due to dependencies across the original document,
contextual information is often essential for accurately interpreting each
chunk. To address this, prior work has explored encoding longer context windows
to produce embeddings for longer chunks. Despite these efforts, gains in
retrieval and downstream tasks remain limited. This is because (1) longer
chunks strain the capacity of embedding models due to the increased amount of
information they must encode, and (2) many real-world applications still
require returning localized evidence due to constraints on model or human
bandwidth.
  We propose an alternative approach to this challenge by representing short
chunks in a way that is conditioned on a broader context window to enhance
retrieval performance -- i.e., situating a chunk's meaning within its context.
We further show that existing embedding models are not well-equipped to encode
such situated context effectively, and thus introduce a new training paradigm
and develop the situated embedding models (SitEmb). To evaluate our method, we
curate a book-plot retrieval dataset specifically designed to assess situated
retrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3
substantially outperforms state-of-the-art embedding models, including several
with up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model
further improves performance by over 10% and shows strong results across
different languages and several downstream applications.

</details>


### [51] [TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2508.01977)
*Fan Gao,Cheng Huang,Nyima Tashi,Yutong Liu,Xiangxiang Wang,Thupten Tsering,Ban Ma-bao,Renzeg Duojie,Gadeng Luosang,Rinchen Dongrub,Dorje Tashi,Xiao Feng,Hao Wang,Yongbin Yu*

Main category: cs.CL

TL;DR: The paper introduces TIBSTC-CoT, a large-scale Tibetan dataset created using LLMs, and Sunshine-thinking LLMs trained on it, advancing Tibetan language processing.


<details>
  <summary>Details</summary>
Motivation: To address data scarcity in Tibetan, a low-resource language, and enable high-quality language processing.

Method: Automated dataset construction via chain-of-thought prompting with LLMs, followed by training Tibetan-centric LLMs (Sunshine-thinking) on this dataset.

Result: Sunshine-thinking LLMs show strong reasoning and generation performance, comparable to SOTA multilingual LLMs.

Conclusion: The work advances inclusive AI by providing scalable resources and models for Tibetan language processing.

Abstract: To address the severe data scarcity in Tibetan, a low-resource language
spoken by over six million people, we introduce TIBSTC-CoT, the large-scale,
multi-domain Tibetan dataset automatically constructed via chain-of-thought
prompting with large language models (LLMs). TIBSTC-CoT establishes a scalable
and reproducible framework for dataset creation in low-resource settings,
covering diverse domains and reasoning patterns essential for language
understanding and generation. Building on this dataset, we develop the
Sunshine-thinking LLM family, a series of Tibetan-centric LLMs equipped with
chain-of-thought capabilities. Trained entirely on TIBSTC-CoT,
Sunshine-thinking has demonstrated strong reasoning and generation performance,
comparable to state-of-the-art (SOTA) multilingual LLMs. Our work marks a
significant step toward inclusive AI by enabling high-quality Tibetan language
processing through both resource creation and model innovation. All data are
available: https://github.com/Vicentvankor/sun-shine.

</details>


### [52] [Contextually Aware E-Commerce Product Question Answering using RAG](https://arxiv.org/abs/2508.01990)
*Praveen Tangarajan,Anand A. Rajasekar,Manish Rathi,Vinay Rao Dandin,Ozan Ersoy*

Main category: cs.CL

TL;DR: A scalable, end-to-end framework for e-commerce Product Question Answering (PQA) using Retrieval Augmented Generation (RAG) is proposed, integrating user context and product info for personalized answers.


<details>
  <summary>Details</summary>
Motivation: E-commerce product pages overwhelm users with mixed info, and existing PQA systems lack effective use of context and diverse data.

Method: The framework uses RAG to combine conversational history, user profiles, and product attributes for answering queries.

Result: The system handles diverse queries, identifies catalog gaps, and introduces new metrics for RAG evaluation.

Conclusion: The proposed framework improves PQA by leveraging context and diverse data, with novel metrics for broader RAG system evaluation.

Abstract: E-commerce product pages contain a mix of structured specifications,
unstructured reviews, and contextual elements like personalized offers or
regional variants. Although informative, this volume can lead to cognitive
overload, making it difficult for users to quickly and accurately find the
information they need. Existing Product Question Answering (PQA) systems often
fail to utilize rich user context and diverse product information effectively.
We propose a scalable, end-to-end framework for e-commerce PQA using Retrieval
Augmented Generation (RAG) that deeply integrates contextual understanding. Our
system leverages conversational history, user profiles, and product attributes
to deliver relevant and personalized answers. It adeptly handles objective,
subjective, and multi-intent queries across heterogeneous sources, while also
identifying information gaps in the catalog to support ongoing content
improvement. We also introduce novel metrics to measure the framework's
performance which are broadly applicable for RAG system evaluations.

</details>


### [53] [Prompting Large Language Models to Detect Dementia Family Caregivers](https://arxiv.org/abs/2508.01999)
*Md Badsha Biswas,Özlem Uzuner*

Main category: cs.CL

TL;DR: A system for detecting tweets by dementia caregivers using LLMs, achieving high accuracy with a zero-shot prompt on a fine-tuned model.


<details>
  <summary>Details</summary>
Motivation: To identify tweets by dementia caregivers for developing online support interventions.

Method: Binary classification using large language models (LLMs) with various prompting methods.

Result: Achieved a macro F1-score of 0.95 on validation and test sets.

Conclusion: Zero-shot prompting on a fine-tuned LLM is effective for this task.

Abstract: Social media, such as Twitter, provides opportunities for caregivers of
dementia patients to share their experiences and seek support for a variety of
reasons. Availability of this information online also paves the way for the
development of internet-based interventions in their support. However, for this
purpose, tweets written by caregivers of dementia patients must first be
identified. This paper demonstrates our system for the SMM4H 2025 shared task
3, which focuses on detecting tweets posted by individuals who have a family
member with dementia. The task is outlined as a binary classification problem,
differentiating between tweets that mention dementia in the context of a family
member and those that do not. Our solution to this problem explores large
language models (LLMs) with various prompting methods. Our results show that a
simple zero-shot prompt on a fine-tuned model yielded the best results. Our
final system achieved a macro F1-score of 0.95 on the validation set and the
test set. Our full code is available on GitHub.

</details>


### [54] [SpeechRole: A Large-Scale Dataset and Benchmark for Evaluating Speech Role-Playing Agents](https://arxiv.org/abs/2508.02013)
*Changhao Jiang,Jiajun Sun,Yifei Cao,Jiabao Zhuang,Hui Li,Xiaoran Fan,Ming Zhang,Junjie Ye,Shihan Dou,Zhiheng Xi,Jingqi Tong,Yilong Wu,Baoyu Fan,Zhen Wang,Tao Liang,Zhihui Fei,Mingyang Wan,Guojun Ma,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: The paper introduces SpeechRole-Data, a dataset for Speech Role-Playing Agents (SRPAs), and SpeechRole-Eval, a benchmark for evaluating SRPAs, addressing gaps in speech-based role-playing research.


<details>
  <summary>Details</summary>
Motivation: Existing research on role-playing agents focuses on text, ignoring speech in realistic interactions. There's a lack of systematic evaluation for SRPAs.

Method: Constructed SpeechRole-Data (98 roles, 112k conversations) and proposed SpeechRole-Eval for multidimensional evaluation.

Result: Revealed strengths and challenges of cascaded and end-to-end SRPAs in vocal style consistency and role coherence.

Conclusion: The released data, code, and models aim to advance speech-driven multimodal role-playing research.

Abstract: Recently, role-playing agents have emerged as a promising paradigm for
achieving personalized interaction and emotional resonance. Existing research
primarily focuses on the textual modality, neglecting the critical dimension of
speech in realistic interactive scenarios. In particular, there is a lack of
systematic evaluation for Speech Role-Playing Agents (SRPAs). To address this
gap, we construct SpeechRole-Data, a large-scale, high-quality dataset that
comprises 98 diverse roles and 112k speech-based single-turn and multi-turn
conversations. Each role demonstrates distinct vocal characteristics, including
timbre and prosody, thereby enabling more sophisticated speech role-playing.
Furthermore, we propose SpeechRole-Eval, a multidimensional evaluation
benchmark that systematically assesses SRPAs performance in key aspects such as
fundamental interaction ability, speech expressiveness, and role-playing
fidelity. Experimental results reveal the advantages and challenges of both
cascaded and end-to-end speech role-playing agents in maintaining vocal style
consistency and role coherence. We release all data, code, and baseline models
to provide a solid foundation for speech-driven multimodal role-playing
research and to foster further developments in this field.

</details>


### [55] [SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models](https://arxiv.org/abs/2508.02018)
*Wanqi Yang,Yanda Li,Yunchao Wei,Meng Fang,Ling Chen*

Main category: cs.CL

TL;DR: SpeechR is a benchmark for evaluating reasoning in large audio-language models (LALMs), focusing on factual retrieval, procedural inference, and normative judgment. It reveals that high transcription accuracy doesn't ensure strong reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of LALMs focus on surface-level perception, neglecting contextual and inference-driven reasoning in speech.

Method: SpeechR evaluates models using multiple-choice, generative, and acoustic-feature formats across three reasoning dimensions.

Result: Evaluations on 11 LALMs show transcription accuracy doesn't correlate with reasoning performance.

Conclusion: SpeechR provides a structured benchmark for assessing reasoning in spoken language, aiding targeted analysis of model capabilities.

Abstract: Large audio-language models (LALMs) have achieved near-human performance in
sentence-level transcription and emotion recognition. However, existing
evaluations focus mainly on surface-level perception, leaving the capacity of
models for contextual and inference-driven reasoning in speech-based scenarios
insufficiently examined. To address this gap, we introduce SpeechR, a unified
benchmark for evaluating reasoning over speech in large audio-language models.
SpeechR evaluates models along three key dimensions: factual retrieval,
procedural inference, and normative judgment. It includes three distinct
evaluation formats. The multiple-choice version measures answer selection
accuracy. The generative version assesses the coherence and logical consistency
of reasoning chains. The acoustic-feature version investigates whether
variations in stress and emotion affect reasoning performance. Evaluations on
eleven state-of-the-art LALMs reveal that high transcription accuracy does not
translate into strong reasoning capabilities. SpeechR establishes a structured
benchmark for evaluating reasoning in spoken language, enabling more targeted
analysis of model capabilities across diverse dialogue-based tasks.

</details>


### [56] [Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a Time](https://arxiv.org/abs/2508.02037)
*Huihan Li,You Chen,Siyuan Wang,Yixin He,Ninareh Mehrabi,Rahul Gupta,Xiang Ren*

Main category: cs.CL

TL;DR: STIM is a framework to identify memorization sources in LLMs' reasoning chains, showing local memorization drives errors and helps predict incorrect tokens.


<details>
  <summary>Details</summary>
Motivation: Address concerns about LLMs' reliance on memorization in reasoning tasks, especially in CoT, where memorized patterns cause errors.

Method: Introduces STIM to attribute tokens in reasoning chains to memorization sources (local, mid-range, long-range) using statistical co-occurrence from pretraining data.

Result: Models rely more on memorization in complex cases, with local memorization causing up to 67% of wrong tokens. STIM scores predict incorrect tokens effectively.

Conclusion: STIM is a valuable tool for diagnosing and improving model reasoning, applicable to other structured generation tasks.

Abstract: Large Language Models (LLMs) perform well on reasoning benchmarks but often
fail when inputs alter slightly, raising concerns about the extent to which
their success relies on memorization. This issue is especially acute in
Chain-of-Thought (CoT) reasoning, where spurious memorized patterns can trigger
intermediate errors that cascade into incorrect final answers. We introduce
STIM, a novel framework for Source-aware Token-level Identification of
Memorization, which attributes each token in a reasoning chain to one of
multiple memorization sources - local, mid-range, or long-range - based on
their statistical co-occurrence with the token in the pretraining corpus. Our
token-level analysis across tasks and distributional settings reveals that
models rely more on memorization in complex or long-tail cases, and that local
memorization is often the dominant driver of errors, leading to up to 67% of
wrong tokens. We also show that memorization scores from STIM can be effective
in predicting the wrong tokens in the wrong reasoning step. STIM offers a
powerful tool for diagnosing and improving model reasoning and can generalize
to other structured step-wise generation tasks.

</details>


### [57] [Marco-Voice Technical Report](https://arxiv.org/abs/2508.02038)
*Fengping Tian,Chenyang Lyu,Xuanfan Ni,Haoqin Sun,Qingjuan Li,Zhiqiang Qian,Haijun Li,Longyue Wang,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: A multifunctional speech synthesis system, Marco-Voice, integrates voice cloning and emotion control, achieving expressive and natural speech while preserving speaker identity.


<details>
  <summary>Details</summary>
Motivation: Address challenges in expressive, controllable, and natural speech generation that preserves speaker identity across diverse contexts.

Method: Introduces speaker-emotion disentanglement with in-batch contrastive learning and rotational emotional embedding for smooth emotion control. Uses CSEMOTIONS dataset (10 hours of Mandarin speech from six speakers across seven emotions).

Result: Marco-Voice shows substantial improvements in objective and subjective metrics, excelling in speech clarity and emotional richness.

Conclusion: Marco-Voice represents a significant advance in expressive neural speech synthesis, delivering competitive performance.

Abstract: This paper presents a multifunctional speech synthesis system that integrates
voice cloning and emotion control speech synthesis within a unified framework.
The goal of this work is to address longstanding challenges in achieving highly
expressive, controllable, and natural speech generation that faithfully
preserves speaker identity across diverse linguistic and emotional contexts.
Our approach introduces an effective speaker-emotion disentanglement mechanism
with in-batch contrastive learning, enabling independent manipulation of
speaker identity and eemotional style, as well as rotational emotional
embedding integration method for smooth emotion control. To support
comprehensive training and evaluation, we construct CSEMOTIONS, a high-quality
emotional speech dataset containing 10 hours of Mandarin speech from six
professional speakers across seven emotional categories. Extensive experiments
demonstrate that our system, Marco-Voice, achieves substantial improvements in
both objective and subjective metrics. Comprehensive evaluations and analysis
were conducted, results show that MarcoVoice delivers competitive performance
in terms of speech clarity and emotional richness, representing a substantial
advance in the field of expressive neural speech synthesis.

</details>


### [58] [Harnessing Temporal Databases for Systematic Evaluation of Factual Time-Sensitive Question-Answering in Large Language Models](https://arxiv.org/abs/2508.02045)
*Soyeon Kim,Jindong Wang,Xing Xie,Steven Euijong Whang*

Main category: cs.CL

TL;DR: TDBench is a new benchmark for time-sensitive QA tasks, leveraging temporal databases and SQL techniques to enable scalable and comprehensive evaluation of LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for time-sensitive QA are limited by manual curation or fixed templates, hindering scalable evaluation.

Method: TDBench systematically constructs QA pairs using temporal databases, temporal SQL, and functional dependencies, and introduces a time accuracy metric.

Result: The benchmark facilitates scalable TSQA evaluation, reduces human labor, and complements existing approaches by enabling application-specific data evaluation.

Conclusion: TDBench offers a reliable and scalable solution for evaluating LLMs on time-sensitive factual knowledge.

Abstract: Facts evolve over time, making it essential for Large Language Models (LLMs)
to handle time-sensitive factual knowledge accurately and reliably. While
factual Time-Sensitive Question-Answering (TSQA) tasks have been widely
studied, existing benchmarks often rely on manual curation or a small, fixed
set of predefined templates, which restricts scalable and comprehensive TSQA
evaluation. To address these challenges, we propose TDBench, a new benchmark
that systematically constructs TSQA pairs by harnessing temporal databases and
database techniques such as temporal SQL and functional dependencies. We also
introduce a fine-grained evaluation metric called time accuracy, which assesses
the validity of time references in model explanations alongside traditional
answer accuracy to enable a more reliable TSQA evaluation. Extensive
experiments on contemporary LLMs show how \ours{} enables scalable and
comprehensive TSQA evaluation while reducing the reliance on human labor,
complementing existing Wikipedia/Wikidata-based TSQA evaluation approaches by
enabling LLM evaluation on application-specific data and seamless multi-hop
question generation. Code and data are publicly available at:
https://github.com/ssoy0701/tdbench.git.

</details>


### [59] [ProCut: LLM Prompt Compression via Attribution Estimation](https://arxiv.org/abs/2508.02053)
*Zhentao Xu,Fengyi Li,Albert Chen,Xiaofeng Wang*

Main category: cs.CL

TL;DR: ProCut is a training-free framework for compressing large LLM prompts by analyzing and pruning low-impact segments, reducing token count by 78% while maintaining or improving performance.


<details>
  <summary>Details</summary>
Motivation: Large-scale industrial LLM systems face bloated prompts due to iterative additions, causing maintenance issues, latency, and high costs.

Method: ProCut segments prompts into meaningful units, quantifies their impact via attribution analysis, and prunes low-utility components. It includes an LLM-driven estimator for faster compression.

Result: ProCut reduces prompt size by 78% in production, maintains or improves task performance (up to 62% better than alternatives), and cuts compression latency by 50%.

Conclusion: ProCut effectively compresses prompts, integrates with existing frameworks, and enhances efficiency without sacrificing performance.

Abstract: In large-scale industrial LLM systems, prompt templates often expand to
thousands of tokens as teams iteratively incorporate sections such as task
instructions, few-shot examples, and heuristic rules to enhance robustness and
coverage. This expansion leads to bloated prompts that are difficult to
maintain and incur significant inference latency and serving costs. To address
this, we introduce Prompt Compression via Attribution Estimation (ProCut), a
flexible, LLM-agnostic, training-free framework that compresses prompts through
attribution analysis. ProCut segments prompt templates into semantically
meaningful units, quantifies their impact on task performance, and prunes
low-utility components. Through extensive experiments on five public benchmark
datasets and real-world industrial prompts, we show that ProCut achieves
substantial prompt size reductions (78% fewer tokens in production) while
maintaining or even slightly improving task performance (up to 62% better than
alternative methods). We further introduce an LLM-driven attribution estimator
that reduces compression latency by over 50%, and demonstrate that ProCut
integrates seamlessly with existing prompt-optimization frameworks to produce
concise, high-performing prompts.

</details>


### [60] [The SMeL Test: A simple benchmark for media literacy in language models](https://arxiv.org/abs/2508.02074)
*Gustaf Ahdritz,Anat Kleiman*

Main category: cs.CL

TL;DR: The paper introduces the SMeL Test to evaluate LLMs' ability to filter untrustworthy content, finding no model consistently outperforms others, with larger models not necessarily better.


<details>
  <summary>Details</summary>
Motivation: To assess if LLMs can mimic human heuristics for filtering unreliable online content.

Method: Developed the SMeL Test to benchmark instruction-tuned LLMs, including reasoning models, on filtering untrustworthy information.

Result: No model consistently trusted reliable sources; reasoning improved scores but still led to 70% hallucinations. Larger models didn't outperform smaller ones.

Conclusion: Highlights LLMs' limitations in filtering untrustworthy content and calls for new methods to address this hallucination issue.

Abstract: The internet is rife with unattributed, deliberately misleading, or otherwise
untrustworthy content. Though large language models (LLMs) are often tasked
with autonomous web browsing, the extent to which they have learned the simple
heuristics human researchers use to navigate this noisy environment is not
currently known. In this paper, we introduce the Synthetic Media Literacy Test
(SMeL Test), a minimal benchmark that tests the ability of language models to
actively filter out untrustworthy information in context. We benchmark a
variety of commonly used instruction-tuned LLMs, including reasoning models,
and find that no model consistently trusts more reliable sources; while
reasoning in particular is associated with higher scores, even the best API
model we test hallucinates up to 70% of the time. Remarkably, larger and more
capable models do not necessarily outperform their smaller counterparts. We
hope our work sheds more light on this important form of hallucination and
guides the development of new methods to combat it.

</details>


### [61] [When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models](https://arxiv.org/abs/2508.02087)
*Jin Li,Keyu Wang,Shu Yang,Zhuoran Zhang,Di Wang*

Main category: cs.CL

TL;DR: The paper investigates how LLMs exhibit sycophantic behavior, identifying internal mechanisms like late-layer preference shifts and representational divergence, influenced by grammatical perspective but not user authority.


<details>
  <summary>Details</summary>
Motivation: To understand the internal mechanisms behind LLMs' sycophantic behavior, which contradicts factual knowledge despite prior documentation.

Method: Systematic study of user opinions' impact, logit-lens analysis, causal activation patching, and examination of grammatical perspective effects.

Result: Sycophancy arises from late-layer preference shifts and deeper representational divergence, unaffected by user authority but influenced by grammatical perspective (higher with first-person prompts).

Conclusion: Sycophancy in LLMs is a structural override of learned knowledge in deeper layers, with implications for AI alignment and truthful systems.

Abstract: Large Language Models (LLMs) often exhibit sycophantic behavior, agreeing
with user-stated opinions even when those contradict factual knowledge. While
prior work has documented this tendency, the internal mechanisms that enable
such behavior remain poorly understood. In this paper, we provide a mechanistic
account of how sycophancy arises within LLMs. We first systematically study how
user opinions induce sycophancy across different model families. We find that
simple opinion statements reliably induce sycophancy, whereas user expertise
framing has a negligible impact. Through logit-lens analysis and causal
activation patching, we identify a two-stage emergence of sycophancy: (1) a
late-layer output preference shift and (2) deeper representational divergence.
We also verify that user authority fails to influence behavior because models
do not encode it internally. In addition, we examine how grammatical
perspective affects sycophantic behavior, finding that first-person prompts
(``I believe...'') consistently induce higher sycophancy rates than
third-person framings (``They believe...'') by creating stronger
representational perturbations in deeper layers. These findings highlight that
sycophancy is not a surface-level artifact but emerges from a structural
override of learned knowledge in deeper layers, with implications for alignment
and truthful AI systems.

</details>


### [62] ["Harmless to You, Hurtful to Me!": Investigating the Detection of Toxic Languages Grounded in the Perspective of Youth](https://arxiv.org/abs/2508.02094)
*Yaqiong Li,Peng Zhang,Lin Wang,Hansu Gu,Siyuan Qiao,Ning Gu,Tun Lu*

Main category: cs.CL

TL;DR: The paper explores youth-toxicity in social media, focusing on language perceived as toxic by youth but not adults. It constructs a Chinese dataset, analyzes contextual factors, and improves detection accuracy by incorporating meta-information.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding youth's unique toxicity perception, which differs from adults and is overlooked in existing research.

Method: Constructed a Chinese youth-toxicity dataset, analyzed contextual factors (e.g., utterance source, text features), and tested existing detection techniques.

Result: Youth's toxicity perception is linked to contextual factors. Adding meta-information improves detection accuracy.

Conclusion: The study highlights the need for youth-centered toxicity detection and offers insights for future research.

Abstract: Risk perception is subjective, and youth's understanding of toxic content
differs from that of adults. Although previous research has conducted extensive
studies on toxicity detection in social media, the investigation of youth's
unique toxicity, i.e., languages perceived as nontoxic by adults but toxic as
youth, is ignored. To address this gap, we aim to explore: 1) What are the
features of ``youth-toxicity'' languages in social media (RQ1); 2) Can existing
toxicity detection techniques accurately detect these languages (RQ2). For
these questions, we took Chinese youth as the research target, constructed the
first Chinese ``youth-toxicity'' dataset, and then conducted extensive
analysis. Our results suggest that youth's perception of these is associated
with several contextual factors, like the source of an utterance and
text-related features. Incorporating these meta information into current
toxicity detection methods significantly improves accuracy overall. Finally, we
propose several insights into future research on youth-centered toxicity
detection.

</details>


### [63] [Learning Dynamics of Meta-Learning in Small Model Pretraining](https://arxiv.org/abs/2508.02189)
*David Demitri Africa,Yuval Weiss,Paula Buttery,Richard Diehl Martinez*

Main category: cs.CL

TL;DR: Meta-learning improves small language models' pretraining, making them faster, better, and more interpretable.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency and interpretability of small language models using meta-learning.

Method: Integrates first-order MAML with subset-masked LM pretraining, tested on four LLama-style models (11M-570M params).

Result: Achieves faster training (1.6x speedup), better multilingual NER performance, and interpretable training dynamics (diversify-compress pattern).

Conclusion: Meta-learning offers a compact, interpretable way to improve small language models.

Abstract: Large language models are powerful but costly. We ask whether meta-learning
can make the pretraining of small language models not only better but also more
interpretable. We integrate first-order MAML with subset-masked LM pretraining,
producing four LLama-style decoder-only models (11M-570M params), and evaluate
it on a fundamental NLP task with many settings and real-world applications.
Compared with vanilla training, our model (i) reaches the same loss up to 1.6x
sooner, (ii) improves F1 on multilingual Universal NER under equal compute, and
(iii) makes the training dynamics easy to read: first the network's
representations fan out ("diversify") and later they collapse into a smaller,
shared subspace ("compress"). This two-stage shift shows up as a rise-and-fall
in both effective-rank curves and attention-head entropy. The same curves
pinpoint which layers specialise earliest and which later reconverge, giving a
compact, interpretable signature of meta-adaptation. Code, checkpoints and
WandB logs are released.

</details>


### [64] [Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference](https://arxiv.org/abs/2508.02193)
*Yuxuan Song,Zheng Zhang,Cheng Luo,Pengyang Gao,Fan Xia,Hao Luo,Zheng Li,Yuehang Yang,Hongli Yu,Xingwei Qu,Yuwei Fu,Jing Su,Ge Zhang,Wenhao Huang,Mingxuan Wang,Lin Yan,Xiaoying Jia,Jingjing Liu,Wei-Ying Ma,Ya-Qin Zhang,Yonghui Wu,Hao Zhou*

Main category: cs.CL

TL;DR: Seed Diffusion Preview is a fast, parallel-generation language model using discrete-state diffusion, achieving 2,146 token/s inference speed while maintaining competitive performance on code benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the latency issue in token-by-token decoding by leveraging non-sequential, parallel generation for faster inference.

Method: Uses discrete-state diffusion for parallel generation, enabling faster inference compared to sequential models.

Result: Achieves 2,146 token/s inference speed on H20 GPUs, outperforming Mercury and Gemini Diffusion in speed while maintaining benchmark performance.

Conclusion: Sets a new state-of-the-art on the speed-quality Pareto frontier for code models.

Abstract: We present Seed Diffusion Preview, a large-scale language model based on
discrete-state diffusion, offering remarkably fast inference speed. Thanks to
non-sequential, parallel generation, discrete diffusion models provide a
notable speedup to mitigate the inherent latency of token-by-token decoding, as
demonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion
Preview achieves an inference speed of 2,146 token/s over H20 GPUs while
maintaining competitive performance across a sweep of standard code evaluation
benchmarks, significantly faster than contemporary Mercury and Gemini
Diffusion, establishing new state of the art on the speed-quality Pareto
frontier for code models.

</details>


### [65] [Proof2Hybrid: Automatic Mathematical Benchmark Synthesis for Proof-Centric Problems](https://arxiv.org/abs/2508.02208)
*Yebo Peng,Zixiang Liu,Yaoming Li,Zhizhuo Yang,Xinye Xu,Bowen Ye,Weijun Yuan,Zihan Wang,Tong Yang*

Main category: cs.CL

TL;DR: Proof2Hybrid automates benchmark creation for evaluating LLMs' mathematical proof abilities, introducing hybrid-formatted questions and AlgGeoTest to reveal deficits in algebraic geometry comprehension.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLMs' mathematical capabilities are unscalable and costly, especially for proof-centric problems, leaving their true abilities unassessed.

Method: Proposes Proof2Hybrid, an automated framework using Proof2X to convert proofs into verifiable questions, including hybrid-formatted "$m$-out-of-$n$ multiple judge questions" for robust evaluation.

Result: AlgGeoTest, a 456-item benchmark for algebraic geometry, exposes significant deficits in LLMs' comprehension, offering precise measurement of their mathematical capabilities.

Conclusion: Proof2Hybrid and AlgGeoTest enable deeper research into AI's mathematical intelligence by providing scalable, high-quality benchmarks.

Abstract: Evaluating the mathematical capability of Large Language Models (LLMs) is a
critical yet challenging frontier. Existing benchmarks fall short, particularly
for proof-centric problems, as manual creation is unscalable and costly,
leaving the true mathematical abilities of LLMs largely unassessed. To overcome
these barriers, we propose Proof2Hybrid, the first fully automated framework
that synthesizes high-quality, proof-centric benchmarks from natural language
mathematical corpora. The key novelty of our solution is Proof2X, a roadmap of
converting mathematical proofs into various kinds of questions that are easy to
verify. Instructed by this roadmap, we propose a new type of hybrid-formatted
questions, named ``$m$-out-of-$n$ multiple judge questions'', specifically
designed to enable robust, automatic evaluation while being resilient to
guessing and superficial pattern matching inherent in traditional formats. As a
demonstration of our framework, we introduce AlgGeoTest, a benchmark for
algebraic geometry--a frontier domain of modern mathematics--comprising 456
challenging items. Our extensive evaluations on state-of-the-art LLMs using
AlgGeoTest reveal profound deficits in their comprehension of algebraic
geometry, providing a more precise measure of their true mathematical
capabilities. Our framework and benchmark pave the way for a new wave of
in-depth research into the mathematical intelligence of AI systems.

</details>


### [66] [Isolating Culture Neurons in Multilingual Large Language Models](https://arxiv.org/abs/2508.02241)
*Danial Namazifard,Lukas Galke*

Main category: cs.CL

TL;DR: The paper investigates how multilingual large language models encode culture, identifying distinct culture-specific neurons and their interaction with language-specific neurons.


<details>
  <summary>Details</summary>
Motivation: To understand and localize how cultural knowledge is encoded in multilingual LLMs, disentangling it from language-specific neurons.

Method: Extends an established methodology for identifying language-specific neurons to culture-specific neurons, using the MUREL dataset (85.2M tokens across six cultures). Localization and intervention experiments are conducted.

Result: LLMs encode cultures in distinct neuron populations, mainly in upper layers, which can be modulated independently from language or other culture neurons.

Conclusion: Cultural knowledge in LLMs can be selectively isolated and edited, promoting fairness and inclusivity. Code and data are available.

Abstract: Language and culture are deeply intertwined, yet it is so far unclear how and
where multilingual large language models encode culture. Here, we extend upon
an established methodology for identifying language-specific neurons and extend
it to localize and isolate culture-specific neurons, carefully disentangling
their overlap and interaction with language-specific neurons. To facilitate our
experiments, we introduce MUREL, a curated dataset of 85.2 million tokens
spanning six different cultures. Our localization and intervention experiments
show that LLMs encode different cultures in distinct neuron populations,
predominantly in upper layers, and that these culture neurons can be modulated
independently from language-specific neurons or those specific to other
cultures. These findings suggest that cultural knowledge and propensities in
multilingual language models can be selectively isolated and edited - promoting
fairness, inclusivity, and alignment. Code and data is available at
https://github.com/namazifard/Culture_Neurons .

</details>


### [67] [Interference Matrix: Quantifying Cross-Lingual Interference in Transformer Encoders](https://arxiv.org/abs/2508.02256)
*Belen Alastruey,João Maria Janeiro,Alexandre Allauzen,Maha Elbayad,Loïc Barrault,Marta R. Costa-jussà*

Main category: cs.CL

TL;DR: A study on language interference in encoder-only Transformer models across 83 languages, revealing asymmetrical interference patterns tied to script, not traditional linguistics, and showing practical utility for multilingual model design.


<details>
  <summary>Details</summary>
Motivation: To quantify and understand cross-lingual interference in Transformer models, moving beyond traditional linguistic characteristics.

Method: Constructed an interference matrix by training and evaluating small BERT-like models on all possible language pairs.

Result: Interference is asymmetrical and better related to script, not language family or embedding similarity. The matrix predicts downstream task performance.

Conclusion: The interference matrix is a valuable tool for optimizing multilingual model design.

Abstract: In this paper, we present a comprehensive study of language interference in
encoder-only Transformer models across 83 languages. We construct an
interference matrix by training and evaluating small BERT-like models on all
possible language pairs, providing a large-scale quantification of
cross-lingual interference. Our analysis reveals that interference between
languages is asymmetrical and that its patterns do not align with traditional
linguistic characteristics, such as language family, nor with proxies like
embedding similarity, but instead better relate to script. Finally, we
demonstrate that the interference matrix effectively predicts performance on
downstream tasks, serving as a tool to better design multilingual models to
obtain optimal performance.

</details>


### [68] [Decomposing the Entropy-Performance Exchange: The Missing Keys to Unlocking Effective Reinforcement Learning](https://arxiv.org/abs/2508.02260)
*Jia Deng,Jie Chen,Zhipeng Chen,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.CL

TL;DR: The paper analyzes the entropy-performance exchange in RLVR for LLMs, identifying key stages and proposing methods to dynamically adjust rewards for better performance.


<details>
  <summary>Details</summary>
Motivation: To understand and optimize the entropy-performance trade-off in RLVR for enhancing LLM reasoning abilities.

Method: Systematic empirical analysis of entropy dynamics across stages (rising, plateau) and granularities (stage, instance, token), followed by dynamic reward adjustment methods using perplexity and positional info.

Result: Entropy reduction in negative samples aids learning in the rising stage, while high-entropy tokens in low-perplexity samples and sequence ends correlate with learning efficiency in the plateau stage. Proposed methods outperform baselines.

Conclusion: Dynamic reward adjustment based on entropy dynamics improves RLVR performance in LLMs, with specific insights for different training stages and token types.

Abstract: Recently, reinforcement learning with verifiable rewards (RLVR) has been
widely used for enhancing the reasoning abilities of large language models
(LLMs). A core challenge in RLVR involves managing the exchange between entropy
and performance of policies. Despite the importance of this exchange, a
fine-grained understanding of when and how this exchange operates most
effectively remains limited. To bridge this gap, we conduct a systematic
empirical analysis of the entropy-performance exchange mechanism of RLVR across
different levels of granularity. Specifically, we first divide the training
process into two distinct stages based on entropy dynamics, i.e., rising stage
and plateau stage, and then systematically investigate how this mechanism
varies across stage-level, instance-level, and token-level granularitiess. Our
analysis reveals that, in the rising stage, entropy reduction in negative
samples facilitates the learning of effective reasoning patterns, which in turn
drives rapid performance gains. Moreover, in the plateau stage, learning
efficiency strongly correlates with high-entropy tokens present in
low-perplexity samples and those located at the end of sequences. Motivated by
these findings, we propose two methods that dynamically adjust the reward
signal using perplexity and positional information to focus RL updates on
tokens that exhibit high learning potential, achieving improvements compared to
the baseline methods on various LLMs.

</details>


### [69] [SHAMI-MT: A Syrian Arabic Dialect to Modern Standard Arabic Bidirectional Machine Translation System](https://arxiv.org/abs/2508.02268)
*Serry Sibaee,Omer Nacar,Yasser Al-Habashi,Adel Ammar,Wadii Boulila*

Main category: cs.CL

TL;DR: SHAMI-MT is a bidirectional machine translation system for Modern Standard Arabic (MSA) and the Syrian dialect, achieving high-quality translations with dialectal authenticity.


<details>
  <summary>Details</summary>
Motivation: The diglossia between MSA and regional dialects in the Arab world poses challenges for NLP, especially machine translation. This work aims to bridge the gap for the MSA-Syrian dialect pair.

Method: Two specialized models (MSA-to-Shami and Shami-to-MSA) were developed using AraT5v2-base-1024, fine-tuned on the Nabra dataset, and evaluated on MADAR corpus data.

Result: The MSA-to-Shami model scored 4.01/5.0 in quality, verified by GPT-4.1, showing high accuracy and dialectal authenticity.

Conclusion: SHAMI-MT advances dialectal Arabic translation, offering practical applications in localization, cultural heritage, and intercultural communication.

Abstract: The rich linguistic landscape of the Arab world is characterized by a
significant gap between Modern Standard Arabic (MSA), the language of formal
communication, and the diverse regional dialects used in everyday life. This
diglossia presents a formidable challenge for natural language processing,
particularly machine translation. This paper introduces \textbf{SHAMI-MT}, a
bidirectional machine translation system specifically engineered to bridge the
communication gap between MSA and the Syrian dialect. We present two
specialized models, one for MSA-to-Shami and another for Shami-to-MSA
translation, both built upon the state-of-the-art AraT5v2-base-1024
architecture. The models were fine-tuned on the comprehensive Nabra dataset and
rigorously evaluated on unseen data from the MADAR corpus. Our MSA-to-Shami
model achieved an outstanding average quality score of \textbf{4.01 out of 5.0}
when judged by OPENAI model GPT-4.1, demonstrating its ability to produce
translations that are not only accurate but also dialectally authentic. This
work provides a crucial, high-fidelity tool for a previously underserved
language pair, advancing the field of dialectal Arabic translation and offering
significant applications in content localization, cultural heritage, and
intercultural communication.

</details>


### [70] [Dynaword: From One-shot to Continuously Developed Datasets](https://arxiv.org/abs/2508.02271)
*Kenneth Enevoldsen,Kristian Nørgaard Jensen,Jan Kostkan,Balázs Szabó,Márton Kardos,Kirten Vad,Andrea Blasi Núñez,Gianluca Barmina,Jacob Nielsen,Rasmus Larsen,Peter Vahlstrup,Per Møldrup Dalum,Desmond Elliott,Lukas Galke,Peter Schneider-Kamp,Kristoffer Nielbo*

Main category: cs.CL

TL;DR: The paper introduces Dynaword, a framework for creating open, continuously updatable NLP datasets, and Danish Dynaword as its implementation, addressing licensing, static releases, and quality assurance challenges.


<details>
  <summary>Details</summary>
Motivation: Current NLP datasets face issues with restrictive licensing, lack of community updates, and limited quality assurance.

Method: Proposes the Dynaword framework for open, community-updatable datasets and implements Danish Dynaword as a proof of concept.

Result: Danish Dynaword is larger, openly licensed, and community-contributed, with built-in quality checks.

Conclusion: The Dynaword approach offers a sustainable solution for scalable, collaborative NLP datasets.

Abstract: Large-scale datasets are foundational for research and development in natural
language processing. However, current approaches face three key challenges: (1)
reliance on ambiguously licensed sources restricting use, sharing, and
derivative works; (2) static dataset releases that prevent community
contributions and diminish longevity; and (3) quality assurance processes
restricted to publishing teams rather than leveraging community expertise.
  To address these limitations, we introduce two contributions: the Dynaword
approach and Danish Dynaword. The Dynaword approach is a framework for creating
large-scale, open datasets that can be continuously updated through community
collaboration. Danish Dynaword is a concrete implementation that validates this
approach and demonstrates its potential. Danish Dynaword contains over four
times as many tokens as comparable releases, is exclusively openly licensed,
and has received multiple contributions across industry and research. The
repository includes light-weight tests to ensure data formatting, quality, and
documentation, establishing a sustainable framework for ongoing community
contributions and dataset evolution.

</details>


### [71] [A French Version of the OLDI Seed Corpus](https://arxiv.org/abs/2508.02290)
*Malik Marmonier,Benoît Sagot,Rachel Bawden*

Main category: cs.CL

TL;DR: First French partition of the OLDI Seed Corpus for WMT 2025, created using machine translation and native speaker post-editing, aimed at aiding under-resourced regional languages.


<details>
  <summary>Details</summary>
Motivation: To provide a pivot resource for collecting parallel corpora for under-resourced regional languages in France.

Method: Used multiple machine translation systems and a custom interface for post-editing by native speakers.

Result: A French corpus combining technical terminology and stylistic irregularities from Wikipedia user-generated content.

Conclusion: The corpus serves as a pivot resource to support regional language translation efforts.

Abstract: We present the first French partition of the OLDI Seed Corpus, our submission
to the WMT 2025 Open Language Data Initiative (OLDI) shared task. We detail its
creation process, which involved using multiple machine translation systems and
a custom-built interface for post-editing by qualified native speakers. We also
highlight the unique translation challenges presented by the source data, which
combines highly technical, encyclopedic terminology with the stylistic
irregularities characteristic of user-generated content taken from Wikipedia.
This French corpus is not an end in itself, but is intended as a crucial pivot
resource to facilitate the collection of parallel corpora for the
under-resourced regional languages of France.

</details>


### [72] [Simple Methods Defend RAG Systems Well Against Real-World Attacks](https://arxiv.org/abs/2508.02296)
*Ilias Triantafyllopoulos,Renyi Qu,Salvatore Giorgi,Brenda Curtis,Lyle H. Ungar,João Sedoc*

Main category: cs.CL

TL;DR: The paper evaluates four methods (GPT-4o, regression, PCA, Neural Collapse) for detecting Out-Of-Domain (OOD) queries in RAG systems to ensure safety and relevance. It introduces novel dimensionality reduction strategies and validates them on real-world datasets and applications.


<details>
  <summary>Details</summary>
Motivation: Ensuring safety and relevance in RAG systems for critical applications is challenging, especially in handling OOD queries.

Method: Four OOD detection methods are evaluated, including PCA and Neural Collapse adaptations, tested on datasets like StackExchange and real-world scenarios (e.g., COVID-19 chatbot).

Result: External OOD detectors are crucial for maintaining response relevance, validated through human and LLM evaluations.

Conclusion: The study highlights the importance of OOD detection in RAG systems for safety-critical applications, with PCA and Neural Collapse showing promise.

Abstract: Ensuring safety and in-domain responses for Retrieval-Augmented Generation
(RAG) systems is paramount in safety-critical applications, yet remains a
significant challenge. To address this, we evaluate four methodologies for
Out-Of-Domain (OOD) query detection: GPT-4o, regression-based, Principal
Component Analysis (PCA)-based, and Neural Collapse (NC), to ensure the RAG
system only responds to queries confined to the system's knowledge base.
Specifically, our evaluation explores two novel dimensionality reduction and
feature separation strategies: \textit{PCA}, where top components are selected
using explained variance or OOD separability, and an adaptation of
\textit{Neural Collapse Feature Separation}. We validate our approach on
standard datasets (StackExchange and MSMARCO) and real-world applications
(Substance Use and COVID-19), including tests against LLM-simulated and actual
attacks on a COVID-19 vaccine chatbot. Through human and LLM-based evaluations
of response correctness and relevance, we confirm that an external OOD detector
is crucial for maintaining response relevance.

</details>


### [73] [LaMPE: Length-aware Multi-grained Position Encoding for Adaptive Long-context Scaling Without Training](https://arxiv.org/abs/2508.02308)
*Sikui Zhang,Guangze Gao,Ziyun Gan,Chunfeng Yuan,Zefeng Lin,Houwen Peng,Bing Li,Weiming Hu*

Main category: cs.CL

TL;DR: LaMPE is a training-free method for adaptive long-context scaling in LLMs, addressing OOD issues in RoPE by dynamically mapping input length to positional capacity and using multi-grained attention.


<details>
  <summary>Details</summary>
Motivation: Performance degradation in LLMs when input exceeds the pretraining context window due to OOD behavior of RoPE.

Method: LaMPE uses a parametric scaled sigmoid function for dynamic length mapping and a multi-grained attention mechanism for positional resolution.

Result: Significant performance improvements on three LLMs across five benchmarks compared to existing methods.

Conclusion: LaMPE effectively adapts to varying input lengths without training, enhancing long-context performance in RoPE-based LLMs.

Abstract: Large language models (LLMs) experience significant performance degradation
when the input exceeds the pretraining context window, primarily due to the
out-of-distribution (OOD) behavior of Rotary Position Embedding (RoPE). Recent
studies mitigate this problem by remapping OOD positions into the
in-distribution range with fixed mapping strategies, ignoring the dynamic
relationship between input length and the model's effective context window. To
this end, we propose Length-aware Multi-grained Positional Encoding (LaMPE), a
training-free method that fully utilizes the model's effective context window
for adaptive long-context scaling in LLMs. Motivated by the left-skewed
frequency distribution of relative positions, LaMPE establishes a dynamic
relationship between mapping length and input length through a parametric
scaled sigmoid function to adaptively allocate positional capacity across
varying input lengths. Meanwhile, LaMPE devises a novel multi-grained attention
mechanism that strategically allocates positional resolution across different
sequence regions to capture both fine-grained locality and long-range
dependencies. Our method can be seamlessly applied to a wide range of
RoPE-based LLMs without training. Extensive experiments on three representative
LLMs across five mainstream long-context benchmarks demonstrate that LaMPE
achieves significant performance improvements compared to existing length
extrapolation methods. The code will be released at
https://github.com/scar-on/LaMPE.

</details>


### [74] [VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo](https://arxiv.org/abs/2508.02317)
*Qianli Ma,Yaowei Zheng,Zhelun Shi,Zhongkai Zhao,Bin Jia,Ziyue Huang,Zhiqi Lin,Youjie Li,Jiacheng Yang,Yanghua Peng,Zhi Zhang,Xin Liu*

Main category: cs.CL

TL;DR: The paper introduces \veomni, a modular and efficient training framework for omni-modal LLMs, addressing scalability and engineering challenges in existing systems.


<details>
  <summary>Details</summary>
Motivation: Training omni-modal LLMs is challenging due to heterogeneous architectures and inefficient parallel logic in current frameworks, requiring better solutions for scalability and ease of integration.

Method: \veomni decouples communication from computation, enabling efficient 3D parallelism, and offers a flexible interface for integrating new modalities with minimal code changes.

Result: The framework achieves high throughput (2,800 tokens/sec/GPU) and scales to 160K context lengths on 128 GPUs, demonstrating efficiency for large omni-modal LLMs.

Conclusion: \veomni provides a scalable and efficient solution for training omni-modal LLMs, overcoming limitations of existing frameworks.

Abstract: Recent advances in large language models (LLMs) have driven impressive
progress in omni-modal understanding and generation. However, training
omni-modal LLMs remains a significant challenge due to the heterogeneous model
architectures required to process diverse modalities, necessitating
sophisticated system design for efficient large-scale training. Existing
frameworks typically entangle model definition with parallel logic, incurring
limited scalability and substantial engineering overhead for end-to-end
omni-modal training. % We present \veomni, a modular and efficient training
framework to accelerate the development of omni-modal LLMs. \veomni introduces
model-centric distributed recipes that decouples communication from
computation, enabling efficient 3D parallelism on omni-modal LLMs. \veomni also
features a flexible configuration interface supporting seamless integration of
new modalities with minimal code change. % Using \veomni, a omni-modal
mixture-of-experts (MoE) model with 30B parameters can be trained with over
2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D
parallelism on 128 GPUs, showcasing its superior efficiency and scalability for
training large omni-modal LLMs.

</details>


### [75] [CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis](https://arxiv.org/abs/2508.02322)
*Yuzhuang Xu,Xu Han,Yuanchi Zhang,Yixuan Wang,Yijun Liu,Shiyu Ji,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: The paper introduces CAMERA, a lightweight framework for compressing Mixture-of-Experts (MoE) models by analyzing and pruning micro-experts, achieving efficiency without training.


<details>
  <summary>Details</summary>
Motivation: MoE models face computational and storage inefficiencies despite their performance, and existing methods fail to balance performance and efficiency.

Method: The authors propose CAMERA, a training-free framework for identifying redundant micro-experts, and introduce CAMERA-P (pruning) and CAMERA-Q (quantization) for compression.

Result: CAMERA-P outperforms baselines in pruning (20%-60% ratios), and CAMERA-Q excels in 2-bit quantization. The method analyzes large models like Qwen2-57B-A14B efficiently.

Conclusion: CAMERA offers a scalable and efficient solution for MoE model compression, maintaining performance while reducing overhead.

Abstract: Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are
distinguished by their strong performance scaling with increasing parameters
across a wide range of tasks, yet they also suffer from substantial
computational and storage overheads. Notably, the performance gains of MoE
models do not scale proportionally with the growth in expert parameters. While
prior works attempt to reduce parameters via expert-level pruning, merging, or
decomposition, they still suffer from challenges in both performance and
computational efficiency. In this paper, we address these challenges by
introducing micro-expert as a finer-grained compression unit that spans across
matrices. We first establish a more fundamental perspective, viewing MoE layers
as mixtures of micro-experts, and present CAMERA, a lightweight and
training-free framework for identifying micro-expert redundancy. Our analysis
uncovers significant variance in micro-expert contributions during decoding.
Based on this insight, we further propose CAMERA-P, a structured micro-expert
pruning framework, and CAMERA-Q, a mixed-precision quantization idea designed
for micro-experts. Extensive experiments on nine downstream tasks show that
CAMERA-P consistently outperforms strong baselines under pruning ratios ranging
from 20% to 60%. Furthermore, CAMERA-Q achieves superior results under
aggressive 2-bit quantization, surpassing existing matrix- and channel-level
ideas. Notably, our method enables complete micro-expert analysis of
Qwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.

</details>


### [76] [Understanding and Mitigating Political Stance Cross-topic Generalization in Large Language Models](https://arxiv.org/abs/2508.02360)
*Jiayi Zhang,Shu Yang,Junchao Wu,Derek F. Wong,Di Wang*

Main category: cs.CL

TL;DR: Fine-tuning LLMs on political topics can manipulate their stances, even on unrelated topics. This study identifies political neurons and introduces InhibitFT to mitigate cross-topic generalization.


<details>
  <summary>Details</summary>
Motivation: To understand how fine-tuning LLMs on political topics affects their stances and to mitigate unintended cross-topic generalization.

Method: Proposes Political Neuron Localization through Activation Contrasting (PNLAC) to identify general and topic-specific political neurons, then introduces InhibitFT for mitigation.

Result: Identified neuron types exist across models/datasets; InhibitFT reduces cross-topic generalization by 20% while preserving topic-specific performance.

Conclusion: Selectively inhibiting 5% of neurons effectively mitigates cross-topic stance generalization, offering a practical solution.

Abstract: Fine-tuning Large Language Models on a political topic will significantly
manipulate their political stance on various issues and unintentionally affect
their stance on unrelated topics. While previous studies have proposed this
issue, there is still a lack of understanding regarding the internal
representations of these stances and the mechanisms that lead to unintended
cross-topic generalization. In this paper, we systematically explore the
internal mechanisms underlying this phenomenon from a neuron-level perspective
and how to mitigate the cross-topic generalization of political fine-tuning.
Firstly, we propose Political Neuron Localization through Activation
Contrasting (PNLAC) to identify two distinct types of political neurons:
general political neurons, which govern stance across multiple political
topics, and topic-specific neurons} that affect the model's political stance on
individual topics. We find the existence of these political neuron types across
four models and datasets through activation patching experiments. Leveraging
these insights, we introduce InhibitFT, an inhibition-based fine-tuning method,
effectively mitigating the cross-topic stance generalization. Experimental
results demonstrate the robustness of identified neuron types across various
models and datasets, and show that InhibitFT significantly reduces the
cross-topic stance generalization by 20% on average, while preserving
topic-specific performance. Moreover, we demonstrate that selectively
inhibiting only 5% of neurons is sufficient to effectively mitigate the
cross-topic stance generalization.

</details>


### [77] [CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation](https://arxiv.org/abs/2508.02401)
*Xiaolin Lin,Jingcun Wang,Olga Kondrateva,Yiyu Shi,Bing Li,Grace Li Zhang*

Main category: cs.CL

TL;DR: CompressKV improves KV cache efficiency in LLMs by selectively using attention heads and layer-adaptive allocation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency of heuristic token eviction in KV cache compression, which degrades LLM performance by ignoring head functionalities.

Method: Identify and use specific attention heads for token importance, then apply layer-adaptive KV cache allocation.

Result: Outperforms state-of-the-art methods on LongBench and Needle-in-a-Haystack benchmarks.

Conclusion: CompressKV effectively enhances memory and execution efficiency in LLMs by focusing on critical tokens and adaptive layer strategies.

Abstract: Recent advances in large language models (LLMs) have significantly boosted
long-context processing. However, the increasing key-value (KV) cache size
poses critical challenges to memory and execution efficiency. Most KV cache
compression methods rely on heuristic token eviction using all attention heads
in Grouped Query Attention (GQA)-based LLMs. This method ignores the different
functionalities of attention heads, leading to the eviction of critical tokens
and thus degrades the performance of LLMs.
  To address the issue above, instead of using all the attention heads in
GQA-based LLMs to determine important tokens as in the previous work, we first
identify the attention heads in each layer that are not only capable of
retrieving the initial and final tokens of a prompt, but also capable of
retrieving important tokens within the text and attending to their surrounding
semantic context. Afterwards, we exploit such heads to determine the important
tokens and retain their corresponding KV cache pairs. Furthermore, we analyze
the cache eviction error of each layer individually and introduce a
layer-adaptive KV cache allocation strategy. Experimental results demonstrate
the proposed CompressKV consistently outperforms state-of-the-art approaches
under various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.
Our code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.

</details>


### [78] [Learning to Evolve: Bayesian-Guided Continual Knowledge Graph Embedding](https://arxiv.org/abs/2508.02426)
*Linyu Li,Zhi Jin,Yuanpeng He,Dongming Jin,Yichi Zhang,Haoran Duan,Nyima Tash*

Main category: cs.CL

TL;DR: BAKE is a new CKGE model addressing catastrophic forgetting in evolving knowledge graphs using Bayesian updates and continual clustering.


<details>
  <summary>Details</summary>
Motivation: Traditional KGE models are static; CKGE faces catastrophic forgetting, losing prior knowledge.

Method: BAKE uses Bayesian posterior updates for continual learning and introduces continual clustering to constrain knowledge evolution.

Result: BAKE outperforms baseline models in experiments.

Conclusion: BAKE effectively mitigates catastrophic forgetting in evolving knowledge graphs.

Abstract: Since knowledge graphs (KG) will continue to evolve in real scenarios,
traditional KGE models are only suitable for static knowledge graphs.
Therefore, continual knowledge graph embedding (CKGE) has attracted the
attention of researchers. Currently, a key challenge facing CKGE is that the
model is prone to "catastrophic forgetting", resulting in the loss of
previously learned knowledge. In order to effectively alleviate this problem,
we propose a new CKGE model BAKE. First, we note that the Bayesian posterior
update principle provides a natural continual learning strategy that is
insensitive to data order and can theoretically effectively resist the
forgetting of previous knowledge during data evolution. Different from the
existing CKGE method, BAKE regards each batch of new data as a Bayesian update
of the model prior. Under this framework, as long as the posterior distribution
of the model is maintained, the model can better preserve the knowledge of
early snapshots even after evolving through multiple time snapshots. Secondly,
we propose a continual clustering method for CKGE, which further directly
combats knowledge forgetting by constraining the evolution difference (or
change amplitude) between new and old knowledge between different snapshots. We
conduct extensive experiments on BAKE on multiple datasets, and the results
show that BAKE significantly outperforms existing baseline models.

</details>


### [79] [AI-Based Measurement of Innovation: Mapping Expert Insight into Large Language Model Applications](https://arxiv.org/abs/2508.02430)
*Robin Nowak,Patrick Figge,Carolin Haeussler*

Main category: cs.CL

TL;DR: The paper proposes an LLM framework to measure innovation from unstructured text, outperforming traditional methods and other ML/DL models in accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of manual expert evaluations and context-specific proxies in innovation research by leveraging LLMs.

Method: Design an LLM framework to assess innovation from text, validated through studies on software updates and product reviews.

Result: The LLM framework achieved higher F1-scores and consistency than alternative measures and state-of-the-art models.

Conclusion: LLMs offer a reliable, accessible tool for innovation measurement, with design decisions impacting performance.

Abstract: Measuring innovation often relies on context-specific proxies and on expert
evaluation. Hence, empirical innovation research is often limited to settings
where such data is available. We investigate how large language models (LLMs)
can be leveraged to overcome the constraints of manual expert evaluations and
assist researchers in measuring innovation. We design an LLM framework that
reliably approximates domain experts' assessment of innovation from
unstructured text data. We demonstrate the performance and broad applicability
of this framework through two studies in different contexts: (1) the
innovativeness of software application updates and (2) the originality of
user-generated feedback and improvement ideas in product reviews. We compared
the performance (F1-score) and reliability (consistency rate) of our LLM
framework against alternative measures used in prior innovation studies, and to
state-of-the-art machine learning- and deep learning-based models. The LLM
framework achieved higher F1-scores than the other approaches, and its results
are highly consistent (i.e., results do not change across runs). This article
equips R&D personnel in firms, as well as researchers, reviewers, and editors,
with the knowledge and tools to effectively use LLMs for measuring innovation
and evaluating the performance of LLM-based innovation measures. In doing so,
we discuss, the impact of important design decisions-including model selection,
prompt engineering, training data size, training data distribution, and
parameter settings-on performance and reliability. Given the challenges
inherent in using human expert evaluation and existing text-based measures, our
framework has important implications for harnessing LLMs as reliable,
increasingly accessible, and broadly applicable research tools for measuring
innovation.

</details>


### [80] [LatentPrompt: Optimizing Promts in Latent Space](https://arxiv.org/abs/2508.02452)
*Mateusz Bystroński,Grzegorz Piotrowski,Nitesh V. Chawla,Tomasz Kajdanowicz*

Main category: cs.CL

TL;DR: LatentPrompt is a model-agnostic framework for optimizing LLM prompts using latent semantic space, improving task performance without manual rules.


<details>
  <summary>Details</summary>
Motivation: Current prompt optimization relies on heuristics or manual exploration, which is inefficient and not scalable.

Method: Embeds seed prompts in a latent space, explores it to find high-performing prompts, and refines them automatically.

Result: Increased classification accuracy by ~3% on the Financial PhraseBank benchmark after one optimization cycle.

Conclusion: LatentPrompt is broadly applicable, requiring only black-box LLM access and an evaluation metric, making it versatile for various tasks.

Abstract: Recent advances have shown that optimizing prompts for Large Language Models
(LLMs) can significantly improve task performance, yet many optimization
techniques rely on heuristics or manual exploration. We present LatentPrompt, a
model-agnostic framework for prompt optimization that leverages latent semantic
space to automatically generate, evaluate, and refine candidate prompts without
requiring hand-crafted rules. Beginning with a set of seed prompts, our method
embeds them in a continuous latent space and systematically explores this space
to identify prompts that maximize task-specific performance. In a
proof-of-concept study on the Financial PhraseBank sentiment classification
benchmark, LatentPrompt increased classification accuracy by approximately 3
percent after a single optimization cycle. The framework is broadly applicable,
requiring only black-box access to an LLM and an automatic evaluation metric,
making it suitable for diverse domains and tasks.

</details>


### [81] [Monsoon Uprising in Bangladesh: How Facebook Shaped Collective Identity](https://arxiv.org/abs/2508.02498)
*Md Tasin Abir,Arpita Chowdhury,Ashfia Rahman*

Main category: cs.CL

TL;DR: Facebook played a key role in shaping collective identity during Bangladesh's 2024 Monsoon Uprising by unifying protesters through multimodal content like images, memes, and hashtags.


<details>
  <summary>Details</summary>
Motivation: To understand how Facebook facilitated collective identity and resistance during the pro-democracy uprising in Bangladesh.

Method: Qualitative analysis of visual rhetoric, verbal discourse, and digital irony in Facebook content.

Result: Shared symbols, protest art, and slogans on Facebook built solidarity, mobilized sentiment, and challenged authoritarian narratives.

Conclusion: Online platforms like Facebook are powerful tools for identity construction and political mobilization in the digital age.

Abstract: This study investigates how Facebook shaped collective identity during the
July 2024 pro-democracy uprising in Bangladesh, known as the Monsoon Uprising.
During government repression, protesters turned to Facebook as a central space
for resistance, where multimodal expressions, images, memes, videos, hashtags,
and satirical posts played an important role in unifying participants. Using a
qualitative approach, this research analyzes visual rhetoric, verbal discourse,
and digital irony to reveal how shared symbols, protest art, and slogans built
a sense of solidarity. Key elements included the symbolic use of red, the
ironic metaphorical use of the term "Razakar", and the widespread sharing of
visuals representing courage, injustice, and resistance. The findings show that
the combination of visual and verbal strategies on Facebook not only mobilized
public sentiment, but also built a strong collective identity that challenged
authoritarian narratives. This study tries to demonstrate how online platforms
can serve as powerful tools for identity construction and political
mobilization in the digital age.

</details>


### [82] [From Monolingual to Bilingual: Investigating Language Conditioning in Large Language Models for Psycholinguistic Tasks](https://arxiv.org/abs/2508.02502)
*Shuzhou Yuan,Zhan Qu,Mario Tawfelis,Michael Färber*

Main category: cs.CL

TL;DR: LLMs adjust outputs based on prompted language identity, with deeper layers showing stronger psycholinguistic signals, especially in Chinese.


<details>
  <summary>Details</summary>
Motivation: To explore how LLMs encode psycholinguistic knowledge across languages and whether they exhibit human-like responses under different linguistic identities.

Method: Evaluated two models (Llama-3.3-70B-Instruct and Qwen2.5-72B-Instruct) using sound symbolism and word valence tasks under monolingual and bilingual prompting in English, Dutch, and Chinese.

Result: Models adjust outputs based on language identity; Qwen showed greater sensitivity. Psycholinguistic signals were more decodable in deeper layers, with Chinese prompts yielding stronger valence representations.

Conclusion: Language identity influences both output behavior and internal representations in LLMs, offering insights into their use as models of cross-linguistic cognition.

Abstract: Large Language Models (LLMs) exhibit strong linguistic capabilities, but
little is known about how they encode psycholinguistic knowledge across
languages. We investigate whether and how LLMs exhibit human-like
psycholinguistic responses under different linguistic identities using two
tasks: sound symbolism and word valence. We evaluate two models,
Llama-3.3-70B-Instruct and Qwen2.5-72B-Instruct, under monolingual and
bilingual prompting in English, Dutch, and Chinese. Behaviorally, both models
adjust their outputs based on prompted language identity, with Qwen showing
greater sensitivity and sharper distinctions between Dutch and Chinese. Probing
analysis reveals that psycholinguistic signals become more decodable in deeper
layers, with Chinese prompts yielding stronger and more stable valence
representations than Dutch. Our results demonstrate that language identity
conditions both output behavior and internal representations in LLMs, providing
new insights into their application as models of cross-linguistic cognition.

</details>


### [83] [Modular Arithmetic: Language Models Solve Math Digit by Digit](https://arxiv.org/abs/2508.02513)
*Tanja Baeumel,Daniil Gurgurov,Yusser al Ghussin,Josef van Genabith,Simon Ostermann*

Main category: cs.CL

TL;DR: LLMs use digit-position-specific circuits for arithmetic tasks, independent of model size or tokenization. These circuits are identified and validated using Feature Importance and Causal Interventions.


<details>
  <summary>Details</summary>
Motivation: To understand the internal mechanisms LLMs use for arithmetic tasks, as a unified understanding is lacking.

Method: Extend findings on digit-wise number representation, identify circuits using Feature Importance and Causal Interventions.

Result: Digit-position-specific circuits exist and operate independently, validated by interventions altering predictions at targeted positions.

Conclusion: LLMs have compositional, interpretable structures for arithmetic, with digit-position circuits playing a causal role.

Abstract: While recent work has begun to uncover the internal strategies that Large
Language Models (LLMs) employ for simple arithmetic tasks, a unified
understanding of their underlying mechanisms is still lacking. We extend recent
findings showing that LLMs represent numbers in a digit-wise manner and present
evidence for the existence of digit-position-specific circuits that LLMs use to
perform simple arithmetic tasks, i.e. modular subgroups of MLP neurons that
operate independently on different digit positions (units, tens, hundreds).
Notably, such circuits exist independently of model size and of tokenization
strategy, i.e. both for models that encode longer numbers digit-by-digit and as
one token. Using Feature Importance and Causal Interventions, we identify and
validate the digit-position-specific circuits, revealing a compositional and
interpretable structure underlying the solving of arithmetic problems in LLMs.
Our interventions selectively alter the model's prediction at targeted digit
positions, demonstrating the causal role of digit-position circuits in solving
arithmetic tasks.

</details>


### [84] [PoeTone: A Framework for Constrained Generation of Structured Chinese Songci with LLMs](https://arxiv.org/abs/2508.02515)
*Zhan Qu,Shuzhou Yuan,Michael Färber*

Main category: cs.CL

TL;DR: The paper investigates LLMs' ability to generate Songci, a constrained Chinese poetry form, using a multi-faceted evaluation framework. It evaluates 18 LLMs under five prompting strategies and proposes a Generate-Critic architecture to improve performance via fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To understand how well LLMs can generate culturally significant and formally constrained literary texts like Songci, and to improve their performance in this domain.

Method: Developed a comprehensive evaluation framework (formal conformity, automated quality assessment, human evaluation, probing tasks). Evaluated 18 LLMs under five prompting strategies. Proposed a Generate-Critic architecture for fine-tuning.

Result: Fine-tuning lightweight LLMs using the critic's feedback improved formal conformity by up to 5.88%.

Conclusion: The study provides insights into LLMs' strengths and limitations in generating constrained literary texts and demonstrates the effectiveness of the Generate-Critic approach.

Abstract: This paper presents a systematic investigation into the constrained
generation capabilities of large language models (LLMs) in producing Songci, a
classical Chinese poetry form characterized by strict structural, tonal, and
rhyme constraints defined by Cipai templates. We first develop a comprehensive,
multi-faceted evaluation framework that includes: (i) a formal conformity
score, (ii) automated quality assessment using LLMs, (iii) human evaluation,
and (iv) classification-based probing tasks. Using this framework, we evaluate
the generative performance of 18 LLMs, including 3 proprietary models and 15
open-source models across four families, under five prompting strategies:
zero-shot, one-shot, completion-based, instruction-tuned, and chain-of-thought.
Finally, we propose a Generate-Critic architecture in which the evaluation
framework functions as an automated critic. Leveraging the critic's feedback as
a reward signal, we fine-tune three lightweight open-source LLMs via supervised
fine-tuning (SFT), resulting in improvements of up to 5.88% in formal
conformity. Our findings offer new insights into the generative strengths and
limitations of LLMs in producing culturally significant and formally
constrained literary texts.

</details>


### [85] [I Have No Mouth, and I Must Rhyme: Uncovering Internal Phonetic Representations in LLaMA 3.2](https://arxiv.org/abs/2508.02527)
*Jack Merullo,Arjun Khurana,Oliver McLaughlin*

Main category: cs.CL

TL;DR: Llama-3.2-1B-Instruct models phonemes internally for phonetic tasks like rhyming, showing IPA-like vowel organization without explicit training.


<details>
  <summary>Details</summary>
Motivation: To understand how large language models like Llama-3.2-1B-Instruct represent phonetic information without auditory or phonetic grounding.

Method: Investigates token-level phonetic representations in Llama, identifying a 'phoneme mover head' and analyzing its output space.

Result: Llama uses a rich internal phoneme model, with vowel representations resembling the IPA vowel chart, despite no direct supervision.

Conclusion: Llama's latent space organizes phonemes similarly to human phonetic systems, suggesting implicit learning of phonetic structures.

Abstract: Large language models demonstrate proficiency on phonetic tasks, such as
rhyming, without explicit phonetic or auditory grounding. In this work, we
investigate how \verb|Llama-3.2-1B-Instruct| represents token-level phonetic
information. Our results suggest that Llama uses a rich internal model of
phonemes to complete phonetic tasks. We provide evidence for high-level
organization of phoneme representations in its latent space. In doing so, we
also identify a ``phoneme mover head" which promotes phonetic information
during rhyming tasks. We visualize the output space of this head and find that,
while notable differences exist, Llama learns a model of vowels similar to the
standard IPA vowel chart for humans, despite receiving no direct supervision to
do so.

</details>


### [86] [Contextual Graph Transformer: A Small Language Model for Enhanced Engineering Document Information Extraction](https://arxiv.org/abs/2508.02532)
*Karan Reddy,Mayukha Pal*

Main category: cs.CL

TL;DR: The paper introduces the Contextual Graph Transformer (CGT), a hybrid model combining GNNs and Transformers for better performance in technical document QA, outperforming GPT-2 and BERT with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Standard transformers struggle with fine-grained syntax and entity relationships in technical documents, necessitating a specialized solution.

Method: CGT uses dynamic graphs (sequential, skip-gram, semantic edges) processed by GATv2Conv layers, followed by a Transformer encoder for global dependencies. It integrates into a RAG pipeline and is trained in two phases: pretraining and fine-tuning.

Result: CGT achieves 24.7% higher accuracy than GPT-2 with 62.4% fewer parameters, excelling in structural token interactions and semantic coherence.

Conclusion: CGT is a parameter-efficient, adaptable model for technical domains, improving grounding, entity tracking, and retrieval-augmented responses.

Abstract: Standard transformer-based language models, while powerful for general text,
often struggle with the fine-grained syntax and entity relationships in complex
technical, engineering documents. To address this, we propose the Contextual
Graph Transformer (CGT), a hybrid neural architecture that combines Graph
Neural Networks (GNNs) and Transformers for domain-specific question answering.
CGT constructs a dynamic graph over input tokens using sequential, skip-gram,
and semantic similarity edges, which is processed by GATv2Conv layers for local
structure learning. These enriched embeddings are then passed to a Transformer
encoder to capture global dependencies. Unlike generic large models, technical
domains often require specialized language models with stronger
contextualization and structure awareness. CGT offers a parameter-efficient
solution for such use cases. Integrated into a Retrieval-Augmented Generation
(RAG) pipeline, CGT outperforms baselines like GPT-2 and BERT, achieving 24.7%
higher accuracy than GPT-2 with 62.4% fewer parameters. This gain stems from
CGTs ability to jointly model structural token interactions and long-range
semantic coherence. The model is trained from scratch using a two-phase
approach: pretraining on general text followed by fine-tuning on
domain-specific manuals. This highlights CGTs adaptability to technical
language, enabling better grounding, entity tracking, and retrieval-augmented
responses in real-world applications.

</details>


### [87] [What's in the News? Towards Identification of Bias by Commission, Omission, and Source Selection (COSS)](https://arxiv.org/abs/2508.02540)
*Anastasia Zhukova,Terry Ruas,Felix Hamborg,Karsten Donnay,Bela Gipp*

Main category: cs.CL

TL;DR: Proposes a method to automatically identify three types of bias (commission, omission, source selection) jointly, unlike prior work. Includes a pipeline for bias identification and visualization of text reuse patterns.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of discerning reliable and neutral news sources in an information-saturated world.

Method: Jointly identifies bias by commission, omission, and source selection (COSS) using a pipeline approach with visualization of text reuse patterns.

Result: A methodology for automated bias identification and visualization of patterns in news articles.

Conclusion: The proposed approach improves bias detection by jointly addressing COSS, aiding readers in evaluating news reliability.

Abstract: In a world overwhelmed with news, determining which information comes from
reliable sources or how neutral is the reported information in the news
articles poses a challenge to news readers. In this paper, we propose a
methodology for automatically identifying bias by commission, omission, and
source selection (COSS) as a joint three-fold objective, as opposed to the
previous work separately addressing these types of bias. In a pipeline concept,
we describe the goals and tasks of its steps toward bias identification and
provide an example of a visualization that leverages the extracted features and
patterns of text reuse.

</details>


### [88] [Building and Aligning Comparable Corpora](https://arxiv.org/abs/2508.02555)
*Motaz Saad,David Langlois,Kamel Smaili*

Main category: cs.CL

TL;DR: The paper presents a method to build and align comparable corpora from Wikipedia and EURONEWS in English, French, and Arabic, using cross-lingual similarity measures. CL-LSI outperforms dictionary-based methods and is validated on BBC and ALJAZEERA news documents.


<details>
  <summary>Details</summary>
Motivation: Comparable corpora are valuable for multilingual NLP where parallel texts are unavailable, as they reveal topic-specific content across languages.

Method: Build comparable corpora from Wikipedia and EURONEWS; align documents using bilingual dictionary and CL-LSI measures. Validate on BBC and ALJAZEERA news.

Result: CL-LSI outperforms dictionary-based alignment, achieving topic and event-level alignment.

Conclusion: CL-LSI is effective for aligning comparable documents across languages, even at granular event levels.

Abstract: Comparable corpus is a set of topic aligned documents in multiple languages,
which are not necessarily translations of each other. These documents are
useful for multilingual natural language processing when there is no parallel
text available in some domains or languages. In addition, comparable documents
are informative because they can tell what is being said about a topic in
different languages. In this paper, we present a method to build comparable
corpora from Wikipedia encyclopedia and EURONEWS website in English, French and
Arabic languages. We further experiment a method to automatically align
comparable documents using cross-lingual similarity measures. We investigate
two cross-lingual similarity measures to align comparable documents. The first
measure is based on bilingual dictionary, and the second measure is based on
Latent Semantic Indexing (LSI). Experiments on several corpora show that the
Cross-Lingual LSI (CL-LSI) measure outperforms the dictionary based measure.
Finally, we collect English and Arabic news documents from the British
Broadcast Corporation (BBC) and from ALJAZEERA (JSC) news website respectively.
Then we use the CL-LSI similarity measure to automatically align comparable
documents of BBC and JSC. The evaluation of the alignment shows that CL-LSI is
not only able to align cross-lingual documents at the topic level, but also it
is able to do this at the event level.

</details>


### [89] [Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU Neural Networks](https://arxiv.org/abs/2508.02556)
*Ali Noori,Pratik Devkota,Somya Mohanty,Prashanti Manda*

Main category: cs.CL

TL;DR: A neural sequence labeling approach using a Bi-GRU model achieves 90% F1-score for SNOMED CT concept recognition in clinical text, outperforming rule-based systems and matching neural models with lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Manual annotation of clinical text with SNOMED CT concepts is labor-intensive; automated solutions are needed for scalability.

Method: A Bi-GRU model processes text preprocessed with domain-adapted SpaCy and SciBERT tokenization, using overlapping 19-token chunks enriched with contextual, syntactic, and morphological features.

Result: Achieves 90% F1-score, surpassing rule-based systems and matching/exceeding neural models, with effective handling of ambiguous terms and misspellings.

Conclusion: Lightweight RNN-based architectures like Bi-GRU offer high-quality clinical concept annotation with lower computational cost, suitable for real-world deployment.

Abstract: Automated annotation of clinical text with standardized medical concepts is
critical for enabling structured data extraction and decision support. SNOMED
CT provides a rich ontology for labeling clinical entities, but manual
annotation is labor-intensive and impractical at scale. This study introduces a
neural sequence labeling approach for SNOMED CT concept recognition using a
Bidirectional GRU model. Leveraging a subset of MIMIC-IV, we preprocess text
with domain-adapted SpaCy and SciBERT-based tokenization, segmenting sentences
into overlapping 19-token chunks enriched with contextual, syntactic, and
morphological features. The Bi-GRU model assigns IOB tags to identify concept
spans and achieves strong performance with a 90 percent F1-score on the
validation set. These results surpass traditional rule-based systems and match
or exceed existing neural models. Qualitative analysis shows effective handling
of ambiguous terms and misspellings. Our findings highlight that lightweight
RNN-based architectures can deliver high-quality clinical concept annotation
with significantly lower computational cost than transformer-based models,
making them well-suited for real-world deployment.

</details>


### [90] [Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction](https://arxiv.org/abs/2508.02558)
*Yuerong Song,Xiaoran Liu,Ruixiao Li,Zhigeng Liu,Zengfeng Huang,Qipeng Guo,Ziwei He,Xipeng Qiu*

Main category: cs.CL

TL;DR: Sparse-dLLM reduces computational and memory overhead in dLLMs by selectively caching pivotal tokens and evicting unimportant ones, achieving 10x higher throughput with comparable performance.


<details>
  <summary>Details</summary>
Motivation: Current caching techniques in dLLMs impose high memory usage, limiting long-context applications. Persistent cross-layer sparsity in attention patterns suggests selective cache eviction could optimize efficiency.

Method: Proposes Sparse-dLLM, a training-free framework combining dynamic cache eviction with sparse attention. It retains critical tokens and evicts unimportant ones using an attention-guided strategy.

Result: Achieves up to 10x higher throughput than vanilla dLLMs, with similar peak memory costs and comparable performance, outperforming previous methods.

Conclusion: Sparse-dLLM effectively balances efficiency and performance in dLLMs, enabling broader applications with reduced computational overhead.

Abstract: Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and
parallel decoding but suffer from prohibitive quadratic computational
complexity and memory overhead during inference. Current caching techniques
accelerate decoding by storing full-layer states, yet impose substantial memory
usage that limit long-context applications. Our analysis of attention patterns
in dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining
salient across decoding steps and low-relevance tokens staying unimportant,
motivating selective cache eviction. We propose Sparse-dLLM, the first
training-free framework integrating dynamic cache eviction with sparse
attention via delayed bidirectional sparse caching. By leveraging the stability
of token saliency over steps, it retains critical tokens and dynamically evicts
unimportant prefix/suffix entries using an attention-guided strategy. Extensive
experiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to
10$\times$ higher throughput than vanilla dLLMs, with comparable performance
and similar peak memory costs, outperforming previous methods in efficiency and
effectiveness.

</details>


### [91] [Guess or Recall? Training CNNs to Classify and Localize Memorization in LLMs](https://arxiv.org/abs/2508.02573)
*Jérémie Dentan,Davide Buscaldi,Sonia Vanier*

Main category: cs.CL

TL;DR: The paper analyzes verbatim memorization in LLMs, critiques the existing taxonomy, and proposes a new one based on attention weights. It finds few-shot memorization lacks a distinct mechanism and highlights the role of guessing in extractable samples.


<details>
  <summary>Details</summary>
Motivation: To better understand the mechanisms behind verbatim memorization in LLMs and improve the existing taxonomy by aligning it with attention weights.

Method: Train CNNs on LLM attention weights to evaluate and propose a new taxonomy of memorization types.

Result: Existing taxonomy poorly reflects attention mechanisms; new taxonomy identifies three categories. Few-shot memorization lacks distinct attention patterns, and many extractable samples are guessed.

Conclusion: A refined taxonomy and visual technique improve understanding of memorization mechanisms, separating guessed from recalled samples.

Abstract: Verbatim memorization in Large Language Models (LLMs) is a multifaceted
phenomenon involving distinct underlying mechanisms. We introduce a novel
method to analyze the different forms of memorization described by the existing
taxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on the
attention weights of the LLM and evaluate the alignment between this taxonomy
and the attention weights involved in decoding.
  We find that the existing taxonomy performs poorly and fails to reflect
distinct mechanisms within the attention blocks. We propose a new taxonomy that
maximizes alignment with the attention weights, consisting of three categories:
memorized samples that are guessed using language modeling abilities, memorized
samples that are recalled due to high duplication in the training set, and
non-memorized samples. Our results reveal that few-shot verbatim memorization
does not correspond to a distinct attention mechanism. We also show that a
significant proportion of extractable samples are in fact guessed by the model
and should therefore be studied separately. Finally, we develop a custom visual
interpretability technique to localize the regions of the attention weights
involved in each form of memorization.

</details>


### [92] [EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based Sentiment Analysis in Healthcare](https://arxiv.org/abs/2508.02574)
*Eman Alamoudi,Ellis Solaiman*

Main category: cs.CL

TL;DR: EHSAN introduces a hybrid pipeline combining ChatGPT pseudo-labelling and human review to create the first explainable Arabic aspect-based sentiment dataset for healthcare, showing high model accuracy even with minimal human supervision.


<details>
  <summary>Details</summary>
Motivation: Arabic-language patient feedback is under-analysed due to dialect diversity and lack of aspect-level sentiment labels, necessitating a scalable solution.

Method: A hybrid pipeline merges ChatGPT pseudo-labelling with human review to create three dataset versions (fully supervised, semi-supervised, unsupervised). Two transformer models are fine-tuned for aspect and sentiment classification.

Result: High accuracy is achieved even with minimal human supervision, with minor performance drops for ChatGPT-only labels. Reducing aspect classes improves classification metrics.

Conclusion: The approach is effective and scalable for Arabic aspect-based sentiment analysis in healthcare, combining LLM annotation and human expertise for robust, explainable datasets.

Abstract: Arabic-language patient feedback remains under-analysed because dialect
diversity and scarce aspect-level sentiment labels hinder automated assessment.
To address this gap, we introduce EHSAN, a data-centric hybrid pipeline that
merges ChatGPT pseudo-labelling with targeted human review to build the first
explainable Arabic aspect-based sentiment dataset for healthcare. Each sentence
is annotated with an aspect and sentiment label (positive, negative, or
neutral), forming a pioneering Arabic dataset aligned with healthcare themes,
with ChatGPT-generated rationales provided for each label to enhance
transparency. To evaluate the impact of annotation quality on model
performance, we created three versions of the training data: a fully supervised
set with all labels reviewed by humans, a semi-supervised set with 50% human
review, and an unsupervised set with only machine-generated labels. We
fine-tuned two transformer models on these datasets for both aspect and
sentiment classification. Experimental results show that our Arabic-specific
model achieved high accuracy even with minimal human supervision, reflecting
only a minor performance drop when using ChatGPT-only labels. Reducing the
number of aspect classes notably improved classification metrics across the
board. These findings demonstrate an effective, scalable approach to Arabic
aspect-based sentiment analysis (SA) in healthcare, combining large language
model annotation with human expertise to produce a robust and explainable
dataset. Future directions include generalisation across hospitals, prompt
refinement, and interpretable data-driven modelling.

</details>


### [93] [MArgE: Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification](https://arxiv.org/abs/2508.02584)
*Ming Pok Ng,Junqi Jiang,Gabriel Freedman,Antonio Rago,Francesca Toni*

Main category: cs.CL

TL;DR: MArgE introduces a structured framework for combining LLM outputs using argument trees, improving claim verification accuracy over single LLMs and unstructured debates.


<details>
  <summary>Details</summary>
Motivation: To address the lack of faithful justifications in current multi-LLM approaches and mitigate errors like hallucinations.

Method: Uses Argumentative LLMs (ArgLLMs) to construct structured argument trees for claim verification, creating inspectable pathways.

Result: MArgE outperforms single LLMs, open-source models, GPT-4o-mini, and unstructured debate methods.

Conclusion: Formal argumentative reasoning enhances multi-LLM collaboration, providing better accuracy and justification.

Abstract: Leveraging outputs from multiple large language models (LLMs) is emerging as
a method for harnessing their power across a wide range of tasks while
mitigating their capacity for making errors, e.g., hallucinations. However,
current approaches to combining insights from multiple LLMs often involve
unstructured interactions (e.g., free debate), resulting in model generations
that are not faithfully justifiable. In this work, we introduce MArgE, a novel
framework to provide formal structure to the evidence from each LLM, in the
form of a tree of extracted arguments, for the task of claim verification. We
use a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks
and semantics from the field of computational argumentation, to construct
structured argument trees for given claims. This process creates an inspectable
pathway from the initial arguments to the final claim verification decisions,
providing a faithful justification thereof. We show experimentally that MArgE
can significantly outperform single LLMs, including three open-source models
(4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior
methods for unstructured multi-LLM debates. We thus demonstrate the advantages
of incorporating formal, argumentative reasoning mechanisms when combining
multiple LLM outputs.

</details>


### [94] [CharBench: Evaluating the Role of Tokenization in Character-Level Tasks](https://arxiv.org/abs/2508.02591)
*Omri Uzan,Yuval Pinter*

Main category: cs.CL

TL;DR: CharBench is a new benchmark for character-level tasks, revealing modern LLMs struggle with such tasks, averaging 43.6% and 32.3% accuracy. Tokenization's impact varies by task type.


<details>
  <summary>Details</summary>
Motivation: Character-level tasks are challenging for language models, and the role of tokenization is unclear. CharBench aims to address this gap.

Method: Introduce CharBench, a large benchmark for character-level tasks, and evaluate various models on it. Analyze word properties and tokenization effects.

Result: Models perform poorly on CharBench (average 43.6% and 32.3% accuracy). Tokenization weakly affects counting tasks but hinders positional tasks.

Conclusion: CharBench highlights LLMs' struggles with character-level tasks. Future work should use it to improve model performance.

Abstract: Tasks that require character-level reasoning, such as counting or locating
characters within words, remain challenging for contemporary language models. A
common conjecture is that language models' reliance on subword units, rather
than characters, contributes to their struggles with character-level tasks, yet
recent studies offer conflicting conclusions about the role of tokenization,
leaving its impact unclear. To address this gap, we introduce CharBench, a
comprehensive benchmark of character-level tasks that is two orders of
magnitude larger than existing alternatives. We evaluate a diverse range of
leading open-weight and proprietary models on CharBench and find that it
presents a significant challenge to modern LLMs, with an average accuracy of
43.6% and 32.3% on some tasks. We present an in-depth analysis of how intrinsic
properties of words and their segmentations into tokens correspond to model
performance. For counting tasks, we find that tokenization properties are
weakly correlated with correctness, while the length of the queried word and
the actual character count play a more significant part. In contrast, for tasks
requiring intra-word positional understanding, performance is negatively
correlated with the length of the token containing the queried character,
suggesting that longer tokens obscure character position information for LLMs.
We encourage future work to build on the benchmark and evaluation methodology
introduced here as tools for improving model performance on such tasks.

</details>


### [95] [Mitigating Attention Hacking in Preference-Based Reward Modeling via Interaction Distillation](https://arxiv.org/abs/2508.02618)
*Jianxiang Zang,Meiling Ning,Shihan Dou,Jiazheng Zhang,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: The paper addresses the issue of 'attention hacking' in reward models (RMs) for RLHF in LLMs by proposing 'Interaction Distillation,' a method to improve token-level interaction and attention alignment.


<details>
  <summary>Details</summary>
Motivation: Current RMs lack adequate token-level interaction, making them vulnerable to attention hacking due to unidirectional attention and lack of inter-sequence attention.

Method: Proposes 'Interaction Distillation,' using a teacher model to guide RM in simulating better token interaction patterns via attentional alignment.

Result: Interaction Distillation provides more stable and generalizable reward signals compared to existing RM optimization methods.

Conclusion: Attention hacking is a fundamental limitation in RMs, and Interaction Distillation effectively addresses it.

Abstract: The reward model (RM), as the core component of reinforcement learning from
human feedback (RLHF) for large language models (LLMs), responsible for
providing reward signals to generated responses. However, mainstream preference
modeling in RM is inadequate in terms of token-level interaction, making its
judgment signals vulnerable to being hacked by misallocated attention to
context. This stems from two fundamental limitations: (1) Current preference
modeling employs decoder-only architectures, where the unidirectional causal
attention mechanism leads to forward-decaying intra-sequence attention within
the prompt-response sequence. (2) The independent Siamese-encoding paradigm
induces the absence of token-level inter-sequence attention between chosen and
rejected sequences. To address this "attention hacking", we propose
"Interaction Distillation", a novel training framework for more adequate
preference modeling through attention-level optimization. The method introduces
an interaction-based natural language understanding model as the teacher to
provide sophisticated token interaction patterns via comprehensive attention,
and guides the preference modeling to simulate teacher model's interaction
pattern through an attentional alignment objective. Through extensive
experiments, interaction distillation has demonstrated its ability to provide
more stable and generalizable reward signals compared to state-of-the-art RM
optimization methods that target data noise, highlighting the attention hacking
constitute a more fundamental limitation in RM.

</details>


### [96] [Pointer: Linear-Complexity Long-Range Modeling without Pre-training](https://arxiv.org/abs/2508.02631)
*Zixi Li*

Main category: cs.CL

TL;DR: Pointer is a new architecture for long-range sequence modeling with linear complexity, outperforming standard attention in speed and accuracy without pre-training.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency of standard attention mechanisms (O(N^2)) for long sequences and eliminate the need for pre-training.

Method: Uses layer-wise pointer chaining, where each layer's pointer selection depends on previous layers, creating explicit long-distance connections.

Result: Achieves 2–10× speedup on long sequences, >95% accuracy on copy tasks up to 2048 tokens, and learns interpretable pointer patterns.

Conclusion: Pointer is a viable alternative to attention for efficient long-range modeling without pre-training, offering speed, accuracy, and interpretability.

Abstract: We introduce Pointer, a novel architecture that achieves linear $O(NK)$
complexity for long-range sequence modeling while maintaining superior
performance without requiring pre-training. Unlike standard attention
mechanisms that compute $O(N^2)$ pairwise interactions, our approach uses
layer-wise pointer chaining where each layer's pointer selection depends on
previous layer's pointer positions, creating explicit long-distance connections
through pointer chains. We demonstrate that this architecture achieves
$2$--$10\times$ speedup on long sequences compared to standard transformers,
maintains $>95\%$ accuracy on copy tasks at distances up to 2048 tokens, and
learns interpretable pointer patterns that reveal structured dependency
modeling. Our experiments on efficiency benchmarks, long-range dependency
tasks, and interpretability analysis show that Pointer offers a compelling
alternative to attention mechanisms for scenarios requiring efficient
long-range modeling without pre-training dependencies.

</details>


### [97] [Test Set Quality in Multilingual LLM Evaluation](https://arxiv.org/abs/2508.02635)
*Kranti Chalamalasetti,Gabriel Bernier-Colborne,Yvan Gauthier,Sowmya Vajjala*

Main category: cs.CL

TL;DR: The paper highlights quality issues in multilingual benchmark datasets, identifies errors in French and Telugu datasets, and shows performance differences in LLMs when using corrected datasets. It advocates for revisiting and versioning test sets.


<details>
  <summary>Details</summary>
Motivation: To address the lack of attention to dataset quality in multilingual benchmarks, despite known issues in human-annotated test sets.

Method: Manual analysis of multilingual evaluation sets in French and Telugu, identifying errors and comparing LLM performance on original vs. revised datasets.

Result: Found significant performance differences (up to 10%) in LLMs when using corrected datasets, indicating dataset quality impacts model evaluation.

Conclusion: Test sets should be regularly checked, corrected, and versioned. Recommendations are provided for dataset creators and consumers to improve quality.

Abstract: Several multilingual benchmark datasets have been developed in a
semi-automatic manner in the recent past to measure progress and understand the
state-of-the-art in the multilingual capabilities of Large Language Models.
However, there is not a lot of attention paid to the quality of the datasets
themselves, despite the existence of previous work in identifying errors in
even fully human-annotated test sets. In this paper, we manually analyze recent
multilingual evaluation sets in two languages - French and Telugu, identifying
several errors in the process. We compare the performance difference across
several LLMs with the original and revised versions of the datasets and
identify large differences (almost 10% in some cases) in both languages). Based
on these results, we argue that test sets should not be considered immutable
and should be revisited, checked for correctness, and potentially versioned. We
end with some recommendations for both the dataset creators as well as
consumers on addressing the dataset quality issues.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [98] [Team PA-VCG's Solution for Competition on Understanding Chinese College Entrance Exam Papers in ICDAR'25](https://arxiv.org/abs/2508.00834)
*Wei Wu,Wenjie Wang,Yang Tan,Ying Liu,Liang Diao,Lin Huang,Kaihe Xu,Wenfeng Xie,Ziling Lin*

Main category: cs.CV

TL;DR: Team PA-VGG's solution for ICDAR'25 combines high-resolution image processing, multi-image input, and domain-specific post-training, achieving 89.6% accuracy in Gaokao paper analysis.


<details>
  <summary>Details</summary>
Motivation: Address challenges of dense OCR extraction and complex layouts in Chinese Gaokao papers.

Method: High-resolution image processing, multi-image end-to-end input, and domain-specific post-training.

Result: Achieved 89.6% accuracy, securing first place in the competition.

Conclusion: Domain-specific post-training significantly enhances performance in complex document analysis.

Abstract: This report presents Team PA-VGG's solution for the ICDAR'25 Competition on
Understanding Chinese College Entrance Exam Papers. In addition to leveraging
high-resolution image processing and a multi-image end-to-end input strategy to
address the challenges of dense OCR extraction and complex document layouts in
Gaokao papers, our approach introduces domain-specific post-training
strategies. Experimental results demonstrate that our post-training approach
achieves the most outstanding performance, securing first place with an
accuracy rate of 89.6%.

</details>


### [99] [Inclusive Review on Advances in Masked Human Face Recognition Technologies](https://arxiv.org/abs/2508.00841)
*Ali Haitham Abdul Amir,Zainab N. Nemer*

Main category: cs.CV

TL;DR: The paper reviews advancements in Masked Face Recognition (MFR) using deep learning, addressing challenges like partial concealment and lighting, and highlights future trends for improved real-world performance.


<details>
  <summary>Details</summary>
Motivation: The rise of mask usage due to COVID-19 has posed challenges for facial recognition systems, necessitating advancements in MFR technologies.

Method: Focuses on deep learning techniques like CNNs and Siamese networks, data enhancement, and advanced feature extraction to tackle MFR challenges.

Result: Discusses improved accuracy in MFR, overcoming issues like lighting and mask types, and reviews applications in security and medicine.

Conclusion: Future research should focus on efficient algorithms and multimedia integration to enhance MFR performance in real-world scenarios.

Abstract: Masked Face Recognition (MFR) is an increasingly important area in biometric
recognition technologies, especially with the widespread use of masks as a
result of the COVID-19 pandemic. This development has created new challenges
for facial recognition systems due to the partial concealment of basic facial
features. This paper aims to provide a comprehensive review of the latest
developments in the field, with a focus on deep learning techniques, especially
convolutional neural networks (CNNs) and twin networks (Siamese networks),
which have played a pivotal role in improving the accuracy of covering face
recognition. The paper discusses the most prominent challenges, which include
changes in lighting, different facial positions, partial concealment, and the
impact of mask types on the performance of systems. It also reviews advanced
technologies developed to overcome these challenges, including data enhancement
using artificial databases and multimedia methods to improve the ability of
systems to generalize. In addition, the paper highlights advance in deep
network design, feature extraction techniques, evaluation criteria, and data
sets used in this area. Moreover, it reviews the various applications of masked
face recognition in the fields of security and medicine, highlighting the
growing importance of these systems in light of recurrent health crises and
increasing security threats. Finally, the paper focuses on future research
trends such as developing more efficient algorithms and integrating multimedia
technologies to improve the performance of recognition systems in real-world
environments and expand their applications.

</details>


### [100] [HoneyImage: Verifiable, Harmless, and Stealthy Dataset Ownership Verification for Image Models](https://arxiv.org/abs/2508.00892)
*Zhihao Zhu,Jiale Han,Yi Yang*

Main category: cs.CV

TL;DR: HoneyImage is a novel method for verifying dataset ownership in image recognition models by embedding imperceptible traces in hard samples, balancing verification effectiveness and data integrity.


<details>
  <summary>Details</summary>
Motivation: Concerns about unauthorized use of proprietary image data in AI models necessitate reliable ownership verification methods without compromising data integrity.

Method: HoneyImage selectively modifies hard samples to embed verifiable traces, ensuring minimal impact on dataset integrity and downstream performance.

Result: Experiments on four benchmark datasets show HoneyImage achieves strong verification accuracy with imperceptible traces and minimal performance impact.

Conclusion: HoneyImage offers a practical solution for protecting dataset ownership, promoting safe data sharing and advancing data-driven AI.

Abstract: Image-based AI models are increasingly deployed across a wide range of
domains, including healthcare, security, and consumer applications. However,
many image datasets carry sensitive or proprietary content, raising critical
concerns about unauthorized data usage. Data owners therefore need reliable
mechanisms to verify whether their proprietary data has been misused to train
third-party models. Existing solutions, such as backdoor watermarking and
membership inference, face inherent trade-offs between verification
effectiveness and preservation of data integrity. In this work, we propose
HoneyImage, a novel method for dataset ownership verification in image
recognition models. HoneyImage selectively modifies a small number of hard
samples to embed imperceptible yet verifiable traces, enabling reliable
ownership verification while maintaining dataset integrity. Extensive
experiments across four benchmark datasets and multiple model architectures
show that HoneyImage consistently achieves strong verification accuracy with
minimal impact on downstream performance while maintaining imperceptible. The
proposed HoneyImage method could provide data owners with a practical mechanism
to protect ownership over valuable image datasets, encouraging safe sharing and
unlocking the full transformative potential of data-driven AI.

</details>


### [101] [Phase-fraction guided denoising diffusion model for augmenting multiphase steel microstructure segmentation via micrograph image-mask pair synthesis](https://arxiv.org/abs/2508.00896)
*Hoang Hai Nam Nguyen,Minh Tien Tran,Hoheok Kim,Ho Won Lee*

Main category: cs.CV

TL;DR: PF-DiffSeg is a diffusion framework for joint synthesis of microstructure images and masks, improving segmentation accuracy by conditioning on phase-fraction vectors.


<details>
  <summary>Details</summary>
Motivation: Lack of annotated phase masks for rare or complex metal alloy morphologies limits machine learning effectiveness in microstructure segmentation.

Method: A phase-fraction controlled, one-stage denoising diffusion framework (PF-DiffSeg) jointly generates images and masks, emphasizing minority classes.

Result: Notable segmentation accuracy improvements, especially for minority classes, outperforming GAN and two-stage diffusion baselines while reducing inference time.

Conclusion: PF-DiffSeg offers a scalable, unified solution for data augmentation in metallographic applications.

Abstract: The effectiveness of machine learning in metallographic microstructure
segmentation is often constrained by the lack of human-annotated phase masks,
particularly for rare or compositionally complex morphologies within the metal
alloy. We introduce PF-DiffSeg, a phase-fraction controlled, one-stage
denoising diffusion framework that jointly synthesizes microstructure images
and their corresponding segmentation masks in a single generative trajectory to
further improve segmentation accuracy. By conditioning on global phase-fraction
vectors, augmented to represent real data distribution and emphasize minority
classes, our model generates compositionally valid and structurally coherent
microstructure image and mask samples that improve both data diversity and
training efficiency. Evaluated on the MetalDAM benchmark for additively
manufactured multiphase steel, our synthetic augmentation method yields notable
improvements in segmentation accuracy compared to standard augmentation
strategies especially in minority classes and further outperforms a two-stage
mask-guided diffusion and generative adversarial network (GAN) baselines, while
also reducing inference time compared to conventional approach. The method
integrates generation and conditioning into a unified framework, offering a
scalable solution for data augmentation in metallographic applications.

</details>


### [102] [Benefits of Feature Extraction and Temporal Sequence Analysis for Video Frame Prediction: An Evaluation of Hybrid Deep Learning Models](https://arxiv.org/abs/2508.00898)
*Jose M. Sánchez Velázquez,Mingbo Cai,Andrew Coney,Álvaro J. García- Tejedor,Alberto Nogales*

Main category: cs.CV

TL;DR: The paper evaluates hybrid deep learning models (autoencoders with RNNs, 3D CNNs) for video frame prediction, showing improved performance (SSIM 0.69 to 0.82) with 3DCNNs and ConvLSTMs being most effective, especially on grayscale real-world videos.


<details>
  <summary>Details</summary>
Motivation: Video frame prediction has critical applications (e.g., weather forecasting, autonomous systems) but current models need improvement.

Method: Hybrid deep learning approaches combining autoencoders with RNNs, 3D CNNs, and ConvLSTMs, tested on synthetic and real-world datasets.

Result: SSIM metrics improved from 0.69 to 0.82; 3DCNNs and ConvLSTMs performed best, with grayscale real-world videos being easiest to predict.

Conclusion: Hybrid models, especially 3DCNNs and ConvLSTMs, enhance video frame prediction, with real-world grayscale data yielding the best results.

Abstract: In recent years, advances in Artificial Intelligence have significantly
impacted computer science, particularly in the field of computer vision,
enabling solutions to complex problems such as video frame prediction. Video
frame prediction has critical applications in weather forecasting or autonomous
systems and can provide technical improvements, such as video compression and
streaming. Among Artificial Intelligence methods, Deep Learning has emerged as
highly effective for solving vision-related tasks, although current frame
prediction models still have room for enhancement. This paper evaluates several
hybrid deep learning approaches that combine the feature extraction
capabilities of autoencoders with temporal sequence modelling using Recurrent
Neural Networks (RNNs), 3D Convolutional Neural Networks (3D CNNs), and related
architectures. The proposed solutions were rigorously evaluated on three
datasets that differ in terms of synthetic versus real-world scenarios and
grayscale versus color imagery. Results demonstrate that the approaches perform
well, with SSIM metrics increasing from 0.69 to 0.82, indicating that hybrid
models utilizing 3DCNNs and ConvLSTMs are the most effective, and greyscale
videos with real data are the easiest to predict.

</details>


### [103] [TESPEC: Temporally-Enhanced Self-Supervised Pretraining for Event Cameras](https://arxiv.org/abs/2508.00913)
*Mohammad Mohammadi,Ziyi Wu,Igor Gilitschenski*

Main category: cs.CV

TL;DR: TESPEC is a self-supervised pre-training framework for event-based perception, focusing on long-term spatio-temporal information. It outperforms existing methods in downstream tasks like object detection and semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing SSL methods for event-based pre-training ignore temporal information, limiting performance for recurrent models. TESPEC addresses this gap.

Method: TESPEC uses masked image modeling with a novel reconstruction target, accumulating events into pseudo grayscale videos to capture long-term spatio-temporal data.

Result: TESPEC achieves state-of-the-art results in object detection, semantic segmentation, and depth estimation.

Conclusion: TESPEC effectively leverages long-term event sequences, proving superior for recurrent models in event-based perception tasks.

Abstract: Long-term temporal information is crucial for event-based perception tasks,
as raw events only encode pixel brightness changes. Recent works show that when
trained from scratch, recurrent models achieve better results than feedforward
models in these tasks. However, when leveraging self-supervised pre-trained
weights, feedforward models can outperform their recurrent counterparts.
Current self-supervised learning (SSL) methods for event-based pre-training
largely mimic RGB image-based approaches. They pre-train feedforward models on
raw events within a short time interval, ignoring the temporal information of
events. In this work, we introduce TESPEC, a self-supervised pre-training
framework tailored for learning spatio-temporal information. TESPEC is
well-suited for recurrent models, as it is the first framework to leverage long
event sequences during pre-training. TESPEC employs the masked image modeling
paradigm with a new reconstruction target. We design a novel method to
accumulate events into pseudo grayscale videos containing high-level semantic
information about the underlying scene, which is robust to sensor noise and
reduces motion blur. Reconstructing this target thus requires the model to
reason about long-term history of events. Extensive experiments demonstrate our
state-of-the-art results in downstream tasks, including object detection,
semantic segmentation, and monocular depth estimation. Project webpage:
https://mhdmohammadi.github.io/TESPEC_webpage.

</details>


### [104] [Latent Diffusion Based Face Enhancement under Degraded Conditions for Forensic Face Recognition](https://arxiv.org/abs/2508.00941)
*Hassan Ugail,Hamad Mansour Alawar,AbdulNasser Abbas Zehi,Ahmed Mohammad Alkendi,Ismail Lujain Jaleel*

Main category: cs.CV

TL;DR: Latent diffusion-based enhancement significantly improves face recognition accuracy for low-quality forensic images, boosting performance from 29.1% to 84.5%.


<details>
  <summary>Details</summary>
Motivation: Face recognition systems struggle with low-quality forensic imagery, necessitating methods to enhance degraded images for better accuracy.

Method: The study uses the Flux.1 Kontext Dev pipeline with Facezoom LoRA adaptation on a dataset of 3,000 individuals (24,000 recognition attempts) to test against seven degradation types.

Result: Recognition accuracy improved from 29.1% to 84.5%, with significant gains across all degradation categories.

Conclusion: Diffusion-based enhancement holds strong potential for forensic face recognition applications.

Abstract: Face recognition systems experience severe performance degradation when
processing low-quality forensic evidence imagery. This paper presents an
evaluation of latent diffusion-based enhancement for improving face recognition
under forensically relevant degradations. Using a dataset of 3,000 individuals
from LFW with 24,000 recognition attempts, we implement the Flux.1 Kontext Dev
pipeline with Facezoom LoRA adaptation to test against seven degradation
categories, including compression artefacts, blur effects, and noise
contamination. Our approach demonstrates substantial improvements, increasing
overall recognition accuracy from 29.1% to 84.5% (55.4 percentage point
improvement, 95% CI: [54.1, 56.7]). Statistical analysis reveals significant
performance gains across all degradation types, with effect sizes exceeding
conventional thresholds for practical significance. These findings establish
the potential of sophisticated diffusion based enhancement in forensic face
recognition applications.

</details>


### [105] [Optimizing Vision-Language Consistency via Cross-Layer Regional Attention Alignment](https://arxiv.org/abs/2508.00945)
*Yifan Wang,Hongfeng Ai,Quangao Liu,Maowei Jiang,Ruiyuan Kang,Ruiqi Li,Jiahua Dong,Mengting Xiao,Cheng Jiang,Chenzhong Li*

Main category: cs.CV

TL;DR: CCRA improves VLMs by coordinating attention mechanisms with LPWCA and PAI, achieving state-of-the-art performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: VLMs struggle with mismatched attention and suboptimal performance due to uncoordinated cross-modal embedding learning.

Method: Proposes CCRA with LPWCA for fine-grained regional-semantic correlations and PAI to systematically coordinate attention mechanisms.

Result: Outperforms baselines on ten benchmarks with minimal additional parameters, offering better interpretability.

Conclusion: CCRA effectively aligns attention mechanisms, enhancing performance and interpretability in VLMs.

Abstract: Vision Language Models (VLMs) face challenges in effectively coordinating
diverse attention mechanisms for cross-modal embedding learning, leading to
mismatched attention and suboptimal performance. We propose Consistent
Cross-layer Regional Alignment (CCRA), which introduces Layer-Patch-wise Cross
Attention (LPWCA) to capture fine-grained regional-semantic correlations by
jointly weighting patch and layer-wise embedding, and Progressive Attention
Integration (PAI) that systematically coordinates LPWCA, layer-wise, and
patch-wise attention mechanisms in sequence. This progressive design ensures
consistency from semantic to regional levels while preventing attention drift
and maximizing individual attention benefits. Experimental results on ten
diverse vision-language benchmarks demonstrate that our CCRA-enhanced
LLaVA-v1.5-7B model achieves state-of-the-art performance, outperforming all
baseline methods with only 3.55M additional parameters, while providing
enhanced interpretability through more regionally focused and semantically
aligned attention patterns.

</details>


### [106] [ThermoCycleNet: Stereo-based Thermogram Labeling for Model Transition to Cycling](https://arxiv.org/abs/2508.00974)
*Daniel Andrés López,Vincent Weber,Severin Zentgraf,Barlo Hillen,Perikles Simon,Elmar Schömer*

Main category: cs.CV

TL;DR: Infrared thermography aids sports medicine by analyzing thermal radiation during exercise. A method combining automatic and manual annotations improves deep neural network performance for tasks like transitioning from treadmill to bicycle analysis.


<details>
  <summary>Details</summary>
Motivation: To adapt a stereo- and multimodal-based labeling approach from treadmill running to ergometer cycling, enhancing the efficiency of deep neural networks in sports medicine.

Method: Training a semantic segmentation network with automatic labels and fine-tuning on high-quality manually annotated images, comparing different data set combinations.

Result: Fine-tuning with a small fraction of manual data improves deep neural network performance. Combining automatic and manual annotations accelerates adaptation to new use cases.

Conclusion: Combining automatic and manual annotations is effective for adapting deep neural networks to new scenarios, like transitioning from treadmill to bicycle analysis.

Abstract: Infrared thermography is emerging as a powerful tool in sports medicine,
allowing assessment of thermal radiation during exercise and analysis of
anatomical regions of interest, such as the well-exposed calves. Building on
our previous advanced automatic annotation method, we aimed to transfer the
stereo- and multimodal-based labeling approach from treadmill running to
ergometer cycling. Therefore, the training of the semantic segmentation network
with automatic labels and fine-tuning on high-quality manually annotated images
has been examined and compared in different data set combinations. The results
indicate that fine-tuning with a small fraction of manual data is sufficient to
improve the overall performance of the deep neural network. Finally, combining
automatically generated labels with small manually annotated data sets
accelerates the adaptation of deep neural networks to new use cases, such as
the transition from treadmill to bicycle.

</details>


### [107] [ROVI: A VLM-LLM Re-Captioned Dataset for Open-Vocabulary Instance-Grounded Text-to-Image Generation](https://arxiv.org/abs/2508.01008)
*Cihang Peng,Qiming Hou,Zhong Ren,Kun Zhou*

Main category: cs.CV

TL;DR: ROVI is a synthetic dataset for text-to-image generation, created by labeling 1M web images using a re-captioning strategy with VLM and LLM. It outperforms existing datasets in quality and resolution, enabling better instance grounding in models like GLIGEN.


<details>
  <summary>Details</summary>
Motivation: To improve instance-grounded text-to-image generation by creating a high-quality dataset with comprehensive visual descriptions and open-vocabulary categories.

Method: Uses re-captioning: VLM generates visual descriptions, LLM extracts categories for OVDs, linking global prompts to instance annotations.

Result: ROVI surpasses existing datasets in quality, resolution, and category diversity. GLIGEN trained on ROVI outperforms SOTA models in accuracy and aesthetics.

Conclusion: ROVI provides a superior dataset for text-to-image generation, enhancing instance grounding and visual fidelity. The pipeline is open-source.

Abstract: We present ROVI, a high-quality synthetic dataset for instance-grounded
text-to-image generation, created by labeling 1M curated web images. Our key
innovation is a strategy called re-captioning, focusing on the pre-detection
stage, where a VLM (Vision-Language Model) generates comprehensive visual
descriptions that are then processed by an LLM (Large Language Model) to
extract a flat list of potential categories for OVDs (Open-Vocabulary
Detectors) to detect. This approach yields a global prompt inherently linked to
instance annotations while capturing secondary visual elements humans typically
overlook. Evaluations show that ROVI exceeds existing detection datasets in
image quality and resolution while containing two orders of magnitude more
categories with an open-vocabulary nature. For demonstrative purposes, a
text-to-image model GLIGEN trained on ROVI significantly outperforms
state-of-the-art alternatives in instance grounding accuracy, prompt fidelity,
and aesthetic quality. Our dataset and reproducible pipeline are available at
https://github.com/CihangPeng/ROVI.

</details>


### [108] [AutoSIGHT: Automatic Eye Tracking-based System for Immediate Grading of Human experTise](https://arxiv.org/abs/2508.01015)
*Byron Dowling,Jozef Probcin,Adam Czajka*

Main category: cs.CV

TL;DR: AutoSIGHT uses eye-tracking data to classify human expertise in visual tasks, achieving high accuracy with small evaluation windows.


<details>
  <summary>Details</summary>
Motivation: To automate the assessment of human expertise in visual tasks using eye-tracking data, enabling dynamic human-AI collaboration.

Method: AutoSIGHT employs an ensemble of features from eye-tracking data, tested on iris Presentation Attack Detection (PAD) with varying evaluation windows.

Result: Achieves AUROC of 0.751 (5s window) and 0.8306 (30s window), demonstrating viability and improved performance with longer windows.

Conclusion: AutoSIGHT proves effective for expertise classification, offering potential for human-AI pairing and dynamic task adaptation.

Abstract: Can we teach machines to assess the expertise of humans solving visual tasks
automatically based on eye tracking features? This paper proposes AutoSIGHT,
Automatic System for Immediate Grading of Human experTise, that classifies
expert and non-expert performers, and builds upon an ensemble of features
extracted from eye tracking data while the performers were solving a visual
task. Results on the task of iris Presentation Attack Detection (PAD) used for
this study show that with a small evaluation window of just 5 seconds,
AutoSIGHT achieves an average average Area Under the ROC curve performance of
0.751 in subject-disjoint train-test regime, indicating that such detection is
viable. Furthermore, when a larger evaluation window of up to 30 seconds is
available, the Area Under the ROC curve (AUROC) increases to 0.8306, indicating
the model is effectively leveraging more information at a cost of slightly
delayed decisions. This work opens new areas of research on how to incorporate
the automatic weighing of human and machine expertise into human-AI pairing
setups, which need to react dynamically to nonstationary expertise distribution
between the human and AI players (e.g. when the experts need to be replaced, or
the task at hand changes rapidly). Along with this paper, we offer the eye
tracking data used in this study collected from 6 experts and 53 non-experts
solving iris PAD visual task.

</details>


### [109] [3D Reconstruction via Incremental Structure From Motion](https://arxiv.org/abs/2508.01019)
*Muhammad Zeeshan,Umer Zaki,Syed Ahmed Pasha,Zaar Khizar*

Main category: cs.CV

TL;DR: The paper details an incremental Structure from Motion (SfM) pipeline for 3D reconstruction, emphasizing geometric consistency and iterative refinement, validated with real-world data.


<details>
  <summary>Details</summary>
Motivation: Accurate 3D reconstruction from unstructured images is crucial for robotics, mapping, and scene understanding, but global SfM methods can struggle with noise or sparse data.

Method: The paper implements an incremental SfM pipeline, focusing on geometric consistency and iterative refinement via bundle adjustment.

Result: The approach is tested on real datasets, showing reliable performance in sparse or partially overlapping scenarios, with quality assessed via reprojection error and camera trajectory coherence.

Conclusion: Incremental SfM is a practical and reliable method for sparse 3D reconstruction in structured environments.

Abstract: Accurate 3D reconstruction from unstructured image collections is a key
requirement in applications such as robotics, mapping, and scene understanding.
While global Structure from Motion (SfM) techniques rely on full image
connectivity and can be sensitive to noise or missing data, incremental SfM
offers a more flexible alternative. By progressively incorporating new views
into the reconstruction, it enables the system to recover scene structure and
camera motion even in sparse or partially overlapping datasets. In this paper,
we present a detailed implementation of the incremental SfM pipeline, focusing
on the consistency of geometric estimation and the effect of iterative
refinement through bundle adjustment. We demonstrate the approach using a real
dataset and assess reconstruction quality through reprojection error and camera
trajectory coherence. The results support the practical utility of incremental
SfM as a reliable method for sparse 3D reconstruction in visually structured
environments.

</details>


### [110] [Structured Spectral Graph Learning for Anomaly Classification in 3D Chest CT Scans](https://arxiv.org/abs/2508.01045)
*Theo Di Piazza,Carole Lazarus,Olivier Nempont,Loic Boussel*

Main category: cs.CV

TL;DR: A graph-based approach for multi-label classification of 3D CT scans is proposed, addressing limitations of 3D CNNs and Vision Transformers by leveraging spectral domain convolution on structured graphs.


<details>
  <summary>Details</summary>
Motivation: The increasing workload of radiologists and the complexity of 3D CT scan data necessitate automated methods for tasks like anomaly detection. Existing methods (3D CNNs and Vision Transformers) have limitations in modeling long-range dependencies or require extensive pre-training.

Method: A new graph-based approach models CT scans as structured graphs, using axial slice triplets and spectral domain convolution to improve multi-label anomaly classification.

Result: The method shows strong cross-dataset generalization, competitive performance, and robustness to z-axis translation. An ablation study validates the contributions of each component.

Conclusion: The proposed graph-based method offers an efficient and effective alternative for multi-label classification of 3D CT scans, addressing key challenges in the field.

Abstract: With the increasing number of CT scan examinations, there is a need for
automated methods such as organ segmentation, anomaly detection and report
generation to assist radiologists in managing their increasing workload.
Multi-label classification of 3D CT scans remains a critical yet challenging
task due to the complex spatial relationships within volumetric data and the
variety of observed anomalies. Existing approaches based on 3D convolutional
networks have limited abilities to model long-range dependencies while Vision
Transformers suffer from high computational costs and often require extensive
pre-training on large-scale datasets from the same domain to achieve
competitive performance. In this work, we propose an alternative by introducing
a new graph-based approach that models CT scans as structured graphs,
leveraging axial slice triplets nodes processed through spectral domain
convolution to enhance multi-label anomaly classification performance. Our
method exhibits strong cross-dataset generalization, and competitive
performance while achieving robustness to z-axis translation. An ablation study
evaluates the contribution of each proposed component.

</details>


### [111] [Evading Data Provenance in Deep Neural Networks](https://arxiv.org/abs/2508.01074)
*Hongyu Zhu,Sichu Liang,Wenwen Wang,Zhuomeng Zhang,Fangqi Li,Shi-Lin Wang*

Main category: cs.CV

TL;DR: The paper introduces a unified evasion framework to bypass Dataset Ownership Verification (DOV) methods, showing vulnerabilities in current DOV approaches.


<details>
  <summary>Details</summary>
Motivation: Current DOV methods rely on oversimplistic evasion attacks, creating a false sense of security. The paper aims to expose these weaknesses.

Method: A teacher model learns from a copyright dataset, then transfers task-relevant knowledge to a surrogate student using an OOD dataset. Vision-Language and Large Language Models curate informative subsets for effective transfer.

Result: The framework outperforms nine state-of-the-art evasion attacks, eliminating copyright identifiers while maintaining generalization and effectiveness.

Conclusion: The study reveals key vulnerabilities in DOV methods, emphasizing the need for long-term improvements to enhance practicality.

Abstract: Modern over-parameterized deep models are highly data-dependent, with large
scale general-purpose and domain-specific datasets serving as the bedrock for
rapid advancements. However, many datasets are proprietary or contain sensitive
information, making unrestricted model training problematic. In the open world
where data thefts cannot be fully prevented, Dataset Ownership Verification
(DOV) has emerged as a promising method to protect copyright by detecting
unauthorized model training and tracing illicit activities. Due to its
diversity and superior stealth, evading DOV is considered extremely
challenging. However, this paper identifies that previous studies have relied
on oversimplistic evasion attacks for evaluation, leading to a false sense of
security. We introduce a unified evasion framework, in which a teacher model
first learns from the copyright dataset and then transfers task-relevant yet
identifier-independent domain knowledge to a surrogate student using an
out-of-distribution (OOD) dataset as the intermediary. Leveraging
Vision-Language Models and Large Language Models, we curate the most
informative and reliable subsets from the OOD gallery set as the final transfer
set, and propose selectively transferring task-oriented knowledge to achieve a
better trade-off between generalization and evasion effectiveness. Experiments
across diverse datasets covering eleven DOV methods demonstrate our approach
simultaneously eliminates all copyright identifiers and significantly
outperforms nine state-of-the-art evasion attacks in both generalization and
effectiveness, with moderate computational overhead. As a proof of concept, we
reveal key vulnerabilities in current DOV methods, highlighting the need for
long-term development to enhance practicality.

</details>


### [112] [DreamSat-2.0: Towards a General Single-View Asteroid 3D Reconstruction](https://arxiv.org/abs/2508.01079)
*Santiago Diaz,Xinghui Hu,Josiane Uwumukiza,Giovanni Lavezzi,Victor Rodriguez-Fernandez,Richard Linares*

Main category: cs.CV

TL;DR: DreamSat-2.0 benchmarks three 3D reconstruction models (Hunyuan-3D, Trellis-3D, Ouroboros-3D) on spacecraft and asteroid datasets, showing domain-dependent performance. Hunyuan-3D excels in image quality for spacecraft and geometric accuracy for asteroids.


<details>
  <summary>Details</summary>
Motivation: To improve asteroid exploration and autonomous spacecraft navigation by evaluating and advancing 3D reconstruction techniques.

Method: Benchmarking three state-of-the-art 3D reconstruction models using 2D perceptual and 3D geometric metrics on custom datasets.

Result: Model performance is domain-dependent: better image quality for spacecraft, better geometric accuracy for asteroids. Hunyuan-3D leads in both domains.

Conclusion: DreamSat-2.0 establishes new benchmarks, with Hunyuan-3D outperforming others, marking progress in 3D reconstruction for space applications.

Abstract: To enhance asteroid exploration and autonomous spacecraft navigation, we
introduce DreamSat-2.0, a pipeline that benchmarks three state-of-the-art 3D
reconstruction models-Hunyuan-3D, Trellis-3D, and Ouroboros-3D-on custom
spacecraft and asteroid datasets. Our systematic analysis, using 2D perceptual
(image quality) and 3D geometric (shape accuracy) metrics, reveals that model
performance is domain-dependent. While models produce higher-quality images of
complex spacecraft, they achieve better geometric reconstructions for the
simpler forms of asteroids. New benchmarks are established, with Hunyuan-3D
achieving top perceptual scores on spacecraft but its best geometric accuracy
on asteroids, marking a significant advance over our prior work.

</details>


### [113] [COSTARR: Consolidated Open Set Technique with Attenuation for Robust Recognition](https://arxiv.org/abs/2508.01087)
*Ryan Rabinowitz,Steve Cruz,Walter Scheirer,Terrance E. Boult*

Main category: cs.CV

TL;DR: COSTARR improves open-set recognition by leveraging attenuated features, outperforming prior methods across various architectures.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of novelty detection in visual recognition by utilizing overlooked attenuation information.

Method: Proposes COSTARR, combining familiar and unfamiliar features, with ablation studies to validate contributions of pre- and post-attenuated features.

Result: COSTARR outperforms state-of-the-art methods, generalizing well across architectures like ViTs, ConvNeXts, and ResNet.

Conclusion: COSTARR advances open-set recognition by effectively using attenuation information, enhancing novelty detection.

Abstract: Handling novelty remains a key challenge in visual recognition systems.
Existing open-set recognition (OSR) methods rely on the familiarity hypothesis,
detecting novelty by the absence of familiar features. We propose a novel
attenuation hypothesis: small weights learned during training attenuate
features and serve a dual role-differentiating known classes while discarding
information useful for distinguishing known from unknown classes. To leverage
this overlooked information, we present COSTARR, a novel approach that combines
both the requirement of familiar features and the lack of unfamiliar ones. We
provide a probabilistic interpretation of the COSTARR score, linking it to the
likelihood of correct classification and belonging in a known class. To
determine the individual contributions of the pre- and post-attenuated features
to COSTARR's performance, we conduct ablation studies that show both
pre-attenuated deep features and the underutilized post-attenuated Hadamard
product features are essential for improving OSR. Also, we evaluate COSTARR in
a large-scale setting using ImageNet2012-1K as known data and NINCO,
iNaturalist, OpenImage-O, and other datasets as unknowns, across multiple
modern pre-trained architectures (ViTs, ConvNeXts, and ResNet). The experiments
demonstrate that COSTARR generalizes effectively across various architectures
and significantly outperforms prior state-of-the-art methods by incorporating
previously discarded attenuation information, advancing open-set recognition
capabilities.

</details>


### [114] [AURA: A Hybrid Spatiotemporal-Chromatic Framework for Robust, Real-Time Detection of Industrial Smoke Emissions](https://arxiv.org/abs/2508.01095)
*Mikhail Bychkov,Matey Yordanov,Andrei Kuchma*

Main category: cs.CV

TL;DR: AURA is a hybrid spatiotemporal-chromatic framework for real-time industrial smoke detection and classification, improving accuracy and reducing false positives.


<details>
  <summary>Details</summary>
Motivation: Current monitoring systems lack specificity in distinguishing smoke types and struggle with environmental variability.

Method: AURA uses dynamic movement patterns and color characteristics of industrial smoke for detection and classification.

Result: Enhanced accuracy and reduced false positives in smoke detection and classification.

Conclusion: AURA improves environmental compliance, operational safety, and public health through precise automated monitoring.

Abstract: This paper introduces AURA, a novel hybrid spatiotemporal-chromatic framework
designed for robust, real-time detection and classification of industrial smoke
emissions. The framework addresses critical limitations of current monitoring
systems, which often lack the specificity to distinguish smoke types and
struggle with environmental variability. AURA leverages both the dynamic
movement patterns and the distinct color characteristics of industrial smoke to
provide enhanced accuracy and reduced false positives. This framework aims to
significantly improve environmental compliance, operational safety, and public
health outcomes by enabling precise, automated monitoring of industrial
emissions.

</details>


### [115] [Trans-Adapter: A Plug-and-Play Framework for Transparent Image Inpainting](https://arxiv.org/abs/2508.01098)
*Yuekun Dai,Haitian Li,Shangchen Zhou,Chen Change Loy*

Main category: cs.CV

TL;DR: Proposes Trans-Adapter, a plug-and-play adapter for diffusion-based inpainting models to handle RGBA images directly, addressing transparency consistency and edge quality issues.


<details>
  <summary>Details</summary>
Motivation: Existing inpainting methods are limited to RGB images, and conventional RGBA inpainting pipelines struggle with transparency consistency and edge quality.

Method: Introduces Trans-Adapter, a plug-and-play adapter for diffusion models, supporting controllable editing via ControlNet and integration with community models. Evaluated using LayerBench and a new alpha edge quality metric.

Result: Demonstrates effectiveness in preserving transparency and improving edge quality in RGBA image inpainting.

Conclusion: Trans-Adapter successfully addresses the limitations of existing methods, offering a versatile solution for transparent image inpainting.

Abstract: RGBA images, with the additional alpha channel, are crucial for any
application that needs blending, masking, or transparency effects, making them
more versatile than standard RGB images. Nevertheless, existing image
inpainting methods are designed exclusively for RGB images. Conventional
approaches to transparent image inpainting typically involve placing a
background underneath RGBA images and employing a two-stage process: image
inpainting followed by image matting. This pipeline, however, struggles to
preserve transparency consistency in edited regions, and matting can introduce
jagged edges along transparency boundaries. To address these challenges, we
propose Trans-Adapter, a plug-and-play adapter that enables diffusion-based
inpainting models to process transparent images directly. Trans-Adapter also
supports controllable editing via ControlNet and can be seamlessly integrated
into various community models. To evaluate our method, we introduce LayerBench,
along with a novel non-reference alpha edge quality evaluation metric for
assessing transparency edge quality. We conduct extensive experiments on
LayerBench to demonstrate the effectiveness of our approach.

</details>


### [116] [MASIV: Toward Material-Agnostic System Identification from Videos](https://arxiv.org/abs/2508.01112)
*Yizhou Zhao,Haoyu Chen,Chunjiang Liu,Zhenyang Li,Charles Herrmann,Junhwa Hur,Yinxiao Li,Ming-Hsuan Yang,Bhiksha Raj,Min Xu*

Main category: cs.CV

TL;DR: MASIV is a vision-based framework for material-agnostic system identification, using learnable neural constitutive models to infer object dynamics without predefined material priors. It addresses challenges like unstable optimization with dense geometric guidance and achieves top performance in accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on predefined material priors, limiting their ability to handle unknown materials. MASIV aims to overcome this by being material-agnostic.

Method: MASIV employs learnable neural constitutive models and introduces dense geometric guidance by reconstructing continuum particle trajectories for stable optimization.

Result: MASIV achieves state-of-the-art performance in geometric accuracy, rendering quality, and generalization ability.

Conclusion: MASIV successfully addresses the limitations of existing methods by being material-agnostic and leveraging dense geometric guidance, demonstrating superior performance.

Abstract: System identification from videos aims to recover object geometry and
governing physical laws. Existing methods integrate differentiable rendering
with simulation but rely on predefined material priors, limiting their ability
to handle unknown ones. We introduce MASIV, the first vision-based framework
for material-agnostic system identification. Unlike existing approaches that
depend on hand-crafted constitutive laws, MASIV employs learnable neural
constitutive models, inferring object dynamics without assuming a
scene-specific material prior. However, the absence of full particle state
information imposes unique challenges, leading to unstable optimization and
physically implausible behaviors. To address this, we introduce dense geometric
guidance by reconstructing continuum particle trajectories, providing
temporally rich motion constraints beyond sparse visual cues. Comprehensive
experiments show that MASIV achieves state-of-the-art performance in geometric
accuracy, rendering quality, and generalization ability.

</details>


### [117] [The Promise of RL for Autoregressive Image Editing](https://arxiv.org/abs/2508.01119)
*Saba Ahmadi,Rabiul Awal,Ankur Sikarwar,Amirhossein Kazemnejad,Ge Ya Luo,Juan A. Rodriguez,Sai Rajeswar,Siva Reddy,Christopher Pal,Benno Krojer,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: The paper introduces EARL, an RL-based image editing model, comparing SFT, RL, and CoT strategies, with RL outperforming others.


<details>
  <summary>Details</summary>
Motivation: To enhance performance in image editing tasks by exploring and comparing SFT, RL, and CoT strategies in a unified framework.

Method: Uses an autoregressive multimodal model to process text and visual tokens, combining RL with a large multi-modal LLM verifier.

Result: RL with a multi-modal verifier is most effective; EARL performs competitively with less training data.

Conclusion: EARL advances autoregressive multimodal models in image editing, with code and models publicly released.

Abstract: We explore three strategies to enhance performance on a wide range of image
editing tasks: supervised fine-tuning (SFT), reinforcement learning (RL), and
Chain-of-Thought (CoT) reasoning. In order to study all these components in one
consistent framework, we adopt an autoregressive multimodal model that
processes textual and visual tokens in a unified manner. We find RL combined
with a large multi-modal LLM verifier to be the most effective of these
strategies. As a result, we release EARL: Editing with Autoregression and RL, a
strong RL-based image editing model that performs competitively on a diverse
range of edits compared to strong baselines, despite using much less training
data. Thus, EARL pushes the frontier of autoregressive multimodal models on
image editing. We release our code, training data, and trained models at
https://github.com/mair-lab/EARL.

</details>


### [118] [UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction, Forecasting, and Generation](https://arxiv.org/abs/2508.01126)
*Chaitanya Patel,Hiroki Nakamura,Yuta Kyuragi,Kazuki Kozuka,Juan Carlos Niebles,Ehsan Adeli*

Main category: cs.CV

TL;DR: The paper introduces UniEgoMotion, a unified model for egocentric motion generation and forecasting using first-person images, addressing limitations of third-person methods.


<details>
  <summary>Details</summary>
Motivation: Enhancing AR/VR, human-robot interaction, assistive tech, and healthcare requires accurate egocentric motion prediction, but current methods lack effectiveness due to reliance on third-person views and structured 3D scenes.

Method: Proposes UniEgoMotion, a conditional motion diffusion model with head-centric motion representation, leveraging first-person visual inputs for unified motion tasks.

Result: UniEgoMotion achieves state-of-the-art performance in egocentric motion reconstruction and is the first to generate motion from a single egocentric image.

Conclusion: The framework sets a new benchmark for egocentric motion modeling, enabling novel applications in AR/VR, robotics, and healthcare.

Abstract: Egocentric human motion generation and forecasting with scene-context is
crucial for enhancing AR/VR experiences, improving human-robot interaction,
advancing assistive technologies, and enabling adaptive healthcare solutions by
accurately predicting and simulating movement from a first-person perspective.
However, existing methods primarily focus on third-person motion synthesis with
structured 3D scene contexts, limiting their effectiveness in real-world
egocentric settings where limited field of view, frequent occlusions, and
dynamic cameras hinder scene perception. To bridge this gap, we introduce
Egocentric Motion Generation and Egocentric Motion Forecasting, two novel tasks
that utilize first-person images for scene-aware motion synthesis without
relying on explicit 3D scene. We propose UniEgoMotion, a unified conditional
motion diffusion model with a novel head-centric motion representation tailored
for egocentric devices. UniEgoMotion's simple yet effective design supports
egocentric motion reconstruction, forecasting, and generation from first-person
visual inputs in a unified framework. Unlike previous works that overlook scene
semantics, our model effectively extracts image-based scene context to infer
plausible 3D motion. To facilitate training, we introduce EE4D-Motion, a
large-scale dataset derived from EgoExo4D, augmented with pseudo-ground-truth
3D motion annotations. UniEgoMotion achieves state-of-the-art performance in
egocentric motion reconstruction and is the first to generate motion from a
single egocentric image. Extensive evaluations demonstrate the effectiveness of
our unified framework, setting a new benchmark for egocentric motion modeling
and unlocking new possibilities for egocentric applications.

</details>


### [119] [Semi-Supervised Anomaly Detection in Brain MRI Using a Domain-Agnostic Deep Reinforcement Learning Approach](https://arxiv.org/abs/2508.01137)
*Zeduo Zhang,Yalda Mohsenzadeh*

Main category: cs.CV

TL;DR: A domain-agnostic, semi-supervised anomaly detection framework using deep reinforcement learning (DRL) was developed, achieving high performance on brain MRI and industrial datasets.


<details>
  <summary>Details</summary>
Motivation: To address challenges like large-scale data, overfitting, and class imbalance in anomaly detection, particularly for brain MRI volumes.

Method: The framework integrates DRL with feature representations, using publicly available brain MRI datasets (IXI and BraTS 2021) and industrial datasets. Preprocessing included normalization and skull-stripping. Performance was evaluated using AUROC and Dice scores.

Result: Achieved AUROC of 88.7% (pixel-level) and 96.7% (image-level) on brain MRI datasets, outperforming SOTA methods. Also showed competitive performance on industrial datasets (AUROC = 99.8% pixel-level, 99.3% image-level).

Conclusion: The DRL-based approach is robust, generalizable, and efficient, showing promise for real-world clinical and industrial applications.

Abstract: To develop a domain-agnostic, semi-supervised anomaly detection framework
that integrates deep reinforcement learning (DRL) to address challenges such as
large-scale data, overfitting, and class imbalance, focusing on brain MRI
volumes. This retrospective study used publicly available brain MRI datasets
collected between 2005 and 2021. The IXI dataset provided 581 T1-weighted and
578 T2-weighted MRI volumes (from healthy subjects) for training, while the
BraTS 2021 dataset provided 251 volumes for validation and 1000 for testing
(unhealthy subjects with Glioblastomas). Preprocessing included normalization,
skull-stripping, and co-registering to a uniform voxel size. Experiments were
conducted on both T1- and T2-weighted modalities. Additional experiments and
ablation analyses were also carried out on the industrial datasets. The
proposed method integrates DRL with feature representations to handle label
scarcity, large-scale data and overfitting. Statistical analysis was based on
several detection and segmentation metrics including AUROC and Dice score. The
proposed method achieved an AUROC of 88.7% (pixel-level) and 96.7%
(image-level) on brain MRI datasets, outperforming State-of-The-Art (SOTA)
methods. On industrial surface datasets, the model also showed competitive
performance (AUROC = 99.8% pixel-level, 99.3% image-level) on MVTec AD dataset,
indicating strong cross-domain generalization. Studies on anomaly sample size
showed a monotonic increase in AUROC as more anomalies were seen, without
evidence of overfitting or additional computational cost. The domain-agnostic
semi-supervised approach using DRL shows significant promise for MRI anomaly
detection, achieving strong performance on both medical and industrial
datasets. Its robustness, generalizability and efficiency highlight its
potential for real-world clinical applications.

</details>


### [120] [Dataset Condensation with Color Compensation](https://arxiv.org/abs/2508.01139)
*Huyu Wu,Duo Su,Junjie Hou,Guang Li*

Main category: cs.CV

TL;DR: DC3 introduces a dataset condensation framework with color compensation, improving color diversity to enhance performance and fidelity in condensed datasets.


<details>
  <summary>Details</summary>
Motivation: Existing dataset condensation methods struggle with inefficiency or semantic distortion, overlooking color's dual role as an information carrier and semantic unit.

Method: DC3 uses a calibrated selection strategy and a latent diffusion model to enhance color diversity in condensed images.

Result: DC3 outperforms SOTA methods, demonstrating superior performance and generalization without model collapse.

Conclusion: DC3 is the first to fine-tune pre-trained diffusion models with condensed datasets, proving feasibility and high quality in downstream tasks.

Abstract: Dataset condensation always faces a constitutive trade-off: balancing
performance and fidelity under extreme compression. Existing methods struggle
with two bottlenecks: image-level selection methods (Coreset Selection, Dataset
Quantization) suffer from inefficiency condensation, while pixel-level
optimization (Dataset Distillation) introduces semantic distortion due to
over-parameterization. With empirical observations, we find that a critical
problem in dataset condensation is the oversight of color's dual role as an
information carrier and a basic semantic representation unit. We argue that
improving the colorfulness of condensed images is beneficial for representation
learning. Motivated by this, we propose DC3: a Dataset Condensation framework
with Color Compensation. After a calibrated selection strategy, DC3 utilizes
the latent diffusion model to enhance the color diversity of an image rather
than creating a brand-new one. Extensive experiments demonstrate the superior
performance and generalization of DC3 that outperforms SOTA methods across
multiple benchmarks. To the best of our knowledge, besides focusing on
downstream tasks, DC3 is the first research to fine-tune pre-trained diffusion
models with condensed datasets. The FID results prove that training networks
with our high-quality datasets is feasible without model collapse or other
degradation issues. Code and generated data will be released soon.

</details>


### [121] [OpenGS-Fusion: Open-Vocabulary Dense Mapping with Hybrid 3D Gaussian Splatting for Refined Object-Level Understanding](https://arxiv.org/abs/2508.01150)
*Dianyi Yang,Xihan Wang,Yu Gao,Shiyang Liu,Bohan Ren,Yufeng Yue,Yi Yang*

Main category: cs.CV

TL;DR: OpenGS-Fusion is a new framework for open-vocabulary 3D scene understanding, combining 3D Gaussian representation and adaptive thresholding to improve semantic modeling and object-level segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D scene understanding lack flexibility and precision in handling open-vocabulary queries, limiting their effectiveness in VR/AR and robotics.

Method: The framework integrates 3D Gaussian representation with Truncated Signed Distance Field for semantic feature fusion and introduces MLLM-Assisted Adaptive Thresholding for refined object segmentation.

Result: OpenGS-Fusion achieves a 17% improvement in 3D mIoU over fixed threshold methods and excels in 3D object understanding and scene reconstruction.

Conclusion: The proposed method advances open-vocabulary 3D scene interaction, offering superior performance and adaptability for real-world applications.

Abstract: Recent advancements in 3D scene understanding have made significant strides
in enabling interaction with scenes using open-vocabulary queries, particularly
for VR/AR and robotic applications. Nevertheless, existing methods are hindered
by rigid offline pipelines and the inability to provide precise 3D object-level
understanding given open-ended queries. In this paper, we present
OpenGS-Fusion, an innovative open-vocabulary dense mapping framework that
improves semantic modeling and refines object-level understanding.
OpenGS-Fusion combines 3D Gaussian representation with a Truncated Signed
Distance Field to facilitate lossless fusion of semantic features on-the-fly.
Furthermore, we introduce a novel multimodal language-guided approach named
MLLM-Assisted Adaptive Thresholding, which refines the segmentation of 3D
objects by adaptively adjusting similarity thresholds, achieving an improvement
17\% in 3D mIoU compared to the fixed threshold strategy. Extensive experiments
demonstrate that our method outperforms existing methods in 3D object
understanding and scene reconstruction quality, as well as showcasing its
effectiveness in language-guided scene interaction. The code is available at
https://young-bit.github.io/opengs-fusion.github.io/ .

</details>


### [122] [Personalized Safety Alignment for Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.01151)
*Yu Lei,Jinbin Bai,Qingyu Shi,Aosong Feng,Kaidong Yu*

Main category: cs.CV

TL;DR: Proposes Personalized Safety Alignment (PSA) for user-specific safety control in text-to-image diffusion models, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current safety mechanisms in text-to-image models lack personalization, ignoring diverse user safety boundaries.

Method: Introduces PSA, integrating personalized user profiles via cross-attention to adjust model behavior.

Result: PSA outperforms in harmful content suppression and aligns better with user constraints, achieving higher Win Rate and Pass Rate.

Conclusion: PSA effectively personalizes safety in generative models while maintaining image quality, with code and data publicly available.

Abstract: Text-to-image diffusion models have revolutionized visual content generation,
but current safety mechanisms apply uniform standards that often fail to
account for individual user preferences. These models overlook the diverse
safety boundaries shaped by factors like age, mental health, and personal
beliefs. To address this, we propose Personalized Safety Alignment (PSA), a
framework that allows user-specific control over safety behaviors in generative
models. PSA integrates personalized user profiles into the diffusion process,
adjusting the model's behavior to match individual safety preferences while
preserving image quality. We introduce a new dataset, Sage, which captures
user-specific safety preferences and incorporates these profiles through a
cross-attention mechanism. Experiments show that PSA outperforms existing
methods in harmful content suppression and aligns generated content better with
user constraints, achieving higher Win Rate and Pass Rate scores. Our code,
data, and models are publicly available at
https://torpedo2648.github.io/PSAlign/.

</details>


### [123] [LawDIS: Language-Window-based Controllable Dichotomous Image Segmentation](https://arxiv.org/abs/2508.01152)
*Xinyu Yan,Meijun Sun,Ge-Peng Ji,Fahad Shahbaz Khan,Salman Khan,Deng-Ping Fan*

Main category: cs.CV

TL;DR: LawDIS is a framework for dichotomous image segmentation (DIS) using language and window controls for high-quality mask generation, outperforming 11 state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To improve DIS by integrating user controls (language prompts and adjustable windows) for personalized and high-accuracy applications.

Method: Uses a latent diffusion model with macro (language-controlled segmentation) and micro (window-controlled refinement) modes, coordinated by a mode switcher.

Result: Outperforms 11 cutting-edge methods, achieving significant gains in metrics like Fβω (4.6% with LS+WR, 3.6% with LS alone).

Conclusion: LawDIS is effective for high-quality DIS, offering flexible user controls and superior performance.

Abstract: We present LawDIS, a language-window-based controllable dichotomous image
segmentation (DIS) framework that produces high-quality object masks. Our
framework recasts DIS as an image-conditioned mask generation task within a
latent diffusion model, enabling seamless integration of user controls. LawDIS
is enhanced with macro-to-micro control modes. Specifically, in macro mode, we
introduce a language-controlled segmentation strategy (LS) to generate an
initial mask based on user-provided language prompts. In micro mode, a
window-controlled refinement strategy (WR) allows flexible refinement of
user-defined regions (i.e., size-adjustable windows) within the initial mask.
Coordinated by a mode switcher, these modes can operate independently or
jointly, making the framework well-suited for high-accuracy, personalised
applications. Extensive experiments on the DIS5K benchmark reveal that our
LawDIS significantly outperforms 11 cutting-edge methods across all metrics.
Notably, compared to the second-best model MVANet, we achieve $F_\beta^\omega$
gains of 4.6\% with both the LS and WR strategies and 3.6\% gains with only the
LS strategy on DIS-TE. Codes will be made available at
https://github.com/XinyuYanTJU/LawDIS.

</details>


### [124] [TEACH: Text Encoding as Curriculum Hints for Scene Text Recognition](https://arxiv.org/abs/2508.01153)
*Xiahan Yang,Hui Zheng*

Main category: cs.CV

TL;DR: TEACH is a novel training paradigm for STR that uses ground-truth text as auxiliary input, gradually reducing its influence to guide the model from label-dependent to visual recognition.


<details>
  <summary>Details</summary>
Motivation: STR is challenging due to complex visuals and limited semantic priors. TEACH aims to improve accuracy without external pretraining or inference overhead.

Method: TEACH encodes target labels into embeddings and uses loss-aware masking to simulate curriculum learning, integrating into encoder-decoder frameworks.

Result: Experiments show TEACH improves accuracy across benchmarks, especially in challenging conditions, proving its robustness.

Conclusion: TEACH is a model-agnostic, effective solution for STR, enhancing performance without added complexity.

Abstract: Scene Text Recognition (STR) remains a challenging task due to complex visual
appearances and limited semantic priors. We propose TEACH, a novel training
paradigm that injects ground-truth text into the model as auxiliary input and
progressively reduces its influence during training. By encoding target labels
into the embedding space and applying loss-aware masking, TEACH simulates a
curriculum learning process that guides the model from label-dependent learning
to fully visual recognition. Unlike language model-based approaches, TEACH
requires no external pretraining and introduces no inference overhead. It is
model-agnostic and can be seamlessly integrated into existing encoder-decoder
frameworks. Extensive experiments across multiple public benchmarks show that
models trained with TEACH achieve consistently improved accuracy, especially
under challenging conditions, validating its robustness and general
applicability.

</details>


### [125] [DELTAv2: Accelerating Dense 3D Tracking](https://arxiv.org/abs/2508.01170)
*Tuan Duc Ngo,Ashkan Mirzaei,Guocheng Qian,Hanwen Liang,Chuang Gan,Evangelos Kalogerakis,Peter Wonka,Chaoyang Wang*

Main category: cs.CV

TL;DR: A novel algorithm for faster 3D point tracking in videos, addressing computational bottlenecks with a coarse-to-fine strategy and optimized feature computation, achieving 5-100x speedup without losing accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for dense long-term 3D point tracking are computationally expensive, especially with large numbers of trajectories and costly correlation feature computations.

Method: Introduces a coarse-to-fine tracking strategy and a learnable interpolation module, alongside optimizations for correlation feature computation.

Result: Achieves a 5-100x speedup over prior methods while maintaining state-of-the-art tracking accuracy.

Conclusion: The proposed algorithm effectively addresses computational bottlenecks, enabling faster and accurate 3D point tracking.

Abstract: We propose a novel algorithm for accelerating dense long-term 3D point
tracking in videos. Through analysis of existing state-of-the-art methods, we
identify two major computational bottlenecks. First, transformer-based
iterative tracking becomes expensive when handling a large number of
trajectories. To address this, we introduce a coarse-to-fine strategy that
begins tracking with a small subset of points and progressively expands the set
of tracked trajectories. The newly added trajectories are initialized using a
learnable interpolation module, which is trained end-to-end alongside the
tracking network. Second, we propose an optimization that significantly reduces
the cost of correlation feature computation, another key bottleneck in prior
methods. Together, these improvements lead to a 5-100x speedup over existing
approaches while maintaining state-of-the-art tracking accuracy.

</details>


### [126] [No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views](https://arxiv.org/abs/2508.01171)
*Ranran Huang,Krystian Mikolajczyk*

Main category: cs.CV

TL;DR: SPFSplat is a pose-free framework for 3D Gaussian splatting from sparse multi-view images, achieving state-of-the-art novel view synthesis and pose estimation without ground-truth poses.


<details>
  <summary>Details</summary>
Motivation: To enable 3D Gaussian splatting from unposed multi-view images without requiring ground-truth poses, addressing practical limitations in pose supervision.

Method: Uses a shared feature extraction backbone to predict 3D Gaussian primitives and camera poses in a canonical space in one feed-forward step, with rendering and reprojection losses for geometric constraints.

Result: Achieves state-of-the-art performance in novel view synthesis and surpasses methods with geometry priors in pose estimation, even with significant viewpoint changes and limited image overlap.

Conclusion: SPFSplat's pose-free training and efficient design make it highly practical for real-world applications, outperforming existing methods without pose supervision.

Abstract: We introduce SPFSplat, an efficient framework for 3D Gaussian splatting from
sparse multi-view images, requiring no ground-truth poses during training or
inference. It employs a shared feature extraction backbone, enabling
simultaneous prediction of 3D Gaussian primitives and camera poses in a
canonical space from unposed inputs within a single feed-forward step.
Alongside the rendering loss based on estimated novel-view poses, a
reprojection loss is integrated to enforce the learning of pixel-aligned
Gaussian primitives for enhanced geometric constraints. This pose-free training
paradigm and efficient one-step feed-forward design make SPFSplat well-suited
for practical applications. Remarkably, despite the absence of pose
supervision, SPFSplat achieves state-of-the-art performance in novel view
synthesis even under significant viewpoint changes and limited image overlap.
It also surpasses recent methods trained with geometry priors in relative pose
estimation. Code and trained models are available on our project page:
https://ranrhuang.github.io/spfsplat/.

</details>


### [127] [Object Affordance Recognition and Grounding via Multi-scale Cross-modal Representation Learning](https://arxiv.org/abs/2508.01184)
*Xinhang Wan,Dongqiang Gou,Xinwang Liu,En Zhu,Xuming He*

Main category: cs.CV

TL;DR: The paper proposes a method for 3D affordance grounding and classification in Embodied AI, addressing inconsistencies and scale issues by learning an affordance-aware 3D representation and using a stage-wise inference strategy.


<details>
  <summary>Details</summary>
Motivation: To improve object manipulation learning in Embodied AI by addressing the limitations of previous methods, which tackled affordance grounding and classification separately and struggled with incomplete or fixed-scale predictions.

Method: Develops a cross-modal 3D representation with efficient fusion and multi-scale geometric feature propagation, followed by a two-stage prediction mechanism to couple grounding and classification tasks.

Result: The method shows improved performance in both affordance grounding and classification, demonstrating its effectiveness.

Conclusion: The proposed approach successfully addresses the dependency and scale issues in affordance understanding, offering a more robust solution for Embodied AI.

Abstract: A core problem of Embodied AI is to learn object manipulation from
observation, as humans do. To achieve this, it is important to localize 3D
object affordance areas through observation such as images (3D affordance
grounding) and understand their functionalities (affordance classification).
Previous attempts usually tackle these two tasks separately, leading to
inconsistent predictions due to lacking proper modeling of their dependency. In
addition, these methods typically only ground the incomplete affordance areas
depicted in images, failing to predict the full potential affordance areas, and
operate at a fixed scale, resulting in difficulty in coping with affordances
significantly varying in scale with respect to the whole object. To address
these issues, we propose a novel approach that learns an affordance-aware 3D
representation and employs a stage-wise inference strategy leveraging the
dependency between grounding and classification tasks. Specifically, we first
develop a cross-modal 3D representation through efficient fusion and
multi-scale geometric feature propagation, enabling inference of full potential
affordance areas at a suitable regional scale. Moreover, we adopt a simple
two-stage prediction mechanism, effectively coupling grounding and
classification for better affordance understanding. Experiments demonstrate the
effectiveness of our method, showing improved performance in both affordance
grounding and classification.

</details>


### [128] [A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding](https://arxiv.org/abs/2508.01197)
*Zhan Shi,Song Wang,Junbo Chen,Jianke Zhu*

Main category: cs.CV

TL;DR: A benchmark for 3D occupancy grounding is introduced to address the limitations of bounding boxes in visual grounding, with a proposed model, GroundingOcc, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing visual grounding tasks rely on bounding boxes, which lack fine-grained details and accuracy in object representation.

Method: Proposes GroundingOcc, an end-to-end model combining visual, textual, and point cloud features for voxel-level occupancy grounding, enhanced by 2D grounding and depth estimation modules.

Result: GroundingOcc outperforms existing baselines on 3D occupancy grounding, demonstrating improved precision.

Conclusion: The benchmark and model provide a more accurate solution for 3D occupancy grounding in outdoor scenes, advancing spatially aware perception.

Abstract: Visual grounding aims to identify objects or regions in a scene based on
natural language descriptions, essential for spatially aware perception in
autonomous driving. However, existing visual grounding tasks typically depend
on bounding boxes that often fail to capture fine-grained details. Not all
voxels within a bounding box are occupied, resulting in inaccurate object
representations. To address this, we introduce a benchmark for 3D occupancy
grounding in challenging outdoor scenes. Built on the nuScenes dataset, it
integrates natural language with voxel-level occupancy annotations, offering
more precise object perception compared to the traditional grounding task.
Moreover, we propose GroundingOcc, an end-to-end model designed for 3D
occupancy grounding through multi-modal learning. It combines visual, textual,
and point cloud features to predict object location and occupancy information
from coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder
for feature extraction, an occupancy head for voxel-wise predictions, and a
grounding head to refine localization. Additionally, a 2D grounding module and
a depth estimation module enhance geometric understanding, thereby boosting
model performance. Extensive experiments on the benchmark demonstrate that our
method outperforms existing baselines on 3D occupancy grounding. The dataset is
available at https://github.com/RONINGOD/GroundingOcc.

</details>


### [129] [Deep Learning for Pavement Condition Evaluation Using Satellite Imagery](https://arxiv.org/abs/2508.01206)
*Prathyush Kumar Reddy Lebaku,Lu Gao,Pan Lu,Jingran Sun*

Main category: cs.CV

TL;DR: The paper explores using deep learning on satellite images for cost-effective pavement condition evaluation, achieving over 90% accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional infrastructure inspection methods are labor-intensive and time-consuming; satellite technology offers a more efficient alternative.

Method: Deep learning models were applied to analyze over 3,000 satellite images of pavements, paired with evaluation ratings from TxDOT's PMIS database.

Result: The study achieved an accuracy rate exceeding 90% in evaluating pavement conditions.

Conclusion: This research demonstrates a rapid, cost-effective method for pavement network evaluation using satellite imagery and deep learning.

Abstract: Civil infrastructure systems covers large land areas and needs frequent
inspections to maintain their public service capabilities. The conventional
approaches of manual surveys or vehicle-based automated surveys to assess
infrastructure conditions are often labor-intensive and time-consuming. For
this reason, it is worthwhile to explore more cost-effective methods for
monitoring and maintaining these infrastructures. Fortunately, recent
advancements in satellite systems and image processing algorithms have opened
up new possibilities. Numerous satellite systems have been employed to monitor
infrastructure conditions and identify damages. Due to the improvement in
ground sample distance (GSD), the level of detail that can be captured has
significantly increased. Taking advantage of these technology advancement, this
research investigated to evaluate pavement conditions using deep learning
models for analyzing satellite images. We gathered over 3,000 satellite images
of pavement sections, together with pavement evaluation ratings from TxDOT's
PMIS database. The results of our study show an accuracy rate is exceeding 90%.
This research paves the way for a rapid and cost-effective approach to
evaluating the pavement network in the future.

</details>


### [130] [RoadMamba: A Dual Branch Visual State Space Model for Road Surface Classification](https://arxiv.org/abs/2508.01210)
*Tianze Wang,Zhang Zhang,Chao Yue,Nuoran Li,Chao Sun*

Main category: cs.CV

TL;DR: RoadMamba, a novel method combining local and global perception, achieves state-of-the-art performance in road surface classification using visual Mamba architectures.


<details>
  <summary>Details</summary>
Motivation: Improving autonomous vehicle safety and comfort by accurately classifying road surface conditions using visual technologies.

Method: Proposes RoadMamba, utilizing Dual State Space Model (DualSSM) for global and local feature extraction and Dual Attention Fusion (DAF) for feature decoding. Includes a dual auxiliary loss to balance feature reliance.

Result: Achieves state-of-the-art performance on a dataset with 1 million samples.

Conclusion: RoadMamba effectively combines local and global perception for superior road surface classification, enhancing autonomous vehicle systems.

Abstract: Acquiring the road surface conditions in advance based on visual technologies
provides effective information for the planning and control system of
autonomous vehicles, thus improving the safety and driving comfort of the
vehicles. Recently, the Mamba architecture based on state-space models has
shown remarkable performance in visual processing tasks, benefiting from the
efficient global receptive field. However, existing Mamba architectures
struggle to achieve state-of-the-art visual road surface classification due to
their lack of effective extraction of the local texture of the road surface. In
this paper, we explore for the first time the potential of visual Mamba
architectures for road surface classification task and propose a method that
effectively combines local and global perception, called RoadMamba.
Specifically, we utilize the Dual State Space Model (DualSSM) to effectively
extract the global semantics and local texture of the road surface and decode
and fuse the dual features through the Dual Attention Fusion (DAF). In
addition, we propose a dual auxiliary loss to explicitly constrain dual
branches, preventing the network from relying only on global semantic
information from the deep large receptive field and ignoring the local texture.
The proposed RoadMamba achieves the state-of-the-art performance in experiments
on a large-scale road surface classification dataset containing 1 million
samples.

</details>


### [131] [StyDeco: Unsupervised Style Transfer with Distilling Priors and Semantic Decoupling](https://arxiv.org/abs/2508.01215)
*Yuanlin Yang,Quanjian Song,Zhexian Gao,Ge Wang,Shanshan Li,Xiaoyan Zhang*

Main category: cs.CV

TL;DR: StyDeco is an unsupervised framework for style transfer that addresses the semantic gap in text-driven diffusion models by learning tailored text representations, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitation of text-driven diffusion models in style transfer, which treat textual descriptions uniformly, leading to loss of semantic structure and fine-grained details.

Method: StyDeco uses Prior-Guided Data Distillation (PGD) to distill stylistic knowledge without supervision and Contrastive Semantic Decoupling (CSD) to adapt text encoders for style transfer.

Result: Experiments on three benchmarks show StyDeco outperforms existing methods in stylistic fidelity and structural preservation, and supports de-stylization.

Conclusion: StyDeco effectively bridges the semantic gap in text-driven style transfer and demonstrates extensibility through de-stylization.

Abstract: Diffusion models have emerged as the dominant paradigm for style transfer,
but their text-driven mechanism is hindered by a core limitation: it treats
textual descriptions as uniform, monolithic guidance. This limitation overlooks
the semantic gap between the non-spatial nature of textual descriptions and the
spatially-aware attributes of visual style, often leading to the loss of
semantic structure and fine-grained details during stylization. In this paper,
we propose StyDeco, an unsupervised framework that resolves this limitation by
learning text representations specifically tailored for the style transfer
task. Our framework first employs Prior-Guided Data Distillation (PGD), a
strategy designed to distill stylistic knowledge without human supervision. It
leverages a powerful frozen generative model to automatically synthesize
pseudo-paired data. Subsequently, we introduce Contrastive Semantic Decoupling
(CSD), a task-specific objective that adapts a text encoder using
domain-specific weights. CSD performs a two-class clustering in the semantic
space, encouraging source and target representations to form distinct clusters.
Extensive experiments on three classic benchmarks demonstrate that our
framework outperforms several existing approaches in both stylistic fidelity
and structural preservation, highlighting its effectiveness in style transfer
with semantic preservation. In addition, our framework supports a unique
de-stylization process, further demonstrating its extensibility. Our code is
vailable at https://github.com/QuanjianSong/StyDeco.

</details>


### [132] [Perspective from a Broader Context: Can Room Style Knowledge Help Visual Floorplan Localization?](https://arxiv.org/abs/2508.01216)
*Bolei Chen,Shengsheng Yan,Yongzheng Cui,Jiaxu Kang,Ping Zhong,Jianxin Wang*

Main category: cs.CV

TL;DR: The paper proposes using visual scene context to improve floorplan localization (FLoc) by pre-training a room discriminator with unsupervised learning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Floorplans have repetitive structures causing ambiguous localization; existing methods ignore richer visual context.

Method: Unsupervised learning with clustering constraints to pre-train a room discriminator, injecting scene context into FLoc.

Result: Outperforms state-of-the-art methods, improving robustness and accuracy on benchmarks.

Conclusion: Scene context enhances FLoc, with the proposed method achieving superior performance.

Abstract: Since a building's floorplan remains consistent over time and is inherently
robust to changes in visual appearance, visual Floorplan Localization (FLoc)
has received increasing attention from researchers. However, as a compact and
minimalist representation of the building's layout, floorplans contain many
repetitive structures (e.g., hallways and corners), thus easily result in
ambiguous localization. Existing methods either pin their hopes on matching 2D
structural cues in floorplans or rely on 3D geometry-constrained visual
pre-trainings, ignoring the richer contextual information provided by visual
images. In this paper, we suggest using broader visual scene context to empower
FLoc algorithms with scene layout priors to eliminate localization uncertainty.
In particular, we propose an unsupervised learning technique with clustering
constraints to pre-train a room discriminator on self-collected unlabeled room
images. Such a discriminator can empirically extract the hidden room type of
the observed image and distinguish it from other room types. By injecting the
scene context information summarized by the discriminator into an FLoc
algorithm, the room style knowledge is effectively exploited to guide definite
visual FLoc. We conducted sufficient comparative studies on two standard visual
Floc benchmarks. Our experiments show that our approach outperforms
state-of-the-art methods and achieves significant improvements in robustness
and accuracy.

</details>


### [133] [MoGaFace: Momentum-Guided and Texture-Aware Gaussian Avatars for Consistent Facial Geometry](https://arxiv.org/abs/2508.01218)
*Yujian Liu,Linlang Cao,Chuang Chen,Fanyu Geng,Dongxu Shen,Peng Cao,Shidang Xu,Xiaoli Liu*

Main category: cs.CV

TL;DR: MoGaFace improves 3D head avatar reconstruction by refining geometry and texture during Gaussian rendering, addressing misalignment issues with a momentum-guided module and latent texture attention.


<details>
  <summary>Details</summary>
Motivation: Existing methods suffer from misalignment between estimated FLAME meshes and target images, leading to poor rendering quality and loss of details.

Method: MoGaFace introduces a Momentum-Guided Consistent Geometry module for alignment and Latent Texture Attention for texture refinement.

Result: The framework achieves high-fidelity reconstruction and better novel-view synthesis, even with inaccurate mesh initialization.

Conclusion: MoGaFace outperforms existing methods by ensuring consistent geometry and refined textures, enhancing avatar realism.

Abstract: Existing 3D head avatar reconstruction methods adopt a two-stage process,
relying on tracked FLAME meshes derived from facial landmarks, followed by
Gaussian-based rendering. However, misalignment between the estimated mesh and
target images often leads to suboptimal rendering quality and loss of fine
visual details. In this paper, we present MoGaFace, a novel 3D head avatar
modeling framework that continuously refines facial geometry and texture
attributes throughout the Gaussian rendering process. To address the
misalignment between estimated FLAME meshes and target images, we introduce the
Momentum-Guided Consistent Geometry module, which incorporates a
momentum-updated expression bank and an expression-aware correction mechanism
to ensure temporal and multi-view consistency. Additionally, we propose Latent
Texture Attention, which encodes compact multi-view features into head-aware
representations, enabling geometry-aware texture refinement via integration
into Gaussians. Extensive experiments show that MoGaFace achieves high-fidelity
head avatar reconstruction and significantly improves novel-view synthesis
quality, even under inaccurate mesh initialization and unconstrained real-world
settings.

</details>


### [134] [Eigen Neural Network: Unlocking Generalizable Vision with Eigenbasis](https://arxiv.org/abs/2508.01219)
*Anzhe Cheng,Chenzhong Yin,Mingxi Cheng,Shukai Duan,Shahin Nazarian,Paul Bogdan*

Main category: cs.CV

TL;DR: ENN introduces a novel architecture using orthonormal eigenbasis to improve weight structure, outperforming SOTA methods in image classification and cross-modal retrieval, while enabling efficient BP-free training.


<details>
  <summary>Details</summary>
Motivation: Address the disordered weight structures in DNNs that harm feature clarity and learning dynamics.

Method: Reparameterizes each layer's weights in a learned orthonormal eigenbasis, enforcing decorrelated weight dynamics.

Result: Outperforms SOTA on ImageNet and sets a new benchmark in cross-modal retrieval; BP-free variant achieves 2x speedup and higher accuracy.

Conclusion: ENN remedies representational flaws in BP, enhancing performance and enabling efficient parallel training.

Abstract: The remarkable success of Deep Neural Networks(DNN) is driven by
gradient-based optimization, yet this process is often undermined by its
tendency to produce disordered weight structures, which harms feature clarity
and degrades learning dynamics. To address this fundamental representational
flaw, we introduced the Eigen Neural Network (ENN), a novel architecture that
reparameterizes each layer's weights in a layer-shared, learned orthonormal
eigenbasis. This design enforces decorrelated, well-aligned weight dynamics
axiomatically, rather than through regularization, leading to more structured
and discriminative feature representations. When integrated with standard BP,
ENN consistently outperforms state-of-the-art methods on large-scale image
classification benchmarks, including ImageNet, and its superior representations
generalize to set a new benchmark in cross-modal image-text retrieval.
Furthermore, ENN's principled structure enables a highly efficient,
backpropagation-free(BP-free) local learning variant, ENN-$\ell$. This variant
not only resolves BP's procedural bottlenecks to achieve over 2$\times$
training speedup via parallelism, but also, remarkably, surpasses the accuracy
of end-to-end backpropagation. ENN thus presents a new architectural paradigm
that directly remedies the representational deficiencies of BP, leading to
enhanced performance and enabling a more efficient, parallelizable training
regime.

</details>


### [135] [ParaRevSNN: A Parallel Reversible Spiking Neural Network for Efficient Training and Inference](https://arxiv.org/abs/2508.01223)
*Changqing Xu,Guoqing Sun,Yi Liu,Xinfang Liao,Yintang Yang*

Main category: cs.CV

TL;DR: ParaRevSNN introduces parallel reversible Spiking Neural Networks to reduce latency while maintaining memory efficiency, achieving faster training and inference without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: RevSNNs suffer from high latency due to sequential computation, limiting their practical use. ParaRevSNN aims to address this by enabling parallelism.

Method: ParaRevSNN decouples sequential dependencies between reversible blocks, allowing inter-block parallelism while preserving reversibility.

Result: Experiments show ParaRevSNN matches or exceeds RevSNN accuracy, reducing training time by 35.2% and inference time to 18.15%.

Conclusion: ParaRevSNN is efficient for resource-constrained scenarios, combining speed and memory benefits.

Abstract: Reversible Spiking Neural Networks (RevSNNs) enable memory-efficient training
by reconstructing forward activations during backpropagation, but suffer from
high latency due to strictly sequential computation. To overcome this
limitation, we propose ParaRevSNN, a parallel reversible SNN architecture that
decouples sequential dependencies between reversible blocks while preserving
reversibility. This design enables inter-block parallelism, significantly
accelerating training and inference while retaining the memory-saving benefits
of reversibility. Experiments on CIFAR10, CIFAR100, CIFAR10-DVS, and DVS128
Gesture demonstrate that ParaRevSNN matches or exceeds the accuracy of standard
RevSNNs, while reducing training time by up to 35.2\% and inference time to
18.15\%, making it well-suited for deployment in resource-constrained
scenarios.

</details>


### [136] [Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of Vision-Language Models](https://arxiv.org/abs/2508.01225)
*Xinyu Chen,Haotian Zhai,Can Zhang,Xiupeng Shi,Ruirui Li*

Main category: cs.CV

TL;DR: The paper proposes MCP and MCP++, multi-cache enhanced prototype-based test-time adaptation methods, improving generalization by ensuring intra-class compactness and leveraging cross-modal alignment.


<details>
  <summary>Details</summary>
Motivation: Existing cache-enhanced TTA methods rely on unreliable low-entropy samples for prototype construction, failing to ensure intra-class compactness under distribution shifts.

Method: Introduces MCP with three caches (entropy, align, negative) and MCP++ with cross-modal prototype alignment and residual learning.

Result: Achieves state-of-the-art performance across 15 downstream tasks.

Conclusion: The proposed methods enhance TTA by addressing intra-class compactness and leveraging multi-cache and cross-modal alignment.

Abstract: In zero-shot setting, test-time adaptation adjusts pre-trained models using
unlabeled data from the test phase to enhance performance on unknown test
distributions. Existing cache-enhanced TTA methods rely on a low-entropy
criterion to select samples for prototype construction, assuming intra-class
compactness. However, low-entropy samples may be unreliable under distribution
shifts, and the resulting prototypes may not ensure compact intra-class
distributions. This study identifies a positive correlation between
cache-enhanced performance and intra-class compactness. Based on this
observation, we propose a Multi-Cache enhanced Prototype-based Test-Time
Adaptation (MCP) featuring three caches: an entropy cache for initializing
prototype representations with low-entropy samples, an align cache for
integrating visual and textual information to achieve compact intra-class
distributions, and a negative cache for prediction calibration using
high-entropy samples. We further developed MCP++, a framework incorporating
cross-modal prototype alignment and residual learning, introducing prototype
residual fine-tuning. Comparative and ablation experiments across 15 downstream
tasks demonstrate that the proposed method and framework achieve
state-of-the-art generalization performance.

</details>


### [137] [Enhancing Multi-view Open-set Learning via Ambiguity Uncertainty Calibration and View-wise Debiasing](https://arxiv.org/abs/2508.01227)
*Zihan Fang,Zhiyong Xu,Lan Du,Shide Du,Zhiling Cai,Shiping Wang*

Main category: cs.CV

TL;DR: A multi-view open-set learning framework addresses class completeness and view-induced biases via ambiguity uncertainty calibration and view-wise debiasing, improving unknown-class recognition.


<details>
  <summary>Details</summary>
Motivation: Existing multi-view learning models fail in open-set scenarios due to class completeness assumptions and static view-induced biases.

Method: Proposes O-Mix for generating ambiguous samples and an ambiguity perception network, plus an HSIC-based contrastive debiasing module.

Result: Enhances unknown-class recognition while maintaining closed-set performance on diverse benchmarks.

Conclusion: The framework effectively tackles open-set challenges in multi-view learning.

Abstract: Existing multi-view learning models struggle in open-set scenarios due to
their implicit assumption of class completeness. Moreover, static view-induced
biases, which arise from spurious view-label associations formed during
training, further degrade their ability to recognize unknown categories. In
this paper, we propose a multi-view open-set learning framework via ambiguity
uncertainty calibration and view-wise debiasing. To simulate ambiguous samples,
we design O-Mix, a novel synthesis strategy to generate virtual samples with
calibrated open-set ambiguity uncertainty. These samples are further processed
by an auxiliary ambiguity perception network that captures atypical patterns
for improved open-set adaptation. Furthermore, we incorporate an HSIC-based
contrastive debiasing module that enforces independence between view-specific
ambiguous and view-consistent representations, encouraging the model to learn
generalizable features. Extensive experiments on diverse multi-view benchmarks
demonstrate that the proposed framework consistently enhances unknown-class
recognition while preserving strong closed-set performance.

</details>


### [138] [Mitigating Information Loss under High Pruning Rates for Efficient Large Vision Language Models](https://arxiv.org/abs/2508.01236)
*Mingyu Fu,Wei Suo,Ji Ma,Lin Yuanbo Wu,Peng Wang,Yanning Zhang*

Main category: cs.CV

TL;DR: ACCM reduces computational costs in LVLMs by compensating for visual information loss with adaptive captions, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: High computational costs of LVLMs due to long visual sequences limit their applications, and existing pruning methods degrade performance.

Method: ACCM uses a lightweight caption model and selector to generate and choose contextually appropriate captions, trained via self-supervised learning.

Result: ACCM surpasses SOTA by 20.6% with 6.5% fewer FLOPs across seven benchmarks.

Conclusion: ACCM effectively mitigates visual information loss and reduces computational costs, enhancing LVLM efficiency.

Abstract: Despite the great success of Large Vision Language Models (LVLMs), their high
computational cost severely limits their broad applications. The computational
cost of LVLMs mainly stems from the visual sequence of the input, which
consists of hundreds or even thousands of tokens. Although existing methods
have made progress by removing redundant tokens, they suffer from severe
performance degradation with high pruning rates due to the loss of visual
information. In this paper, we propose an Adaptive Content Compensation Method
(ACCM), which can effectively mitigate the visual information loss via an image
caption. Specifically, ACCM comprises two key components: a lightweight caption
model and a selector. Firstly the caption model generates question-related
descriptions under the guidance of the user instruction. Then the selector
further identifies a contextually appropriate caption from multiple candidates.
Leveraging self-supervised learning, our modules could be learned efficiently
without any human or automated labeling. We conduct extensive experiments
across seven benchmarks and the results show that ACCM significantly
outperforms existing methods with lower FLOPs (e.g., surpassing SOTA by 20.6%
with 6.5% fewer FLOPs).

</details>


### [139] [OCSplats: Observation Completeness Quantification and Label Noise Separation in 3DGS](https://arxiv.org/abs/2508.01239)
*Han Ling,Xian Xu,Yinghui Sun,Quansen Sun*

Main category: cs.CV

TL;DR: OCSplats improves 3DGS anti-noise reconstruction by leveraging epistemic uncertainty, hybrid noise assessment, and dynamic anchor points, achieving robust performance across diverse scenarios without parameter tuning.


<details>
  <summary>Details</summary>
Motivation: Label noise in real-world 3D reconstruction (e.g., moving objects, non-Lambertian surfaces) causes errors, and existing methods lack adaptability or require fine-tuning.

Method: Proposes OCSplats, combining hybrid noise assessment, observation-based cognitive correction, and dynamic anchor points for noise classification.

Result: OCSplats achieves leading reconstruction accuracy and precise noise classification in varied scenarios without parameter adjustments.

Conclusion: OCSplats offers a practical, adaptable solution for noise-resistant 3DGS reconstruction, outperforming existing methods.

Abstract: 3D Gaussian Splatting (3DGS) has become one of the most promising 3D
reconstruction technologies. However, label noise in real-world scenarios-such
as moving objects, non-Lambertian surfaces, and shadows-often leads to
reconstruction errors. Existing 3DGS-Bsed anti-noise reconstruction methods
either fail to separate noise effectively or require scene-specific fine-tuning
of hyperparameters, making them difficult to apply in practice. This paper
re-examines the problem of anti-noise reconstruction from the perspective of
epistemic uncertainty, proposing a novel framework, OCSplats. By combining key
technologies such as hybrid noise assessment and observation-based cognitive
correction, the accuracy of noise classification in areas with cognitive
differences has been significantly improved. Moreover, to address the issue of
varying noise proportions in different scenarios, we have designed a label
noise classification pipeline based on dynamic anchor points. This pipeline
enables OCSplats to be applied simultaneously to scenarios with vastly
different noise proportions without adjusting parameters. Extensive experiments
demonstrate that OCSplats always achieve leading reconstruction performance and
precise label noise classification in scenes of different complexity levels.

</details>


### [140] [NS-Net: Decoupling CLIP Semantic Information through NULL-Space for Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2508.01248)
*Jiazhen Yan,Fan Wang,Weiwei Jiang,Ziqiang Li,Zhangjie Fu*

Main category: cs.CV

TL;DR: NS-Net improves AI-generated image detection by decoupling semantic info from CLIP features and using contrastive learning, achieving 7.4% better accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the failure of existing detectors to generalize to unknown generative models due to semantic alignment between real and fake images.

Method: Proposes NS-Net, leveraging NULL-Space projection and contrastive learning, with Patch Selection to mitigate semantic bias.

Result: Outperforms state-of-the-art methods by 7.4% on a benchmark of 40 generative models.

Conclusion: NS-Net demonstrates strong generalization across GAN- and diffusion-based techniques.

Abstract: The rapid progress of generative models, such as GANs and diffusion models,
has facilitated the creation of highly realistic images, raising growing
concerns over their misuse in security-sensitive domains. While existing
detectors perform well under known generative settings, they often fail to
generalize to unknown generative models, especially when semantic content
between real and fake images is closely aligned. In this paper, we revisit the
use of CLIP features for AI-generated image detection and uncover a critical
limitation: the high-level semantic information embedded in CLIP's visual
features hinders effective discrimination. To address this, we propose NS-Net,
a novel detection framework that leverages NULL-Space projection to decouple
semantic information from CLIP's visual features, followed by contrastive
learning to capture intrinsic distributional differences between real and
generated images. Furthermore, we design a Patch Selection strategy to preserve
fine-grained artifacts by mitigating semantic bias caused by global image
structures. Extensive experiments on an open-world benchmark comprising images
generated by 40 diverse generative models show that NS-Net outperforms existing
state-of-the-art methods, achieving a 7.4\% improvement in detection accuracy,
thereby demonstrating strong generalization across both GAN- and
diffusion-based image generation techniques.

</details>


### [141] [DisFaceRep: Representation Disentanglement for Co-occurring Facial Components in Weakly Supervised Face Parsing](https://arxiv.org/abs/2508.01250)
*Xiaoqin Wang,Xianxu Hou,Meidan Ding,Junliang Chen,Kaijun Deng,Jinheng Xie,Linlin Shen*

Main category: cs.CV

TL;DR: WSFP introduces weakly supervised face parsing to reduce annotation costs, using DisFaceRep to disentangle co-occurring facial components for improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing face parsing methods require expensive dense annotations. WSFP aims to achieve similar results with cheaper weak supervision like image-level labels and text descriptions.

Method: DisFaceRep disentangles facial components via explicit (co-occurring component disentanglement) and implicit (text-guided loss) mechanisms.

Result: DisFaceRep outperforms existing weakly supervised methods on datasets like CelebAMask-HQ, LaPa, and Helen.

Conclusion: WSFP is challenging but feasible with DisFaceRep, which effectively leverages weak supervision for accurate face parsing.

Abstract: Face parsing aims to segment facial images into key components such as eyes,
lips, and eyebrows. While existing methods rely on dense pixel-level
annotations, such annotations are expensive and labor-intensive to obtain. To
reduce annotation cost, we introduce Weakly Supervised Face Parsing (WSFP), a
new task setting that performs dense facial component segmentation using only
weak supervision, such as image-level labels and natural language descriptions.
WSFP introduces unique challenges due to the high co-occurrence and visual
similarity of facial components, which lead to ambiguous activations and
degraded parsing performance. To address this, we propose DisFaceRep, a
representation disentanglement framework designed to separate co-occurring
facial components through both explicit and implicit mechanisms. Specifically,
we introduce a co-occurring component disentanglement strategy to explicitly
reduce dataset-level bias, and a text-guided component disentanglement loss to
guide component separation using language supervision implicitly. Extensive
experiments on CelebAMask-HQ, LaPa, and Helen demonstrate the difficulty of
WSFP and the effectiveness of DisFaceRep, which significantly outperforms
existing weakly supervised semantic segmentation methods. The code will be
released at
\href{https://github.com/CVI-SZU/DisFaceRep}{\textcolor{cyan}{https://github.com/CVI-SZU/DisFaceRep}}.

</details>


### [142] [ODOV: Towards Open-Domain Open-Vocabulary Object Detection](https://arxiv.org/abs/2508.01253)
*Yupeng Zhang,Ruize Han,Fangnan Zhou,Song Wang,Wei Feng,Liang Wan*

Main category: cs.CV

TL;DR: The paper introduces Open-Domain Open-Vocabulary (ODOV) object detection, addressing domain and category shifts. It presents OD-LVIS, a benchmark with 46,949 images across 18 domains and 1,203 categories, and a novel method using large language models for domain-agnostic text prompts and domain-specific embeddings.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of object detection in real-world scenarios with domain and category shifts, ensuring adaptability.

Method: Leverages large language models for domain-agnostic text prompts and learns domain embeddings from images, integrating them for customized domain-specific category embeddings.

Result: The proposed method is validated through benchmark evaluations, showing its effectiveness and the utility of the OD-LVIS dataset.

Conclusion: The work successfully introduces ODOV detection, a new benchmark, and a superior method, demonstrating adaptability to real-world complexities.

Abstract: In this work, we handle a new problem of Open-Domain Open-Vocabulary (ODOV)
object detection, which considers the detection model's adaptability to the
real world including both domain and category shifts. For this problem, we
first construct a new benchmark OD-LVIS, which includes 46,949 images, covers
18 complex real-world domains and 1,203 categories, and provides a
comprehensive dataset for evaluating real-world object detection. Besides, we
develop a novel baseline method for ODOV detection.The proposed method first
leverages large language models to generate the domain-agnostic text prompts
for category embedding. It further learns the domain embedding from the given
image, which, during testing, can be integrated into the category embedding to
form the customized domain-specific category embedding for each test image. We
provide sufficient benchmark evaluations for the proposed ODOV detection task
and report the results, which verify the rationale of ODOV detection, the
usefulness of our benchmark, and the superiority of the proposed method.

</details>


### [143] [Self-Enhanced Image Clustering with Cross-Modal Semantic Consistency](https://arxiv.org/abs/2508.01254)
*Zihan Li,Wei Sun,Jing Hu,Jianhua Yin,Jianlong Wu,Liqiang Nie*

Main category: cs.CV

TL;DR: A self-enhanced framework for image clustering leverages cross-modal semantic consistency and fine-tuning to outperform existing methods by aligning pre-trained model representations with clustering tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods freeze pre-trained encoders, creating a mismatch between task-agnostic features and clustering demands, limiting performance.

Method: The framework uses cross-modal semantic consistency to align clustering heads with pre-trained semantics, followed by self-enhanced fine-tuning to optimize the encoder and clustering heads.

Result: Outperforms existing deep clustering methods on six datasets, with a ViT-B/32 model matching or surpassing larger ViT-L/14-based methods.

Conclusion: The proposed framework effectively bridges the gap between pre-trained models and clustering tasks, achieving superior performance.

Abstract: While large language-image pre-trained models like CLIP offer powerful
generic features for image clustering, existing methods typically freeze the
encoder. This creates a fundamental mismatch between the model's task-agnostic
representations and the demands of a specific clustering task, imposing a
ceiling on performance. To break this ceiling, we propose a self-enhanced
framework based on cross-modal semantic consistency for efficient image
clustering. Our framework first builds a strong foundation via Cross-Modal
Semantic Consistency and then specializes the encoder through Self-Enhancement.
In the first stage, we focus on Cross-Modal Semantic Consistency. By mining
consistency between generated image-text pairs at the instance, cluster
assignment, and cluster center levels, we train lightweight clustering heads to
align with the rich semantics of the pre-trained model. This alignment process
is bolstered by a novel method for generating higher-quality cluster centers
and a dynamic balancing regularizer to ensure well-distributed assignments. In
the second stage, we introduce a Self-Enhanced fine-tuning strategy. The
well-aligned model from the first stage acts as a reliable pseudo-label
generator. These self-generated supervisory signals are then used to feed back
the efficient, joint optimization of the vision encoder and clustering heads,
unlocking their full potential. Extensive experiments on six mainstream
datasets show that our method outperforms existing deep clustering methods by
significant margins. Notably, our ViT-B/32 model already matches or even
surpasses the accuracy of state-of-the-art methods built upon the far larger
ViT-L/14.

</details>


### [144] [SpatioTemporal Difference Network for Video Depth Super-Resolution](https://arxiv.org/abs/2508.01259)
*Zhengxue Wang,Yuan Wu,Xiang Li,Zhiqiang Yan,Jian Yang*

Main category: cs.CV

TL;DR: STDNet addresses long-tailed distribution issues in video depth super-resolution using spatial and temporal difference mechanisms, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Video depth super-resolution suffers from long-tailed distributions in spatial non-smooth regions and temporal variation zones, degrading reconstruction quality.

Method: Proposes STDNet with two branches: spatial difference (aligns RGB features for intra-frame depth calibration) and temporal difference (propagates temporal variation for motion compensation).

Result: Extensive experiments show STDNet outperforms existing approaches.

Conclusion: STDNet effectively mitigates long-tailed effects, enhancing depth super-resolution quality.

Abstract: Depth super-resolution has achieved impressive performance, and the
incorporation of multi-frame information further enhances reconstruction
quality. Nevertheless, statistical analyses reveal that video depth
super-resolution remains affected by pronounced long-tailed distributions, with
the long-tailed effects primarily manifesting in spatial non-smooth regions and
temporal variation zones. To address these challenges, we propose a novel
SpatioTemporal Difference Network (STDNet) comprising two core branches: a
spatial difference branch and a temporal difference branch. In the spatial
difference branch, we introduce a spatial difference mechanism to mitigate the
long-tailed issues in spatial non-smooth regions. This mechanism dynamically
aligns RGB features with learned spatial difference representations, enabling
intra-frame RGB-D aggregation for depth calibration. In the temporal difference
branch, we further design a temporal difference strategy that preferentially
propagates temporal variation information from adjacent RGB and depth frames to
the current depth frame, leveraging temporal difference representations to
achieve precise motion compensation in temporal long-tailed areas. Extensive
experimental results across multiple datasets demonstrate the effectiveness of
our STDNet, outperforming existing approaches.

</details>


### [145] [Enhancing Diffusion-based Dataset Distillation via Adversary-Guided Curriculum Sampling](https://arxiv.org/abs/2508.01264)
*Lexiao Zou,Gongwei Chen,Yanda Chen,Miao Zhang*

Main category: cs.CV

TL;DR: The paper introduces Adversary-guided Curriculum Sampling (ACS) to enhance dataset distillation by improving diversity and reducing redundancy in images sampled from diffusion models.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of diversity in images sampled from diffusion models, which leads to information redundancy in distilled datasets.

Method: ACS partitions the dataset into curricula, using adversarial loss to guide diffusion sampling and train a discriminator, ensuring diverse and progressively complex images.

Result: ACS improves performance by 4.1% on Imagewoof and 2.1% on ImageNet-1k over state-of-the-art methods.

Conclusion: ACS effectively mitigates redundancy and enhances diversity in distilled datasets, achieving superior performance.

Abstract: Dataset distillation aims to encapsulate the rich information contained in
dataset into a compact distilled dataset but it faces performance degradation
as the image-per-class (IPC) setting or image resolution grows larger. Recent
advancements demonstrate that integrating diffusion generative models can
effectively facilitate the compression of large-scale datasets while
maintaining efficiency due to their superiority in matching data distribution
and summarizing representative patterns. However, images sampled from diffusion
models are always blamed for lack of diversity which may lead to information
redundancy when multiple independent sampled images are aggregated as a
distilled dataset. To address this issue, we propose Adversary-guided
Curriculum Sampling (ACS), which partitions the distilled dataset into multiple
curricula. For generating each curriculum, ACS guides diffusion sampling
process by an adversarial loss to challenge a discriminator trained on sampled
images, thus mitigating information overlap between curricula and fostering a
more diverse distilled dataset. Additionally, as the discriminator evolves with
the progression of curricula, ACS generates images from simpler to more
complex, ensuring efficient and systematic coverage of target data
informational spectrum. Extensive experiments demonstrate the effectiveness of
ACS, which achieves substantial improvements of 4.1\% on Imagewoof and 2.1\% on
ImageNet-1k over the state-of-the-art.

</details>


### [146] [ModelNet40-E: An Uncertainty-Aware Benchmark for Point Cloud Classification](https://arxiv.org/abs/2508.01269)
*Pedro Alonso,Tianrui Li,Chongshou Li*

Main category: cs.CV

TL;DR: ModelNet40-E is a new benchmark for evaluating point cloud classification models under LiDAR-like noise, featuring noise-corrupted data and uncertainty annotations. Three models were tested, with Point Transformer v3 showing the best calibration.


<details>
  <summary>Details</summary>
Motivation: To assess robustness and calibration of point cloud models under synthetic noise, addressing gaps in existing benchmarks.

Method: Introduces ModelNet40-E with noise-corrupted point clouds and uncertainty annotations. Evaluates PointNet, DGCNN, and Point Transformer v3 using accuracy, calibration, and uncertainty-awareness metrics.

Result: All models degrade with noise, but Point Transformer v3 outperforms in calibration, aligning uncertainties with measurement noise.

Conclusion: ModelNet40-E enables fine-grained evaluation, highlighting Point Transformer v3's superior calibration under noise.

Abstract: We introduce ModelNet40-E, a new benchmark designed to assess the robustness
and calibration of point cloud classification models under synthetic LiDAR-like
noise. Unlike existing benchmarks, ModelNet40-E provides both noise-corrupted
point clouds and point-wise uncertainty annotations via Gaussian noise
parameters ({\sigma}, {\mu}), enabling fine-grained evaluation of uncertainty
modeling. We evaluate three popular models-PointNet, DGCNN, and Point
Transformer v3-across multiple noise levels using classification accuracy,
calibration metrics, and uncertainty-awareness. While all models degrade under
increasing noise, Point Transformer v3 demonstrates superior calibration, with
predicted uncertainties more closely aligned with the underlying measurement
uncertainty.

</details>


### [147] [SGCap: Decoding Semantic Group for Zero-shot Video Captioning](https://arxiv.org/abs/2508.01270)
*Zeyu Pan,Ping Li,Wenxiao Wang*

Main category: cs.CV

TL;DR: SGCap introduces a zero-shot video captioning method using Semantic Group Decoding and novel supervision modules to improve temporal dynamics and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot image captioning methods fail to capture video temporal dynamics and lack sufficient video-level supervision.

Method: Proposes Semantic Group Captioning (SGCap) with Semantic Group Decoding (SGD), Key Sentences Selection (KSS), and Probability Sampling Supervision (PSS) modules.

Result: Outperforms state-of-the-art zero-shot methods and competes with fully supervised ones on benchmarks.

Conclusion: SGCap effectively addresses the challenges of zero-shot video captioning by modeling temporal relationships and enhancing supervision.

Abstract: Zero-shot video captioning aims to generate sentences for describing videos
without training the model on video-text pairs, which remains underexplored.
Existing zero-shot image captioning methods typically adopt a text-only
training paradigm, where a language decoder reconstructs single-sentence
embeddings obtained from CLIP. However, directly extending them to the video
domain is suboptimal, as applying average pooling over all frames neglects
temporal dynamics. To address this challenge, we propose a Semantic Group
Captioning (SGCap) method for zero-shot video captioning. In particular, it
develops the Semantic Group Decoding (SGD) strategy to employ multi-frame
information while explicitly modeling inter-frame temporal relationships.
Furthermore, existing zero-shot captioning methods that rely on cosine
similarity for sentence retrieval and reconstruct the description supervised by
a single frame-level caption, fail to provide sufficient video-level
supervision. To alleviate this, we introduce two key components, including the
Key Sentences Selection (KSS) module and the Probability Sampling Supervision
(PSS) module. The two modules construct semantically-diverse sentence groups
that models temporal dynamics and guide the model to capture inter-sentence
causal relationships, thereby enhancing its generalization ability to video
captioning. Experimental results on several benchmarks demonstrate that SGCap
significantly outperforms previous state-of-the-art zero-shot alternatives and
even achieves performance competitive with fully supervised ones. Code is
available at https://github.com/mlvccn/SGCap_Video.

</details>


### [148] [PromptSafe: Gated Prompt Tuning for Safe Text-to-Image Generation](https://arxiv.org/abs/2508.01272)
*Zonglei Jing,Xiao Yang,Xiaoqian Li,Siyuan Liang,Aishan Liu,Mingchuan Zhang,Xianglong Liu*

Main category: cs.CV

TL;DR: PromptSafe is a gated prompt tuning framework for text-to-image models to prevent NSFW content, balancing safety and image quality by dynamically adjusting defensive strength based on prompt toxicity.


<details>
  <summary>Details</summary>
Motivation: Existing moderation methods for T2I models are computationally expensive, degrade benign image quality, and lack adaptability to diverse safety needs.

Method: PromptSafe uses a text-only training corpus (rewritten unsafe prompts via LLM) and a gated control network to optimize a universal soft prompt, adjusting defense intensity based on prompt toxicity.

Result: Achieves a 2.36% unsafe generation rate, preserves benign fidelity, and generalizes well to unseen harmful categories and model architectures.

Conclusion: PromptSafe offers a scalable, adaptable solution for safe T2I generation, outperforming static defenses in both safety and quality.

Abstract: Text-to-image (T2I) models have demonstrated remarkable generative
capabilities but remain vulnerable to producing not-safe-for-work (NSFW)
content, such as violent or explicit imagery. While recent moderation efforts
have introduced soft prompt-guided tuning by appending defensive tokens to the
input, these approaches often rely on large-scale curated image-text datasets
and apply static, one-size-fits-all defenses at inference time. However, this
results not only in high computational cost and degraded benign image quality,
but also in limited adaptability to the diverse and nuanced safety requirements
of real-world prompts. To address these challenges, we propose PromptSafe, a
gated prompt tuning framework that combines a lightweight, text-only supervised
soft embedding with an inference-time gated control network. Instead of
training on expensive image-text datasets, we first rewrite unsafe prompts into
semantically aligned but safe alternatives using an LLM, constructing an
efficient text-only training corpus. Based on this, we optimize a universal
soft prompt that repels unsafe and attracts safe embeddings during the
diffusion denoising process. To avoid over-suppressing benign prompts, we
introduce a gated mechanism that adaptively adjusts the defensive strength
based on estimated prompt toxicity, thereby aligning defense intensity with
prompt risk and ensuring strong protection for harmful inputs while preserving
benign generation quality. Extensive experiments across multiple benchmarks and
T2I models show that PromptSafe achieves a SOTA unsafe generation rate (2.36%),
while preserving high benign fidelity. Furthermore, PromptSafe demonstrates
strong generalization to unseen harmful categories, robust transferability
across diffusion model architectures, and resilience under adaptive adversarial
attacks, highlighting its practical value for safe and scalable deployment.

</details>


### [149] [Integrating Disparity Confidence Estimation into Relative Depth Prior-Guided Unsupervised Stereo Matching](https://arxiv.org/abs/2508.01275)
*Chuang-Wei Liu,Mingjian Sun,Cairong Zhao,Hanli Wang,Alexander Dvorkovich,Rui Fan*

Main category: cs.CV

TL;DR: The paper proposes an unsupervised stereo matching framework using disparity confidence estimation and depth prior-guided loss functions to improve accuracy, outperforming existing methods on KITTI benchmarks.


<details>
  <summary>Details</summary>
Motivation: Unsupervised stereo matching avoids costly disparity annotations but suffers from ambiguities like repetitive patterns. Existing methods inefficiently use 3D geometric knowledge and introduce noise.

Method: A plug-and-play disparity confidence estimation algorithm and two depth prior-guided loss functions are introduced. Quasi-dense correspondences are built using confident disparity estimates for efficient depth ranking learning.

Result: The method achieves state-of-the-art accuracy on KITTI Stereo benchmarks among unsupervised methods.

Conclusion: The proposed framework effectively addresses stereo matching ambiguities and outperforms existing unsupervised methods.

Abstract: Unsupervised stereo matching has garnered significant attention for its
independence from costly disparity annotations. Typical unsupervised methods
rely on the multi-view consistency assumption for training networks, which
suffer considerably from stereo matching ambiguities, such as repetitive
patterns and texture-less regions. A feasible solution lies in transferring 3D
geometric knowledge from a relative depth map to the stereo matching networks.
However, existing knowledge transfer methods learn depth ranking information
from randomly built sparse correspondences, which makes inefficient utilization
of 3D geometric knowledge and introduces noise from mistaken disparity
estimates. This work proposes a novel unsupervised learning framework to
address these challenges, which comprises a plug-and-play disparity confidence
estimation algorithm and two depth prior-guided loss functions. Specifically,
the local coherence consistency between neighboring disparities and their
corresponding relative depths is first checked to obtain disparity confidence.
Afterwards, quasi-dense correspondences are built using only confident
disparity estimates to facilitate efficient depth ranking learning. Finally, a
dual disparity smoothness loss is proposed to boost stereo matching performance
at disparity discontinuities. Experimental results demonstrate that our method
achieves state-of-the-art stereo matching accuracy on the KITTI Stereo
benchmarks among all unsupervised stereo matching methods.

</details>


### [150] [GMAT: Grounded Multi-Agent Clinical Description Generation for Text Encoder in Vision-Language MIL for Whole Slide Image Classification](https://arxiv.org/abs/2508.01293)
*Ngoc Bui Lam Quang,Nam Le Nguyen Binh,Thanh-Huy Nguyen,Le Thien Phuc Nguyen,Quan Nguyen,Ulas Bagci*

Main category: cs.CV

TL;DR: A vision-language MIL framework improves WSI classification by generating diverse, grounded clinical descriptions and using multi-description text encoding for better alignment with visual features.


<details>
  <summary>Details</summary>
Motivation: Current MIL methods for WSI classification using VLMs are limited by token capacity and lack domain-specific grounding in clinical descriptions, leading to suboptimal performance.

Method: Proposes a grounded multi-agent description generation system and a text encoding strategy using multiple descriptions for better alignment with visual features.

Result: The framework outperforms single-prompt baselines and matches state-of-the-art models on renal and lung cancer datasets.

Conclusion: The approach enhances the expressiveness and domain specificity of clinical descriptions, improving WSI classification performance.

Abstract: Multiple Instance Learning (MIL) is the leading approach for whole slide
image (WSI) classification, enabling efficient analysis of gigapixel pathology
slides. Recent work has introduced vision-language models (VLMs) into MIL
pipelines to incorporate medical knowledge through text-based class
descriptions rather than simple class names. However, when these methods rely
on large language models (LLMs) to generate clinical descriptions or use
fixed-length prompts to represent complex pathology concepts, the limited token
capacity of VLMs often constrains the expressiveness and richness of the
encoded class information. Additionally, descriptions generated solely by LLMs
may lack domain grounding and fine-grained medical specificity, leading to
suboptimal alignment with visual features. To address these challenges, we
propose a vision-language MIL framework with two key contributions: (1) A
grounded multi-agent description generation system that leverages curated
pathology textbooks and agent specialization (e.g., morphology, spatial
context) to produce accurate and diverse clinical descriptions; (2) A text
encoding strategy using a list of descriptions rather than a single prompt,
capturing fine-grained and complementary clinical signals for better alignment
with visual features. Integrated into a VLM-MIL pipeline, our approach shows
improved performance over single-prompt class baselines and achieves results
comparable to state-of-the-art models, as demonstrated on renal and lung cancer
datasets.

</details>


### [151] [Domain Generalized Stereo Matching with Uncertainty-guided Data Augmentation](https://arxiv.org/abs/2508.01303)
*Shuangli Du,Jing Wang,Minghua Zhao,Zhenyu Xu,Jie Li*

Main category: cs.CV

TL;DR: The paper proposes an uncertainty-guided data augmentation (UgDA) method to improve stereo matching models' generalization from synthetic to real data by perturbing RGB image statistics and enforcing feature consistency.


<details>
  <summary>Details</summary>
Motivation: Synthetic-trained stereo matching models struggle with real data due to domain differences like color and texture.

Method: UgDA perturbs RGB image statistics (mean, standard deviation) to simulate unseen domains, using Gaussian distributions for uncertainty modeling, and enforces feature consistency.

Result: Extensive experiments show UgDA significantly boosts generalization performance of stereo matching networks.

Conclusion: UgDA is a simple, effective, and architecture-agnostic solution for improving cross-domain stereo matching.

Abstract: State-of-the-art stereo matching (SM) models trained on synthetic data often
fail to generalize to real data domains due to domain differences, such as
color, illumination, contrast, and texture. To address this challenge, we
leverage data augmentation to expand the training domain, encouraging the model
to acquire robust cross-domain feature representations instead of
domain-dependent shortcuts. This paper proposes an uncertainty-guided data
augmentation (UgDA) method, which argues that the image statistics in RGB space
(mean and standard deviation) carry the domain characteristics. Thus, samples
in unseen domains can be generated by properly perturbing these statistics.
Furthermore, to simulate more potential domains, Gaussian distributions founded
on batch-level statistics are poposed to model the unceratinty of perturbation
direction and intensity. Additionally, we further enforce feature consistency
between original and augmented data for the same scene, encouraging the model
to learn structure aware, shortcuts-invariant feature representations. Our
approach is simple, architecture-agnostic, and can be integrated into any SM
networks. Extensive experiments on several challenging benchmarks have
demonstrated that our method can significantly improve the generalization
performance of existing SM networks.

</details>


### [152] [C3D-AD: Toward Continual 3D Anomaly Detection via Kernel Attention with Learnable Advisor](https://arxiv.org/abs/2508.01311)
*Haoquan Lu,Hanzhe Liang,Jie Zhang,Chenxi Hu,Jinbao Wang,Can Gao*

Main category: cs.CV

TL;DR: The paper introduces C3D-AD, a continual learning framework for 3D anomaly detection, addressing limitations of class-specific training and handling emerging classes. It uses Kernel Attention mechanisms and a reconstruction module to maintain performance across tasks.


<details>
  <summary>Details</summary>
Motivation: Existing 3D anomaly detection methods are class-specific and cannot adapt to new classes. The study aims to develop a framework for multi-class learning and continual adaptation.

Method: Proposes C3D-AD with Kernel Attention Layers (KAL) for feature extraction, Kernel Attention with Advisor (KAA) for efficient learning of new classes, and Reconstruction with Parameter Perturbation (RPP) for representation consistency.

Result: Achieves 66.4%, 83.1%, and 63.4% AUROC on Real3D-AD, Anomaly-ShapeNet, and MulSen-AD datasets, demonstrating effectiveness.

Conclusion: C3D-AD successfully addresses continual learning in 3D anomaly detection, outperforming existing methods and adapting to new classes efficiently.

Abstract: 3D Anomaly Detection (AD) has shown great potential in detecting anomalies or
defects of high-precision industrial products. However, existing methods are
typically trained in a class-specific manner and also lack the capability of
learning from emerging classes. In this study, we proposed a continual learning
framework named Continual 3D Anomaly Detection (C3D-AD), which can not only
learn generalized representations for multi-class point clouds but also handle
new classes emerging over time.Specifically, in the feature extraction module,
to extract generalized local features from diverse product types of different
tasks efficiently, Kernel Attention with random feature Layer (KAL) is
introduced, which normalizes the feature space. Then, to reconstruct data
correctly and continually, an efficient Kernel Attention with learnable Advisor
(KAA) mechanism is proposed, which learns the information from new categories
while discarding redundant old information within both the encoder and decoder.
Finally, to keep the representation consistency over tasks, a Reconstruction
with Parameter Perturbation (RPP) module is proposed by designing a
representation rehearsal loss function, which ensures that the model remembers
previous category information and returns category-adaptive
representation.Extensive experiments on three public datasets demonstrate the
effectiveness of the proposed method, achieving an average performance of
66.4%, 83.1%, and 63.4% AUROC on Real3D-AD, Anomaly-ShapeNet, and MulSen-AD,
respectively.

</details>


### [153] [P3P Made Easy](https://arxiv.org/abs/2508.01312)
*Seong Hun Lee,Patrick Vandewalle,Javier Civera*

Main category: cs.CV

TL;DR: A novel algebraic solution for the P3P problem, offering simplicity, accuracy, and efficiency comparable to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To provide a computationally efficient and interpretable solution for recovering camera pose from 2D-3D correspondences, suitable for real-time systems and education.

Method: Reformulates the P3P problem into a quartic polynomial with analytically simple and efficient coefficients.

Result: Achieves accuracy and runtime performance on par with leading methods, validated by synthetic dataset experiments.

Conclusion: The solver's simplicity and performance make it ideal for real-time applications and educational use, emphasizing interpretability and reliability.

Abstract: We present a novel algebraic solution to the Perspective-Three-Point (P3P)
problem, which aims to recover the absolute pose of a calibrated camera from
three 2D-3D correspondences. Our method reformulates the problem into a quartic
polynomial with coefficients that are analytically simple and computationally
efficient. Despite its simplicity, the proposed solver achieves accuracy and
runtime performance comparable to state-of-the-art methods. Extensive
experiments on synthetic datasets validate its robustness and efficiency. This
combination of simplicity and performance makes our solver appealing for both
real-time systems and educational contexts, where interpretability and
reliability are critical.

</details>


### [154] [Multimodal Attention-Aware Fusion for Diagnosing Distal Myopathy: Evaluating Model Interpretability and Clinician Trust](https://arxiv.org/abs/2508.01316)
*Mohsen Abbaspour Onari,Lucie Charlotte Magister,Yaoxin Wu,Amalia Lupi,Dario Creazzo,Mattia Tordin,Luigi Di Donatantonio,Emilio Quaia,Chao Zhang,Isel Grau,Marco S. Nobile,Yingqian Zhang,Pietro Liò*

Main category: cs.CV

TL;DR: A novel multimodal attention-aware fusion architecture improves classification accuracy and interpretability for distal myopathy diagnosis, though gaps in anatomical specificity remain.


<details>
  <summary>Details</summary>
Motivation: Diagnosing distal myopathy is challenging due to its genetic heterogeneity and varied clinical manifestations. The paper aims to enhance diagnostic accuracy and transparency in radiology.

Method: Proposes a fusion architecture combining global and local deep learning features via an attention gate mechanism. Evaluated on BUSI and a proprietary dataset.

Result: Achieves high classification accuracy and generates clinically relevant saliency maps. However, gaps in anatomical specificity and clinical usefulness persist.

Conclusion: The study highlights the need for richer interpretability methods and human-in-the-loop feedback to meet real-world clinical expectations.

Abstract: Distal myopathy represents a genetically heterogeneous group of skeletal
muscle disorders with broad clinical manifestations, posing diagnostic
challenges in radiology. To address this, we propose a novel multimodal
attention-aware fusion architecture that combines features extracted from two
distinct deep learning models, one capturing global contextual information and
the other focusing on local details, representing complementary aspects of the
input data. Uniquely, our approach integrates these features through an
attention gate mechanism, enhancing both predictive performance and
interpretability. Our method achieves a high classification accuracy on the
BUSI benchmark and a proprietary distal myopathy dataset, while also generating
clinically relevant saliency maps that support transparent decision-making in
medical diagnosis. We rigorously evaluated interpretability through (1)
functionally grounded metrics, coherence scoring against reference masks and
incremental deletion analysis, and (2) application-grounded validation with
seven expert radiologists. While our fusion strategy boosts predictive
performance relative to single-stream and alternative fusion strategies, both
quantitative and qualitative evaluations reveal persistent gaps in anatomical
specificity and clinical usefulness of the interpretability. These findings
highlight the need for richer, context-aware interpretability methods and
human-in-the-loop feedback to meet clinicians' expectations in real-world
diagnostic settings.

</details>


### [155] [Referring Remote Sensing Image Segmentation with Cross-view Semantics Interaction Network](https://arxiv.org/abs/2508.01331)
*Jiaxing Yang,Lihe Zhang,Huchuan Lu*

Main category: cs.CV

TL;DR: The paper introduces CSINet, a novel framework for Referring Remote Sensing Image Segmentation (RRSIS), addressing scale variation and ambiguity in targets by combining remote and close-view cues with cross-view attention and collaborative decoding.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with tiny or ambiguous targets in RRSIS due to reliance on single-view structures and saliency-preferring techniques. The work is inspired by human observation behavior to improve segmentation accuracy.

Method: Proposes CSINet, featuring Cross-View Window-attention (CVWin) for global/local semantics integration and Collaboratively Dilated Attention Decoder (CDAD) for multiscale feature fusion and orientation awareness.

Result: CSINet significantly outperforms existing methods in segmentation accuracy while maintaining efficient processing speed.

Conclusion: The framework effectively addresses scale variation and ambiguity in RRSIS by leveraging cross-view semantics and collaborative attention, achieving superior performance.

Abstract: Recently, Referring Remote Sensing Image Segmentation (RRSIS) has aroused
wide attention. To handle drastic scale variation of remote targets, existing
methods only use the full image as input and nest the saliency-preferring
techniques of cross-scale information interaction into traditional single-view
structure. Although effective for visually salient targets, they still struggle
in handling tiny, ambiguous ones in lots of real scenarios. In this work, we
instead propose a paralleled yet unified segmentation framework Cross-view
Semantics Interaction Network (CSINet) to solve the limitations. Motivated by
human behavior in observing targets of interest, the network orchestrates
visual cues from remote and close distances to conduct synergistic prediction.
In its every encoding stage, a Cross-View Window-attention module (CVWin) is
utilized to supplement global and local semantics into close-view and
remote-view branch features, finally promoting the unified representation of
feature in every encoding stage. In addition, we develop a Collaboratively
Dilated Attention enhanced Decoder (CDAD) to mine the orientation property of
target and meanwhile integrate cross-view multiscale features. The proposed
network seamlessly enhances the exploitation of global and local semantics,
achieving significant improvements over others while maintaining satisfactory
speed.

</details>


### [156] [Zero-shot Segmentation of Skin Conditions: Erythema with Edit-Friendly Inversion](https://arxiv.org/abs/2508.01334)
*Konstantinos Moutselos,Ilias Maglogiannis*

Main category: cs.CV

TL;DR: A zero-shot image segmentation framework using diffusion models detects skin erythema by synthesizing reference images and aligning them with originals, reducing reliance on labeled datasets.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting erythema without needing labeled dermatological datasets, leveraging generative models for scalable and flexible diagnostic support.

Method: Uses edit-friendly inversion in diffusion models to synthesize erythema-free reference images, aligns them with original images, and performs color-space analysis for segmentation.

Result: Successfully isolated facial erythema in diverse cases, outperforming baseline threshold-based techniques.

Conclusion: Combining generative diffusion models with color segmentation enables efficient erythema detection without prior training data, showing promise for computer-aided dermatology.

Abstract: This study proposes a zero-shot image segmentation framework for detecting
erythema (redness of the skin) using edit-friendly inversion in diffusion
models. The method synthesizes reference images of the same patient that are
free from erythema via generative editing and then accurately aligns these
references with the original images. Color-space analysis is performed with
minimal user intervention to identify erythematous regions. This approach
significantly reduces the reliance on labeled dermatological datasets while
providing a scalable and flexible diagnostic support tool by avoiding the need
for any annotated training masks. In our initial qualitative experiments, the
pipeline successfully isolated facial erythema in diverse cases, demonstrating
performance improvements over baseline threshold-based techniques. These
results highlight the potential of combining generative diffusion models and
statistical color segmentation for computer-aided dermatology, enabling
efficient erythema detection without prior training data.

</details>


### [157] [StyleSentinel: Reliable Artistic Copyright Verification via Stylistic Fingerprints](https://arxiv.org/abs/2508.01335)
*Lingxiao Chen,Liqin Wang,Wei Lu*

Main category: cs.CV

TL;DR: StyleSentinel protects artwork copyright by verifying stylistic fingerprints, outperforming existing methods in one-sample verification.


<details>
  <summary>Details</summary>
Motivation: Unauthorized use of personal artwork via diffusion models threatens artists' intellectual property; current defenses like watermarks are inadequate.

Method: Uses semantic self-reconstruction for stylistic expressiveness, fuses multi-layer features for stylistic fingerprints, and models style as a hypersphere boundary for one-class learning.

Result: Superior performance in one-sample verification and effectiveness on online platforms.

Conclusion: StyleSentinel offers a robust solution for copyright protection by leveraging inherent stylistic fingerprints.

Abstract: The versatility of diffusion models in generating customized images has led
to unauthorized usage of personal artwork, which poses a significant threat to
the intellectual property of artists. Existing approaches relying on embedding
additional information, such as perturbations, watermarks, and backdoors,
suffer from limited defensive capabilities and fail to protect artwork
published online. In this paper, we propose StyleSentinel, an approach for
copyright protection of artwork by verifying an inherent stylistic fingerprint
in the artist's artwork. Specifically, we employ a semantic self-reconstruction
process to enhance stylistic expressiveness within the artwork, which
establishes a dense and style-consistent manifold foundation for feature
learning. Subsequently, we adaptively fuse multi-layer image features to encode
abstract artistic style into a compact stylistic fingerprint. Finally, we model
the target artist's style as a minimal enclosing hypersphere boundary in the
feature space, transforming complex copyright verification into a robust
one-class learning task. Extensive experiments demonstrate that compared with
the state-of-the-art, StyleSentinel achieves superior performance on the
one-sample verification task. We also demonstrate the effectiveness through
online platforms.

</details>


### [158] [Weakly-Supervised Image Forgery Localization via Vision-Language Collaborative Reasoning Framework](https://arxiv.org/abs/2508.01338)
*Ziqi Sheng,Junyan Wu,Wei Lu,Jiantao Zhou*

Main category: cs.CV

TL;DR: ViLaCo introduces a vision-language framework for weakly supervised image forgery localization, leveraging pre-trained VLMs to enhance semantic guidance and achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing weakly supervised methods that rely on intra-image clues and lack external semantic guidance, ViLaCo integrates vision-language models for better localization.

Method: ViLaCo uses a vision-language feature modeling network to extract textual and visual priors, aligns them via mutual interactions, and employs dual prediction heads for image-level classification and pixel-level localization. A contrastive patch consistency module further refines forgery discrimination.

Result: ViLaCo outperforms existing WSIFL methods on multiple datasets, achieving superior detection and localization accuracy.

Conclusion: ViLaCo effectively bridges the gap between weak supervision and fine-grained localization, demonstrating the value of vision-language collaboration in forgery detection.

Abstract: Image forgery localization aims to precisely identify tampered regions within
images, but it commonly depends on costly pixel-level annotations. To alleviate
this annotation burden, weakly supervised image forgery localization (WSIFL)
has emerged, yet existing methods still achieve limited localization
performance as they mainly exploit intra-image consistency clues and lack
external semantic guidance to compensate for weak supervision. In this paper,
we propose ViLaCo, a vision-language collaborative reasoning framework that
introduces auxiliary semantic supervision distilled from pre-trained
vision-language models (VLMs), enabling accurate pixel-level localization using
only image-level labels. Specifically, ViLaCo first incorporates semantic
knowledge through a vision-language feature modeling network, which jointly
extracts textual and visual priors using pre-trained VLMs. Next, an adaptive
vision-language reasoning network aligns textual semantics and visual features
through mutual interactions, producing semantically aligned representations.
Subsequently, these representations are passed into dual prediction heads,
where the coarse head performs image-level classification and the fine head
generates pixel-level localization masks, thereby bridging the gap between weak
supervision and fine-grained localization. Moreover, a contrastive patch
consistency module is introduced to cluster tampered features while separating
authentic ones, facilitating more reliable forgery discrimination. Extensive
experiments on multiple public datasets demonstrate that ViLaCo substantially
outperforms existing WSIFL methods, achieving state-of-the-art performance in
both detection and localization accuracy.

</details>


### [159] [SBP-YOLO:A Lightweight Real-Time Model for Detecting Speed Bumps and Potholes](https://arxiv.org/abs/2508.01339)
*Chuanqi Liang,Jie Fu,Lei Luo,Miao Yu*

Main category: cs.CV

TL;DR: SBP-YOLO, a lightweight YOLOv11-based framework, enhances real-time speed bump and pothole detection for predictive suspension control, achieving 87.0% mAP and 139.5 FPS on embedded hardware.


<details>
  <summary>Details</summary>
Motivation: The increasing demand for ride comfort in new energy vehicles necessitates accurate, real-time detection of road irregularities like speed bumps and potholes for predictive suspension systems.

Method: SBP-YOLO integrates GhostConv for efficiency, VoVGSCSPC for multi-scale features, and a Lightweight Efficiency Detection Head (LEDH). It uses hybrid training with NWD loss, knowledge distillation, and weather augmentation.

Result: Achieves 87.0% mAP (5.8% higher than YOLOv11n) and runs at 139.5 FPS on Jetson AGX Xavier with TensorRT FP16 quantization.

Conclusion: SBP-YOLO is effective for real-time road condition perception in intelligent suspension systems, validated by its performance and efficiency.

Abstract: With increasing demand for ride comfort in new energy vehicles, accurate
real-time detection of speed bumps and potholes is critical for predictive
suspension control. This paper proposes SBP-YOLO, a lightweight detection
framework based on YOLOv11, optimized for embedded deployment. The model
integrates GhostConv for efficient computation, VoVGSCSPC for multi-scale
feature enhancement, and a Lightweight Efficiency Detection Head (LEDH) to
reduce early-stage feature processing costs. A hybrid training strategy
combining NWD loss, knowledge distillation, and Albumentations-based weather
augmentation improves detection robustness, especially for small and distant
targets. Experiments show SBP-YOLO achieves 87.0% mAP (outperforming YOLOv11n
by 5.8%) and runs at 139.5 FPS on a Jetson AGX Xavier with TensorRT FP16
quantization. The results validate its effectiveness for real-time road
condition perception in intelligent suspension systems.

</details>


### [160] [Predicting Video Slot Attention Queries from Random Slot-Feature Pairs](https://arxiv.org/abs/2508.01345)
*Rongzhen Zhao,Jian Li,Juho Kannala,Joni Pajarinen*

Main category: cs.CV

TL;DR: Proposes RandSF.Q, a method for unsupervised video Object-Centric Learning (OCL) that improves query prediction by incorporating next-frame features and learning transition dynamics, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current video OCL methods neglect next-frame features and fail to learn transition dynamics, limiting their effectiveness.

Method: Introduces RandSF.Q, which uses a new transitioner to incorporate slots and features and trains it with randomly sampled slot-feature pairs to learn dynamics.

Result: Significantly outperforms existing methods, e.g., up to 10 points in object discovery, and benefits downstream tasks like dynamics modeling.

Conclusion: RandSF.Q advances video OCL by addressing key limitations, achieving state-of-the-art performance and practical benefits.

Abstract: Unsupervised video Object-Centric Learning (OCL) is promising as it enables
object-level scene representation and dynamics modeling as we humans do.
Mainstream video OCL methods adopt a recurrent architecture: An aggregator
aggregates current video frame into object features, termed slots, under some
queries; A transitioner transits current slots to queries for the next frame.
This is an effective architecture but all existing implementations both
(\textit{i1}) neglect to incorporate next frame features, the most informative
source for query prediction, and (\textit{i2}) fail to learn transition
dynamics, the knowledge essential for query prediction. To address these
issues, we propose Random Slot-Feature pair for learning Query prediction
(RandSF.Q): (\textit{t1}) We design a new transitioner to incorporate both
slots and features, which provides more information for query prediction;
(\textit{t2}) We train the transitioner to predict queries from slot-feature
pairs randomly sampled from available recurrences, which drives it to learn
transition dynamics. Experiments on scene representation demonstrate that our
method surpass existing video OCL methods significantly, e.g., up to 10 points
on object discovery, setting new state-of-the-art. Such superiority also
benefits downstream tasks like dynamics modeling. Our core source code and
training logs are available as the supplement.

</details>


### [161] [Effective Damage Data Generation by Fusing Imagery with Human Knowledge Using Vision-Language Models](https://arxiv.org/abs/2508.01380)
*Jie Wei,Erika Ardiles-Cruz,Aleksey Panasyuk,Erik Blasch*

Main category: cs.CV

TL;DR: The paper proposes using vision-language models (VLMs) to improve damage assessment in HADR by addressing data imbalance and labeling inaccuracies, showing promising results in classifying structural damage.


<details>
  <summary>Details</summary>
Motivation: Current deep learning methods for HADR damage assessment struggle with data imbalance, lack of moderate damage examples, and human labeling errors.

Method: The approach leverages VLMs to fuse imagery with human knowledge, generating diversified image-based damage data.

Result: Initial experiments show improved classification of structural damage levels in buildings, roads, and infrastructure.

Conclusion: VLMs offer a viable solution to enhance damage assessment accuracy in HADR by overcoming data limitations.

Abstract: It is of crucial importance to assess damages promptly and accurately in
humanitarian assistance and disaster response (HADR). Current deep learning
approaches struggle to generalize effectively due to the imbalance of data
classes, scarcity of moderate damage examples, and human inaccuracy in pixel
labeling during HADR situations. To accommodate for these limitations and
exploit state-of-the-art techniques in vision-language models (VLMs) to fuse
imagery with human knowledge understanding, there is an opportunity to generate
a diversified set of image-based damage data effectively. Our initial
experimental results suggest encouraging data generation quality, which
demonstrates an improvement in classifying scenes with different levels of
structural damage to buildings, roads, and infrastructures.

</details>


### [162] [A Full-Stage Refined Proposal Algorithm for Suppressing False Positives in Two-Stage CNN-Based Detection Methods](https://arxiv.org/abs/2508.01382)
*Qiang Guo,Rubo Zhang,Bingbing Zhang,Junjie Liu,Jianqing Liu*

Main category: cs.CV

TL;DR: The paper proposes a Full-stage Refined Proposal (FRP) algorithm to reduce false positives in pedestrian detection by refining proposals during training and testing stages using feature re-evaluation strategies.


<details>
  <summary>Details</summary>
Motivation: False positives in pedestrian detection remain unresolved, prompting the need for an effective solution to improve detection accuracy.

Method: The FRP algorithm includes Training mode FRP (TFRP) for validating proposals during training, and Classifier-guided FRP (CFRP) and Split-proposal FRP (SFRP) for refining proposals during inference.

Result: Experiments show the FRP algorithm effectively reduces false positives and enhances detection capabilities, even on resource-constrained devices.

Conclusion: The proposed FRP algorithm significantly improves pedestrian detection by suppressing false positives across all stages, validated by benchmarks and real-world datasets.

Abstract: False positives in pedestrian detection remain a challenge that has yet to be
effectively resolved. To address this issue, this paper proposes a Full-stage
Refined Proposal (FRP) algorithm aimed at eliminating these false positives
within a two-stage CNN-based pedestrian detection framework. The main
innovation of this work lies in employing various pedestrian feature
re-evaluation strategies to filter out low-quality pedestrian proposals during
both the training and testing stages. Specifically, in the training phase, the
Training mode FRP algorithm (TFRP) introduces a novel approach for validating
pedestrian proposals to effectively guide the model training process, thereby
constructing a model with strong capabilities for false positive suppression.
During the inference phase, two innovative strategies are implemented: the
Classifier-guided FRP (CFRP) algorithm integrates a pedestrian classifier into
the proposal generation pipeline to yield high-quality proposals through
pedestrian feature evaluation, and the Split-proposal FRP (SFRP) algorithm
vertically divides all proposals, sending both the original and the sub-region
proposals to the subsequent subnetwork to evaluate their confidence scores,
filtering out those with lower sub-region pedestrian confidence scores. As a
result, the proposed algorithm enhances the model's ability to suppress
pedestrian false positives across all stages. Various experiments conducted on
multiple benchmarks and the SY-Metro datasets demonstrate that the model,
supported by different combinations of the FRP algorithm, can effectively
eliminate false positives to varying extents. Furthermore, experiments
conducted on embedded platforms underscore the algorithm's effectiveness in
enhancing the comprehensive pedestrian detection capabilities of the small
pedestrian detector in resource-constrained edge devices.

</details>


### [163] [Lightweight Backbone Networks Only Require Adaptive Lightweight Self-Attention Mechanisms](https://arxiv.org/abs/2508.01385)
*Fengyun Li,Chao Zheng,Yangyang Fang,Jialiang Lan,Jianhua Liang,Luhao Zhang,Fa Si*

Main category: cs.CV

TL;DR: The paper proposes Fast Window Attention (FWA), a lightweight SoftMax attention mechanism with adaptive feature map sizes, and integrates it into a hybrid backbone network, LOLViT, outperforming CNNs in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the computational imbalance between CNNs and attention mechanisms, and improve long-sequence modeling in lightweight hybrid models.

Method: Introduces FWA for adaptive feature map reduction, uses ReLU to simulate SoftMax, and designs a global-local fusion mechanism with GhostNet.

Result: LOLViT shows superior performance in classification, detection, and segmentation tasks, with LOLViT-X being 5x faster than MobileViT-X.

Conclusion: FWA and LOLViT effectively balance computational efficiency and accuracy, advancing lightweight hybrid models.

Abstract: Currently, lightweight hybrid backbone networks have partially alleviated the
issue of computational saturation, but the imbalance in computational
efficiencys between convolutional neural networks (CNNs) and attention
mechanisms is becoming increasingly apparent. Specifically, although linear
attention mechanisms and their variants have made progress in lightweight
design, they still fail to meet the demands of hybrid models for long-sequence
modeling. On the other hand, existing lightweight SoftMax attention
computations typically reduce the feature map to a fixed size to decrease the
number of sequences, thereby compressing the computational scale. However, the
process of determining the feature map reduction ratio is cumbersome, and
computational saturation issues still persist. To address this issue, this
paper proposes a lightweight SoftMax attention mechanism with adaptive feature
map sizes, named Fast Window Attention (FWA), which generates a small number of
key sequences (Key and Value) through window aggregation for attention
computation. Additionally, it explains the rationality of using ReLU to
simulate SoftMax operations in lightweight global attention mechanisms.
Finally, the paper designs a global-local feature fusion mechanism and combines
it with GhostNet to propose a lightweight hybrid backbone network, LOLViT.
Through visual tasks such as classification (ImageNet 1K), detection (COCO
2017), and segmentation (BDD100K), along with extensive ablation studies, it is
demonstrated that LOLViT outperforms CNN models of the same level in both
inference speed and model accuracy. Notably, the inference speed of LOLViT-X is
5x that of MobileViT-X.

</details>


### [164] [Construction of Digital Terrain Maps from Multi-view Satellite Imagery using Neural Volume Rendering](https://arxiv.org/abs/2508.01386)
*Josef X. Biberstein,Guilherme Cavalheiro,Juyeop Han,Sertac Karaman*

Main category: cs.CV

TL;DR: The paper introduces Neural Terrain Maps (NTM), a method using neural volume rendering to create digital terrain maps from satellite imagery, eliminating the need for manual preprocessing and structural priors.


<details>
  <summary>Details</summary>
Motivation: Current methods for producing digital terrain maps (DTMs) are cumbersome and require significant manual preprocessing. The paper aims to simplify this process using neural techniques.

Method: NTM adapts neural volume rendering to learn terrain maps directly from satellite imagery, requiring only pixel loci and no depth or structural priors.

Result: NTM achieves terrain prediction precision nearly matching satellite image resolution, even with imperfect camera data, as validated on Earth and Mars datasets.

Conclusion: NTM offers a promising alternative to traditional multi-view stereo pipelines for DTM generation, reducing manual effort while maintaining accuracy.

Abstract: Digital terrain maps (DTMs) are an important part of planetary exploration,
enabling operations such as terrain relative navigation during entry, descent,
and landing for spacecraft and aiding in navigation on the ground. As robotic
exploration missions become more ambitious, the need for high quality DTMs will
only increase. However, producing DTMs via multi-view stereo pipelines for
satellite imagery, the current state-of-the-art, can be cumbersome and require
significant manual image preprocessing to produce satisfactory results. In this
work, we seek to address these shortcomings by adapting neural volume rendering
techniques to learn textured digital terrain maps directly from satellite
imagery. Our method, neural terrain maps (NTM), only requires the locus for
each image pixel and does not rely on depth or any other structural priors. We
demonstrate our method on both synthetic and real satellite data from Earth and
Mars encompassing scenes on the order of $100 \textrm{km}^2$. We evaluate the
accuracy of our output terrain maps by comparing with existing high-quality
DTMs produced using traditional multi-view stereo pipelines. Our method shows
promising results, with the precision of terrain prediction almost equal to the
resolution of the satellite images even in the presence of imperfect camera
intrinsics and extrinsics.

</details>


### [165] [Video-based Vehicle Surveillance in the Wild: License Plate, Make, and Model Recognition with Self Reflective Vision-Language Models](https://arxiv.org/abs/2508.01387)
*Pouya Parsa,Keya Li,Kara M. Kockelman,Seongjin Choi*

Main category: cs.CV

TL;DR: The paper evaluates vision-language models (VLMs) for license plate and vehicle make/model recognition in dynamic video settings, achieving high accuracy with a self-reflection module.


<details>
  <summary>Details</summary>
Motivation: To address challenges in ALPR and vehicle recognition using handheld or non-static cameras, leveraging VLMs for scalable, cost-effective solutions.

Method: Proposes pipelines for ALPR and make/model recognition using VLMs with multimodal prompts and a self-reflection module for correction.

Result: Achieves 91.67% ALPR and 66.67% make/model accuracy on a smartphone dataset, with further improvements via self-reflection.

Conclusion: VLMs offer a viable, scalable solution for dynamic traffic video analysis, outperforming traditional methods.

Abstract: Automatic license plate recognition (ALPR) and vehicle make and model
recognition underpin intelligent transportation systems, supporting law
enforcement, toll collection, and post-incident investigation. Applying these
methods to videos captured by handheld smartphones or non-static
vehicle-mounted cameras presents unique challenges compared to fixed
installations, including frequent camera motion, varying viewpoints,
occlusions, and unknown road geometry. Traditional ALPR solutions, dependent on
specialized hardware and handcrafted OCR pipelines, often degrade under these
conditions. Recent advances in large vision-language models (VLMs) enable
direct recognition of textual and semantic attributes from arbitrary imagery.
This study evaluates the potential of VLMs for ALPR and makes and models
recognition using monocular videos captured with handheld smartphones and
non-static mounted cameras. The proposed license plate recognition pipeline
filters to sharp frames, then sends a multimodal prompt to a VLM using several
prompt strategies. Make and model recognition pipeline runs the same VLM with a
revised prompt and an optional self-reflection module. In the self-reflection
module, the model contrasts the query image with a reference from a 134-class
dataset, correcting mismatches. Experiments on a smartphone dataset collected
on the campus of the University of Texas at Austin, achieve top-1 accuracies of
91.67% for ALPR and 66.67% for make and model recognition. On the public
UFPR-ALPR dataset, the approach attains 83.05% and 61.07%, respectively. The
self-reflection module further improves results by 5.72% on average for make
and model recognition. These findings demonstrate that VLMs provide a
cost-effective solution for scalable, in-motion traffic video analysis.

</details>


### [166] [Open-Attribute Recognition for Person Retrieval: Finding People Through Distinctive and Novel Attributes](https://arxiv.org/abs/2508.01389)
*Minjeong Park,Hongbeen Park,Sangwon Lee,Yoonha Jang,Jinkyu Kim*

Main category: cs.CV

TL;DR: The paper introduces Open-Attribute Recognition for Person Retrieval (OAPR) to handle novel attributes in real-world scenarios, proposing a framework for generalizable body part representations and reconstructing datasets for validation.


<details>
  <summary>Details</summary>
Motivation: Existing PAR methods assume closed-set attributes, limiting real-world applicability where novel attributes emerge. Predefined attributes are also less discriminative for person retrieval.

Method: Proposes the OAPR task and a framework for learning generalizable body part representations. Reconstructs four datasets for open-attribute recognition.

Result: Experiments validate the necessity of OAPR and the effectiveness of the proposed framework.

Conclusion: The OAPR task addresses limitations of closed-set PAR, and the framework shows promise for real-world applications.

Abstract: Pedestrian Attribute Recognition (PAR) plays a crucial role in various vision
tasks such as person retrieval and identification. Most existing
attribute-based retrieval methods operate under the closed-set assumption that
all attribute classes are consistently available during both training and
inference. However, this assumption limits their applicability in real-world
scenarios where novel attributes may emerge. Moreover, predefined attributes in
benchmark datasets are often generic and shared across individuals, making them
less discriminative for retrieving the target person. To address these
challenges, we propose the Open-Attribute Recognition for Person Retrieval
(OAPR) task, which aims to retrieve individuals based on attribute cues,
regardless of whether those attributes were seen during training. To support
this task, we introduce a novel framework designed to learn generalizable body
part representations that cover a broad range of attribute categories.
Furthermore, we reconstruct four widely used datasets for open-attribute
recognition. Comprehensive experiments on these datasets demonstrate the
necessity of the OAPR task and the effectiveness of our framework. The source
code and pre-trained models will be publicly available upon publication.

</details>


### [167] [Spatial-Frequency Aware for Object Detection in RAW Image](https://arxiv.org/abs/2508.01396)
*Zhuohua Ye,Liming Zhang,Hongru Han*

Main category: cs.CV

TL;DR: The paper proposes SFAE, a framework combining spatial and frequency domains to enhance RAW image object detection by recovering suppressed details.


<details>
  <summary>Details</summary>
Motivation: Existing methods in the spatial domain struggle to recover suppressed details in RAW images due to their wide dynamic range and linear response.

Method: SFAE transforms frequency bands into spatial maps, uses cross-domain fusion attention, and applies adaptive nonlinear adjustments in both domains.

Result: The framework effectively recovers object details by leveraging frequency domain features and spatial interactions.

Conclusion: SFAE improves RAW-based object detection by synergizing spatial and frequency representations.

Abstract: Direct RAW-based object detection offers great promise by utilizing RAW data
(unprocessed sensor data), but faces inherent challenges due to its wide
dynamic range and linear response, which tends to suppress crucial object
details. In particular, existing enhancement methods are almost all performed
in the spatial domain, making it difficult to effectively recover these
suppressed details from the skewed pixel distribution of RAW images. To address
this limitation, we turn to the frequency domain, where features, such as
object contours and textures, can be naturally separated based on frequency. In
this paper, we propose Space-Frequency Aware RAW Image Object Detection
Enhancer (SFAE), a novel framework that synergizes spatial and frequency
representations. Our contribution is threefold. The first lies in the
``spatialization" of frequency bands. Different from the traditional paradigm
of directly manipulating abstract spectra in deep networks, our method
inversely transforms individual frequency bands back into tangible spatial
maps, thus preserving direct physical intuition. Then the cross-domain fusion
attention module is developed to enable deep multimodal interactions between
these maps and the original spatial features. Finally, the framework performs
adaptive nonlinear adjustments by predicting and applying different gamma
parameters for the two domains.

</details>


### [168] [ForenX: Towards Explainable AI-Generated Image Detection with Multimodal Large Language Models](https://arxiv.org/abs/2508.01402)
*Chuangchuang Tan,Jinglu Wang,Xiang Ming,Renshuai Tao,Yunchao Wei,Yao Zhao,Yan Lu*

Main category: cs.CV

TL;DR: ForenX is a novel method using multimodal large language models (MLLMs) to detect AI-generated images and provide human-like explanations, enhanced by a specialized forensic prompt and the ForgReason dataset.


<details>
  <summary>Details</summary>
Motivation: The gap between AI-generated image detection methods and human cognitive forensic analysis drives the need for a system that not only identifies authenticity but also explains findings in a human-understandable way.

Method: ForenX employs MLLMs with a forensic prompt to focus on forgery-indicative attributes and uses the ForgReason dataset, curated via LLM-human collaboration, to improve performance.

Result: ForenX demonstrates effectiveness on major benchmarks, with improved generalization and explainability verified through subjective evaluations.

Conclusion: ForenX bridges the gap between AI and human forensic analysis, offering accurate detection and comprehensive explanations, further enhanced by limited manual annotations.

Abstract: Advances in generative models have led to AI-generated images visually
indistinguishable from authentic ones. Despite numerous studies on detecting
AI-generated images with classifiers, a gap persists between such methods and
human cognitive forensic analysis. We present ForenX, a novel method that not
only identifies the authenticity of images but also provides explanations that
resonate with human thoughts. ForenX employs the powerful multimodal large
language models (MLLMs) to analyze and interpret forensic cues. Furthermore, we
overcome the limitations of standard MLLMs in detecting forgeries by
incorporating a specialized forensic prompt that directs the MLLMs attention to
forgery-indicative attributes. This approach not only enhance the
generalization of forgery detection but also empowers the MLLMs to provide
explanations that are accurate, relevant, and comprehensive. Additionally, we
introduce ForgReason, a dataset dedicated to descriptions of forgery evidences
in AI-generated images. Curated through collaboration between an LLM-based
agent and a team of human annotators, this process provides refined data that
further enhances our model's performance. We demonstrate that even limited
manual annotations significantly improve explanation quality. We evaluate the
effectiveness of ForenX on two major benchmarks. The model's explainability is
verified by comprehensive subjective evaluations.

</details>


### [169] [3DRot: 3D Rotation Augmentation for RGB-Based 3D Tasks](https://arxiv.org/abs/2508.01423)
*Shitian Yang,Deyu Li,Xiaoke Jiang,Lei Zhang*

Main category: cs.CV

TL;DR: 3DRot is a plug-and-play augmentation method for RGB-based 3D tasks that preserves geometric consistency by rotating/mirroring images about the camera's optical center, improving performance in monocular 3D detection.


<details>
  <summary>Details</summary>
Motivation: RGB-based 3D tasks face challenges due to scarce annotations and limited augmentation options that disrupt geometric consistency.

Method: 3DRot rotates and mirrors images about the camera's optical center while updating RGB images, camera intrinsics, object poses, and 3D annotations to maintain projective geometry.

Result: On SUN RGB-D, 3DRot improves IoU3D (43.21 to 44.51), reduces rotation error (22.91° to 20.93°), and boosts mAP0.5 (35.70 to 38.11).

Conclusion: 3DRot is effective for monocular 3D detection and transferable to other 3D tasks due to its camera-space transform approach.

Abstract: RGB-based 3D tasks, e.g., 3D detection, depth estimation, 3D keypoint
estimation, still suffer from scarce, expensive annotations and a thin
augmentation toolbox, since most image transforms, including resize and
rotation, disrupt geometric consistency. In this paper, we introduce 3DRot, a
plug-and-play augmentation that rotates and mirrors images about the camera's
optical center while synchronously updating RGB images, camera intrinsics,
object poses, and 3D annotations to preserve projective geometry-achieving
geometry-consistent rotations and reflections without relying on any scene
depth. We validate 3DRot with a classical 3D task, monocular 3D detection. On
SUN RGB-D dataset, 3DRot raises $IoU_{3D}$ from 43.21 to 44.51, cuts rotation
error (ROT) from 22.91$^\circ$ to 20.93$^\circ$, and boosts $mAP_{0.5}$ from
35.70 to 38.11. As a comparison, Cube R-CNN adds 3 other datasets together with
SUN RGB-D for monocular 3D estimation, with a similar mechanism and test
dataset, increases $IoU_{3D}$ from 36.2 to 37.8, boosts $mAP_{0.5}$ from 34.7
to 35.4. Because it operates purely through camera-space transforms, 3DRot is
readily transferable to other 3D tasks.

</details>


### [170] [Capturing More: Learning Multi-Domain Representations for Robust Online Handwriting Verification](https://arxiv.org/abs/2508.01427)
*Peirong Zhang,Kai Ding,Lianwen Jin*

Main category: cs.CV

TL;DR: SPECTRUM is a temporal-frequency model for online handwriting verification, combining temporal and frequency features for superior performance.


<details>
  <summary>Details</summary>
Motivation: To unlock the potential of multi-domain representation learning in online handwriting verification (OHV) by integrating temporal and frequency features.

Method: SPECTRUM uses a multi-scale interactor, self-gated fusion module, and multi-domain distance-based verifier to synergistically combine temporal and frequency features.

Result: Outperforms existing OHV methods, demonstrating the effectiveness of temporal-frequency multi-domain learning.

Conclusion: Multi-domain learning enhances discriminative power in OHV and opens avenues for future multi-domain research.

Abstract: In this paper, we propose SPECTRUM, a temporal-frequency synergistic model
that unlocks the untapped potential of multi-domain representation learning for
online handwriting verification (OHV). SPECTRUM comprises three core
components: (1) a multi-scale interactor that finely combines temporal and
frequency features through dual-modal sequence interaction and multi-scale
aggregation, (2) a self-gated fusion module that dynamically integrates global
temporal and frequency features via self-driven balancing. These two components
work synergistically to achieve micro-to-macro spectral-temporal integration.
(3) A multi-domain distance-based verifier then utilizes both temporal and
frequency representations to improve discrimination between genuine and forged
handwriting, surpassing conventional temporal-only approaches. Extensive
experiments demonstrate SPECTRUM's superior performance over existing OHV
methods, underscoring the effectiveness of temporal-frequency multi-domain
learning. Furthermore, we reveal that incorporating multiple handwritten
biometrics fundamentally enhances the discriminative power of handwriting
representations and facilitates verification. These findings not only validate
the efficacy of multi-domain learning in OHV but also pave the way for future
research in multi-domain approaches across both feature and biometric domains.
Code is publicly available at https://github.com/NiceRingNode/SPECTRUM.

</details>


### [171] [Hyperspectral Image Recovery Constrained by Multi-Granularity Non-Local Self-Similarity Priors](https://arxiv.org/abs/2508.01435)
*Zhuoran Peng,Yiqing Shen*

Main category: cs.CV

TL;DR: The paper proposes a multi-granularity non-local self-similarity prior model for HSI recovery, addressing limitations of fixed-format methods by alternating coarse and fine-grained tensor decompositions.


<details>
  <summary>Details</summary>
Motivation: Existing HSI recovery methods use fixed-format factors for non-local self-similarity, limiting adaptability to diverse missing scenarios.

Method: The model alternates coarse-grained (Tucker decomposition) and fine-grained (FCTN decomposition) tensor decompositions to capture global and local details.

Result: The model shows strong applicability and outstanding recovery in various missing scenarios (e.g., pixels, stripes).

Conclusion: The proposed multi-granularity approach effectively unifies global, local, and non-local priors for HSI recovery.

Abstract: Hyperspectral image (HSI) recovery, as an upstream image processing task,
  holds significant importance for downstream tasks such as classification,
  segmentation, and detection. In recent years, HSI recovery methods based on
  non-local prior representations have demonstrated outstanding performance.
However,
  these methods employ a fixed-format factor to represent the non-local
self-similarity
  tensor groups, making them unable to adapt to diverse missing scenarios. To
address
  this issue, we introduce the concept of granularity in tensor decomposition
for the first
  time and propose an HSI recovery model constrained by multi-granularity
non-local
  self-similarity priors. Specifically, the proposed model alternately performs
  coarse-grained decomposition and fine-grained decomposition on the non-local
  self-similarity tensor groups. Among them, the coarse-grained decomposition
builds
  upon Tucker tensor decomposition, which extracts global structural
information of the
  image by performing singular value shrinkage on the mode-unfolded matrices.
The
  fine-grained decomposition employs the FCTN decomposition, capturing local
detail
  information through modeling pairwise correlations among factor tensors. This
  architectural approach achieves a unified representation of global, local,
and non-local
  priors for HSIs. Experimental results demonstrate that the model has strong
  applicability and exhibits outstanding recovery effects in various types of
missing
  scenes such as pixels and stripes.

</details>


### [172] [Uncertainty-Aware Segmentation Quality Prediction via Deep Learning Bayesian Modeling: Comprehensive Evaluation and Interpretation on Skin Cancer and Liver Segmentation](https://arxiv.org/abs/2508.01460)
*Sikha O K,Meritxell Riera-Marín,Adrian Galdran,Javier García Lopez,Julia Rodríguez-Comas,Gemma Piella,Miguel A. González Ballester*

Main category: cs.CV

TL;DR: Proposes a framework for predicting segmentation quality without ground truth, using uncertainty maps and Bayesian methods, achieving high accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of assessing segmentation quality in clinical settings without manual annotations, aiming to improve model reliability.

Method: Introduces two frameworks leveraging uncertainty maps and Bayesian adaptations of SwinUNet and Feature Pyramid Network, using Monte Carlo Dropout, Ensemble, and Test Time Augmentation.

Result: Achieves high R2 scores (93.25) and Pearson correlations (96.58) on skin lesion datasets and demonstrates robustness in 3D liver segmentation.

Conclusion: The framework provides reliable segmentation quality assessment without ground truth, with interpretability via Grad-CAM and UMAP analysis.

Abstract: Image segmentation is a critical step in computational biomedical image
analysis, typically evaluated using metrics like the Dice coefficient during
training and validation. However, in clinical settings without manual
annotations, assessing segmentation quality becomes challenging, and models
lacking reliability indicators face adoption barriers. To address this gap, we
propose a novel framework for predicting segmentation quality without requiring
ground truth annotations during test time. Our approach introduces two
complementary frameworks: one leveraging predicted segmentation and uncertainty
maps, and another integrating the original input image, uncertainty maps, and
predicted segmentation maps. We present Bayesian adaptations of two benchmark
segmentation models-SwinUNet and Feature Pyramid Network with ResNet50-using
Monte Carlo Dropout, Ensemble, and Test Time Augmentation to quantify
uncertainty. We evaluate four uncertainty estimates: confidence map, entropy,
mutual information, and expected pairwise Kullback-Leibler divergence on 2D
skin lesion and 3D liver segmentation datasets, analyzing their correlation
with segmentation quality metrics. Our framework achieves an R2 score of 93.25
and Pearson correlation of 96.58 on the HAM10000 dataset, outperforming
previous segmentation quality assessment methods. For 3D liver segmentation,
Test Time Augmentation with entropy achieves an R2 score of 85.03 and a Pearson
correlation of 65.02, demonstrating cross-modality robustness. Additionally, we
propose an aggregation strategy that combines multiple uncertainty estimates
into a single score per image, offering a more robust and comprehensive
assessment of segmentation quality. Finally, we use Grad-CAM and UMAP-based
embedding analysis to interpret the model's behavior and reliability,
highlighting the impact of uncertainty integration.

</details>


### [173] [Can3Tok: Canonical 3D Tokenization and Latent Modeling of Scene-Level 3D Gaussians](https://arxiv.org/abs/2508.01464)
*Quankai Gao,Iliyan Georgiev,Tuanfeng Y. Wang,Krishna Kumar Singh,Ulrich Neumann,Jae Shin Yoon*

Main category: cs.CV

TL;DR: Can3Tok is the first 3D scene-level VAE that encodes Gaussian primitives into a low-dimensional latent embedding, addressing scale inconsistency and enabling novel scene generation.


<details>
  <summary>Details</summary>
Motivation: Existing 3D generation focuses on object-level, lacking scalable models for scene-level data due to unbounded and inconsistent 3D scenes.

Method: Introduces Can3Tok, a VAE for encoding Gaussian primitives, and a pipeline for 3D scene data processing to handle scale inconsistency.

Result: Validated on DL3DV-10K, Can3Tok generalizes to novel scenes, unlike other methods that fail to converge or generalize.

Conclusion: Can3Tok enables effective scene-level generation, demonstrated through image-to-3DGS and text-to-3DGS applications.

Abstract: 3D generation has made significant progress, however, it still largely
remains at the object-level. Feedforward 3D scene-level generation has been
rarely explored due to the lack of models capable of scaling-up latent
representation learning on 3D scene-level data. Unlike object-level generative
models, which are trained on well-labeled 3D data in a bounded canonical space,
scene-level generations with 3D scenes represented by 3D Gaussian Splatting
(3DGS) are unbounded and exhibit scale inconsistency across different scenes,
making unified latent representation learning for generative purposes extremely
challenging. In this paper, we introduce Can3Tok, the first 3D scene-level
variational autoencoder (VAE) capable of encoding a large number of Gaussian
primitives into a low-dimensional latent embedding, which effectively captures
both semantic and spatial information of the inputs. Beyond model design, we
propose a general pipeline for 3D scene data processing to address scale
inconsistency issue. We validate our method on the recent scene-level 3D
dataset DL3DV-10K, where we found that only Can3Tok successfully generalizes to
novel 3D scenes, while compared methods fail to converge on even a few hundred
scene inputs during training and exhibit zero generalization ability during
inference. Finally, we demonstrate image-to-3DGS and text-to-3DGS generation as
our applications to demonstrate its ability to facilitate downstream generation
tasks.

</details>


### [174] [EfficientGFormer: Multimodal Brain Tumor Segmentation via Pruned Graph-Augmented Transformer](https://arxiv.org/abs/2508.01465)
*Fatemeh Ziaeetabar*

Main category: cs.CV

TL;DR: EfficientGFormer integrates pretrained models with graph-based reasoning for efficient 3D brain tumor segmentation, achieving state-of-the-art accuracy with reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: Brain tumor segmentation is challenging due to tumor heterogeneity and high computational demands.

Method: Combines nnFormer for encoding MRI volumes into embeddings, a dual-edge graph for spatial and semantic relationships, and a pruned GAT for efficient reasoning. Uses distillation for real-time deployment.

Result: Outperforms baselines on MSD Task01 and BraTS 2021 datasets, with reduced memory and inference time.

Conclusion: EfficientGFormer provides a scalable, interpretable, and generalizable solution for clinical tumor segmentation.

Abstract: Accurate and efficient brain tumor segmentation remains a critical challenge
in neuroimaging due to the heterogeneous nature of tumor subregions and the
high computational cost of volumetric inference. In this paper, we propose
EfficientGFormer, a novel architecture that integrates pretrained foundation
models with graph-based reasoning and lightweight efficiency mechanisms for
robust 3D brain tumor segmentation. Our framework leverages nnFormer as a
modality-aware encoder, transforming multi-modal MRI volumes into patch-level
embeddings. These features are structured into a dual-edge graph that captures
both spatial adjacency and semantic similarity. A pruned, edge-type-aware Graph
Attention Network (GAT) enables efficient relational reasoning across tumor
subregions, while a distillation module transfers knowledge from a
full-capacity teacher to a compact student model for real-time deployment.
Experiments on the MSD Task01 and BraTS 2021 datasets demonstrate that
EfficientGFormer achieves state-of-the-art accuracy with significantly reduced
memory and inference time, outperforming recent transformer-based and
graph-based baselines. This work offers a clinically viable solution for fast
and accurate volumetric tumor delineation, combining scalability,
interpretability, and generalization.

</details>


### [175] [MiraGe: Multimodal Discriminative Representation Learning for Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2508.01525)
*Kuo Shi,Jie Lu,Shanshan Ye,Guangquan Zhang,Zhen Fang*

Main category: cs.CV

TL;DR: MiraGe introduces a method for detecting AI-generated images by learning generator-invariant features, improving generalizability across unseen models.


<details>
  <summary>Details</summary>
Motivation: Existing detectors struggle with new or unseen generative models due to overlapping features, necessitating a more robust solution.

Method: MiraGe minimizes intra-class variation and maximizes inter-class separation, using multimodal prompt learning with CLIP for better feature discriminability.

Result: MiraGe achieves state-of-the-art performance, including robustness against unseen generators like Sora.

Conclusion: The method effectively generalizes AI-generated image detection, addressing limitations of current approaches.

Abstract: Recent advances in generative models have highlighted the need for robust
detectors capable of distinguishing real images from AI-generated images. While
existing methods perform well on known generators, their performance often
declines when tested with newly emerging or unseen generative models due to
overlapping feature embeddings that hinder accurate cross-generator
classification. In this paper, we propose Multimodal Discriminative
Representation Learning for Generalizable AI-generated Image Detection
(MiraGe), a method designed to learn generator-invariant features. Motivated by
theoretical insights on intra-class variation minimization and inter-class
separation, MiraGe tightly aligns features within the same class while
maximizing separation between classes, enhancing feature discriminability.
Moreover, we apply multimodal prompt learning to further refine these
principles into CLIP, leveraging text embeddings as semantic anchors for
effective discriminative representation learning, thereby improving
generalizability. Comprehensive experiments across multiple benchmarks show
that MiraGe achieves state-of-the-art performance, maintaining robustness even
against unseen generators like Sora.

</details>


### [176] [ReasonAct: Progressive Training for Fine-Grained Video Reasoning in Small Models](https://arxiv.org/abs/2508.01533)
*Jiaxin Liu,Zhaolu Kang*

Main category: cs.CV

TL;DR: ReasonAct improves video reasoning in small models via a three-stage training process, achieving significant accuracy gains on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Small-scale multimodal models struggle with fine-grained temporal reasoning in video understanding.

Method: A three-stage training process: text-only reasoning, video fine-tuning, and temporal-aware reinforcement learning with T-GRPO and sub-action decomposition.

Result: Achieves 67.2%, 94.1%, and 78.9% accuracy on HMDB51, UCF-101, and Kinetics-400, outperforming baselines by 17.9, 15.8, and 12.3 points.

Conclusion: Progressive training enables smaller models to achieve competitive video reasoning efficiently.

Abstract: While recent multimodal models have shown progress in vision-language tasks,
small-scale variants still struggle with the fine-grained temporal reasoning
required for video understanding. We introduce ReasonAct, a method that
enhances video reasoning in smaller models through a three-stage training
process: first building a foundation with text-only reasoning, then fine-tuning
on video, and finally refining with temporal-aware reinforcement learning. We
build upon Temporal Group Relative Policy Optimization (T-GRPO) by
incorporating temporal consistency modeling into policy optimization. We also
propose a biomechanically-motivated sub-action decomposition mechanism that
provides graduated rewards for constituent action phases. Through experiments
on HMDB51, UCF-101, and Kinetics-400, our 3B-parameter model achieves 67.2%,
94.1%, and 78.9% accuracy respectively, demonstrating improvements of 17.9,
15.8, and 12.3 points over baselines. Ablation studies validate that our
progressive training methodology enables smaller models to achieve competitive
video reasoning performance while maintaining computational efficiency.

</details>


### [177] [MagicVL-2B: Empowering Vision-Language Models on Mobile Devices with Lightweight Visual Encoders via Curriculum Learning](https://arxiv.org/abs/2508.01540)
*Yi Liu,Xiao Xu,Zeyu Xu,Meng Zhang,Yibo Li,Haoyu Chen,Junkang Zhang,Qiang Wang,Jifa Sun,Siling Lin,Shengxun Cheng,Lingshu Zhang,Kang Wang*

Main category: cs.CV

TL;DR: MagicVL-2B is a lightweight Vision-Language Model optimized for smartphones, reducing power consumption by 41.1% while matching state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: The high computational and storage demands of VLMs hinder their deployment on mobile devices, which are the most accessible computing platforms.

Method: MagicVL-2B uses a lightweight visual encoder (<100M parameters), a dynamic resolution scheme, and a multimodal curriculum learning strategy to enhance performance.

Result: The model matches state-of-the-art accuracy on VLM benchmarks while reducing on-device power consumption by 41.1%.

Conclusion: MagicVL-2B is a practical solution for mobile vision-language applications, enabling advanced multimodal intelligence on smartphones.

Abstract: Vision-Language Models (VLMs) have achieved remarkable breakthroughs in
recent years, enabling a diverse array of applications in everyday life.
However, the substantial computational and storage demands of VLMs pose
significant challenges for their efficient deployment on mobile devices, which
represent the most ubiquitous and accessible computing platforms today. In this
work, we introduce MagicVL-2B, a novel VLM meticulously optimized for flagship
smartphones. MagicVL-2B leverages a lightweight visual encoder with fewer than
100M parameters and features a redesigned dynamic resolution scheme that
adaptively generates image tokens without excessive modification of image
dimensions. To further enhance the performance of this compact encoder within
VLMs, we propose a multimodal curriculum learning strategy that incrementally
increases task difficulty and data information density throughout training.
This approach substantially improves the model's performance across a variety
of sub-tasks. Extensive evaluations on standard VLM benchmarks demonstrate that
MagicVL-2B matches the accuracy of current state-of-the-art models while
reducing on-device power consumption by 41.1%. These results establish
MagicVL-2B as a practical and robust solution for real-world mobile
vision-language applications, enabling advanced multimodal intelligence to run
directly on smartphones.

</details>


### [178] [E-VRAG: Enhancing Long Video Understanding with Resource-Efficient Retrieval Augmented Generation](https://arxiv.org/abs/2508.01546)
*Zeyu Xu,Junkang Zhang,Qiang Wang,Yi Liu*

Main category: cs.CV

TL;DR: E-VRAG is a novel video RAG framework that improves efficiency and accuracy in video understanding by combining hierarchical query decomposition, lightweight VLM scoring, and multi-view question answering.


<details>
  <summary>Details</summary>
Motivation: Existing video RAG methods struggle with balancing retrieval efficiency and accuracy, especially for diverse and complex video content.

Method: E-VRAG uses frame pre-filtering, lightweight VLM scoring, global statistical distribution for retrieval, and multi-view question answering.

Result: E-VRAG reduces computational costs by ~70% and achieves higher accuracy on four benchmarks without additional training.

Conclusion: E-VRAG effectively enhances efficiency and accuracy in video RAG tasks.

Abstract: Vision-Language Models (VLMs) have enabled substantial progress in video
understanding by leveraging cross-modal reasoning capabilities. However, their
effectiveness is limited by the restricted context window and the high
computational cost required to process long videos with thousands of frames.
Retrieval-augmented generation (RAG) addresses this challenge by selecting only
the most relevant frames as input, thereby reducing the computational burden.
Nevertheless, existing video RAG methods struggle to balance retrieval
efficiency and accuracy, particularly when handling diverse and complex video
content. To address these limitations, we propose E-VRAG, a novel and efficient
video RAG framework for video understanding. We first apply a frame
pre-filtering method based on hierarchical query decomposition to eliminate
irrelevant frames, reducing computational costs at the data level. We then
employ a lightweight VLM for frame scoring, further reducing computational
costs at the model level. Additionally, we propose a frame retrieval strategy
that leverages the global statistical distribution of inter-frame scores to
mitigate the potential performance degradation from using a lightweight VLM.
Finally, we introduce a multi-view question answering scheme for the retrieved
frames, enhancing the VLM's capability to extract and comprehend information
from long video contexts. Experiments on four public benchmarks show that
E-VRAG achieves about 70% reduction in computational cost and higher accuracy
compared to baseline methods, all without additional training. These results
demonstrate the effectiveness of E-VRAG in improving both efficiency and
accuracy for video RAG tasks.

</details>


### [179] [A Glimpse to Compress: Dynamic Visual Token Pruning for Large Vision-Language Models](https://arxiv.org/abs/2508.01548)
*Quan-Sheng Zeng,Yunheng Li,Qilong Wang,Peng-Tao Jiang,Zuxuan Wu,Ming-Ming Cheng,Qibin Hou*

Main category: cs.CV

TL;DR: GlimpsePrune dynamically prunes visual tokens in LVLMs, retaining performance while reducing computational cost.


<details>
  <summary>Details</summary>
Motivation: Fixed compression ratios in LVLMs often discard informative tokens, degrading performance.

Method: Introduces GlimpsePrune, a dynamic pruning framework inspired by human cognition, to prune irrelevant tokens in one forward pass.

Result: Prunes 92.6% of tokens while maintaining baseline performance; enhanced version achieves 110% performance.

Conclusion: GlimpsePrune offers a more efficient and powerful approach for LVLMs.

Abstract: Visual token compression is critical for Large Vision-Language Models (LVLMs)
to efficiently process high-resolution inputs. Existing methods that typically
adopt fixed compression ratios cannot adapt to scenes of varying complexity,
often causing imprecise pruning that discards informative visual tokens and
results in degraded model performance. To address this issue, we introduce a
dynamic pruning framework, GlimpsePrune, inspired by human cognition. It takes
a data-driven ''glimpse'' and prunes irrelevant visual tokens in a single
forward pass before answer generation. This approach prunes 92.6% of visual
tokens while on average fully retaining the baseline performance on free-form
VQA tasks. The reduced computational cost also enables more effective
fine-tuning: an enhanced GlimpsePrune+ achieves 110% of the baseline
performance while maintaining a similarly high pruning rate. Our work paves a
new way for building more powerful and efficient LVLMs.

</details>


### [180] [EvoVLMA: Evolutionary Vision-Language Model Adaptation](https://arxiv.org/abs/2508.01558)
*Kun Ding,Ying Wang,Shiming Xiang*

Main category: cs.CV

TL;DR: EvoVLMA automates the search for efficient training-free adaptation algorithms for Vision-Language Models using LLM-assisted evolutionary methods, improving performance over manual designs.


<details>
  <summary>Details</summary>
Motivation: Existing adaptation methods for VLMs are manually designed, requiring expertise and time. Automating this process can enhance efficiency and performance.

Method: A two-stage LLM-assisted evolutionary algorithm optimizes feature selection and logits computation, using divide-and-conquer to manage the search space. Low-precision code conversion and web-based execution improve stability.

Result: EvoVLMA improves the APE algorithm by 1.91 points in 8-shot image classification accuracy.

Conclusion: EvoVLMA demonstrates the potential for automating adaptation algorithm design for multimodal models, offering better performance than manual methods.

Abstract: Pre-trained Vision-Language Models (VLMs) have been exploited in various
Computer Vision tasks (e.g., few-shot recognition) via model adaptation, such
as prompt tuning and adapters. However, existing adaptation methods are
designed by human experts, requiring significant time cost and experience.
Inspired by recent advances in Large Language Models (LLMs) based code
generation, we propose an Evolutionary Vision-Language Model Adaptation
(EvoVLMA) method to automatically search training-free efficient adaptation
algorithms for VLMs. We recognize feature selection and logits computation as
the key functions in training-free VLM adaptation, and propose a two-stage
LLM-assisted evolutionary algorithm for optimizing these parts in a sequential
manner, effectively addressing the challenge posed by the expansive search
space through a divide-and-conquer strategy. Besides, to enhance the stability
and efficiency of searching process, we propose low-precision code conversion,
web based code execution and process monitoring, leading to a highly effective
automatic algorithm design system. Extensive experiments demonstrate that the
algorithms found by EvoVLMA can obtain promising results compared to previous
manually-designed ones. More specifically, in the 8-shot image classification
setting, the classical APE algorithm can be improved by 1.91 points in
recognition accuracy. This research opens new possibilities for automating the
optimization of adaptation algorithms of pre-trained multimodal models. Code is
available at: https://github.com/kding1225/EvoVLMA

</details>


### [181] [Adaptive LiDAR Scanning: Harnessing Temporal Cues for Efficient 3D Object Detection via Multi-Modal Fusion](https://arxiv.org/abs/2508.01562)
*Sara Shoouri,Morteza Tavakoli Taba,Hun-Seok Kim*

Main category: cs.CV

TL;DR: A predictive, history-aware adaptive scanning framework reduces LiDAR energy consumption by over 65% while maintaining strong 3D object detection performance.


<details>
  <summary>Details</summary>
Motivation: Conventional LiDAR sensors ignore temporal continuity, causing redundancy and high power consumption, limiting practicality on resource-constrained platforms.

Method: A lightweight predictor network uses historical data to identify ROIs, guiding a Mask Generator for adaptive scanning with Gumbel-Softmax sampling.

Result: Reduces LiDAR energy consumption by over 65% while maintaining competitive 3D object detection performance on nuScenes and Lyft benchmarks.

Conclusion: The proposed framework efficiently balances energy savings and detection accuracy, enhancing practicality for resource-limited platforms.

Abstract: Multi-sensor fusion using LiDAR and RGB cameras significantly enhances 3D
object detection task. However, conventional LiDAR sensors perform dense,
stateless scans, ignoring the strong temporal continuity in real-world scenes.
This leads to substantial sensing redundancy and excessive power consumption,
limiting their practicality on resource-constrained platforms. To address this
inefficiency, we propose a predictive, history-aware adaptive scanning
framework that anticipates informative regions of interest (ROI) based on past
observations. Our approach introduces a lightweight predictor network that
distills historical spatial and temporal contexts into refined query
embeddings. These embeddings guide a differentiable Mask Generator network,
which leverages Gumbel-Softmax sampling to produce binary masks identifying
critical ROIs for the upcoming frame. Our method significantly reduces
unnecessary data acquisition by concentrating dense LiDAR scanning only within
these ROIs and sparsely sampling elsewhere. Experiments on nuScenes and Lyft
benchmarks demonstrate that our adaptive scanning strategy reduces LiDAR energy
consumption by over 65% while maintaining competitive or even superior 3D
object detection performance compared to traditional LiDAR-camera fusion
methods with dense LiDAR scanning.

</details>


### [182] [LetheViT: Selective Machine Unlearning for Vision Transformers via Attention-Guided Contrastive Learning](https://arxiv.org/abs/2508.01569)
*Yujia Tong,Tian Zhang,Jingling Yuan,Yuze Wang,Chuang Hu*

Main category: cs.CV

TL;DR: LetheViT is a contrastive unlearning method for Vision Transformers (ViTs) that addresses privacy regulations by enabling selective forgetting of specific data while maintaining model performance.


<details>
  <summary>Details</summary>
Motivation: Privacy regulations like GDPR and CCPA require models to forget user data upon request, posing challenges for ViTs. Machine unlearning is needed to remove data influence without retraining.

Method: LetheViT uses masked images (positive logits) and original images (negative logits) to guide the model to forget specific details while retaining general class outlines.

Result: LetheViT achieves state-of-the-art performance, balancing privacy compliance and model efficacy.

Conclusion: LetheViT effectively addresses the challenge of random data forgetting in ViTs, ensuring privacy compliance without sacrificing performance.

Abstract: Vision Transformers (ViTs) have revolutionized computer vision tasks with
their exceptional performance. However, the introduction of privacy regulations
such as GDPR and CCPA has brought new challenges to them. These laws grant
users the right to withdraw their data, necessitating not only the deletion of
data but also the complete removal of its influence from trained models.
Machine unlearning emerges as a critical solution, with exact unlearning being
computationally prohibitive and approximate methods offering a more practical
approach. This work addresses the particularly challenging scenario of random
data forgetting in ViTs, where the model must forget specific samples while
retaining others, even within the same class. We first reveal the core
characteristics of ViTs through selective masking experiments: when
high-attention areas are masked, the model retains its recognition capability
but significantly weakens its memorization ability. Based on the above
insights, we propose LetheViT, a contrastive unlearning method tailored for
ViTs. LetheViT uses masked image inputs to generate positive logits and
original image inputs to generate negative logits, guiding the model to forget
specific details while retaining the general cl category outlines. Experimental
results demonstrate that LetheViT achieves state-of-the-art performance,
effectively balancing privacy compliance with model efficacy.

</details>


### [183] [TopoImages: Incorporating Local Topology Encoding into Deep Learning Models for Medical Image Classification](https://arxiv.org/abs/2508.01574)
*Pengfei Gu,Hongxiao Wang,Yejia Zhang,Huimin Li,Chaoli Wang,Danny Chen*

Main category: cs.CV

TL;DR: The paper introduces TopoImages, a method to encode local topological features of image patches using persistent homology, enhancing deep learning frameworks for improved image classification.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods often lack sensitivity to topological structures in images, which are crucial for understanding content like biomedical objects.

Method: The approach computes persistence diagrams of image patches, vectorizes them into TopoImages, and generates multi-view TopoImages using various filtration functions for enriched representation.

Result: Experiments on medical image datasets show improved classification accuracy over state-of-the-art methods.

Conclusion: TopoImages provide a versatile and effective way to integrate topological information into deep learning frameworks, enhancing performance.

Abstract: Topological structures in image data, such as connected components and loops,
play a crucial role in understanding image content (e.g., biomedical objects).
% Despite remarkable successes of numerous image processing methods that rely
on appearance information, these methods often lack sensitivity to topological
structures when used in general deep learning (DL) frameworks. % In this paper,
we introduce a new general approach, called TopoImages (for Topology Images),
which computes a new representation of input images by encoding local topology
of patches. % In TopoImages, we leverage persistent homology (PH) to encode
geometric and topological features inherent in image patches. % Our main
objective is to capture topological information in local patches of an input
image into a vectorized form. % Specifically, we first compute persistence
diagrams (PDs) of the patches, % and then vectorize and arrange these PDs into
long vectors for pixels of the patches. % The resulting multi-channel
image-form representation is called a TopoImage. % TopoImages offers a new
perspective for data analysis. % To garner diverse and significant topological
features in image data and ensure a more comprehensive and enriched
representation, we further generate multiple TopoImages of the input image
using various filtration functions, which we call multi-view TopoImages. % The
multi-view TopoImages are fused with the input image for DL-based
classification, with considerable improvement. % Our TopoImages approach is
highly versatile and can be seamlessly integrated into common DL frameworks.
Experiments on three public medical image classification datasets demonstrate
noticeably improved accuracy over state-of-the-art methods.

</details>


### [184] [Harnessing Textual Semantic Priors for Knowledge Transfer and Refinement in CLIP-Driven Continual Learning](https://arxiv.org/abs/2508.01579)
*Lingfeng He,De Cheng,Huaijie Wang,Nannan Wang*

Main category: cs.CV

TL;DR: SECA leverages CLIP's textual priors for continual learning, addressing stability-plasticity issues with semantic-guided knowledge transfer and visual prototype refinement.


<details>
  <summary>Details</summary>
Motivation: Current CL methods overlook semantic relevance in knowledge transfer and face modality gaps in CLIP, limiting plasticity and stability.

Method: Proposes SECA with SG-AKT for semantic-aware knowledge transfer and SE-VPR for refining visual prototypes using textual embeddings.

Result: Extensive experiments show SECA's effectiveness in balancing stability and plasticity.

Conclusion: SECA successfully integrates textual and visual semantics for improved continual learning performance.

Abstract: Continual learning (CL) aims to equip models with the ability to learn from a
stream of tasks without forgetting previous knowledge. With the progress of
vision-language models like Contrastive Language-Image Pre-training (CLIP),
their promise for CL has attracted increasing attention due to their strong
generalizability. However, the potential of rich textual semantic priors in
CLIP in addressing the stability-plasticity dilemma remains underexplored.
During backbone training, most approaches transfer past knowledge without
considering semantic relevance, leading to interference from unrelated tasks
that disrupt the balance between stability and plasticity. Besides, while
text-based classifiers provide strong generalization, they suffer from limited
plasticity due to the inherent modality gap in CLIP. Visual classifiers help
bridge this gap, but their prototypes lack rich and precise semantics. To
address these challenges, we propose Semantic-Enriched Continual Adaptation
(SECA), a unified framework that harnesses the anti-forgetting and structured
nature of textual priors to guide semantic-aware knowledge transfer in the
backbone and reinforce the semantic structure of the visual classifier.
Specifically, a Semantic-Guided Adaptive Knowledge Transfer (SG-AKT) module is
proposed to assess new images' relevance to diverse historical visual knowledge
via textual cues, and aggregate relevant knowledge in an instance-adaptive
manner as distillation signals. Moreover, a Semantic-Enhanced Visual Prototype
Refinement (SE-VPR) module is introduced to refine visual prototypes using
inter-class semantic relations captured in class-wise textual embeddings.
Extensive experiments on multiple benchmarks validate the effectiveness of our
approach.

</details>


### [185] [Set Pivot Learning: Redefining Generalized Segmentation with Vision Foundation Models](https://arxiv.org/abs/2508.01582)
*Xinhui Li,Xinyu He,Qiming Hu,Xiaojie Guo*

Main category: cs.CV

TL;DR: The paper introduces Set Pivot Learning (SPL), a new paradigm for domain generalization using Vision Foundation Models (VFMs), focusing on dynamic adaptation and VFM-centric tuning.


<details>
  <summary>Details</summary>
Motivation: Traditional domain generalization assumes inaccessible target domains, but VFMs challenge this. SPL addresses this gap by redefining domain migration tasks.

Method: SPL emphasizes dynamic adaptation and VFM-centric tuning, proposing Dynamic Prompt Fine-Tuning with a Dynamic Class-aware Prompter and Prompt-guided Feature Focuser.

Result: Experiments show SPL's superiority over state-of-the-art methods, especially in generalized segmentation.

Conclusion: SPL offers a more adaptive and robust approach to domain generalization, aligning better with real-world conditions.

Abstract: In this paper, we introduce, for the first time, the concept of Set Pivot
Learning, a paradigm shift that redefines domain generalization (DG) based on
Vision Foundation Models (VFMs). Traditional DG assumes that the target domain
is inaccessible during training, but the emergence of VFMs, trained on vast and
diverse data, renders this assumption unclear and obsolete. Traditional DG
assumes that the target domain is inaccessible during training, but the
emergence of VFMs, which are trained on vast and diverse datasets, renders this
assumption unclear and obsolete. To address this challenge, we propose Set
Pivot Learning (SPL), a new definition of domain migration task based on VFMs,
which is more suitable for current research and application requirements.
Unlike conventional DG methods, SPL prioritizes adaptive refinement over rigid
domain transfer, ensuring continuous alignment with evolving real-world
conditions. Specifically, SPL features two key attributes: (i) Dynamic
adaptation, transitioning from static domain alignment to flexible, task-driven
feature optimization, enabling models to evolve with downstream scenarios; (ii)
VFM-centric tuning, leveraging pretrained knowledge as a pivot to hone
task-specific representations while preserving cross-domain robustness.
Building on SPL, we propose a Dynamic Prompt Fine-Tuning method, which combines
a Dynamic Class-aware Prompter with a Prompt-guided Feature Focuser, to elevate
VFM performance in targeted scenarios. Extensive experiments on benchmark
datasets show the effectiveness of our method, highlighting its superiority
over state-of-the-art methods, particularly in generalized segmentation.

</details>


### [186] [A Spatio-temporal Continuous Network for Stochastic 3D Human Motion Prediction](https://arxiv.org/abs/2508.01585)
*Hua Yu,Yaqing Hou,Xu Gui,Shanshan Feng,Dongsheng Zhou,Qiang Zhang*

Main category: cs.CV

TL;DR: Proposes STCN, a two-stage method for stochastic human motion prediction, addressing mode collapse and improving motion diversity and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with continuous temporal dynamics and stochastic motion prediction, often leading to mode collapse and ignoring motion flexibility.

Method: STCN uses a spatio-temporal continuous network for smoother sequences and introduces an anchor set to prevent mode collapse. It learns a GMM of motion sequences and samples multiple sequences per anchor to reduce intra-class differences.

Result: Competitive performance on diversity and accuracy is achieved on Human3.6M and HumanEva-I datasets.

Conclusion: STCN effectively addresses challenges in stochastic human motion prediction, offering improved flexibility and reduced mode collapse.

Abstract: Stochastic Human Motion Prediction (HMP) has received increasing attention
due to its wide applications. Despite the rapid progress in generative fields,
existing methods often face challenges in learning continuous temporal dynamics
and predicting stochastic motion sequences. They tend to overlook the
flexibility inherent in complex human motions and are prone to mode collapse.
To alleviate these issues, we propose a novel method called STCN, for
stochastic and continuous human motion prediction, which consists of two
stages. Specifically, in the first stage, we propose a spatio-temporal
continuous network to generate smoother human motion sequences. In addition,
the anchor set is innovatively introduced into the stochastic HMP task to
prevent mode collapse, which refers to the potential human motion patterns. In
the second stage, STCN endeavors to acquire the Gaussian mixture distribution
(GMM) of observed motion sequences with the aid of the anchor set. It also
focuses on the probability associated with each anchor, and employs the
strategy of sampling multiple sequences from each anchor to alleviate
intra-class differences in human motions. Experimental results on two
widely-used datasets (Human3.6M and HumanEva-I) demonstrate that our model
obtains competitive performance on both diversity and accuracy.

</details>


### [187] [Lifelong Person Re-identification via Privacy-Preserving Data Replay](https://arxiv.org/abs/2508.01587)
*Mingyu Wang,Haojie Liu,Zhiyong Li,Wei Jiang*

Main category: cs.CV

TL;DR: The paper introduces Privacy-Preserving Replay (Pr²R), a method for lifelong person re-identification that condenses sequential data into pixel-space replay samples, preserving privacy while improving performance.


<details>
  <summary>Details</summary>
Motivation: Addressing privacy concerns in replay-based lifelong person re-identification by avoiding raw data storage and mitigating performance degradation in exemplar-free methods.

Method: Condenses training characteristics of multiple images into single pixel-space samples, using a dual-alignment strategy for style replay to align domains and adapt samples.

Result: Pr²R achieves 4% and 6% higher accuracy on sequential tasks compared to state-of-the-art and replay-based methods, respectively.

Conclusion: Pr²R effectively balances privacy preservation and performance in lifelong person re-identification, outperforming existing methods.

Abstract: Lifelong person re-identification (LReID) aims to incrementally accumulate
knowledge across a sequence of tasks under domain shifts. Recently,
replay-based methods have demonstrated strong effectiveness in LReID by
rehearsing past samples stored in an auxiliary memory. However, storing
historical exemplars raises concerns over data privacy. To avoid this,
exemplar-free approaches attempt to match the distribution of past data without
storing raw samples. Despite being privacy-friendly, these methods often suffer
from performance degradation due to the forgetting of specific past knowledge
representations. To this end, we propose to condense information from
sequential data into the pixel space in the replay memory, enabling
Privacy-Preserving Replay (Pr^2R). More specifically, by distilling the
training characteristics of multiple real images into a single image, the
condensed samples undergo pixel-level changes. This not only protects the
privacy of the original data but also makes the replay samples more
representative for sequential tasks. During the style replay phase, we align
the current domain to the previous one while simultaneously adapting the replay
samples to match the style of the current domain. This dual-alignment strategy
effectively mitigates both class-incremental challenges and forgetting caused
by domain shifts. Extensive experiments on multiple benchmarks show that the
proposed method significantly improves replay effectiveness while preserving
data privacy. Specifically, Pr^2R achieves 4% and 6% higher accuracy on
sequential tasks compared to the current state-of-the-art and other
replay-based methods, respectively.

</details>


### [188] [Self-Navigated Residual Mamba for Universal Industrial Anomaly Detection](https://arxiv.org/abs/2508.01591)
*Hanxi Li,Jingqi Wu,Lin Yuanbo Wu,Mingliang Li,Deyin Liu,Jialie Shen,Chunhua Shen*

Main category: cs.CV

TL;DR: SNARM is a novel framework for industrial anomaly detection using self-referential learning and dynamic reference selection, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: To improve anomaly detection by dynamically refining detection using in-image references, unlike static pre-trained methods.

Method: Computes inter- and intra-residual features, uses a Mamba module with dynamic navigation, and aggregates results via ensemble learning.

Result: Achieves SOTA performance on MVTec AD, MVTec 3D, and VisA benchmarks with improved metrics.

Conclusion: SNARM enhances anomaly detection by leveraging self-referential learning and dynamic feature refinement.

Abstract: In this paper, we propose Self-Navigated Residual Mamba (SNARM), a novel
framework for universal industrial anomaly detection that leverages
``self-referential learning'' within test images to enhance anomaly
discrimination. Unlike conventional methods that depend solely on pre-trained
features from normal training data, SNARM dynamically refines anomaly detection
by iteratively comparing test patches against adaptively selected in-image
references. Specifically, we first compute the ``inter-residuals'' features by
contrasting test image patches with the training feature bank. Patches
exhibiting small-norm residuals (indicating high normality) are then utilized
as self-generated reference patches to compute ``intra-residuals'', amplifying
discriminative signals. These inter- and intra-residual features are
concatenated and fed into a novel Mamba module with multiple heads, which are
dynamically navigated by residual properties to focus on anomalous regions.
Finally, AD results are obtained by aggregating the outputs of a self-navigated
Mamba in an ensemble learning paradigm. Extensive experiments on MVTec AD,
MVTec 3D, and VisA benchmarks demonstrate that SNARM achieves state-of-the-art
(SOTA) performance, with notable improvements in all metrics, including
Image-AUROC, Pixel-AURC, PRO, and AP.

</details>


### [189] [DMTrack: Spatio-Temporal Multimodal Tracking via Dual-Adapter](https://arxiv.org/abs/2508.01592)
*Weihong Li,Shaohua Dong,Haonan Lu,Yanhao Zhang,Heng Fan,Libo Zhang*

Main category: cs.CV

TL;DR: DMTrack introduces a dual-adapter architecture for spatio-temporal multimodal tracking, achieving state-of-the-art results with minimal trainable parameters.


<details>
  <summary>Details</summary>
Motivation: To improve cross-modality fusion and tracking performance in spatio-temporal multimodal tasks.

Method: Uses two modules: STMA for self-prompting spatio-temporal features and PMCA for progressive cross-modality prompting with shallow and deep adapters.

Result: Achieves top performance on five benchmarks with only 0.93M trainable parameters.

Conclusion: DMTrack is effective for spatio-temporal multimodal tracking, offering a lightweight yet powerful solution.

Abstract: In this paper, we explore adapter tuning and introduce a novel dual-adapter
architecture for spatio-temporal multimodal tracking, dubbed DMTrack. The key
of our DMTrack lies in two simple yet effective modules, including a
spatio-temporal modality adapter (STMA) and a progressive modality
complementary adapter (PMCA) module. The former, applied to each modality
alone, aims to adjust spatio-temporal features extracted from a frozen backbone
by self-prompting, which to some extent can bridge the gap between different
modalities and thus allows better cross-modality fusion. The latter seeks to
facilitate cross-modality prompting progressively with two specially designed
pixel-wise shallow and deep adapters. The shallow adapter employs shared
parameters between the two modalities, aiming to bridge the information flow
between the two modality branches, thereby laying the foundation for following
modality fusion, while the deep adapter modulates the preliminarily fused
information flow with pixel-wise inner-modal attention and further generates
modality-aware prompts through pixel-wise inter-modal attention. With such
designs, DMTrack achieves promising spatio-temporal multimodal tracking
performance with merely \textbf{0.93M} trainable parameters. Extensive
experiments on five benchmarks show that DMTrack achieves state-of-the-art
results. Code will be available.

</details>


### [190] [CLIMD: A Curriculum Learning Framework for Imbalanced Multimodal Diagnosis](https://arxiv.org/abs/2508.01594)
*Kai Han,Chongwen Lyu,Lele Ma,Chengxuan Qian,Siqi Ma,Zheng Pang,Jun Chen,Zhe Liu*

Main category: cs.CV

TL;DR: The paper proposes CLIMD, a curriculum learning framework for imbalanced multimodal diagnosis, addressing class imbalance and cross-modal interactions in medical data.


<details>
  <summary>Details</summary>
Motivation: Class imbalance in multimodal medical data hinders accurate diagnosis, and existing methods like resampling or loss reweighting often lead to overfitting or underfitting.

Method: CLIMD uses a multimodal curriculum measurer (intra-modal confidence and inter-modal complementarity) and a class distribution-guided training scheduler to adapt to imbalanced data.

Result: CLIMD outperforms state-of-the-art methods on multiple datasets and handles imbalanced data effectively.

Conclusion: CLIMD is a plug-and-play framework that improves multimodal disease diagnosis accuracy and can be integrated into other models.

Abstract: Clinicians usually combine information from multiple sources to achieve the
most accurate diagnosis, and this has sparked increasing interest in leveraging
multimodal deep learning for diagnosis. However, in real clinical scenarios,
due to differences in incidence rates, multimodal medical data commonly face
the issue of class imbalance, which makes it difficult to adequately learn the
features of minority classes. Most existing methods tackle this issue with
resampling or loss reweighting, but they are prone to overfitting or
underfitting and fail to capture cross-modal interactions. Therefore, we
propose a Curriculum Learning framework for Imbalanced Multimodal Diagnosis
(CLIMD). Specifically, we first design multimodal curriculum measurer that
combines two indicators, intra-modal confidence and inter-modal
complementarity, to enable the model to focus on key samples and gradually
adapt to complex category distributions. Additionally, a class
distribution-guided training scheduler is introduced, which enables the model
to progressively adapt to the imbalanced class distribution during training.
Extensive experiments on multiple multimodal medical datasets demonstrate that
the proposed method outperforms state-of-the-art approaches across various
metrics and excels in handling imbalanced multimodal medical data. Furthermore,
as a plug-and-play CL framework, CLIMD can be easily integrated into other
models, offering a promising path for improving multimodal disease diagnosis
accuracy. Code is publicly available at https://github.com/KHan-UJS/CLIMD.

</details>


### [191] [Enhancing Zero-Shot Brain Tumor Subtype Classification via Fine-Grained Patch-Text Alignment](https://arxiv.org/abs/2508.01602)
*Lubin Gan,Jing Zhang,Linhao Qu,Yijun Wang,Siying Wu,Xiaoyan Sun*

Main category: cs.CV

TL;DR: FG-PAN, a zero-shot framework for fine-grained brain tumor classification, improves subtype discrimination by aligning refined visual features with LLM-generated semantic prototypes.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in subtle morphological variations and limited annotated data for brain tumor subtype classification, with existing vision-language models lacking fine-grained feature capture.

Method: FG-PAN includes a local feature refinement module for patch-level visual enhancement and a text description generation module using LLMs for semantic prototypes.

Result: FG-PAN achieves state-of-the-art performance and robust generalization in zero-shot classification on datasets like EBRAINS and TCGA.

Conclusion: FG-PAN effectively addresses fine-grained classification challenges in digital pathology, offering improved separability and generalization.

Abstract: The fine-grained classification of brain tumor subtypes from
histopathological whole slide images is highly challenging due to subtle
morphological variations and the scarcity of annotated data. Although
vision-language models have enabled promising zero-shot classification, their
ability to capture fine-grained pathological features remains limited,
resulting in suboptimal subtype discrimination. To address these challenges, we
propose the Fine-Grained Patch Alignment Network (FG-PAN), a novel zero-shot
framework tailored for digital pathology. FG-PAN consists of two key modules:
(1) a local feature refinement module that enhances patch-level visual features
by modeling spatial relationships among representative patches, and (2) a
fine-grained text description generation module that leverages large language
models to produce pathology-aware, class-specific semantic prototypes. By
aligning refined visual features with LLM-generated fine-grained descriptions,
FG-PAN effectively increases class separability in both visual and semantic
spaces. Extensive experiments on multiple public pathology datasets, including
EBRAINS and TCGA, demonstrate that FG-PAN achieves state-of-the-art performance
and robust generalization in zero-shot brain tumor subtype classification.

</details>


### [192] [Towards Generalizable AI-Generated Image Detection via Image-Adaptive Prompt Learning](https://arxiv.org/abs/2508.01603)
*Yiheng Li,Zichang Tan,Zhen Lei,Xu Zhou,Yang Yang*

Main category: cs.CV

TL;DR: Proposes Image-Adaptive Prompt Learning (IAPL) for detecting AI-generated images, enhancing adaptability to unseen generators with two adaptive modules.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to generalize to unknown image generators; IAPL aims to improve flexibility and detection accuracy.

Method: Uses Conditional Information Learner and Confidence-Driven Adaptive Prediction to adjust prompts dynamically based on input images.

Result: Achieves 95.61% and 96.7% mean accuracy on UniversalFakeDetect and GenImage datasets.

Conclusion: IAPL outperforms existing methods, offering robust detection of diverse AI-generated images.

Abstract: A major struggle for AI-generated image detection is identifying fake images
from unseen generators. Existing cutting-edge methods typically customize
pre-trained foundation models to this task via partial-parameter fine-tuning.
However, these parameters trained on a narrow range of generators may fail to
generalize to unknown sources. In light of this, we propose a novel framework
named Image-Adaptive Prompt Learning (IAPL), which enhances flexibility in
processing diverse testing images. It consists of two adaptive modules, i.e.,
the Conditional Information Learner and the Confidence-Driven Adaptive
Prediction. The former employs CNN-based feature extractors to learn
forgery-specific and image-specific conditions, which are then propagated to
learnable tokens via a gated mechanism. The latter optimizes the shallowest
learnable tokens based on a single test sample and selects the cropped view
with the highest prediction confidence for final detection. These two modules
enable the prompts fed into the foundation model to be automatically adjusted
based on the input image, rather than being fixed after training, thereby
enhancing the model's adaptability to various forged images. Extensive
experiments show that IAPL achieves state-of-the-art performance, with 95.61%
and 96.7% mean accuracy on two widely used UniversalFakeDetect and GenImage
datasets, respectively.

</details>


### [193] [From Pixels to Places: A Systematic Benchmark for Evaluating Image Geolocalization Ability in Large Language Models](https://arxiv.org/abs/2508.01608)
*Lingyao Li,Runlong Yu,Qikai Hu,Bowei Li,Min Deng,Yang Zhou,Xiaowei Jia*

Main category: cs.CV

TL;DR: The paper introduces IMAGEO-Bench, a benchmark to evaluate LLMs for image geolocalization, revealing performance disparities and geospatial biases.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' capabilities in image geolocalization, crucial for applications like crisis response and digital forensics.

Method: Systematic evaluation using three diverse datasets and 10 state-of-the-art LLMs, analyzing accuracy, distance error, bias, and reasoning.

Result: Closed-source models outperform open-source ones, with biases favoring high-resource regions. Success depends on recognizing urban settings and landmarks.

Conclusion: IMAGEO-Bench highlights LLMs' spatial reasoning limitations and biases, guiding future geolocation-aware AI development.

Abstract: Image geolocalization, the task of identifying the geographic location
depicted in an image, is important for applications in crisis response, digital
forensics, and location-based intelligence. While recent advances in large
language models (LLMs) offer new opportunities for visual reasoning, their
ability to perform image geolocalization remains underexplored. In this study,
we introduce a benchmark called IMAGEO-Bench that systematically evaluates
accuracy, distance error, geospatial bias, and reasoning process. Our benchmark
includes three diverse datasets covering global street scenes, points of
interest (POIs) in the United States, and a private collection of unseen
images. Through experiments on 10 state-of-the-art LLMs, including both open-
and closed-source models, we reveal clear performance disparities, with
closed-source models generally showing stronger reasoning. Importantly, we
uncover geospatial biases as LLMs tend to perform better in high-resource
regions (e.g., North America, Western Europe, and California) while exhibiting
degraded performance in underrepresented areas. Regression diagnostics
demonstrate that successful geolocalization is primarily dependent on
recognizing urban settings, outdoor environments, street-level imagery, and
identifiable landmarks. Overall, IMAGEO-Bench provides a rigorous lens into the
spatial reasoning capabilities of LLMs and offers implications for building
geolocation-aware AI systems.

</details>


### [194] [LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical Image Understanding](https://arxiv.org/abs/2508.01617)
*Xuanzhao Dong,Wenhui Zhu,Xiwen Chen,Zhipeng Wang,Peijie Qiu,Shao Tang,Xin Li,Yalin Wang*

Main category: cs.CV

TL;DR: LLaDA-MedV is a new biomedical vision-language model using masked diffusion, outperforming ARMs and setting state-of-the-art results in biomedical VQA tasks.


<details>
  <summary>Details</summary>
Motivation: To explore masked diffusion models in biomedical VLMs, where ARMs dominate, and improve performance in biomedical image understanding.

Method: LLaDA-MedV employs vision instruction tuning and diffusion models, with detailed analysis of training and inference stages.

Result: Achieves 7.855% gain over LLaVA-Med and 1.867% over LLaDA-V, with top accuracy on VQA-RAD (84.93%), SLAKE (92.31%), and PathVQA (95.15%).

Conclusion: LLaDA-MedV demonstrates the potential of diffusion models in biomedical VLMs, offering longer, more informative responses and superior performance.

Abstract: Autoregressive models (ARMs) have long dominated the landscape of biomedical
vision-language models (VLMs). Recently, masked diffusion models such as LLaDA
have emerged as promising alternatives, yet their application in the biomedical
domain remains largely underexplored. To bridge this gap, we introduce
\textbf{LLaDA-MedV}, the first large language diffusion model tailored for
biomedical image understanding through vision instruction tuning. LLaDA-MedV
achieves relative performance gains of 7.855\% over LLaVA-Med and 1.867\% over
LLaDA-V in the open-ended biomedical visual conversation task, and sets new
state-of-the-art accuracy on the closed-form subset of three VQA benchmarks:
84.93\% on VQA-RAD, 92.31\% on SLAKE, and 95.15\% on PathVQA. Furthermore, a
detailed comparison with LLaVA-Med suggests that LLaDA-MedV is capable of
generating reasonably longer responses by explicitly controlling response
length, which can lead to more informative outputs. We also conduct an in-depth
analysis of both the training and inference stages, highlighting the critical
roles of initialization weight selection, fine-tuning strategies, and the
interplay between sampling steps and response repetition. The code and model
weight is released at https://github.com/LLM-VLM-GSL/LLaDA-MedV.

</details>


### [195] [Rate-distortion Optimized Point Cloud Preprocessing for Geometry-based Point Cloud Compression](https://arxiv.org/abs/2508.01633)
*Wanhao Ma,Wei Zhang,Shuai Wan,Fuzheng Yang*

Main category: cs.CV

TL;DR: A novel preprocessing framework enhances G-PCC efficiency by integrating a voxelization network and a differentiable surrogate model, achieving a 38.84% BD-rate reduction without modifying the decoder.


<details>
  <summary>Details</summary>
Motivation: G-PCC underperforms compared to deep learning-based methods despite lower computational power. The goal is to improve G-PCC efficiency while maintaining interoperability and flexibility.

Method: Proposes a compression-oriented voxelization network and a differentiable G-PCC surrogate model, jointly optimized during training. The surrogate mimics G-PCC's rate-distortion behavior for end-to-end gradient propagation.

Result: Achieves a 38.84% average BD-rate reduction over G-PCC, with no computational overhead for end users.

Conclusion: The framework bridges classical codecs with deep learning, enhancing legacy standards while preserving backward compatibility, ideal for real-world deployment.

Abstract: Geometry-based point cloud compression (G-PCC), an international standard
designed by MPEG, provides a generic framework for compressing diverse types of
point clouds while ensuring interoperability across applications and devices.
However, G-PCC underperforms compared to recent deep learning-based PCC methods
despite its lower computational power consumption. To enhance the efficiency of
G-PCC without sacrificing its interoperability or computational flexibility, we
propose a novel preprocessing framework that integrates a compression-oriented
voxelization network with a differentiable G-PCC surrogate model, jointly
optimized in the training phase. The surrogate model mimics the rate-distortion
behaviour of the non-differentiable G-PCC codec, enabling end-to-end gradient
propagation. The versatile voxelization network adaptively transforms input
point clouds using learning-based voxelization and effectively manipulates
point clouds via global scaling, fine-grained pruning, and point-level editing
for rate-distortion trade-offs. During inference, only the lightweight
voxelization network is appended to the G-PCC encoder, requiring no
modifications to the decoder, thus introducing no computational overhead for
end users. Extensive experiments demonstrate a 38.84% average BD-rate reduction
over G-PCC. By bridging classical codecs with deep learning, this work offers a
practical pathway to enhance legacy compression standards while preserving
their backward compatibility, making it ideal for real-world deployment.

</details>


### [196] [Glass Surface Segmentation with an RGB-D Camera via Weighted Feature Fusion for Service Robots](https://arxiv.org/abs/2508.01639)
*Henghong Lin,Zihan Zhu,Tao Wang,Anastasia Ioannou,Yuanshui Huang*

Main category: cs.CV

TL;DR: The paper introduces a Weighted Feature Fusion (WFF) module for RGB-D glass surface segmentation, improving accuracy and robustness, and presents the MJU-Glass dataset for benchmarking.


<details>
  <summary>Details</summary>
Motivation: Glass surfaces pose challenges like transparency and reflections, requiring effective fusion of RGB and depth data for accurate segmentation in robotics.

Method: Proposes a WFF module to dynamically combine RGB and depth features, compatible with various deep neural networks, and introduces the MJU-Glass dataset.

Result: WFF improves segmentation, achieving a 7.49% boost in boundary IoU with PSPNet, and the dataset aids benchmarking.

Conclusion: The WFF module and MJU-Glass dataset enhance glass segmentation, reducing collision risks in robotics.

Abstract: We address the problem of glass surface segmentation with an RGB-D camera,
with a focus on effectively fusing RGB and depth information. To this end, we
propose a Weighted Feature Fusion (WFF) module that dynamically and adaptively
combines RGB and depth features to tackle issues such as transparency,
reflections, and occlusions. This module can be seamlessly integrated with
various deep neural network backbones as a plug-and-play solution.
Additionally, we introduce the MJU-Glass dataset, a comprehensive RGB-D dataset
collected by a service robot navigating real-world environments, providing a
valuable benchmark for evaluating segmentation models. Experimental results
show significant improvements in segmentation accuracy and robustness, with the
WFF module enhancing performance in both mean Intersection over Union (mIoU)
and boundary IoU (bIoU), achieving a 7.49% improvement in bIoU when integrated
with PSPNet. The proposed module and dataset provide a robust framework for
advancing glass surface segmentation in robotics and reducing the risk of
collisions with glass objects.

</details>


### [197] [Minimal High-Resolution Patches Are Sufficient for Whole Slide Image Representation via Cascaded Dual-Scale Reconstruction](https://arxiv.org/abs/2508.01641)
*Yujian Liu,Yuechuan Lin,Dongxu Shen,Haoran Li,Yutong Wang,Xiaoli Liu,Shidang Xu*

Main category: cs.CV

TL;DR: The paper proposes a Cascaded Dual-Scale Reconstruction (CDSR) framework for efficient and accurate whole-slide image (WSI) analysis, using only 9 high-resolution patches per WSI. It outperforms existing methods with fewer patches.


<details>
  <summary>Details</summary>
Motivation: Current MIL approaches overlook feature extractor impact, leading to domain gaps and suboptimal representations. SSL methods split WSIs into small patches, degrading performance and increasing costs.

Method: CDSR uses a two-stage selective sampling strategy to identify informative regions and a Local-to-Global Network to reconstruct high-resolution WSI representations.

Result: CDSR improves accuracy by 6.3% and AUC by 5.5% on classification tasks, using only 4.5% of patches compared to state-of-the-art methods.

Conclusion: CDSR is efficient and maintains morphological fidelity, outperforming existing methods with significantly fewer patches.

Abstract: Whole-slide image (WSI) analysis remains challenging due to the gigapixel
scale and sparsely distributed diagnostic regions. Multiple Instance Learning
(MIL) mitigates this by modeling the WSI as bags of patches for slide-level
prediction. However, most MIL approaches emphasize aggregator design while
overlooking the impact of the feature extractor of the feature extraction
stage, which is often pretrained on natural images. This leads to domain gap
and suboptimal representations. Self-supervised learning (SSL) has shown
promise in bridging domain gap via pretext tasks, but it still primarily builds
upon generic backbones, thus requiring WSIs to be split into small patches.
This inevitably splits histological structures and generates both redundant and
interdependent patches, which in turn degrades aggregator performance and
drastically increases training costs. To address this challenge, we propose a
Cascaded Dual-Scale Reconstruction (CDSR) framework, demonstrating that only an
average of 9 high-resolution patches per WSI are sufficient for robust
slide-level representation. CDSR employs a two-stage selective sampling
strategy that identifies the most informative representative regions from both
model-based and semantic perspectives. These patches are then fed into a
Local-to-Global Network, which reconstructs spatially coherent high-resolution
WSI representations by integrating fine-grained local detail with global
contextual information. Unlike existing dense-sampling or SSL pipelines, CDSR
is optimized for efficiency and morphological fidelity. Experiments on
Camelyon16, TCGA-NSCLC, and TCGA-RCC demonstrate that CDSR achieves
improvements of 6.3% in accuracy and 5.5% in area under ROC curve on downstream
classification tasks with only 7,070 (4.5% of total) high-resolution patches
per dataset on average, outperforming state-of-the-art methods trained on over
10,000,000 patches.

</details>


### [198] [StrandDesigner: Towards Practical Strand Generation with Sketch Guidance](https://arxiv.org/abs/2508.01650)
*Na Zhang,Moran Li,Chengming Xu,Han Feng,Xiaobin Hu,Jiangning Zhang,Weijian Cao,Chengjie Wang,Yanwei Fu*

Main category: cs.CV

TL;DR: A sketch-based strand generation model for realistic hair, outperforming existing methods in realism and precision.


<details>
  <summary>Details</summary>
Motivation: Existing methods using text or images lack precision and user-friendliness for hair strand generation.

Method: Uses learnable strand upsampling and multi-scale adaptive conditioning with a transformer and diffusion heads.

Result: Outperforms benchmarks in realism and precision, with qualitative results confirming effectiveness.

Conclusion: Proposed model offers finer control and user-friendliness for realistic hair strand generation.

Abstract: Realistic hair strand generation is crucial for applications like computer
graphics and virtual reality. While diffusion models can generate hairstyles
from text or images, these inputs lack precision and user-friendliness.
Instead, we propose the first sketch-based strand generation model, which
offers finer control while remaining user-friendly. Our framework tackles key
challenges, such as modeling complex strand interactions and diverse sketch
patterns, through two main innovations: a learnable strand upsampling strategy
that encodes 3D strands into multi-scale latent spaces, and a multi-scale
adaptive conditioning mechanism using a transformer with diffusion heads to
ensure consistency across granularity levels. Experiments on several benchmark
datasets show our method outperforms existing approaches in realism and
precision. Qualitative results further confirm its effectiveness. Code will be
released at [GitHub](https://github.com/fighting-Zhang/StrandDesigner).

</details>


### [199] [DAG: Unleash the Potential of Diffusion Model for Open-Vocabulary 3D Affordance Grounding](https://arxiv.org/abs/2508.01651)
*Hanqing Wang,Zhenhao Zhang,Kaiyang Ji,Mingyu Liu,Wenti Yin,Yuchao Chen,Zhirui Liu,Xiangyu Zeng,Tianxiang Gui,Hangxing Zhang*

Main category: cs.CV

TL;DR: The paper introduces DAG, a diffusion-based framework for 3D affordance grounding, leveraging text-to-image diffusion models to improve generalization and performance.


<details>
  <summary>Details</summary>
Motivation: Current methods for 3D affordance grounding struggle with generalization due to limited affordance knowledge in demonstration images.

Method: Proposes DAG, using frozen representations from text-to-image diffusion models, an affordance block, and a multi-source decoder for 3D dense prediction.

Result: DAG outperforms established methods and shows strong open-world generalization.

Conclusion: The framework effectively unlocks affordance knowledge in diffusion models, advancing 3D affordance grounding.

Abstract: 3D object affordance grounding aims to predict the touchable regions on a 3d
object, which is crucial for human-object interaction, human-robot interaction,
embodied perception, and robot learning. Recent advances tackle this problem
via learning from demonstration images. However, these methods fail to capture
the general affordance knowledge within the image, leading to poor
generalization. To address this issue, we propose to use text-to-image
diffusion models to extract the general affordance knowledge because we find
that such models can generate semantically valid HOI images, which demonstrate
that their internal representation space is highly correlated with real-world
affordance concepts. Specifically, we introduce the DAG, a diffusion-based 3d
affordance grounding framework, which leverages the frozen internal
representations of the text-to-image diffusion model and unlocks affordance
knowledge within the diffusion model to perform 3D affordance grounding. We
further introduce an affordance block and a multi-source affordance decoder to
endow 3D dense affordance prediction. Extensive experimental evaluations show
that our model excels over well-established methods and exhibits open-world
generalization.

</details>


### [200] [MAP: Mitigating Hallucinations in Large Vision-Language Models with Map-Level Attention Processing](https://arxiv.org/abs/2508.01653)
*Chenxi Li,Yichen Guo,Benfang Qian,Jinhao You,Kai Tang,Yaosong Du,Zonghao Zhang,Xiande Huang*

Main category: cs.CV

TL;DR: The paper introduces Map-Level Attention Processing (MAP), a training-free method to reduce hallucinations in Large Vision-Language Models by leveraging a 2D semantic map of hidden states for improved factual consistency.


<details>
  <summary>Details</summary>
Motivation: LVLMs often generate grammatically correct but factually inconsistent content (hallucinations). Existing methods focus on localized regions, missing broader factual information.

Method: Proposes MAP, using Layer-Wise Criss-Cross Attention and Global-Local Logit Fusion to refine token representations and predictions by aggregating inter- and intra-layer information.

Result: MAP improves truthfulness and performance across benchmarks like POPE, MME, and MMHal-Bench.

Conclusion: The map-level decoding strategy effectively mitigates hallucinations in LVLMs, demonstrating its potential for enhancing factual consistency.

Abstract: Large Vision-Language Models (LVLMs) have achieved impressive performance in
multimodal tasks, but they still suffer from hallucinations, i.e., generating
content that is grammatically accurate but inconsistent with visual inputs. In
this work, we introduce a novel map-level perspective to mitigate
hallucinations in LVLMs, interpreting the hidden states of the model as a 2D
semantic map. We observe that factual information is widely distributed across
this map, extending beyond the localized inter- or intra-layer regions targeted
by most existing methods (e.g., contrastive decoding and layer-wise
consistency). Building on this insight, we propose Map-Level Attention
Processing (MAP), a training-free decoding method that effectively leverages
factual information through attention-based map-level operations to improve
factual consistency. Specifically, we employ Layer-Wise Criss-Cross Attention
to progressively refine token representations at each decoding layer by
aggregating tokens from both inter- and intra-layer dimensions. Additionally, a
Global-Local Logit Fusion mechanism combines logits obtained before and after
global attention to further refine predictions and improve accuracy. Our method
consistently improves the truthfulness and performance of LVLMs across
benchmarks, such as POPE, MME, and MMHal-Bench, demonstrating the potential of
the map-level decoding strategy.

</details>


### [201] [Single Point, Full Mask: Velocity-Guided Level Set Evolution for End-to-End Amodal Segmentation](https://arxiv.org/abs/2508.01661)
*Zhixuan Li,Yujia Liu,Chen Hui,Weisi Lin*

Main category: cs.CV

TL;DR: VELA is a novel method for amodal segmentation using point-based prompts and explicit contour evolution, outperforming existing methods with interpretable geometric modeling.


<details>
  <summary>Details</summary>
Motivation: Existing amodal segmentation methods rely on costly strong prompts or lack interpretability, limiting their practicality and generalization in complex occlusion scenarios.

Method: VELA uses an end-to-end velocity-driven level-set approach, evolving an initial contour from point prompts via a differentiable network predicting shape-specific motion fields.

Result: VELA outperforms strongly prompted methods on benchmarks (COCOA-cls, D2SA, KINS) using only a single-point prompt.

Conclusion: VELA demonstrates the effectiveness of interpretable geometric modeling for amodal segmentation under weak guidance, with code to be released.

Abstract: Amodal segmentation aims to recover complete object shapes, including
occluded regions with no visual appearance, whereas conventional segmentation
focuses solely on visible areas. Existing methods typically rely on strong
prompts, such as visible masks or bounding boxes, which are costly or
impractical to obtain in real-world settings. While recent approaches such as
the Segment Anything Model (SAM) support point-based prompts for guidance, they
often perform direct mask regression without explicitly modeling shape
evolution, limiting generalization in complex occlusion scenarios. Moreover,
most existing methods suffer from a black-box nature, lacking geometric
interpretability and offering limited insight into how occluded shapes are
inferred. To deal with these limitations, we propose VELA, an end-to-end
VElocity-driven Level-set Amodal segmentation method that performs explicit
contour evolution from point-based prompts. VELA first constructs an initial
level set function from image features and the point input, which then
progressively evolves into the final amodal mask under the guidance of a
shape-specific motion field predicted by a fully differentiable network. This
network learns to generate evolution dynamics at each step, enabling
geometrically grounded and topologically flexible contour modeling. Extensive
experiments on COCOA-cls, D2SA, and KINS benchmarks demonstrate that VELA
outperforms existing strongly prompted methods while requiring only a
single-point prompt, validating the effectiveness of interpretable geometric
modeling under weak guidance. The code will be publicly released.

</details>


### [202] [Shape Distribution Matters: Shape-specific Mixture-of-Experts for Amodal Segmentation under Diverse Occlusions](https://arxiv.org/abs/2508.01664)
*Zhixuan Li,Yujia Liu,Chen Hui,Jeonghaeng Lee,Sanghoon Lee,Weisi Lin*

Main category: cs.CV

TL;DR: ShapeMoE introduces a shape-specific sparse Mixture-of-Experts framework for amodal segmentation, improving performance by dynamically routing objects to specialized experts based on shape characteristics.


<details>
  <summary>Details</summary>
Motivation: Existing approaches struggle with diverse amodal shapes due to limited representation capacity, leading to mismatched expert routing and insufficient specialization.

Method: ShapeMoE learns a latent shape distribution space, encodes objects into Gaussian embeddings, and uses a Shape-Aware Sparse Router to route objects to specialized lightweight experts.

Result: ShapeMoE outperforms state-of-the-art methods on COCOA-cls, KINS, and D2SA datasets, particularly in occluded region segmentation.

Conclusion: ShapeMoE provides interpretable, efficient, and high-capacity amodal segmentation by leveraging shape-aware expert routing.

Abstract: Amodal segmentation targets to predict complete object masks, covering both
visible and occluded regions. This task poses significant challenges due to
complex occlusions and extreme shape variation, from rigid furniture to highly
deformable clothing. Existing one-size-fits-all approaches rely on a single
model to handle all shape types, struggling to capture and reason about diverse
amodal shapes due to limited representation capacity. A natural solution is to
adopt a Mixture-of-Experts (MoE) framework, assigning experts to different
shape patterns. However, naively applying MoE without considering the object's
underlying shape distribution can lead to mismatched expert routing and
insufficient expert specialization, resulting in redundant or underutilized
experts. To deal with these issues, we introduce ShapeMoE, a shape-specific
sparse Mixture-of-Experts framework for amodal segmentation. The key idea is to
learn a latent shape distribution space and dynamically route each object to a
lightweight expert tailored to its shape characteristics. Specifically,
ShapeMoE encodes each object into a compact Gaussian embedding that captures
key shape characteristics. A Shape-Aware Sparse Router then maps the object to
the most suitable expert, enabling precise and efficient shape-aware expert
routing. Each expert is designed as lightweight and specialized in predicting
occluded regions for specific shape patterns. ShapeMoE offers well
interpretability via clear shape-to-expert correspondence, while maintaining
high capacity and efficiency. Experiments on COCOA-cls, KINS, and D2SA show
that ShapeMoE consistently outperforms state-of-the-art methods, especially in
occluded region segmentation. The code will be released.

</details>


### [203] [Rein++: Efficient Generalization and Adaptation for Semantic Segmentation with Vision Foundation Models](https://arxiv.org/abs/2508.01667)
*Zhixiang Wei,Xiaoxiao Ma,Ruishen Yan,Tao Tu,Huaian Chen,Jinjin Zheng,Yi Jin,Enhong Chen*

Main category: cs.CV

TL;DR: Rein++ is an efficient VFM-based segmentation framework addressing data scale disparity and domain shifts, combining domain generalization (Rein-G) and adaptation (Rein-A) for superior performance.


<details>
  <summary>Details</summary>
Motivation: Overcome challenges in applying Vision Foundation Models (VFMs) to semantic segmentation due to small datasets and domain shifts.

Method: Rein++ integrates Rein-G (instance-aware tokens for feature refinement) and Rein-A (unsupervised domain adaptation at instance/logit levels, plus semantic transfer).

Result: Outperforms state-of-the-art methods with efficient training, generalizing and adapting well even for large models.

Conclusion: Rein++ is an efficient, generalizable, and adaptive segmentation solution for VFMs, validated by comprehensive experiments.

Abstract: Vision Foundation Models(VFMs) have achieved remarkable success in various
computer vision tasks. However, their application to semantic segmentation is
hindered by two significant challenges: (1) the disparity in data scale, as
segmentation datasets are typically much smaller than those used for VFM
pre-training, and (2) domain distribution shifts, where real-world segmentation
scenarios are diverse and often underrepresented during pre-training. To
overcome these limitations, we present Rein++, an efficient VFM-based
segmentation framework that demonstrates superior generalization from limited
data and enables effective adaptation to diverse unlabeled scenarios.
Specifically, Rein++ comprises a domain generalization solution Rein-G and a
domain adaptation solution Rein-A. Rein-G introduces a set of trainable,
instance-aware tokens that effectively refine the VFM's features for the
segmentation task. This parameter-efficient approach fine-tunes less than 1% of
the backbone's parameters, enabling robust generalization. Building on the
Rein-G, Rein-A performs unsupervised domain adaptation at both the instance and
logit levels to mitigate domain shifts. In addition, it incorporates a semantic
transfer module that leverages the class-agnostic capabilities of the segment
anything model to enhance boundary details in the target domain. The integrated
Rein++ pipeline first learns a generalizable model on a source domain (e.g.,
daytime scenes) and subsequently adapts it to diverse target domains (e.g.,
nighttime scenes) without any target labels. Comprehensive experiments
demonstrate that Rein++ significantly outperforms state-of-the-art methods with
efficient training, underscoring its roles an efficient, generalizable, and
adaptive segmentation solution for VFMs, even for large models with billions of
parameters. The code is available at https://github.com/wloves/Rein.

</details>


### [204] [Benchmarking Adversarial Patch Selection and Location](https://arxiv.org/abs/2508.01676)
*Shai Kimhi,Avi Mendlson,Moshe Kimhi*

Main category: cs.CV

TL;DR: PatchMap is the first exhaustive benchmark for adversarial patch placement, revealing vulnerable spots in vision models and improving attack success rates.


<details>
  <summary>Details</summary>
Motivation: Adversarial patch attacks undermine vision model reliability, necessitating a systematic study of patch placement effects.

Method: PatchMap evaluates over 1.5e8 forward passes on ImageNet, identifying hot-spots for patch attacks. A segmentation-guided heuristic is proposed for efficient patch placement.

Result: PatchMap boosts attack success rates by 8-13 percentage points across five architectures, including adversarially trained models.

Conclusion: PatchMap accelerates research on location-aware defenses and adaptive attacks, with plans to release a larger benchmark.

Abstract: Adversarial patch attacks threaten the reliability of modern vision models.
We present PatchMap, the first spatially exhaustive benchmark of patch
placement, built by evaluating over 1.5e8 forward passes on ImageNet validation
images. PatchMap reveals systematic hot-spots where small patches (as little as
2% of the image) induce confident misclassifications and large drops in model
confidence. To demonstrate its utility, we propose a simple segmentation guided
placement heuristic that leverages off the shelf masks to identify vulnerable
regions without any gradient queries. Across five architectures-including
adversarially trained ResNet50, our method boosts attack success rates by 8 to
13 percentage points compared to random or fixed placements. We publicly
release PatchMap and the code implementation. The full PatchMap bench (6.5B
predictions, multiple backbones) will be released soon to further accelerate
research on location-aware defenses and adaptive attacks.

</details>


### [205] [Cure or Poison? Embedding Instructions Visually Alters Hallucination in Vision-Language Models](https://arxiv.org/abs/2508.01678)
*Zhaochen Wang,Yiwei Wang,Yujun Cai*

Main category: cs.CV

TL;DR: Prompt-in-Image embeds text instructions into images to reduce VLMs' hallucination by aligning multimodal info through a single visual channel. It improves Qwen2.5-VL but harms LLaVA-1.5 and InstructBLIP due to attention bias.


<details>
  <summary>Details</summary>
Motivation: Address hallucination in VLMs caused by misalignment of multimodal information.

Method: Embed textual instructions directly into images, forcing models to process all content visually.

Result: Qwen2.5-VL's performance improves (POPE accuracy +4.1%), while LLaVA-1.5 and InstructBLIP suffer severe drops due to attention bias.

Conclusion: Prompt-in-Image enhances cross-modal alignment for some VLMs but reveals encoder-specific limitations.

Abstract: Vision-Language Models (VLMs) often suffer from hallucination, partly due to
challenges in aligning multimodal information. We propose Prompt-in-Image, a
simple method that embeds textual instructions directly into images. This
removes the need for separate text inputs and forces the model to process all
content through the visual channel. We evaluate this method on three popular
open-source VLMs: Qwen2.5-VL, LLaVA-1.5, and InstructBLIP. The results reveal
sharp differences. Prompt-in-Image improves Qwen2.5-VL's performance,
increasing POPE accuracy by 4.1 percent (from 80.2 percent to 84.3 percent) and
also reducing hallucination rates on MS-COCO. In contrast, LLaVA-1.5 and
InstructBLIP experience a severe performance drop, with accuracy falling from
around 84 percent to near-random levels. Through detailed analysis, we found
that CLIP-based encoders in LLaVA and InstructBLIP exhibit excessive attention
bias toward embedded text regions, disrupting visual understanding. In
contrast, Qwen's vision encoder handles text-embedded images robustly.
Crucially, Prompt-in-Image reduces Qwen's modality gap, enhancing cross-modal
alignment by unifying information processing through a single modality.

</details>


### [206] [DisCo3D: Distilling Multi-View Consistency for 3D Scene Editing](https://arxiv.org/abs/2508.01684)
*Yufeng Chi,Huimin Ma,Kafeng Wang,Jianmin Li*

Main category: cs.CV

TL;DR: DisCo3D is a novel framework for 3D editing that distills 3D consistency priors into a 2D editor, achieving stable multi-view consistency and superior editing quality.


<details>
  <summary>Details</summary>
Motivation: Extending diffusion models to 3D editing is challenging due to multi-view consistency issues, with existing methods suffering from slow convergence, blurry artifacts, and fine-grained inconsistencies.

Method: DisCo3D fine-tunes a 3D generator with multi-view inputs, trains a 2D editor via consistency distillation, and optimizes edited outputs into 3D using Gaussian Splatting.

Result: DisCo3D outperforms state-of-the-art methods in editing quality and maintains stable multi-view consistency.

Conclusion: DisCo3D effectively addresses multi-view consistency challenges in 3D editing, offering improved performance over existing approaches.

Abstract: While diffusion models have demonstrated remarkable progress in 2D image
generation and editing, extending these capabilities to 3D editing remains
challenging, particularly in maintaining multi-view consistency. Classical
approaches typically update 3D representations through iterative refinement
based on a single editing view. However, these methods often suffer from slow
convergence and blurry artifacts caused by cross-view inconsistencies. Recent
methods improve efficiency by propagating 2D editing attention features, yet
still exhibit fine-grained inconsistencies and failure modes in complex scenes
due to insufficient constraints. To address this, we propose \textbf{DisCo3D},
a novel framework that distills 3D consistency priors into a 2D editor. Our
method first fine-tunes a 3D generator using multi-view inputs for scene
adaptation, then trains a 2D editor through consistency distillation. The
edited multi-view outputs are finally optimized into 3D representations via
Gaussian Splatting. Experimental results show DisCo3D achieves stable
multi-view consistency and outperforms state-of-the-art methods in editing
quality.

</details>


### [207] [Register Anything: Estimating "Corresponding Prompts" for Segment Anything Model](https://arxiv.org/abs/2508.01697)
*Shiqi Huang,Tingfa Xu,Wen Yan,Dean Barratt,Yipeng Hu*

Main category: cs.CV

TL;DR: The paper introduces PromptReg, a training-free image registration method that simplifies region-based correspondence by directly searching for corresponding prompts using pre-trained segmentation models like SAM.


<details>
  <summary>Details</summary>
Motivation: Traditional region-based correspondence requires two steps: segmenting ROIs and matching them. The paper aims to simplify this into one step by leveraging pre-trained models for direct prompt-based registration.

Method: The method involves identifying corresponding prompts between images, using an "inverse prompt" solution to invert prompts, and marginalizing these prompts across dimensions to find paired ROIs.

Result: PromptReg outperforms intensity-based and learning-based methods across five applications, achieving competitive results with weakly-supervised approaches.

Conclusion: The proposed PromptReg method simplifies and improves image registration by leveraging pre-trained models for prompt-based correspondence, demonstrating superior performance across diverse datasets.

Abstract: Establishing pixel/voxel-level or region-level correspondences is the core
challenge in image registration. The latter, also known as region-based
correspondence representation, leverages paired regions of interest (ROIs) to
enable regional matching while preserving fine-grained capability at
pixel/voxel level. Traditionally, this representation is implemented via two
steps: segmenting ROIs in each image then matching them between the two images.
In this paper, we simplify this into one step by directly "searching for
corresponding prompts", using extensively pre-trained segmentation models
(e.g., SAM) for a training-free registration approach, PromptReg. Firstly, we
introduce the "corresponding prompt problem", which aims to identify a
corresponding Prompt Y in Image Y for any given visual Prompt X in Image X,
such that the two respectively prompt-conditioned segmentations are a pair of
corresponding ROIs from the two images. Secondly, we present an "inverse
prompt" solution that generates primary and optionally auxiliary prompts,
inverting Prompt X into the prompt space of Image Y. Thirdly, we propose a
novel registration algorithm that identifies multiple paired corresponding ROIs
by marginalizing the inverted Prompt X across both prompt and spatial
dimensions. Comprehensive experiments are conducted on five applications of
registering 3D prostate MR, 3D abdomen MR, 3D lung CT, 2D histopathology and,
as a non-medical example, 2D aerial images. Based on metrics including Dice and
target registration errors on anatomical structures, the proposed registration
outperforms both intensity-based iterative algorithms and learning-based
DDF-predicting networks, even yielding competitive performance with
weakly-supervised approaches that require fully-segmented training data.

</details>


### [208] [Versatile Transition Generation with Image-to-Video Diffusion](https://arxiv.org/abs/2508.01698)
*Zuhao Yang,Jiahui Zhang,Yingchen Yu,Shijian Lu,Song Bai*

Main category: cs.CV

TL;DR: VTG is a framework for generating smooth, high-quality video transitions between two frames using text prompts, addressing challenges like motion smoothness and fidelity.


<details>
  <summary>Details</summary>
Motivation: Current methods lack exploration in generating smooth and rational video transitions between given frames with text prompts.

Method: VTG uses interpolation-based initialization, dual-directional motion fine-tuning, and representation alignment regularization to improve transition quality.

Result: VTG outperforms existing methods on TransitBench, a new benchmark for transition generation tasks.

Conclusion: VTG effectively generates smooth, high-fidelity transitions, advancing the field of video generation.

Abstract: Leveraging text, images, structure maps, or motion trajectories as
conditional guidance, diffusion models have achieved great success in automated
and high-quality video generation. However, generating smooth and rational
transition videos given the first and last video frames as well as descriptive
text prompts is far underexplored. We present VTG, a Versatile Transition video
Generation framework that can generate smooth, high-fidelity, and semantically
coherent video transitions. VTG introduces interpolation-based initialization
that helps preserve object identity and handle abrupt content changes
effectively. In addition, it incorporates dual-directional motion fine-tuning
and representation alignment regularization to mitigate the limitations of
pre-trained image-to-video diffusion models in motion smoothness and generation
fidelity, respectively. To evaluate VTG and facilitate future studies on
unified transition generation, we collected TransitBench, a comprehensive
benchmark for transition generation covering two representative transition
tasks: concept blending and scene transition. Extensive experiments show that
VTG achieves superior transition performance consistently across all four
tasks.

</details>


### [209] [CSLRConformer: A Data-Centric Conformer Approach for Continuous Arabic Sign Language Recognition on the Isharah Datase](https://arxiv.org/abs/2508.01791)
*Fatimah Mohamed Emad Elden*

Main category: cs.CV

TL;DR: The paper proposes a data-centric approach for Continuous Sign Language Recognition (CSLR), focusing on feature engineering, preprocessing, and a novel CSLRConformer architecture, achieving competitive results.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of signer-independent recognition to improve CSLR generalization across diverse signers.

Method: Systematic feature engineering, preprocessing (DBSCAN-based outlier filtering, spatial normalization), and the CSLRConformer architecture (hybrid CNN-Transformer).

Result: Achieved WER of 5.60% (dev set) and 12.01% (test set), securing 3rd place in the competition.

Conclusion: Validates cross-domain adaptation of the Conformer model, establishing state-of-the-art performance in CSLR.

Abstract: The field of Continuous Sign Language Recognition (CSLR) poses substantial
technical challenges, including fluid inter-sign transitions, the absence of
temporal boundaries, and co-articulation effects. This paper, developed for the
MSLR 2025 Workshop Challenge at ICCV 2025, addresses the critical challenge of
signer-independent recognition to advance the generalization capabilities of
CSLR systems across diverse signers. A data-centric methodology is proposed,
centered on systematic feature engineering, a robust preprocessing pipeline,
and an optimized model architecture. Key contributions include a principled
feature selection process guided by Exploratory Data Analysis (EDA) to isolate
communicative keypoints, a rigorous preprocessing pipeline incorporating
DBSCAN-based outlier filtering and spatial normalization, and the novel
CSLRConformer architecture. This architecture adapts the hybrid CNN-Transformer
design of the Conformer model, leveraging its capacity to model local temporal
dependencies and global sequence context; a characteristic uniquely suited for
the spatio-temporal dynamics of sign language. The proposed methodology
achieved a competitive performance, with a Word Error Rate (WER) of 5.60% on
the development set and 12.01% on the test set, a result that secured a 3rd
place ranking on the official competition platform. This research validates the
efficacy of cross-domain architectural adaptation, demonstrating that the
Conformer model, originally conceived for speech recognition, can be
successfully repurposed to establish a new state-of-the-art performance in
keypoint-based CSLR.

</details>


### [210] [TimeExpert: An Expert-Guided Video LLM for Video Temporal Grounding](https://arxiv.org/abs/2508.01699)
*Zuhao Yang,Yingchen Yu,Yunqing Zhao,Shijian Lu,Song Bai*

Main category: cs.CV

TL;DR: TimeExpert is a Mixture-of-Experts-based Video-LLM that dynamically routes task-specific tokens to specialized experts for improved Video Temporal Grounding (VTG) performance.


<details>
  <summary>Details</summary>
Motivation: Existing Video-LLMs process all task tokens uniformly, failing to address the distinct needs of temporal localization, saliency assessment, and textual generation in VTG tasks.

Method: TimeExpert uses a Mixture-of-Experts framework to dynamically route task-specific tokens (e.g., timestamps, saliency scores) to specialized experts, enhancing computational efficiency and subtask precision.

Result: TimeExpert achieves state-of-the-art performance on VTG tasks like Dense Video Captioning, Moment Retrieval, and Video Highlight Detection.

Conclusion: TimeExpert effectively addresses the limitations of static Video-LLMs by specializing task processing, leading to superior VTG performance.

Abstract: Video Temporal Grounding (VTG) aims to precisely identify video event
segments in response to textual queries. The outputs of VTG tasks manifest as
sequences of events, each defined by precise timestamps, saliency scores, and
textual descriptions. Despite recent advances, a fundamental limitation
persists in existing Video Large Language Models (Video-LLMs): they process all
task tokens through identical and static pathways, failing to recognize that
temporal localization, saliency assessment, and textual generation represent
fundamentally distinct tasks requiring specialized processing. To address this,
we introduce TimeExpert, a Mixture-of-Experts (MoE)-based Video-LLM that
effectively decomposes VTG tasks by dynamically routing task-specific tokens
(e.g., timestamps, saliency scores) to specialized experts, with increased
computational efficiency. Our design choices enable precise handling of each
subtask, leading to improved event modeling across diverse VTG applications.
Extensive experiments demonstrate that TimeExpert consistently achieves
state-of-the-art performance on various VTG tasks such as Dense Video
Captioning, Moment Retrieval, and Video Highlight Detection.

</details>


### [211] [LT-Gaussian: Long-Term Map Update Using 3D Gaussian Splatting for Autonomous Driving](https://arxiv.org/abs/2508.01704)
*Luqi Cheng,Zhangshuo Qi,Zijie Zhou,Chao Lu,Guangming Xiong*

Main category: cs.CV

TL;DR: LT-Gaussian is a method for updating 3D-GS-based maps in autonomous driving, using multimodal splatting, change detection, and targeted updates to efficiently handle environmental changes.


<details>
  <summary>Details</summary>
Motivation: Updating 3D-Gaussian Splatting maps is challenging due to high computational costs; LT-Gaussian addresses this by leveraging old and new scene data.

Method: Uses Multimodal Gaussian Splatting, Structural Change Detection, and Gaussian-Map Update Modules to compare old maps with new LiDAR data for targeted updates.

Result: LT-Gaussian efficiently updates maps, handles environmental changes, and produces higher-quality reconstructions than rebuilding from scratch.

Conclusion: LT-Gaussian is an effective and efficient solution for updating 3D-GS maps in autonomous driving, outperforming traditional methods.

Abstract: Maps play an important role in autonomous driving systems. The recently
proposed 3D Gaussian Splatting (3D-GS) produces rendering-quality explicit
scene reconstruction results, demonstrating the potential for map construction
in autonomous driving scenarios. However, because of the time and computational
costs involved in generating Gaussian scenes, how to update the map becomes a
significant challenge. In this paper, we propose LT-Gaussian, a map update
method for 3D-GS-based maps. LT-Gaussian consists of three main components:
Multimodal Gaussian Splatting, Structural Change Detection Module, and
Gaussian-Map Update Module. Firstly, the Gaussian map of the old scene is
generated using our proposed Multimodal Gaussian Splatting. Subsequently,
during the map update process, we compare the outdated Gaussian map with the
current LiDAR data stream to identify structural changes. Finally, we perform
targeted updates to the Gaussian-map to generate an up-to-date map. We
establish a benchmark for map updating on the nuScenes dataset to
quantitatively evaluate our method. The experimental results show that
LT-Gaussian can effectively and efficiently update the Gaussian-map, handling
common environmental changes in autonomous driving scenarios. Furthermore, by
taking full advantage of information from both new and old scenes, LT-Gaussian
is able to produce higher quality reconstruction results compared to map update
strategies that reconstruct maps from scratch. Our open-source code is
available at https://github.com/ChengLuqi/LT-gaussian.

</details>


### [212] [GAID: Frame-Level Gated Audio-Visual Integration with Directional Perturbation for Text-Video Retrieval](https://arxiv.org/abs/2508.01711)
*Bowen Yang,Yun Cao,Chen He,Xiaosu Su*

Main category: cs.CV

TL;DR: GAID improves text-to-video retrieval by integrating audio-visual features adaptively (FGF) and enhancing text embeddings with perturbations (DASP), achieving state-of-the-art results efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook audio semantics or use coarse fusion, leading to suboptimal multimodal representations.

Method: GAID uses Frame-level Gated Fusion (FGF) for fine-grained audio-visual alignment and Directional Adaptive Semantic Perturbation (DASP) for robust text embeddings.

Result: Achieves state-of-the-art results on MSR-VTT, DiDeMo, LSMDC, and VATEX with efficiency gains.

Conclusion: GAID's fusion and perturbation modules complement each other, yielding stable and expressive multimodal representations.

Abstract: Text-to-video retrieval requires precise alignment between language and
temporally rich video signals. Existing methods predominantly exploit visual
cues and often overlook complementary audio semantics or adopt coarse fusion
strategies, leading to suboptimal multimodal representations. We present GAID,
a framework that jointly address this gap via two key components: (i) a
Frame-level Gated Fusion (FGF) that adaptively integrates audio and visual
features under textual guidance, enabling fine-grained temporal alignment; and
(ii) a Directional Adaptive Semantic Perturbation (DASP) that injects
structure-aware perturbations into text embeddings, enhancing robustness and
discrimination without incurring multi-pass inference. These modules complement
each other -- fusion reduces modality gaps while perturbation regularizes
cross-modal matching -- yielding more stable and expressive representations.
Extensive experiments on MSR-VTT, DiDeMo, LSMDC, and VATEX show consistent
state-of-the-art results across all retrieval metrics with notable efficiency
gains. Our code is available at https://github.com/YangBowenn/GAID.

</details>


### [213] [HateClipSeg: A Segment-Level Annotated Dataset for Fine-Grained Hate Video Detection](https://arxiv.org/abs/2508.01712)
*Han Wang,Zhuoran Wang,Roy Ka-Wei Lee*

Main category: cs.CV

TL;DR: HateClipSeg is a large-scale multimodal dataset for hate speech detection in videos, featuring fine-grained annotations and high inter-annotator agreement. It introduces three benchmark tasks, revealing gaps in current models.


<details>
  <summary>Details</summary>
Motivation: The challenge of detecting hate speech in videos due to multimodal complexity and lack of fine-grained annotations in existing datasets.

Method: Creation of HateClipSeg dataset with video-level and segment-level annotations, using a three-stage annotation process. Three benchmark tasks are proposed.

Result: High inter-annotator agreement (alpha = 0.817). Current models show significant performance gaps in the proposed tasks.

Conclusion: HateClipSeg addresses dataset limitations and highlights the need for advanced multimodal and temporally aware approaches in hate speech detection.

Abstract: Detecting hate speech in videos remains challenging due to the complexity of
multimodal content and the lack of fine-grained annotations in existing
datasets. We present HateClipSeg, a large-scale multimodal dataset with both
video-level and segment-level annotations, comprising over 11,714 segments
labeled as Normal or across five Offensive categories: Hateful, Insulting,
Sexual, Violence, Self-Harm, along with explicit target victim labels. Our
three-stage annotation process yields high inter-annotator agreement
(Krippendorff's alpha = 0.817). We propose three tasks to benchmark
performance: (1) Trimmed Hateful Video Classification, (2) Temporal Hateful
Video Localization, and (3) Online Hateful Video Classification. Results
highlight substantial gaps in current models, emphasizing the need for more
sophisticated multimodal and temporally aware approaches. The HateClipSeg
dataset are publicly available at
https://github.com/Social-AI-Studio/HateClipSeg.git.

</details>


### [214] [Dynamic Robot-Assisted Surgery with Hierarchical Class-Incremental Semantic Segmentation](https://arxiv.org/abs/2508.01713)
*Julia Hindel,Ema Mekic,Enamundram Naga Karthik,Rohit Mohan,Daniele Cattaneo,Maria Kalweit,Abhinav Valada*

Main category: cs.CV

TL;DR: The paper introduces TOPICS+, an enhanced variant of the TOPICS method, for class-incremental semantic segmentation in robotic surgery, addressing class imbalances and dynamic environments. It proposes new benchmarks and a refined dataset for evaluation.


<details>
  <summary>Details</summary>
Motivation: Robotic surgeries require real-time scene understanding, but static datasets limit segmentation models. Class-incremental learning is needed to adapt to new classes without forgetting prior knowledge.

Method: TOPICS+ incorporates Dice loss, hierarchical pseudo-labeling, and tailored taxonomies for surgical scenes. It also introduces six CISS benchmarks and a refined dataset with 144+ classes.

Result: The method improves segmentation robustness in dynamic surgical environments and provides benchmarks for future research.

Conclusion: TOPICS+ advances CISS for robotic surgery, offering practical tools and datasets for real-world deployment.

Abstract: Robot-assisted surgeries rely on accurate and real-time scene understanding
to safely guide surgical instruments. However, segmentation models trained on
static datasets face key limitations when deployed in these dynamic and
evolving surgical environments. Class-incremental semantic segmentation (CISS)
allows models to continually adapt to new classes while avoiding catastrophic
forgetting of prior knowledge, without training on previous data. In this work,
we build upon the recently introduced Taxonomy-Oriented Poincar\'e-regularized
Incremental Class Segmentation (TOPICS) approach and propose an enhanced
variant, termed TOPICS+, specifically tailored for robust segmentation of
surgical scenes. Concretely, we incorporate the Dice loss into the hierarchical
loss formulation to handle strong class imbalances, introduce hierarchical
pseudo-labeling, and design tailored label taxonomies for robotic surgery
environments. We also propose six novel CISS benchmarks designed for robotic
surgery environments including multiple incremental steps and several semantic
categories to emulate realistic class-incremental settings in surgical
environments. In addition, we introduce a refined set of labels with more than
144 classes on the Syn-Mediverse synthetic dataset, hosted online as an
evaluation benchmark. We make the code and trained models publicly available at
http://topics.cs.uni-freiburg.de.

</details>


### [215] [Granular Concept Circuits: Toward a Fine-Grained Circuit Discovery for Concept Representations](https://arxiv.org/abs/2508.01728)
*Dahee Kwon,Sehyun Lee,Jaesik Choi*

Main category: cs.CV

TL;DR: The paper introduces Granular Concept Circuit (GCC), a method to discover circuits in deep vision models that encode specific visual concepts, offering fine-grained interpretability.


<details>
  <summary>Details</summary>
Motivation: Understanding where and how visual concepts are encoded in deep vision models is challenging due to distributed representations.

Method: GCC iteratively assesses inter-neuron connectivity, focusing on functional dependencies and semantic alignment to construct circuits for specific concepts.

Result: GCC effectively identifies fine-grained circuits tied to visual concepts across various deep image classification models.

Conclusion: GCC provides a profound, concept-wise interpretation of deep vision models, advancing model interpretability.

Abstract: Deep vision models have achieved remarkable classification performance by
leveraging a hierarchical architecture in which human-interpretable concepts
emerge through the composition of individual neurons across layers. Given the
distributed nature of representations, pinpointing where specific visual
concepts are encoded within a model remains a crucial yet challenging task. In
this paper, we introduce an effective circuit discovery method, called Granular
Concept Circuit (GCC), in which each circuit represents a concept relevant to a
given query. To construct each circuit, our method iteratively assesses
inter-neuron connectivity, focusing on both functional dependencies and
semantic alignment. By automatically discovering multiple circuits, each
capturing specific concepts within that query, our approach offers a profound,
concept-wise interpretation of models and is the first to identify circuits
tied to specific visual concepts at a fine-grained level. We validate the
versatility and effectiveness of GCCs across various deep image classification
models.

</details>


### [216] [Tracking the Unstable: Appearance-Guided Motion Modeling for Robust Multi-Object Tracking in UAV-Captured Videos](https://arxiv.org/abs/2508.01730)
*Jianbo Ma,Hui Luo,Qi Chen,Yuankai Qi,Yumei Sun,Amin Beheshti,Jianlin Zhang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: AMOT improves multi-object tracking in UAV videos by jointly leveraging appearance and motion cues with an AMC matrix and MTC module, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Challenges in UAV videos, like viewpoint changes and complex motion, cause unstable tracking. Existing methods treat appearance and motion separately, leading to suboptimal performance.

Method: Proposes AMOT with an Appearance-Motion Consistency (AMC) matrix for reliable association and a Motion-aware Track Continuation (MTC) module to reduce broken trajectories.

Result: AMOT outperforms state-of-the-art methods on UAV benchmarks (VisDrone2019, UAVDT, VT-MOT-UAV) and generalizes well without training.

Conclusion: AMOT effectively addresses UAV tracking challenges by integrating appearance and motion cues, offering robust and training-free performance.

Abstract: Multi-object tracking (MOT) aims to track multiple objects while maintaining
consistent identities across frames of a given video. In unmanned aerial
vehicle (UAV) recorded videos, frequent viewpoint changes and complex
UAV-ground relative motion dynamics pose significant challenges, which often
lead to unstable affinity measurement and ambiguous association. Existing
methods typically model motion and appearance cues separately, overlooking
their spatio-temporal interplay and resulting in suboptimal tracking
performance. In this work, we propose AMOT, which jointly exploits appearance
and motion cues through two key components: an Appearance-Motion Consistency
(AMC) matrix and a Motion-aware Track Continuation (MTC) module. Specifically,
the AMC matrix computes bi-directional spatial consistency under the guidance
of appearance features, enabling more reliable and context-aware identity
association. The MTC module complements AMC by reactivating unmatched tracks
through appearance-guided predictions that align with Kalman-based predictions,
thereby reducing broken trajectories caused by missed detections. Extensive
experiments on three UAV benchmarks, including VisDrone2019, UAVDT, and
VT-MOT-UAV, demonstrate that our AMOT outperforms current state-of-the-art
methods and generalizes well in a plug-and-play and training-free manner.

</details>


### [217] [SpectralX: Parameter-efficient Domain Generalization for Spectral Remote Sensing Foundation Models](https://arxiv.org/abs/2508.01731)
*Yuxiang Zhang,Wei Li,Mengmeng Zhang,Jiawei Han,Ran Tao,Shunlin Liang*

Main category: cs.CV

TL;DR: SpectralX is a parameter-efficient fine-tuning framework adapting existing RSFMs for diverse spectral modalities without extensive pretraining, improving domain generalization.


<details>
  <summary>Details</summary>
Motivation: Address the lack of foundation models for multispectral/hyperspectral data by leveraging existing RSFMs.

Method: Two-stage training: masked-reconstruction with Hyper Tokenizer and Attribute-oriented Mixture of Adapter, followed by semantic segmentation with Attribute-refined Adapter.

Result: Improved domain generalization for interpreting spectral imagery from new regions or seasons.

Conclusion: SpectralX effectively adapts RSFMs for diverse spectral inputs, enhancing performance without extensive pretraining.

Abstract: Recent advances in Remote Sensing Foundation Models (RSFMs) have led to
significant breakthroughs in the field. While many RSFMs have been pretrained
with massive optical imagery, more multispectral/hyperspectral data remain lack
of the corresponding foundation models. To leverage the advantages of spectral
imagery in earth observation, we explore whether existing RSFMs can be
effectively adapted to process diverse spectral modalities without requiring
extensive spectral pretraining. In response to this challenge, we proposed
SpectralX, an innovative parameter-efficient fine-tuning framework that adapt
existing RSFMs as backbone while introducing a two-stage training approach to
handle various spectral inputs, thereby significantly improving domain
generalization performance. In the first stage, we employ a
masked-reconstruction task and design a specialized Hyper Tokenizer (HyperT) to
extract attribute tokens from both spatial and spectral dimensions.
Simultaneously, we develop an Attribute-oriented Mixture of Adapter (AoMoA)
that dynamically aggregates multi-attribute expert knowledge while performing
layer-wise fine-tuning. With semantic segmentation as downstream task in the
second stage, we insert an Attribute-refined Adapter (Are-adapter) into the
first stage framework. By iteratively querying low-level semantic features with
high-level representations, the model learns to focus on task-beneficial
attributes, enabling customized adjustment of RSFMs. Following this two-phase
adaptation process, SpectralX is capable of interpreting spectral imagery from
new regions or seasons. The codes will be available from the website:
https://github.com/YuxiangZhang-BIT.

</details>


### [218] [AG$^2$aussian: Anchor-Graph Structured Gaussian Splatting for Instance-Level 3D Scene Understanding and Editing](https://arxiv.org/abs/2508.01740)
*Zhaonan Wang,Manyi Li,Changhe Tu*

Main category: cs.CV

TL;DR: AG$^2$aussian introduces an anchor-graph structure to organize semantic features in 3D Gaussian Splatting, improving instance-level segmentation and enabling diverse applications.


<details>
  <summary>Details</summary>
Motivation: Existing methods for semantic-aware 3D Gaussian representations produce noisy segmentation and messy Gaussian selections, limiting scene understanding and editing.

Method: AG$^2$aussian uses an anchor-graph structure to regulate Gaussian primitives and propagate semantic features, ensuring compact and instance-aware distributions.

Result: The framework achieves clean and accurate instance-level Gaussian selection, validated across four applications (interactive query, text-driven query, object removal, and physics simulation).

Conclusion: AG$^2$aussian effectively addresses limitations of existing approaches, demonstrating superior performance and versatility in semantic-aware 3DGS applications.

Abstract: 3D Gaussian Splatting (3DGS) has witnessed exponential adoption across
diverse applications, driving a critical need for semantic-aware 3D Gaussian
representations to enable scene understanding and editing tasks. Existing
approaches typically attach semantic features to a collection of free Gaussians
and distill the features via differentiable rendering, leading to noisy
segmentation and a messy selection of Gaussians. In this paper, we introduce
AG$^2$aussian, a novel framework that leverages an anchor-graph structure to
organize semantic features and regulate Gaussian primitives. Our anchor-graph
structure not only promotes compact and instance-aware Gaussian distributions,
but also facilitates graph-based propagation, achieving a clean and accurate
instance-level Gaussian selection. Extensive validation across four
applications, i.e. interactive click-based query, open-vocabulary text-driven
query, object removal editing, and physics simulation, demonstrates the
advantages of our approach and its benefits to various applications. The
experiments and ablation studies further evaluate the effectiveness of the key
designs of our approach.

</details>


### [219] [Subject or Style: Adaptive and Training-Free Mixture of LoRAs](https://arxiv.org/abs/2508.02165)
*Jia-Chen Zhang,Yu-Jie Xiong*

Main category: cs.CV

TL;DR: EST-LoRA is a training-free adaptive LoRA fusion method that balances subject and style in generation tasks by considering energy, style discrepancy, and time steps, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current LoRA fusion methods struggle to balance subject and style and often require additional training. K-LoRA, a recent training-free method, is hindered by multiple hyperparameters.

Method: EST-LoRA integrates three factors (Energy, Style discrepancy, Time steps) and uses an adaptive selection mechanism, akin to Mixture of Experts, to choose between subject and style LoRA in each attention layer.

Result: EST-LoRA outperforms state-of-the-art methods in quality and speed, achieving balanced generation without additional training.

Conclusion: EST-LoRA provides an efficient, training-free solution for adaptive LoRA fusion, improving performance and generation speed.

Abstract: Fine-tuning models via Low-Rank Adaptation (LoRA) demonstrates remarkable
performance in subject-driven or style-driven generation tasks. Studies have
explored combinations of different LoRAs to jointly generate learned styles and
content. However, current methods struggle to balance the original subject and
style, and often require additional training. Recently, K-LoRA proposed a
training-free LoRA fusion method. But it involves multiple hyperparameters,
making it difficult to adapt to all styles and subjects. In this paper, we
propose EST-LoRA, a training-free adaptive LoRA fusion method. It
comprehensively considers three critical factors: \underline{E}nergy of matrix,
\underline{S}tyle discrepancy scores and \underline{T}ime steps. Analogous to
the Mixture of Experts (MoE) architecture, the model adaptively selects between
subject LoRA and style LoRA within each attention layer. This integrated
selection mechanism ensures balanced contributions from both components during
the generation process. Experimental results show that EST-LoRA outperforms
state-of-the-art methods in both qualitative and quantitative evaluations and
achieves faster generation speed compared to other efficient fusion approaches.
Our code is publicly available at:
https://anonymous.4open.science/r/EST-LoRA-F318.

</details>


### [220] [Simulated Ensemble Attack: Transferring Jailbreaks Across Fine-tuned Vision-Language Models](https://arxiv.org/abs/2508.01741)
*Ruofan Wang,Xin Wang,Yang Yao,Xuan Tong,Xingjun Ma*

Main category: cs.CV

TL;DR: The paper introduces SEA, a grey-box jailbreak method for fine-tuned VLMs, demonstrating high transferability of attacks due to inherited vulnerabilities from base models.


<details>
  <summary>Details</summary>
Motivation: To expose the risk of transferable jailbreak attacks in fine-tuned VLMs, highlighting vulnerabilities retained from base models.

Method: SEA combines Fine-tuning Trajectory Simulation (FTS) and Targeted Prompt Guidance (TPG) to generate transferable adversarial attacks.

Result: SEA achieves 86.5% attack success and 49.5% toxicity rates across fine-tuned VLMs, even safety-enhanced ones.

Conclusion: Urges safeguarding fine-tuned VLMs against inherited vulnerabilities, advocating for defenses across the model lifecycle.

Abstract: Fine-tuning open-source Vision-Language Models (VLMs) creates a critical yet
underexplored attack surface: vulnerabilities in the base VLM could be retained
in fine-tuned variants, rendering them susceptible to transferable jailbreak
attacks. To demonstrate this risk, we introduce the Simulated Ensemble Attack
(SEA), a novel grey-box jailbreak method in which the adversary has full access
to the base VLM but no knowledge of the fine-tuned target's weights or training
configuration. To improve jailbreak transferability across fine-tuned VLMs, SEA
combines two key techniques: Fine-tuning Trajectory Simulation (FTS) and
Targeted Prompt Guidance (TPG). FTS generates transferable adversarial images
by simulating the vision encoder's parameter shifts, while TPG is a textual
strategy that steers the language decoder toward adversarially optimized
outputs. Experiments on the Qwen2-VL family (2B and 7B) demonstrate that SEA
achieves high transfer attack success rates exceeding 86.5% and toxicity rates
near 49.5% across diverse fine-tuned variants, even those specifically
fine-tuned to improve safety behaviors. Notably, while direct PGD-based image
jailbreaks rarely transfer across fine-tuned VLMs, SEA reliably exploits
inherited vulnerabilities from the base model, significantly enhancing
transferability. These findings highlight an urgent need to safeguard
fine-tuned proprietary VLMs against transferable vulnerabilities inherited from
open-source foundations, motivating the development of holistic defenses across
the entire model lifecycle.

</details>


### [221] [Intention-Guided Cognitive Reasoning for Egocentric Long-Term Action Anticipation](https://arxiv.org/abs/2508.01742)
*Qiaohui Chu,Haoyu Zhang,Meng Liu,Yisen Feng,Haoxiang Shi,Liqiang Nie*

Main category: cs.CV

TL;DR: INSIGHT is a two-stage framework for egocentric action anticipation, addressing limitations in fine-grained visual cues, semantic dependencies, and cognitive reasoning, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Improving long-term action anticipation from egocentric video for proactive AI assistance by addressing underutilized visual cues, neglected semantic dependencies, and lack of cognitive reasoning.

Method: Two-stage framework: 1) extracts features from hand-object interactions and uses a verb-noun co-occurrence matrix; 2) employs reinforcement learning for cognitive reasoning (perception, inference, anticipation).

Result: State-of-the-art performance on Ego4D, EPIC-Kitchens-55, and EGTEA Gaze+ benchmarks, demonstrating strong generalization.

Conclusion: INSIGHT effectively overcomes key limitations and advances egocentric action anticipation with its unified approach.

Abstract: Long-term action anticipation from egocentric video is critical for
applications such as human-computer interaction and assistive technologies,
where anticipating user intent enables proactive and context-aware AI
assistance. However, existing approaches suffer from three key limitations: 1)
underutilization of fine-grained visual cues from hand-object interactions, 2)
neglect of semantic dependencies between verbs and nouns, and 3) lack of
explicit cognitive reasoning, limiting generalization and long-term forecasting
ability. To overcome these challenges, we propose INSIGHT, a unified two-stage
framework for egocentric action anticipation. In the first stage, INSIGHT
focuses on extracting semantically rich features from hand-object interaction
regions and enhances action representations using a verb-noun co-occurrence
matrix. In the second stage, it introduces a reinforcement learning-based
module that simulates explicit cognitive reasoning through a structured
process: visual perception (think) -> intention inference (reason) -> action
anticipation (answer). Extensive experiments on Ego4D, EPIC-Kitchens-55, and
EGTEA Gaze+ benchmarks show that INSIGHT achieves state-of-the-art performance,
demonstrating its effectiveness and strong generalization capability.

</details>


### [222] [Improving Noise Efficiency in Privacy-preserving Dataset Distillation](https://arxiv.org/abs/2508.01749)
*Runkai Zheng,Vishnu Asutosh Dasu,Yinong Oliver Wang,Haohan Wang,Fernando De la Torre*

Main category: cs.CV

TL;DR: A novel framework improves privacy-preserving dataset distillation by decoupling sampling from optimization and enhancing signal quality, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Addressing inefficiencies in current private dataset distillation methods, such as synchronized sampling-optimization and noisy training signals, to better utilize private information.

Method: Decouples sampling from optimization and improves signal quality by matching in an informative subspace, reducing the impact of DP noise.

Result: Achieves 10.0% improvement with 50 images per class and 8.3% increase with one-fifth the distilled set size on CIFAR-10.

Conclusion: The framework advances privacy-preserving dataset distillation by efficiently utilizing private information and outperforming state-of-the-art methods.

Abstract: Modern machine learning models heavily rely on large datasets that often
include sensitive and private information, raising serious privacy concerns.
Differentially private (DP) data generation offers a solution by creating
synthetic datasets that limit the leakage of private information within a
predefined privacy budget; however, it requires a substantial amount of data to
achieve performance comparable to models trained on the original data. To
mitigate the significant expense incurred with synthetic data generation,
Dataset Distillation (DD) stands out for its remarkable training and storage
efficiency. This efficiency is particularly advantageous when integrated with
DP mechanisms, curating compact yet informative synthetic datasets without
compromising privacy. However, current state-of-the-art private DD methods
suffer from a synchronized sampling-optimization process and the dependency on
noisy training signals from randomly initialized networks. This results in the
inefficient utilization of private information due to the addition of excessive
noise. To address these issues, we introduce a novel framework that decouples
sampling from optimization for better convergence and improves signal quality
by mitigating the impact of DP noise through matching in an informative
subspace. On CIFAR-10, our method achieves a \textbf{10.0\%} improvement with
50 images per class and \textbf{8.3\%} increase with just \textbf{one-fifth}
the distilled set size of previous state-of-the-art methods, demonstrating
significant potential to advance privacy-preserving DD.

</details>


### [223] [Vision transformer-based multi-camera multi-object tracking framework for dairy cow monitoring](https://arxiv.org/abs/2508.01752)
*Kumail Abbas,Zeeshan Afzal,Aqeel Raza,Taha Mansouri,Andrew W. Dowsey,Chaidate Inchaisri,Ali Alameer*

Main category: cs.CV

TL;DR: A multi-camera, real-time tracking system using advanced computer vision techniques was developed to monitor dairy cow activity accurately, outperforming existing methods with high MOTA and IDF1 scores.


<details>
  <summary>Details</summary>
Motivation: Manual monitoring of dairy cow activity is laborious and inconsistent, necessitating an automated system for better health and welfare management.

Method: The system integrates six camera feeds with homographic transformations, uses YOLO11-m for detection, SAMURAI for segmentation, and a motion-aware Kalman filter for tracking.

Result: Achieved high accuracy (MOTA 98.7%-99.3%, IDF1 >99%) with minimal identity switches, outperforming Deep SORT Realtime.

Conclusion: The system enables real-time, precise cow tracking in complex environments, aiding early disease prediction and farm productivity.

Abstract: Activity and behaviour correlate with dairy cow health and welfare, making
continual and accurate monitoring crucial for disease identification and farm
productivity. Manual observation and frequent assessments are laborious and
inconsistent for activity monitoring. In this study, we developed a unique
multi-camera, real-time tracking system for indoor-housed Holstein Friesian
dairy cows. This technology uses cutting-edge computer vision techniques,
including instance segmentation and tracking algorithms to monitor cow activity
seamlessly and accurately. An integrated top-down barn panorama was created by
geometrically aligning six camera feeds using homographic transformations. The
detection phase used a refined YOLO11-m model trained on an overhead cow
dataset, obtaining high accuracy (mAP\@0.50 = 0.97, F1 = 0.95). SAMURAI, an
upgraded Segment Anything Model 2.1, generated pixel-precise cow masks for
instance segmentation utilizing zero-shot learning and motion-aware memory.
Even with occlusion and fluctuating posture, a motion-aware Linear Kalman
filter and IoU-based data association reliably identified cows over time for
object tracking. The proposed system significantly outperformed Deep SORT
Realtime. Multi-Object Tracking Accuracy (MOTA) was 98.7% and 99.3% in two
benchmark video sequences, with IDF1 scores above 99% and near-zero identity
switches. This unified multi-camera system can track dairy cows in complex
interior surroundings in real time, according to our data. The system reduces
redundant detections across overlapping cameras, maintains continuity as cows
move between viewpoints, with the aim of improving early sickness prediction
through activity quantification and behavioural classification.

</details>


### [224] [VPN: Visual Prompt Navigation](https://arxiv.org/abs/2508.01766)
*Shuo Feng,Zihan Wang,Yuchen Li,Rui Kong,Hengyi Cai,Shuaiqiang Wang,Gim Hee Lee,Piji Li,Shuqiang Jiang*

Main category: cs.CV

TL;DR: The paper introduces Visual Prompt Navigation (VPN), a language-free method for guiding embodied agents using visual prompts on 2D maps, reducing ambiguity and improving usability for non-experts.


<details>
  <summary>Details</summary>
Motivation: Natural language instructions are often ambiguous and verbose, hindering effective navigation in complex environments. VPN aims to provide intuitive, spatially grounded guidance without relying on language.

Method: VPN uses visual prompts (trajectory markings on 2D maps) to guide agents. Two datasets (R2R-VP and R2R-CE-VP) are created, and VPNet is introduced as a baseline model with view-level and trajectory-level data augmentation.

Result: Experiments evaluate the impact of visual prompt forms, map formats, and augmentation strategies on navigation performance.

Conclusion: VPN offers a more intuitive and less ambiguous alternative to language-guided navigation, with potential for broader usability.

Abstract: While natural language is commonly used to guide embodied agents, the
inherent ambiguity and verbosity of language often hinder the effectiveness of
language-guided navigation in complex environments. To this end, we propose
Visual Prompt Navigation (VPN), a novel paradigm that guides agents to navigate
using only user-provided visual prompts within 2D top-view maps. This visual
prompt primarily focuses on marking the visual navigation trajectory on a
top-down view of a scene, offering intuitive and spatially grounded guidance
without relying on language instructions. It is more friendly for non-expert
users and reduces interpretive ambiguity. We build VPN tasks in both discrete
and continuous navigation settings, constructing two new datasets, R2R-VP and
R2R-CE-VP, by extending existing R2R and R2R-CE episodes with corresponding
visual prompts. Furthermore, we introduce VPNet, a dedicated baseline network
to handle the VPN tasks, with two data augmentation strategies: view-level
augmentation (altering initial headings and prompt orientations) and
trajectory-level augmentation (incorporating diverse trajectories from
large-scale 3D scenes), to enhance navigation performance. Extensive
experiments evaluate how visual prompt forms, top-view map formats, and data
augmentation strategies affect the performance of visual prompt navigation. The
code is available at https://github.com/farlit/VPN.

</details>


### [225] [DiffSemanticFusion: Semantic Raster BEV Fusion for Autonomous Driving via Online HD Map Diffusion](https://arxiv.org/abs/2508.01778)
*Zhigang Sun,Yiru Wang,Anqing Jiang,Shuo Wang,Yu Gao,Yuwen Heng,Shouyi Zhang,An He,Hao Jiang,Jinhao Chai,Zichong Gu,Wang Jijun,Shichen Tang,Lavdim Halilaj,Juergen Luettin,Hao Sun*

Main category: cs.CV

TL;DR: DiffSemanticFusion combines raster and graph-based representations for autonomous driving, improving trajectory prediction and planning with a map diffusion module.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of raster-based (lacking precision) and graph-based (unstable without precise maps) representations in online HD map generation.

Method: Proposes DiffSemanticFusion, a fusion framework using a semantic raster-fused BEV space and a map diffusion module.

Result: Achieves 5.1% improvement in trajectory prediction on nuScenes and 15% gain in end-to-end driving on NAVSIM.

Conclusion: The framework enhances performance and can be integrated into other vector-based approaches.

Abstract: Autonomous driving requires accurate scene understanding, including road
geometry, traffic agents, and their semantic relationships. In online HD map
generation scenarios, raster-based representations are well-suited to vision
models but lack geometric precision, while graph-based representations retain
structural detail but become unstable without precise maps. To harness the
complementary strengths of both, we propose DiffSemanticFusion -- a fusion
framework for multimodal trajectory prediction and planning. Our approach
reasons over a semantic raster-fused BEV space, enhanced by a map diffusion
module that improves both the stability and expressiveness of online HD map
representations. We validate our framework on two downstream tasks: trajectory
prediction and planning-oriented end-to-end autonomous driving. Experiments on
real-world autonomous driving benchmarks, nuScenes and NAVSIM, demonstrate
improved performance over several state-of-the-art methods. For the prediction
task on nuScenes, we integrate DiffSemanticFusion with the online HD map
informed QCNet, achieving a 5.1\% performance improvement. For end-to-end
autonomous driving in NAVSIM, DiffSemanticFusion achieves state-of-the-art
results, with a 15\% performance gain in NavHard scenarios. In addition,
extensive ablation and sensitivity studies show that our map diffusion module
can be seamlessly integrated into other vector-based approaches to enhance
performance. All artifacts are available at
https://github.com/SunZhigang7/DiffSemanticFusion.

</details>


### [226] [Skip priors and add graph-based anatomical information, for point-based Couinaud segmentation](https://arxiv.org/abs/2508.01785)
*Xiaotong Zhang,Alexander Broersen,Gonnie CM van Erp,Silvia L. Pintea,Jouke Dijkstra*

Main category: cs.CV

TL;DR: A point-based method for Couinaud liver segmentation without explicit prior vessel structure, using graph reasoning to learn anatomical affinities.


<details>
  <summary>Details</summary>
Motivation: Traditional point-based methods require time-consuming prior knowledge of liver vessel structure, which this work aims to eliminate.

Method: Proposes a point-based approach with a graph reasoning module to implicitly learn liver vessel structure from point neighborhoods.

Result: Competitive performance on MSD and LiTS datasets in Dice coefficient and average surface distance scores.

Conclusion: The method effectively segments liver Couinaud regions without explicit vessel structure, leveraging learned anatomical affinities.

Abstract: The preoperative planning of liver surgery relies on Couinaud segmentation
from computed tomography (CT) images, to reduce the risk of bleeding and guide
the resection procedure. Using 3D point-based representations, rather than
voxelizing the CT volume, has the benefit of preserving the physical resolution
of the CT. However, point-based representations need prior knowledge of the
liver vessel structure, which is time consuming to acquire. Here, we propose a
point-based method for Couinaud segmentation, without explicitly providing the
prior liver vessel structure. To allow the model to learn this anatomical liver
vessel structure, we add a graph reasoning module on top of the point features.
This adds implicit anatomical information to the model, by learning affinities
across point neighborhoods. Our method is competitive on the MSD and LiTS
public datasets in Dice coefficient and average surface distance scores
compared to four pioneering point-based methods. Our code is available at
https://github.com/ZhangXiaotong015/GrPn.

</details>


### [227] [SoccerTrack v2: A Full-Pitch Multi-View Soccer Dataset for Game State Reconstruction](https://arxiv.org/abs/2508.01802)
*Atom Scott,Ikuma Uchida,Kento Kuroda,Yufi Kim,Keisuke Fujii*

Main category: cs.CV

TL;DR: SoccerTrack v2 is a 4K panoramic dataset for soccer analytics, offering annotated videos for MOT, GSR, and BAS tasks.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive dataset for advancing research in soccer analytics, addressing limitations of prior datasets with broadcast views or limited scenarios.

Method: Uses 10 full-length, panoramic 4K recordings of university-level matches, annotated with GSR and BAS labels, captured with BePro cameras.

Result: A public dataset with detailed annotations for player tracking, game state reconstruction, and ball action spotting.

Conclusion: SoccerTrack v2 aims to advance computer vision and soccer analytics research, enabling new benchmarks and practical applications.

Abstract: SoccerTrack v2 is a new public dataset for advancing multi-object tracking
(MOT), game state reconstruction (GSR), and ball action spotting (BAS) in
soccer analytics. Unlike prior datasets that use broadcast views or limited
scenarios, SoccerTrack v2 provides 10 full-length, panoramic 4K recordings of
university-level matches, captured with BePro cameras for complete player
visibility. Each video is annotated with GSR labels (2D pitch coordinates,
jersey-based player IDs, roles, teams) and BAS labels for 12 action classes
(e.g., Pass, Drive, Shot). This technical report outlines the datasets
structure, collection pipeline, and annotation process. SoccerTrack v2 is
designed to advance research in computer vision and soccer analytics, enabling
new benchmarks and practical applications in tactical analysis and automated
tools.

</details>


### [228] [Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination via Attention Lens](https://arxiv.org/abs/2508.02419)
*Haohan Zheng,Zhenguo Zhang*

Main category: cs.CV

TL;DR: The paper identifies modality bias in LVLMs as a cause of object hallucination, proposing a training-free method to balance attention weights and reduce reliance on parametric knowledge.


<details>
  <summary>Details</summary>
Motivation: LVLMs suffer from object hallucination due to over-reliance on textual prompts and internal knowledge, ignoring visual and textual modalities.

Method: A training-free approach adjusts attention weights of textual and visual tokens and uses contrastive decoding to balance cross-modal compatibility.

Result: Experiments show widespread modality bias in LVLMs, with the proposed method effectively reducing hallucination across benchmarks.

Conclusion: The method is generalizable and effective, highlighting the importance of balanced cross-modal attention in LVLMs.

Abstract: Large vision-language models (LVLMs) have demonstrated remarkable multimodal
comprehension and reasoning capabilities, but they still suffer from severe
object hallucination. Previous studies primarily attribute the flaw to
linguistic prior caused by the scale mismatch between visual encoders and large
language models (LLMs) in LVLMs. Specifically, as current LVLMs are built upon
LLMs, they tend to over-rely on textual prompts and internal knowledge of LLMs,
generating descriptions inconsistent with visual cues. However, through an
in-depth investigation of the hallucinated mechanisms, we empirically reveal a
previously overlooked phenomenon: LVLMs may ignore not only visual information
but also textual modality during hallucination, a behavior termed as modality
bias, which indicates that LVLMs struggle to simultaneously attend to both
visual and textual modalities, leading to fragmented understanding of
user-provided instructions. Based on this observation, we propose a simple yet
effective training-free method to mitigate object hallucination. Concretely, we
intervene and adjust the attention weights of textual and visual tokens,
balancing cross-modal compatibility for better alignment with user intentions.
Furthermore, we adopt a contrastive decoding strategy to reduce the LVLM's
overreliance on its parametric knowledge, synergistically enhancing our
attention manipulation. Extensive experiments confirm the widespread presence
of modality bias in LVLMs. Notably, our method effectively mitigates
hallucination across multiple open-source LVLMs and benchmarks, highlighting
its generalizability and efficacy.

</details>


### [229] [Diffusion-based 3D Hand Motion Recovery with Intuitive Physics](https://arxiv.org/abs/2508.01835)
*Yufei Zhang,Zijun Cui,Jeffrey O. Kephart,Qiang Ji*

Main category: cs.CV

TL;DR: A novel 3D hand motion recovery framework uses diffusion-based and physics-augmented refinement to improve accuracy and coherence in hand-object interactions, trained solely on motion capture data.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generating accurate and temporally coherent 3D hand motion estimates from videos, especially during hand-object interactions.

Method: A diffusion-based and physics-augmented motion refinement model iteratively denoises initial motion estimates, integrating intuitive physics knowledge like key motion states and constraints.

Result: Significantly improves frame-wise reconstruction methods, achieving SOTA performance on benchmarks.

Conclusion: The framework effectively enhances 3D hand motion recovery by combining diffusion models with physics insights, outperforming existing methods.

Abstract: While 3D hand reconstruction from monocular images has made significant
progress, generating accurate and temporally coherent motion estimates from
videos remains challenging, particularly during hand-object interactions. In
this paper, we present a novel 3D hand motion recovery framework that enhances
image-based reconstructions through a diffusion-based and physics-augmented
motion refinement model. Our model captures the distribution of refined motion
estimates conditioned on initial ones, generating improved sequences through an
iterative denoising process. Instead of relying on scarce annotated video data,
we train our model only using motion capture data without images. We identify
valuable intuitive physics knowledge during hand-object interactions, including
key motion states and their associated motion constraints. We effectively
integrate these physical insights into our diffusion model to improve its
performance. Extensive experiments demonstrate that our approach significantly
improves various frame-wise reconstruction methods, achieving state-of-the-art
(SOTA) performance on existing benchmarks.

</details>


### [230] [A Simple Algebraic Solution for Estimating the Pose of a Camera from Planar Point Features](https://arxiv.org/abs/2508.01836)
*Tarek Bouazza,Tarek Hamel,Claude Samson*

Main category: cs.CV

TL;DR: A hierarchical algebraic method for estimating camera pose relative to a planar target using 4+ reference points, refining normal direction for robustness.


<details>
  <summary>Details</summary>
Motivation: To accurately and robustly estimate camera pose from planar targets with noisy measurements.

Method: Hierarchical approach: first estimates the target plane's normal, then camera position, distance, and full orientation. Uses averaging to refine normal direction for noise robustness.

Result: Validated through experiments, showing accuracy and robustness to noise.

Conclusion: The method effectively estimates camera pose from planar targets, with improved robustness to measurement noise.

Abstract: This paper presents a simple algebraic method to estimate the pose of a
camera relative to a planar target from $n \geq 4$ reference points with known
coordinates in the target frame and their corresponding bearing measurements in
the camera frame. The proposed approach follows a hierarchical structure;
first, the unit vector normal to the target plane is determined, followed by
the camera's position vector, its distance to the target plane, and finally,
the full orientation. To improve the method's robustness to measurement noise,
an averaging methodology is introduced to refine the estimation of the target's
normal direction. The accuracy and robustness of the approach are validated
through extensive experiments.

</details>


### [231] [OmniEvent: Unified Event Representation Learning](https://arxiv.org/abs/2508.01842)
*Weiqi Yan,Chenlu Lin,Youbiao Wang,Zhipeng Cai,Xiuhong Lin,Yangyang Shi,Weiquan Liu,Yu Zang*

Main category: cs.CV

TL;DR: OmniEvent is a unified event representation learning framework for event cameras, eliminating task-specific designs and achieving SOTA performance across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Event networks rely on task-specific designs due to unstructured data and spatial-temporal inhomogeneity, hindering reuse of architectures.

Method: OmniEvent uses a decouple-enhance-fuse paradigm, independently processing spatial and temporal domains, applying space-filling curves, and fusing features via attention.

Result: OmniEvent outperforms task-specific SOTA by up to 68.2% across 3 tasks and 10 datasets.

Conclusion: OmniEvent provides a unified, efficient framework for event data, enabling standard vision models without architecture changes.

Abstract: Event cameras have gained increasing popularity in computer vision due to
their ultra-high dynamic range and temporal resolution. However, event networks
heavily rely on task-specific designs due to the unstructured data distribution
and spatial-temporal (S-T) inhomogeneity, making it hard to reuse existing
architectures for new tasks. We propose OmniEvent, the first unified event
representation learning framework that achieves SOTA performance across diverse
tasks, fully removing the need of task-specific designs. Unlike previous
methods that treat event data as 3D point clouds with manually tuned S-T
scaling weights, OmniEvent proposes a decouple-enhance-fuse paradigm, where the
local feature aggregation and enhancement is done independently on the spatial
and temporal domains to avoid inhomogeneity issues. Space-filling curves are
applied to enable large receptive fields while improving memory and compute
efficiency. The features from individual domains are then fused by attention to
learn S-T interactions. The output of OmniEvent is a grid-shaped tensor, which
enables standard vision models to process event data without architecture
change. With a unified framework and similar hyper-parameters, OmniEvent
out-performs (tasks-specific) SOTA by up to 68.2% across 3 representative tasks
and 10 datasets (Fig.1). Code will be ready in
https://github.com/Wickyan/OmniEvent .

</details>


### [232] [Beyond Vulnerabilities: A Survey of Adversarial Attacks as Both Threats and Defenses in Computer Vision Systems](https://arxiv.org/abs/2508.01845)
*Zhongliang Guo,Yifei Qian,Yanli Li,Weiye Li,Chun Tong Lei,Shuai Zhao,Lei Fang,Ognjen Arandjelović,Chun Pong Lau*

Main category: cs.CV

TL;DR: A survey on adversarial attacks in computer vision, covering attack methods, defensive uses, and research gaps.


<details>
  <summary>Details</summary>
Motivation: To analyze adversarial techniques as both threats and tools, and to identify research gaps for robust systems.

Method: Systematic analysis of adversarial attacks in pixel-space, physically realizable, and latent-space domains, tracing technical evolution.

Result: Reveals dual nature of adversarial techniques, critical gaps in style transfer protection, and computational efficiency.

Conclusion: Contributes taxonomy and future directions to enhance robustness in computer vision systems.

Abstract: Adversarial attacks against computer vision systems have emerged as a
critical research area that challenges the fundamental assumptions about neural
network robustness and security. This comprehensive survey examines the
evolving landscape of adversarial techniques, revealing their dual nature as
both sophisticated security threats and valuable defensive tools. We provide a
systematic analysis of adversarial attack methodologies across three primary
domains: pixel-space attacks, physically realizable attacks, and latent-space
attacks. Our investigation traces the technical evolution from early
gradient-based methods such as FGSM and PGD to sophisticated optimization
techniques incorporating momentum, adaptive step sizes, and advanced
transferability mechanisms. We examine how physically realizable attacks have
successfully bridged the gap between digital vulnerabilities and real-world
threats through adversarial patches, 3D textures, and dynamic optical
perturbations. Additionally, we explore the emergence of latent-space attacks
that leverage semantic structure in internal representations to create more
transferable and meaningful adversarial examples. Beyond traditional offensive
applications, we investigate the constructive use of adversarial techniques for
vulnerability assessment in biometric authentication systems and protection
against malicious generative models. Our analysis reveals critical research
gaps, particularly in neural style transfer protection and computational
efficiency requirements. This survey contributes a comprehensive taxonomy,
evolution analysis, and identification of future research directions, aiming to
advance understanding of adversarial vulnerabilities and inform the development
of more robust and trustworthy computer vision systems.

</details>


### [233] [Context Guided Transformer Entropy Modeling for Video Compression](https://arxiv.org/abs/2508.01852)
*Junlong Tong,Wei Zhang,Yaohui Jin,Xiaoyu Shen*

Main category: cs.CV

TL;DR: The paper introduces the Context Guided Transformer (CGT) entropy model to improve video redundancy reduction by efficiently leveraging temporal and spatial contexts, reducing computational costs and enhancing context modeling.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with computational overhead from temporal context and lack explicit spatial dependency modeling, limiting decoding efficiency.

Method: CGT uses a temporal context resampler with transformer encoders and a teacher-student network for dependency-weighted spatial context.

Result: CGT reduces entropy modeling time by 65% and achieves an 11% BD-Rate reduction.

Conclusion: CGT effectively addresses computational and modeling limitations, outperforming prior methods.

Abstract: Conditional entropy models effectively leverage spatio-temporal contexts to
reduce video redundancy. However, incorporating temporal context often
introduces additional model complexity and increases computational cost. In
parallel, many existing spatial context models lack explicit modeling the
ordering of spatial dependencies, which may limit the availability of relevant
context during decoding. To address these issues, we propose the Context Guided
Transformer (CGT) entropy model, which estimates probability mass functions of
the current frame conditioned on resampled temporal context and
dependency-weighted spatial context. A temporal context resampler learns
predefined latent queries to extract critical temporal information using
transformer encoders, reducing downstream computational overhead. Meanwhile, a
teacher-student network is designed as dependency-weighted spatial context
assigner to explicitly model the dependency of spatial context order. The
teacher generates an attention map to represent token importance and an entropy
map to reflect prediction certainty from randomly masked inputs, guiding the
student to select the weighted top-k tokens with the highest spatial
dependency. During inference, only the student is used to predict undecoded
tokens based on high-dependency context. Experimental results demonstrate that
our CGT model reduces entropy modeling time by approximately 65% and achieves
an 11% BD-Rate reduction compared to the previous state-of-the-art conditional
entropy model.

</details>


### [234] [Distinguishing Target and Non-Target Fixations with EEG and Eye Tracking in Realistic Visual Scenes](https://arxiv.org/abs/2508.01853)
*Mansi Sharma,Camilo Andrés Martínez Martínez,Benedikt Emanuel Wirth,Antonio Krüger,Philipp Müller*

Main category: cs.CV

TL;DR: The paper investigates classifying target vs. non-target fixations during free visual search in realistic scenes using gaze and EEG data, outperforming prior methods with 83.6% accuracy.


<details>
  <summary>Details</summary>
Motivation: Prior research lacked realism, using abstract stimuli and ignoring scene context, limiting generalizability to real-world scenarios.

Method: A 36-participant study with 140 realistic scenes (e.g., icons on desktops, tools in workshops) using gaze and EEG features.

Result: Achieved 83.6% accuracy in cross-user evaluations, significantly outperforming the previous state-of-the-art (56.9%).

Conclusion: The approach advances realistic visual search classification, demonstrating high accuracy and generalizability across scenes.

Abstract: Distinguishing target from non-target fixations during visual search is a
fundamental building block to understand users' intended actions and to build
effective assistance systems. While prior research indicated the feasibility of
classifying target vs. non-target fixations based on eye tracking and
electroencephalography (EEG) data, these studies were conducted with explicitly
instructed search trajectories, abstract visual stimuli, and disregarded any
scene context. This is in stark contrast with the fact that human visual search
is largely driven by scene characteristics and raises questions regarding
generalizability to more realistic scenarios. To close this gap, we, for the
first time, investigate the classification of target vs. non-target fixations
during free visual search in realistic scenes. In particular, we conducted a
36-participants user study using a large variety of 140 realistic visual search
scenes in two highly relevant application scenarios: searching for icons on
desktop backgrounds and finding tools in a cluttered workshop. Our approach
based on gaze and EEG features outperforms the previous state-of-the-art
approach based on a combination of fixation duration and saccade-related
potentials. We perform extensive evaluations to assess the generalizability of
our approach across scene types. Our approach significantly advances the
ability to distinguish between target and non-target fixations in realistic
scenarios, achieving 83.6% accuracy in cross-user evaluations. This
substantially outperforms previous methods based on saccade-related potentials,
which reached only 56.9% accuracy.

</details>


### [235] [DiffusionFF: Face Forgery Detection via Diffusion-based Artifact Localization](https://arxiv.org/abs/2508.01873)
*Siran Peng,Haoyuan Zhang,Li Gao,Tianshuo Zhang,Bao Li,Zhen Lei*

Main category: cs.CV

TL;DR: DiffusionFF enhances face forgery detection using diffusion-based artifact localization, improving accuracy and explainability.


<details>
  <summary>Details</summary>
Motivation: The need for robust face forgery detection and precise artifact localization to improve model explainability and user trust.

Method: Uses a denoising diffusion model to generate DSSIM maps, fused with semantic features from a pretrained detector.

Result: Achieves superior detection performance and fine-grained artifact localization on benchmarks.

Conclusion: DiffusionFF is effective for accurate face forgery detection and localization.

Abstract: The rapid evolution of deepfake generation techniques demands robust and
accurate face forgery detection algorithms. While determining whether an image
has been manipulated remains essential, the ability to precisely localize
forgery artifacts has become increasingly important for improving model
explainability and fostering user trust. To address this challenge, we propose
DiffusionFF, a novel framework that enhances face forgery detection through
diffusion-based artifact localization. Our method utilizes a denoising
diffusion model to generate high-quality Structural Dissimilarity (DSSIM) maps,
which effectively capture subtle traces of manipulation. These DSSIM maps are
then fused with high-level semantic features extracted by a pretrained forgery
detector, leading to significant improvements in detection accuracy. Extensive
experiments on both cross-dataset and intra-dataset benchmarks demonstrate that
DiffusionFF not only achieves superior detection performance but also offers
precise and fine-grained artifact localization, highlighting its overall
effectiveness.

</details>


### [236] [StreamAgent: Towards Anticipatory Agents for Streaming Video Understanding](https://arxiv.org/abs/2508.01875)
*Haolin Yang,Feilong Tang,Linxiao Zhao,Xiang An,Ming Hu,Huifa Li,Xinlin Zhuang,Boqian Wang,Yifan Lu,Xiaofeng Zhang,Abdalla Swikir,Junjun He,Zongyuan Ge,Imran Razzak*

Main category: cs.CV

TL;DR: Proposes StreamAgent for real-time video understanding, integrating anticipation and proactive decision-making to improve responsiveness in dynamic streams.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of existing methods in real-time video processing, which lack proactive planning and future anticipation, hindering responsiveness.

Method: Introduces StreamAgent with anticipatory planning, question semantics integration, and a streaming KV-cache memory for efficient inference.

Result: Outperforms existing methods in accuracy and efficiency for streaming and long video tasks.

Conclusion: StreamAgent enhances real-time video understanding, proving practical for dynamic scenarios like autonomous driving and surveillance.

Abstract: Real-time streaming video understanding in domains such as autonomous driving
and intelligent surveillance poses challenges beyond conventional offline video
processing, requiring continuous perception, proactive decision making, and
responsive interaction based on dynamically evolving visual content. However,
existing methods rely on alternating perception-reaction or asynchronous
triggers, lacking task-driven planning and future anticipation, which limits
their real-time responsiveness and proactive decision making in evolving video
streams. To this end, we propose a StreamAgent that anticipates the temporal
intervals and spatial regions expected to contain future task-relevant
information to enable proactive and goal-driven responses. Specifically, we
integrate question semantics and historical observations through prompting the
anticipatory agent to anticipate the temporal progression of key events, align
current observations with the expected future evidence, and subsequently adjust
the perception action (e.g., attending to task-relevant regions or continuously
tracking in subsequent frames). To enable efficient inference, we design a
streaming KV-cache memory mechanism that constructs a hierarchical memory
structure for selective recall of relevant tokens, enabling efficient semantic
retrieval while reducing the overhead of storing all tokens in the traditional
KV-cache. Extensive experiments on streaming and long video understanding tasks
demonstrate that our method outperforms existing methods in response accuracy
and real-time efficiency, highlighting its practical value for real-world
streaming scenarios.

</details>


### [237] [Medical Image De-Identification Resources: Synthetic DICOM Data and Tools for Validation](https://arxiv.org/abs/2508.01889)
*Michael W. Rutherford,Tracy Nolan,Linmin Pei,Ulrike Wagner,Qinyan Pan,Phillip Farmer,Kirk Smith,Benjamin Kopchick,Laura Opsahl-Ong,Granger Sutton,David Clunie,Keyvan Farahani,Fred Prior*

Main category: cs.CV

TL;DR: The paper introduces the MIDI dataset and framework to benchmark medical image de-identification workflows, addressing privacy concerns in large-scale data sharing.


<details>
  <summary>Details</summary>
Motivation: Ensuring patient privacy in medical imaging data sharing is challenging due to embedded PHI/PII in DICOM files. Existing tools lack objective evaluation, limiting reproducibility and regulatory confidence.

Method: Developed the MIDI dataset with synthetic PHI/PII embedded in DICOM files, along with evaluation tools (Python script, answer keys) for automated benchmarking.

Result: The dataset includes 538 subjects, 53,581 DICOM images, and spans multiple vendors, modalities, and cancer types. The framework aligns with HIPAA, DICOM standards, and TCIA practices.

Conclusion: The MIDI framework enables objective, standards-driven evaluation of de-identification workflows, promoting safer and more consistent medical image sharing.

Abstract: Medical imaging research increasingly depends on large-scale data sharing to
promote reproducibility and train Artificial Intelligence (AI) models. Ensuring
patient privacy remains a significant challenge for open-access data sharing.
Digital Imaging and Communications in Medicine (DICOM), the global standard
data format for medical imaging, encodes both essential clinical metadata and
extensive protected health information (PHI) and personally identifiable
information (PII). Effective de-identification must remove identifiers,
preserve scientific utility, and maintain DICOM validity. Tools exist to
perform de-identification, but few assess its effectiveness, and most rely on
subjective reviews, limiting reproducibility and regulatory confidence. To
address this gap, we developed an openly accessible DICOM dataset infused with
synthetic PHI/PII and an evaluation framework for benchmarking image
de-identification workflows. The Medical Image de-identification (MIDI) dataset
was built using publicly available de-identified data from The Cancer Imaging
Archive (TCIA). It includes 538 subjects (216 for validation, 322 for testing),
605 studies, 708 series, and 53,581 DICOM image instances. These span multiple
vendors, imaging modalities, and cancer types. Synthetic PHI and PII were
embedded into structured data elements, plain text data elements, and pixel
data to simulate real-world identity leaks encountered by TCIA curation teams.
Accompanying evaluation tools include a Python script, answer keys (known
truth), and mapping files that enable automated comparison of curated data
against expected transformations. The framework is aligned with the HIPAA
Privacy Rule "Safe Harbor" method, DICOM PS3.15 Confidentiality Profiles, and
TCIA best practices. It supports objective, standards-driven evaluation of
de-identification workflows, promoting safer and more consistent medical image
sharing.

</details>


### [238] [EgoTrigger: Toward Audio-Driven Image Capture for Human Memory Enhancement in All-Day Energy-Efficient Smart Glasses](https://arxiv.org/abs/2508.01915)
*Akshay Paruchuri,Sinan Hersek,Lavisha Aggarwal,Qiao Yang,Xin Liu,Achin Kulshrestha,Andrea Colaco,Henry Fuchs,Ishan Chatterjee*

Main category: cs.CV

TL;DR: EgoTrigger uses audio cues to selectively activate cameras in smart glasses, reducing energy use by 54% while maintaining performance for memory enhancement tasks.


<details>
  <summary>Details</summary>
Motivation: Smart glasses for all-day use face energy efficiency challenges due to continuous sensing. EgoTrigger aims to balance energy use with functionality for human memory enhancement.

Method: EgoTrigger leverages a lightweight audio model (YAMNet) and custom classification to trigger cameras based on hand-object interaction sounds. Evaluated on QA-Ego4D and HME-QA datasets.

Result: EgoTrigger reduces frame usage by 54%, saving energy in cameras and downstream operations, while maintaining comparable performance on episodic memory tasks.

Conclusion: EgoTrigger's context-aware triggering is a promising approach for energy-efficient smart glasses, supporting applications like memory enhancement for daily activities.

Abstract: All-day smart glasses are likely to emerge as platforms capable of continuous
contextual sensing, uniquely positioning them for unprecedented assistance in
our daily lives. Integrating the multi-modal AI agents required for human
memory enhancement while performing continuous sensing, however, presents a
major energy efficiency challenge for all-day usage. Achieving this balance
requires intelligent, context-aware sensor management. Our approach,
EgoTrigger, leverages audio cues from the microphone to selectively activate
power-intensive cameras, enabling efficient sensing while preserving
substantial utility for human memory enhancement. EgoTrigger uses a lightweight
audio model (YAMNet) and a custom classification head to trigger image capture
from hand-object interaction (HOI) audio cues, such as the sound of a drawer
opening or a medication bottle being opened. In addition to evaluating on the
QA-Ego4D dataset, we introduce and evaluate on the Human Memory Enhancement
Question-Answer (HME-QA) dataset. Our dataset contains 340 human-annotated
first-person QA pairs from full-length Ego4D videos that were curated to ensure
that they contained audio, focusing on HOI moments critical for contextual
understanding and memory. Our results show EgoTrigger can use 54% fewer frames
on average, significantly saving energy in both power-hungry sensing components
(e.g., cameras) and downstream operations (e.g., wireless transmission), while
achieving comparable performance on datasets for an episodic memory task. We
believe this context-aware triggering strategy represents a promising direction
for enabling energy-efficient, functional smart glasses capable of all-day use
-- supporting applications like helping users recall where they placed their
keys or information about their routine activities (e.g., taking medications).

</details>


### [239] [InspectVLM: Unified in Theory, Unreliable in Practice](https://arxiv.org/abs/2508.01921)
*Conor Wallace,Isaac Corley,Jonathan Lwowski*

Main category: cs.CV

TL;DR: Unified vision-language models (VLMs) like InspectVLM show promise for industrial inspection but fall short in robustness and precision compared to traditional models.


<details>
  <summary>Details</summary>
Motivation: To evaluate the viability of unified VLMs for industrial inspection, addressing the inefficiency of disjoint task-specific models.

Method: InspectVLM, a Florence-2-based VLM, trained on InspectMM, a large-scale multimodal dataset, is tested on classification, detection, and keypoint tasks.

Result: InspectVLM performs well on classification and keypoint tasks but struggles with fine-grained detection, prompt variability, and visual grounding.

Conclusion: Current VLMs lack the robustness and precision needed for critical industrial inspections despite their conceptual appeal.

Abstract: Unified vision-language models (VLMs) promise to streamline computer vision
pipelines by reframing multiple visual tasks such as classification, detection,
and keypoint localization within a single language-driven interface. This
architecture is particularly appealing in industrial inspection, where managing
disjoint task-specific models introduces complexity, inefficiency, and
maintenance overhead. In this paper, we critically evaluate the viability of
this unified paradigm using InspectVLM, a Florence-2-based VLM trained on
InspectMM, our new large-scale multimodal, multitask inspection dataset. While
InspectVLM performs competitively on image-level classification and structured
keypoint tasks, we find that it fails to match traditional ResNet-based models
in core inspection metrics. Notably, the model exhibits brittle behavior under
low prompt variability, produces degenerate outputs for fine-grained object
detection, and frequently defaults to memorized language responses regardless
of visual input. Our findings suggest that while language-driven unification
offers conceptual elegance, current VLMs lack the visual grounding and
robustness necessary for deployment in precision critical industrial
inspections.

</details>


### [240] [IAUNet: Instance-Aware U-Net](https://arxiv.org/abs/2508.01928)
*Yaroslav Prytula,Illia Tsiporenko,Ali Zeynalli,Dmytro Fishman*

Main category: cs.CV

TL;DR: IAUNet is a query-based U-Net architecture for biomedical instance segmentation, featuring a lightweight Pixel decoder and Transformer decoder, outperforming state-of-the-art models on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: To explore U-Net's potential in query-based segmentation and address challenges like overlapping cells in biomedical imaging.

Method: Combines U-Net with a lightweight Pixel decoder and Transformer decoder for multi-scale feature refinement.

Result: IAUNet outperforms existing models on public and proprietary datasets, setting a new benchmark.

Conclusion: IAUNet is a strong baseline for cell instance segmentation, with code publicly available.

Abstract: Instance segmentation is critical in biomedical imaging to accurately
distinguish individual objects like cells, which often overlap and vary in
size. Recent query-based methods, where object queries guide segmentation, have
shown strong performance. While U-Net has been a go-to architecture in medical
image segmentation, its potential in query-based approaches remains largely
unexplored. In this work, we present IAUNet, a novel query-based U-Net
architecture. The core design features a full U-Net architecture, enhanced by a
novel lightweight convolutional Pixel decoder, making the model more efficient
and reducing the number of parameters. Additionally, we propose a Transformer
decoder that refines object-specific features across multiple scales. Finally,
we introduce the 2025 Revvity Full Cell Segmentation Dataset, a unique resource
with detailed annotations of overlapping cell cytoplasm in brightfield images,
setting a new benchmark for biomedical instance segmentation. Experiments on
multiple public datasets and our own show that IAUNet outperforms most
state-of-the-art fully convolutional, transformer-based, and query-based models
and cell segmentation-specific models, setting a strong baseline for cell
instance segmentation tasks. Code is available at
https://github.com/SlavkoPrytula/IAUNet

</details>


### [241] [Proactive Disentangled Modeling of Trigger-Object Pairings for Backdoor Defense](https://arxiv.org/abs/2508.01932)
*Kyle Stein,Andrew A. Mahyari,Guillermo Francia III,Eman El-Sheikh*

Main category: cs.CV

TL;DR: DBOM is a proactive framework using disentangled modeling to detect and neutralize backdoor threats in datasets, leveraging VLMs for zero-shot generalization to unseen trigger-object pairings.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks and generative AI are vulnerable to backdoor attacks, including unseen trigger-object configurations, necessitating robust detection methods.

Method: DBOM uses structured disentanglement with VLMs, visual prompt tuning, and separation/diversity losses to decompose and align trigger-object representations.

Result: DBOM effectively detects poisoned images on CIFAR-10 and GTSRB, improving security in DNN training pipelines.

Conclusion: DBOM offers a scalable solution for detecting backdoor threats, including unseen configurations, enhancing AI security.

Abstract: Deep neural networks (DNNs) and generative AI (GenAI) are increasingly
vulnerable to backdoor attacks, where adversaries embed triggers into inputs to
cause models to misclassify or misinterpret target labels. Beyond traditional
single-trigger scenarios, attackers may inject multiple triggers across various
object classes, forming unseen backdoor-object configurations that evade
standard detection pipelines. In this paper, we introduce DBOM (Disentangled
Backdoor-Object Modeling), a proactive framework that leverages structured
disentanglement to identify and neutralize both seen and unseen backdoor
threats at the dataset level. Specifically, DBOM factorizes input image
representations by modeling triggers and objects as independent primitives in
the embedding space through the use of Vision-Language Models (VLMs). By
leveraging the frozen, pre-trained encoders of VLMs, our approach decomposes
the latent representations into distinct components through a learnable visual
prompt repository and prompt prefix tuning, ensuring that the relationships
between triggers and objects are explicitly captured. To separate trigger and
object representations in the visual prompt repository, we introduce the
trigger-object separation and diversity losses that aids in disentangling
trigger and object visual features. Next, by aligning image features with
feature decomposition and fusion, as well as learned contextual prompt tokens
in a shared multimodal space, DBOM enables zero-shot generalization to novel
trigger-object pairings that were unseen during training, thereby offering
deeper insights into adversarial attack patterns. Experimental results on
CIFAR-10 and GTSRB demonstrate that DBOM robustly detects poisoned images prior
to downstream training, significantly enhancing the security of DNN training
pipelines.

</details>


### [242] [CVD-SfM: A Cross-View Deep Front-end Structure-from-Motion System for Sparse Localization in Multi-Altitude Scenes](https://arxiv.org/abs/2508.01936)
*Yaxuan Li,Yewei Huang,Bijay Gaudel,Hamidreza Jafarnejadsani,Brendan Englot*

Main category: cs.CV

TL;DR: A novel multi-altitude camera pose estimation system using cross-view transformers, deep features, and structure-from-motion, validated on new datasets, outperforms existing methods in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of robust and accurate localization across varied altitudes with sparse image input, especially for real-world robotic applications.

Method: Integrates cross-view transformers, deep features, and structure-from-motion into a unified framework. Introduces two new datasets for benchmarking.

Result: Superior performance in accuracy and robustness for multi-altitude sparse pose estimation compared to existing solutions.

Conclusion: The system is well-suited for real-world applications like aerial navigation, search and rescue, and automated inspection.

Abstract: We present a novel multi-altitude camera pose estimation system, addressing
the challenges of robust and accurate localization across varied altitudes when
only considering sparse image input. The system effectively handles diverse
environmental conditions and viewpoint variations by integrating the cross-view
transformer, deep features, and structure-from-motion into a unified framework.
To benchmark our method and foster further research, we introduce two newly
collected datasets specifically tailored for multi-altitude camera pose
estimation; datasets of this nature remain rare in the current literature. The
proposed framework has been validated through extensive comparative analyses on
these datasets, demonstrating that our system achieves superior performance in
both accuracy and robustness for multi-altitude sparse pose estimation tasks
compared to existing solutions, making it well suited for real-world robotic
applications such as aerial navigation, search and rescue, and automated
inspection.

</details>


### [243] [Self-Supervised YOLO: Leveraging Contrastive Learning for Label-Efficient Object Detection](https://arxiv.org/abs/2508.01966)
*Manikanta Kotthapalli,Reshma Bhatia,Nainsi Jain*

Main category: cs.CV

TL;DR: The paper explores contrastive self-supervised learning (SSL) to reduce reliance on labeled data for YOLO object detectors, showing improved performance with unlabeled pretraining.


<details>
  <summary>Details</summary>
Motivation: To reduce dependency on large-scale labeled datasets for training YOLO detectors by leveraging unlabeled data through SSL.

Method: Adapts YOLO backbones as encoders, uses global pooling and projection heads, and optimizes contrastive loss with COCO unlabeled data. Fine-tunes on a cyclist detection task.

Result: SSL pretraining improves mAP, convergence speed, and precision-recall, especially with limited labels. SimCLR-pretrained YOLOv8 outperforms supervised training.

Conclusion: Contrastive SSL is effective for one-stage detectors, demonstrating the potential of unlabeled data for label-efficient object detection.

Abstract: One-stage object detectors such as the YOLO family achieve state-of-the-art
performance in real-time vision applications but remain heavily reliant on
large-scale labeled datasets for training. In this work, we present a
systematic study of contrastive self-supervised learning (SSL) as a means to
reduce this dependency by pretraining YOLOv5 and YOLOv8 backbones on unlabeled
images using the SimCLR framework. Our approach introduces a simple yet
effective pipeline that adapts YOLO's convolutional backbones as encoders,
employs global pooling and projection heads, and optimizes a contrastive loss
using augmentations of the COCO unlabeled dataset (120k images). The pretrained
backbones are then fine-tuned on a cyclist detection task with limited labeled
data. Experimental results show that SSL pretraining leads to consistently
higher mAP, faster convergence, and improved precision-recall performance,
especially in low-label regimes. For example, our SimCLR-pretrained YOLOv8
achieves a mAP@50:95 of 0.7663, outperforming its supervised counterpart
despite using no annotations during pretraining. These findings establish a
strong baseline for applying contrastive SSL to one-stage detectors and
highlight the potential of unlabeled data as a scalable resource for
label-efficient object detection.

</details>


### [244] [On-the-Fly Object-aware Representative Point Selection in Point Cloud](https://arxiv.org/abs/2508.01980)
*Xiaoyu Zhang,Ziwei Wang,Hai Dong,Zhifeng Bao,Jiajun Liu*

Main category: cs.CV

TL;DR: A framework for point cloud downsampling in AVs, preserving object-related data while reducing storage and processing costs.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of high data volume in AV point clouds by efficiently downsizing without losing critical object information.

Method: Two-step approach: (1) Object Presence Detection using unsupervised and supervised classifiers, (2) Sampling Budget Allocation for optimal point selection.

Result: Outperforms state-of-the-art baselines in efficiency and effectiveness on KITTI and nuScenes datasets.

Conclusion: A scalable, model-agnostic solution for AV point cloud downsampling, enhancing storage and processing efficiency.

Abstract: Point clouds are essential for object modeling and play a critical role in
assisting driving tasks for autonomous vehicles (AVs). However, the significant
volume of data generated by AVs creates challenges for storage, bandwidth, and
processing cost. To tackle these challenges, we propose a representative point
selection framework for point cloud downsampling, which preserves critical
object-related information while effectively filtering out irrelevant
background points. Our method involves two steps: (1) Object Presence
Detection, where we introduce an unsupervised density peak-based classifier and
a supervised Na\"ive Bayes classifier to handle diverse scenarios, and (2)
Sampling Budget Allocation, where we propose a strategy that selects
object-relevant points while maintaining a high retention rate of object
information. Extensive experiments on the KITTI and nuScenes datasets
demonstrate that our method consistently outperforms state-of-the-art baselines
in both efficiency and effectiveness across varying sampling rates. As a
model-agnostic solution, our approach integrates seamlessly with diverse
downstream models, making it a valuable and scalable addition to the 3D point
cloud downsampling toolkit for AV applications.

</details>


### [245] [IMoRe: Implicit Program-Guided Reasoning for Human Motion Q&A](https://arxiv.org/abs/2508.01984)
*Chen Li,Chinthani Sugandhika,Yeo Keat Ee,Eric Peh,Hao Zhang,Hong Yang,Deepu Rajan,Basura Fernando*

Main category: cs.CV

TL;DR: The paper introduces IMoRe, an implicit program-guided motion reasoning framework, eliminating the need for manually defined modules and improving scalability and adaptability in human motion Q&A.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on explicit program execution with manually defined modules, limiting scalability and adaptability.

Method: IMoRe uses structured program functions for precise reasoning and a program-guided reading mechanism to dynamically select multi-level motion representations from a pretrained motion ViT.

Result: The model achieves state-of-the-art performance on Babel-QA and generalizes well to a new dataset based on HuMMan.

Conclusion: IMoRe demonstrates superior adaptability and scalability in motion reasoning tasks.

Abstract: Existing human motion Q\&A methods rely on explicit program execution, where
the requirement for manually defined functional modules may limit the
scalability and adaptability. To overcome this, we propose an implicit
program-guided motion reasoning (IMoRe) framework that unifies reasoning across
multiple query types without manually designed modules. Unlike existing
implicit reasoning approaches that infer reasoning operations from question
words, our model directly conditions on structured program functions, ensuring
a more precise execution of reasoning steps. Additionally, we introduce a
program-guided reading mechanism, which dynamically selects multi-level motion
representations from a pretrained motion Vision Transformer (ViT), capturing
both high-level semantics and fine-grained motion cues. The reasoning module
iteratively refines memory representations, leveraging structured program
functions to extract relevant information for different query types. Our model
achieves state-of-the-art performance on Babel-QA and generalizes to a newly
constructed motion Q\&A dataset based on HuMMan, demonstrating its adaptability
across different motion reasoning datasets. Code and dataset are available at:
https://github.com/LUNAProject22/IMoRe.

</details>


### [246] [Deeply Dual Supervised learning for melanoma recognition](https://arxiv.org/abs/2508.01994)
*Rujosh Polma,Krishnan Menon Iyer*

Main category: cs.CV

TL;DR: A novel Deeply Dual Supervised Learning framework improves melanoma recognition by integrating local and global features, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with identifying subtle visual cues in melanoma detection, prompting the need for a more comprehensive approach.

Method: The framework uses a dual-pathway structure with dual attention and multi-scale feature aggregation to enhance feature extraction.

Result: The model achieves higher accuracy and better resilience against false positives on benchmark datasets.

Conclusion: The work advances automated skin cancer recognition and validates dual supervised learning in medical image analysis.

Abstract: As the application of deep learning in dermatology continues to grow, the
recognition of melanoma has garnered significant attention, demonstrating
potential for improving diagnostic accuracy. Despite advancements in image
classification techniques, existing models still face challenges in identifying
subtle visual cues that differentiate melanoma from benign lesions. This paper
presents a novel Deeply Dual Supervised Learning framework that integrates
local and global feature extraction to enhance melanoma recognition. By
employing a dual-pathway structure, the model focuses on both fine-grained
local features and broader contextual information, ensuring a comprehensive
understanding of the image content. The framework utilizes a dual attention
mechanism that dynamically emphasizes critical features, thereby reducing the
risk of overlooking subtle characteristics of melanoma. Additionally, we
introduce a multi-scale feature aggregation strategy to ensure robust
performance across varying image resolutions. Extensive experiments on
benchmark datasets demonstrate that our framework significantly outperforms
state-of-the-art methods in melanoma detection, achieving higher accuracy and
better resilience against false positives. This work lays the foundation for
future research in automated skin cancer recognition and highlights the
effectiveness of dual supervised learning in medical image analysis.

</details>


### [247] [Fast and Memory-efficient Non-line-of-sight Imaging with Quasi-Fresnel Transform](https://arxiv.org/abs/2508.02003)
*Yijun Wei,Jianyu Wang,Leping Xiao,Zuoqiang Shi,Xing Fu,Lingyun Qiu*

Main category: cs.CV

TL;DR: A novel NLOS imaging method uses 2D functions and a Quasi-Fresnel transform to reduce computational costs and memory usage, enabling real-time, high-resolution imaging on lightweight devices.


<details>
  <summary>Details</summary>
Motivation: Existing NLOS imaging methods model scenes in 3D, leading to high computational and memory costs, limiting practicality and real-time performance.

Method: Represent hidden scenes with 2D functions and use a Quasi-Fresnel transform for direct inversion, reducing complexity.

Result: Reduces runtime and memory demands by orders of magnitude while maintaining imaging quality.

Conclusion: This approach enables efficient NLOS imaging on lightweight devices, expanding its practical applications.

Abstract: Non-line-of-sight (NLOS) imaging seeks to reconstruct hidden objects by
analyzing reflections from intermediary surfaces. Existing methods typically
model both the measurement data and the hidden scene in three dimensions,
overlooking the inherently two-dimensional nature of most hidden objects. This
oversight leads to high computational costs and substantial memory consumption,
limiting practical applications and making real-time, high-resolution NLOS
imaging on lightweight devices challenging. In this paper, we introduce a novel
approach that represents the hidden scene using two-dimensional functions and
employs a Quasi-Fresnel transform to establish a direct inversion formula
between the measurement data and the hidden scene. This transformation
leverages the two-dimensional characteristics of the problem to significantly
reduce computational complexity and memory requirements. Our algorithm
efficiently performs fast transformations between these two-dimensional
aggregated data, enabling rapid reconstruction of hidden objects with minimal
memory usage. Compared to existing methods, our approach reduces runtime and
memory demands by several orders of magnitude while maintaining imaging
quality. The substantial reduction in memory usage not only enhances
computational efficiency but also enables NLOS imaging on lightweight devices
such as mobile and embedded systems. We anticipate that this method will
facilitate real-time, high-resolution NLOS imaging and broaden its
applicability across a wider range of platforms.

</details>


### [248] [Devil is in the Detail: Towards Injecting Fine Details of Image Prompt in Image Generation via Conflict-free Guidance and Stratified Attention](https://arxiv.org/abs/2508.02004)
*Kyungmin Jo,Jooyeol Yun,Jaegul Choo*

Main category: cs.CV

TL;DR: The paper addresses limitations in text-to-image diffusion models by improving the use of image prompts through conflict-free guidance and a new self-attention method called Stratified Attention.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to capture intricate details from image prompts and create conflicting signals in classifier-free guidance, limiting the fidelity of generated images.

Method: Proposes conflict-free guidance to avoid conflicting signals and introduces Stratified Attention to balance realism and alignment with image prompts by jointly using keys and values from both images.

Result: The method outperforms existing models in faithfully reflecting image prompts across three image generation tasks.

Conclusion: The proposed approach resolves key issues in image-prompted generation, enhancing both alignment with prompts and realism in outputs.

Abstract: While large-scale text-to-image diffusion models enable the generation of
high-quality, diverse images from text prompts, these prompts struggle to
capture intricate details, such as textures, preventing the user intent from
being reflected. This limitation has led to efforts to generate images
conditioned on user-provided images, referred to as image prompts. Recent work
modifies the self-attention mechanism to impose image conditions in generated
images by replacing or concatenating the keys and values from the image prompt.
This enables the self-attention layer to work like a cross-attention layer,
generally used to incorporate text prompts. In this paper, we identify two
common issues in existing methods of modifying self-attention to generate
images that reflect the details of image prompts. First, existing approaches
neglect the importance of image prompts in classifier-free guidance.
Specifically, current methods use image prompts as both desired and undesired
conditions in classifier-free guidance, causing conflicting signals. To resolve
this, we propose conflict-free guidance by using image prompts only as desired
conditions, ensuring that the generated image faithfully reflects the image
prompt. In addition, we observe that the two most common self-attention
modifications involve a trade-off between the realism of the generated image
and alignment with the image prompt. Specifically, selecting more keys and
values from the image prompt improves alignment, while selecting more from the
generated image enhances realism. To balance both, we propose an new
self-attention modification method, Stratified Attention to jointly use keys
and values from both images rather than selecting between them. Through
extensive experiments across three image generation tasks, we show that the
proposed method outperforms existing image-prompting models in faithfully
reflecting the image prompt.

</details>


### [249] [Bench2ADVLM: A Closed-Loop Benchmark for Vision-language Models in Autonomous Driving](https://arxiv.org/abs/2508.02028)
*Tianyuan Zhang,Ting Jin,Lu Wang,Jiangfan Liu,Siyuan Liang,Mingchuan Zhang,Aishan Liu,Xianglong Liu*

Main category: cs.CV

TL;DR: Bench2ADVLM introduces a closed-loop evaluation framework for Vision-Language Models (VLMs) in autonomous driving, addressing gaps in current open-loop protocols by enabling real-time, interactive testing across simulation and physical platforms.


<details>
  <summary>Details</summary>
Motivation: Current evaluation protocols for VLM-based autonomous driving systems (ADVLMs) are limited to open-loop settings, missing realistic closed-loop interactions and safety assessments.

Method: The framework uses a dual-system adaptation architecture to translate high-level driving commands into mid-level actions for simulation and low-level signals for physical vehicles, with a self-reflective scenario generation module for comprehensive testing.

Result: Experiments show existing ADVLMs underperform in closed-loop conditions, highlighting the framework's diagnostic capabilities.

Conclusion: Bench2ADVLM provides a unified, hierarchical pipeline for evaluating ADVLMs, revealing performance gaps and enhancing safety-critical assessments.

Abstract: Vision-Language Models (VLMs) have recently emerged as a promising paradigm
in autonomous driving (AD). However, current performance evaluation protocols
for VLM-based AD systems (ADVLMs) are predominantly confined to open-loop
settings with static inputs, neglecting the more realistic and informative
closed-loop setting that captures interactive behavior, feedback resilience,
and real-world safety. To address this, we introduce Bench2ADVLM, a unified
hierarchical closed-loop evaluation framework for real-time, interactive
assessment of ADVLMs across both simulation and physical platforms. Inspired by
dual-process theories of cognition, we first adapt diverse ADVLMs to simulation
environments via a dual-system adaptation architecture. In this design,
heterogeneous high-level driving commands generated by target ADVLMs (fast
system) are interpreted by a general-purpose VLM (slow system) into
standardized mid-level control actions suitable for execution in simulation. To
bridge the gap between simulation and reality, we design a physical control
abstraction layer that translates these mid-level actions into low-level
actuation signals, enabling, for the first time, closed-loop testing of ADVLMs
on physical vehicles. To enable more comprehensive evaluation, Bench2ADVLM
introduces a self-reflective scenario generation module that automatically
explores model behavior and uncovers potential failure modes for
safety-critical scenario generation. Overall, Bench2ADVLM establishes a
hierarchical evaluation pipeline that seamlessly integrates high-level abstract
reasoning, mid-level simulation actions, and low-level real-world execution.
Experiments on diverse scenarios across multiple state-of-the-art ADVLMs and
physical platforms validate the diagnostic strength of our framework, revealing
that existing ADVLMs still exhibit limited performance under closed-loop
conditions.

</details>


### [250] [Protego: User-Centric Pose-Invariant Privacy Protection Against Face Recognition-Induced Digital Footprint Exposure](https://arxiv.org/abs/2508.02034)
*Ziling Wang,Shuya Yang,Jialin Lu,Ka-Ho Chow*

Main category: cs.CV

TL;DR: Protego is a privacy protection method that deforms facial images to prevent retrieval-based intrusions, outperforming existing methods by 2x and ensuring visual coherence.


<details>
  <summary>Details</summary>
Motivation: Addressing privacy concerns from FR technologies like Clearview AI and PimEyes, which expose personal data without consent.

Method: Encapsulates 3D facial signatures into pose-invariant 2D representations, dynamically deforming them into natural-looking 3D masks for online sharing.

Result: Significantly reduces retrieval accuracy across black-box FR models, performing 2x better than existing methods with high visual coherence.

Conclusion: Protego effectively combats misuse of FR for surveillance and identity tracing, offering robust privacy protection.

Abstract: Face recognition (FR) technologies are increasingly used to power large-scale
image retrieval systems, raising serious privacy concerns. Services like
Clearview AI and PimEyes allow anyone to upload a facial photo and retrieve a
large amount of online content associated with that person. This not only
enables identity inference but also exposes their digital footprint, such as
social media activity, private photos, and news reports, often without their
consent. In response to this emerging threat, we propose Protego, a
user-centric privacy protection method that safeguards facial images from such
retrieval-based privacy intrusions. Protego encapsulates a user's 3D facial
signatures into a pose-invariant 2D representation, which is dynamically
deformed into a natural-looking 3D mask tailored to the pose and expression of
any facial image of the user, and applied prior to online sharing. Motivated by
a critical limitation of existing methods, Protego amplifies the sensitivity of
FR models so that protected images cannot be matched even among themselves.
Experiments show that Protego significantly reduces retrieval accuracy across a
wide range of black-box FR models and performs at least 2x better than existing
methods. It also offers unprecedented visual coherence, particularly in video
settings where consistency and natural appearance are essential. Overall,
Protego contributes to the fight against the misuse of FR for mass surveillance
and unsolicited identity tracing.

</details>


### [251] [Conditional Diffusion Model with Anatomical-Dose Dual Constraints for End-to-End Multi-Tumor Dose Prediction](https://arxiv.org/abs/2508.02043)
*Hui Xie,Haiqin Hu,Lijuan Ding,Qing Li,Yue Sun,Tao Tan*

Main category: cs.CV

TL;DR: ADDiff-Dose, a conditional diffusion model, improves radiotherapy dose prediction with better accuracy, generalization, and clinical applicability, reducing planning time significantly.


<details>
  <summary>Details</summary>
Motivation: Current radiotherapy planning is time-consuming and expert-dependent, while deep learning methods lack generalization and accuracy.

Method: Uses LightweightVAE3D for CT data compression, integrates multimodal inputs, and employs a progressive noise addition/denoising framework with multi-head attention and a composite loss function.

Result: Outperforms baselines (MAE 0.101-0.154, DICE 0.927), reduces spinal cord dose error to 0.1 Gy, and cuts planning time to 22 seconds per case.

Conclusion: ADDiff-Dose offers a generalizable, efficient solution for automated radiotherapy planning, improving workflow efficiency.

Abstract: Radiotherapy treatment planning often relies on time-consuming,
trial-and-error adjustments that heavily depend on the expertise of
specialists, while existing deep learning methods face limitations in
generalization, prediction accuracy, and clinical applicability. To tackle
these challenges, we propose ADDiff-Dose, an Anatomical-Dose Dual Constraints
Conditional Diffusion Model for end-to-end multi-tumor dose prediction. The
model employs LightweightVAE3D to compress high-dimensional CT data and
integrates multimodal inputs, including target and organ-at-risk (OAR) masks
and beam parameters, within a progressive noise addition and denoising
framework. It incorporates conditional features via a multi-head attention
mechanism and utilizes a composite loss function combining MSE, conditional
terms, and KL divergence to ensure both dosimetric accuracy and compliance with
clinical constraints. Evaluation on a large-scale public dataset (2,877 cases)
and three external institutional cohorts (450 cases in total) demonstrates that
ADDiff-Dose significantly outperforms traditional baselines, achieving an MAE
of 0.101-0.154 (compared to 0.316 for UNet and 0.169 for GAN models), a DICE
coefficient of 0.927 (a 6.8% improvement), and limiting spinal cord maximum
dose error to within 0.1 Gy. The average plan generation time per case is
reduced to 22 seconds. Ablation studies confirm that the structural encoder
enhances compliance with clinical dose constraints by 28.5%. To our knowledge,
this is the first study to introduce a conditional diffusion model framework
for radiotherapy dose prediction, offering a generalizable and efficient
solution for automated treatment planning across diverse tumor sites, with the
potential to substantially reduce planning time and improve clinical workflow
efficiency.

</details>


### [252] [Mapillary Vistas Validation for Fine-Grained Traffic Signs: A Benchmark Revealing Vision-Language Model Limitations](https://arxiv.org/abs/2508.02047)
*Sparsh Garg,Abhishek Aich*

Main category: cs.CV

TL;DR: The paper introduces MVV, a fine-grained validation set for traffic signs derived from Mapillary, and benchmarks VLMs against DINOv2, showing DINOv2's superiority in fine-grained recognition.


<details>
  <summary>Details</summary>
Motivation: Coarse-grained labels in datasets like Mapillary limit accurate decision-making in autonomous driving. Fine-grained annotations are needed for better semantic understanding.

Method: Created MVV by decomposing composite traffic signs into granular categories with pixel-level masks, manually annotated by experts. Benchmarked VLMs against DINOv2.

Result: DINOv2 outperformed VLMs in traffic sign recognition and other categories, highlighting VLMs' limitations in fine-grained understanding.

Conclusion: MVV and DINOv2 provide a foundation for more reliable and scalable perception systems in autonomous driving.

Abstract: Obtaining high-quality fine-grained annotations for traffic signs is critical
for accurate and safe decision-making in autonomous driving. Widely used
datasets, such as Mapillary, often provide only coarse-grained labels - without
distinguishing semantically important types such as stop signs or speed limit
signs. To this end, we present a new validation set for traffic signs derived
from the Mapillary dataset called Mapillary Vistas Validation for Traffic Signs
(MVV), where we decompose composite traffic signs into granular, semantically
meaningful categories. The dataset includes pixel-level instance masks and has
been manually annotated by expert annotators to ensure label fidelity. Further,
we benchmark several state-of-the-art VLMs against the self-supervised DINOv2
model on this dataset and show that DINOv2 consistently outperforms all VLM
baselines-not only on traffic sign recognition, but also on heavily represented
categories like vehicles and humans. Our analysis reveals significant
limitations in current vision-language models for fine-grained visual
understanding and establishes DINOv2 as a strong baseline for dense semantic
matching in autonomous driving scenarios. This dataset and evaluation framework
pave the way for more reliable, interpretable, and scalable perception systems.
  Code and data are available at: https://github.com/nec-labs-ma/relabeling

</details>


### [253] [HCF: Hierarchical Cascade Framework for Distributed Multi-Stage Image Compression](https://arxiv.org/abs/2508.02051)
*Junhao Cai,Taegun An,Chengjun Jin,Sung Il Choi,JuHyun Park,Changhee Joo*

Main category: cs.CV

TL;DR: The paper introduces the Hierarchical Cascade Framework (HCF) for efficient multi-stage image compression, outperforming existing methods in rate-distortion performance and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing inefficiencies in distributed multi-stage image compression, such as underutilized compute resources, repeated costly operations, and lack of flexibility in fixed-parameter models.

Method: Developed HCF with latent-space transformations, policy-driven quantization control, and edge quantization principles for optimized rate-distortion trade-offs.

Result: Achieved up to 0.6dB PSNR gains, 5.56% BD-Rate improvement, and significant computational savings (97.8% FLOPs, 96.5% GPU memory, 90.0% execution time). Outperformed progressive methods by up to 12.64% BD-Rate.

Conclusion: HCF provides a scalable, efficient solution for multi-stage image compression with superior performance and adaptability.

Abstract: Distributed multi-stage image compression -- where visual content traverses
multiple processing nodes under varying quality requirements -- poses
challenges. Progressive methods enable bitstream truncation but underutilize
available compute resources; successive compression repeats costly pixel-domain
operations and suffers cumulative quality loss and inefficiency;
fixed-parameter models lack post-encoding flexibility. In this work, we
developed the Hierarchical Cascade Framework (HCF) that achieves high
rate-distortion performance and better computational efficiency through direct
latent-space transformations across network nodes in distributed multi-stage
image compression system. Under HCF, we introduced policy-driven quantization
control to optimize rate-distortion trade-offs, and established the edge
quantization principle through differential entropy analysis. The configuration
based on this principle demonstrates up to 0.6dB PSNR gains over other
configurations. When comprehensively evaluated on the Kodak, CLIC, and
CLIC2020-mobile datasets, HCF outperforms successive-compression methods by up
to 5.56% BD-Rate in PSNR on CLIC, while saving up to 97.8% FLOPs, 96.5% GPU
memory, and 90.0% execution time. It also outperforms state-of-the-art
progressive compression methods by up to 12.64% BD-Rate on Kodak and enables
retraining-free cross-quality adaptation with 7.13-10.87% BD-Rate reductions on
CLIC2020-mobile.

</details>


### [254] [StarPose: 3D Human Pose Estimation via Spatial-Temporal Autoregressive Diffusion](https://arxiv.org/abs/2508.02056)
*Haoxin Yang,Weihong Chen,Xuemiao Xu,Cheng Xu,Peng Xiao,Cuifeng Sun,Shaoyu Huang,Shengfeng He*

Main category: cs.CV

TL;DR: StarPose is an autoregressive diffusion framework for monocular 3D human pose estimation, enhancing accuracy and temporal consistency by integrating historical pose predictions and spatial-temporal physical guidance.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based methods lack spatial-temporal correlations, leading to inconsistent and inaccurate 3D pose sequences. StarPose addresses this by leveraging historical data and physical guidance.

Method: StarPose models 2D-to-3D pose mapping as an autoregressive diffusion process, using a Historical Pose Integration Module (HPIM) and Spatial-Temporal Physical Guidance (STPG) mechanism.

Result: StarPose outperforms state-of-the-art methods in accuracy and temporal consistency on benchmark datasets.

Conclusion: StarPose effectively improves 3D human pose estimation by combining autoregressive diffusion with historical and physical constraints, offering robust and realistic results.

Abstract: Monocular 3D human pose estimation remains a challenging task due to inherent
depth ambiguities and occlusions. Compared to traditional methods based on
Transformers or Convolutional Neural Networks (CNNs), recent diffusion-based
approaches have shown superior performance, leveraging their probabilistic
nature and high-fidelity generation capabilities. However, these methods often
fail to account for the spatial and temporal correlations across predicted
frames, resulting in limited temporal consistency and inferior accuracy in
predicted 3D pose sequences. To address these shortcomings, this paper proposes
StarPose, an autoregressive diffusion framework that effectively incorporates
historical 3D pose predictions and spatial-temporal physical guidance to
significantly enhance both the accuracy and temporal coherence of pose
predictions. Unlike existing approaches, StarPose models the 2D-to-3D pose
mapping as an autoregressive diffusion process. By synergically integrating
previously predicted 3D poses with 2D pose inputs via a Historical Pose
Integration Module (HPIM), the framework generates rich and informative
historical pose embeddings that guide subsequent denoising steps, ensuring
temporally consistent predictions. In addition, a fully plug-and-play
Spatial-Temporal Physical Guidance (STPG) mechanism is tailored to refine the
denoising process in an iterative manner, which further enforces spatial
anatomical plausibility and temporal motion dynamics, rendering robust and
realistic pose estimates. Extensive experiments on benchmark datasets
demonstrate that StarPose outperforms state-of-the-art methods, achieving
superior accuracy and temporal consistency in 3D human pose estimation. Code is
available at https://github.com/wileychan/StarPose.

</details>


### [255] [YOLOv1 to YOLOv11: A Comprehensive Survey of Real-Time Object Detection Innovations and Challenges](https://arxiv.org/abs/2508.02067)
*Manikanta Kotthapalli,Deepika Ravipati,Reshma Bhatia*

Main category: cs.CV

TL;DR: A review of the YOLO family's evolution, innovations, and applications in real-time vision tasks.


<details>
  <summary>Details</summary>
Motivation: To document the advancements and expanded capabilities of YOLO models in object detection and related tasks.

Method: Comprehensive review of YOLO versions, focusing on architectural and algorithmic improvements.

Result: YOLO models have improved speed, accuracy, and versatility, supporting tasks like segmentation and tracking.

Conclusion: YOLO's continuous evolution and adaptability make it impactful across diverse computer vision domains.

Abstract: Over the past decade, object detection has advanced significantly, with the
YOLO (You Only Look Once) family of models transforming the landscape of
real-time vision applications through unified, end-to-end detection frameworks.
From YOLOv1's pioneering regression-based detection to the latest YOLOv9, each
version has systematically enhanced the balance between speed, accuracy, and
deployment efficiency through continuous architectural and algorithmic
advancements.. Beyond core object detection, modern YOLO architectures have
expanded to support tasks such as instance segmentation, pose estimation,
object tracking, and domain-specific applications including medical imaging and
industrial automation. This paper offers a comprehensive review of the YOLO
family, highlighting architectural innovations, performance benchmarks,
extended capabilities, and real-world use cases. We critically analyze the
evolution of YOLO models and discuss emerging research directions that extend
their impact across diverse computer vision domains.

</details>


### [256] [S-RRG-Bench: Structured Radiology Report Generation with Fine-Grained Evaluation Framework](https://arxiv.org/abs/2508.02082)
*Yingshu Li,Yunyi Liu,Zhanyu Wang,Xinyu Liang,Lingqiao Liu,Lei Wang,Luping Zhou*

Main category: cs.CV

TL;DR: The paper introduces a novel structured radiology report generation (S-RRG) approach, including dataset construction, model training, and a new evaluation framework (S-Score) for clinically relevant report quality.


<details>
  <summary>Details</summary>
Motivation: Traditional free-text radiology reports are redundant and inconsistent, while existing structured approaches lack expressiveness and clinical detail.

Method: The authors create a structured chest X-ray dataset (MIMIC-STRUC) and train an LLM-based model to generate standardized reports. They also propose the S-Score for evaluation.

Result: The approach produces high-quality structured reports and introduces a clinically meaningful evaluation metric aligned with human assessments.

Conclusion: Structured reports and tailored evaluation metrics improve clinical relevance and report quality in radiology.

Abstract: Radiology report generation (RRG) for diagnostic images, such as chest
X-rays, plays a pivotal role in both clinical practice and AI. Traditional
free-text reports suffer from redundancy and inconsistent language,
complicating the extraction of critical clinical details. Structured radiology
report generation (S-RRG) offers a promising solution by organizing information
into standardized, concise formats. However, existing approaches often rely on
classification or visual question answering (VQA) pipelines that require
predefined label sets and produce only fragmented outputs. Template-based
approaches, which generate reports by replacing keywords within fixed sentence
patterns, further compromise expressiveness and often omit clinically important
details. In this work, we present a novel approach to S-RRG that includes
dataset construction, model training, and the introduction of a new evaluation
framework. We first create a robust chest X-ray dataset (MIMIC-STRUC) that
includes disease names, severity levels, probabilities, and anatomical
locations, ensuring that the dataset is both clinically relevant and
well-structured. We train an LLM-based model to generate standardized,
high-quality reports. To assess the generated reports, we propose a specialized
evaluation metric (S-Score) that not only measures disease prediction accuracy
but also evaluates the precision of disease-specific details, thus offering a
clinically meaningful metric for report quality that focuses on elements
critical to clinical decision-making and demonstrates a stronger alignment with
human assessments. Our approach highlights the effectiveness of structured
reports and the importance of a tailored evaluation metric for S-RRG, providing
a more clinically relevant measure of report quality.

</details>


### [257] [VLM4D: Towards Spatiotemporal Awareness in Vision Language Models](https://arxiv.org/abs/2508.02095)
*Shijie Zhou,Alexander Vilesov,Xuehai He,Ziyu Wan,Shuwang Zhang,Aditya Nagachandra,Di Chang,Dongdong Chen,Xin Eric Wang,Achuta Kadambi*

Main category: cs.CV

TL;DR: VLM4D is a new benchmark for evaluating spatiotemporal reasoning in vision language models (VLMs), revealing their limitations compared to humans and suggesting improvements like 4D feature fields and fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current VLMs lack robust dynamic spatiotemporal understanding, a key human ability, prompting the need for a dedicated benchmark.

Method: VLM4D uses diverse real-world and synthetic videos with curated QA pairs to test VLMs on motions, perspective, and continuity.

Result: State-of-the-art VLMs show significant gaps in performance versus humans, struggling with visual cue integration and temporal coherence.

Conclusion: The benchmark highlights VLMs' deficiencies and suggests promising directions like 4D feature fields and fine-tuning for better dynamic understanding.

Abstract: Vision language models (VLMs) have shown remarkable capabilities in
integrating linguistic and visual reasoning but remain fundamentally limited in
understanding dynamic spatiotemporal interactions. Humans effortlessly track
and reason about object movements, rotations, and perspective shifts-abilities
essential for robust dynamic real-world understanding yet notably lacking in
current VLMs. In this paper, we introduce VLM4D, the first benchmark
specifically designed to evaluate the spatiotemporal reasoning capabilities of
VLMs. Our benchmark comprises diverse real-world and synthetic videos
accompanied by carefully curated question-answer pairs emphasizing
translational and rotational motions, perspective awareness, and motion
continuity. Through comprehensive evaluations of state-of-the-art open and
closed-source VLMs, we identify significant performance gaps compared to human
baselines, highlighting fundamental deficiencies in existing models. Extensive
analysis reveals that VLMs struggle particularly with integrating multiple
visual cues and maintaining temporal coherence. We further explore promising
directions, such as leveraging 4D feature field reconstruction and targeted
spatiotemporal supervised fine-tuning, demonstrating their effectiveness in
enhancing spatiotemporal comprehension. Our work aims to encourage deeper
exploration into improving VLMs' spatial and temporal grounding, paving the way
towards more capable and reliable visual intelligence for dynamic environments.

</details>


### [258] [Towards Immersive Human-X Interaction: A Real-Time Framework for Physically Plausible Motion Synthesis](https://arxiv.org/abs/2508.02106)
*Kaiyang Ji,Ye Shi,Zichen Jin,Kangyi Chen,Lan Xu,Yuexin Ma,Jingyi Yu,Jingya Wang*

Main category: cs.CV

TL;DR: Human-X is a real-time framework for physically plausible human interactions in VR/AR and robotics, addressing responsiveness, feasibility, and safety.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack real-time responsiveness and physical feasibility in dynamic human-machine interactions.

Method: Uses an auto-regressive reaction diffusion planner for joint action-reaction prediction and integrates an actor-aware motion tracking policy with reinforcement learning.

Result: Shows improvements in motion quality, interaction continuity, and physical plausibility on Inter-X and InterHuman datasets.

Conclusion: Validated in real-world applications, Human-X advances human-robot collaboration.

Abstract: Real-time synthesis of physically plausible human interactions remains a
critical challenge for immersive VR/AR systems and humanoid robotics. While
existing methods demonstrate progress in kinematic motion generation, they
often fail to address the fundamental tension between real-time responsiveness,
physical feasibility, and safety requirements in dynamic human-machine
interactions. We introduce Human-X, a novel framework designed to enable
immersive and physically plausible human interactions across diverse entities,
including human-avatar, human-humanoid, and human-robot systems. Unlike
existing approaches that focus on post-hoc alignment or simplified physics, our
method jointly predicts actions and reactions in real-time using an
auto-regressive reaction diffusion planner, ensuring seamless synchronization
and context-aware responses. To enhance physical realism and safety, we
integrate an actor-aware motion tracking policy trained with reinforcement
learning, which dynamically adapts to interaction partners' movements while
avoiding artifacts like foot sliding and penetration. Extensive experiments on
the Inter-X and InterHuman datasets demonstrate significant improvements in
motion quality, interaction continuity, and physical plausibility over
state-of-the-art methods. Our framework is validated in real-world
applications, including virtual reality interface for human-robot interaction,
showcasing its potential for advancing human-robot collaboration.

</details>


### [259] [AutoLoRA: Automatic LoRA Retrieval and Fine-Grained Gated Fusion for Text-to-Image Generation](https://arxiv.org/abs/2508.02107)
*Zhiwen Li,Zhongjie Duan,Die Chen,Cen Chen,Daoyuan Chen,Yaliang Li,Yingda Chen*

Main category: cs.CV

TL;DR: A novel framework for semantic-driven LoRA retrieval and dynamic aggregation addresses challenges in deploying large-scale image generation models, improving performance and scalability.


<details>
  <summary>Details</summary>
Motivation: Overcoming limitations in deploying photorealistic image generation models due to sparse metadata, zero-shot adaptation needs, and suboptimal multi-LoRA fusion.

Method: Introduces a weight encoding-based LoRA retriever and fine-grained gated fusion mechanism for dynamic aggregation.

Result: Significant improvement in image generation performance, enabling scalable and data-efficient model enhancement.

Conclusion: Bridges community-developed LoRAs with practical deployment, fostering collaborative model evolution.

Abstract: Despite recent advances in photorealistic image generation through
large-scale models like FLUX and Stable Diffusion v3, the practical deployment
of these architectures remains constrained by their inherent intractability to
parameter fine-tuning. While low-rank adaptation (LoRA) have demonstrated
efficacy in enabling model customization with minimal parameter overhead, the
effective utilization of distributed open-source LoRA modules faces three
critical challenges: sparse metadata annotation, the requirement for zero-shot
adaptation capabilities, and suboptimal fusion strategies for multi-LoRA fusion
strategies. To address these limitations, we introduce a novel framework that
enables semantic-driven LoRA retrieval and dynamic aggregation through two key
components: (1) weight encoding-base LoRA retriever that establishes a shared
semantic space between LoRA parameter matrices and text prompts, eliminating
dependence on original training data, and (2) fine-grained gated fusion
mechanism that computes context-specific fusion weights across network layers
and diffusion timesteps to optimally integrate multiple LoRA modules during
generation. Our approach achieves significant improvement in image generation
perfermance, thereby facilitating scalable and data-efficient enhancement of
foundational models. This work establishes a critical bridge between the
fragmented landscape of community-developed LoRAs and practical deployment
requirements, enabling collaborative model evolution through standardized
adapter integration.

</details>


### [260] [DeflareMamba: Hierarchical Vision Mamba for Contextually Consistent Lens Flare Removal](https://arxiv.org/abs/2508.02113)
*Yihang Huang,Yuanfei Huang,Junhui Lin,Hua Huang*

Main category: cs.CV

TL;DR: DeflareMamba introduces state space models for flare removal, achieving consistent results by capturing local-global dependencies through a hierarchical framework.


<details>
  <summary>Details</summary>
Motivation: Existing flare removal methods often fail to maintain contextual consistency, leading to incomplete results.

Method: Uses state space models with a hierarchical framework for long-range pixel correlations and local-enhanced details.

Result: Effectively removes various flare types while preserving non-flare regions, improving downstream tasks like object recognition.

Conclusion: DeflareMamba is the first to apply state space models to flare removal, demonstrating superior performance and versatility.

Abstract: Lens flare removal remains an information confusion challenge in the
underlying image background and the optical flares, due to the complex optical
interactions between light sources and camera lens. While recent solutions have
shown promise in decoupling the flare corruption from image, they often fail to
maintain contextual consistency, leading to incomplete and inconsistent flare
removal. To eliminate this limitation, we propose DeflareMamba, which leverages
the efficient sequence modeling capabilities of state space models while
maintains the ability to capture local-global dependencies. Particularly, we
design a hierarchical framework that establishes long-range pixel correlations
through varied stride sampling patterns, and utilize local-enhanced state space
models that simultaneously preserves local details. To the best of our
knowledge, this is the first work that introduces state space models to the
flare removal task. Extensive experiments demonstrate that our method
effectively removes various types of flare artifacts, including scattering and
reflective flares, while maintaining the natural appearance of non-flare
regions. Further downstream applications demonstrate the capacity of our method
to improve visual object recognition and cross-modal semantic understanding.
Code is available at https://github.com/BNU-ERC-ITEA/DeflareMamba.

</details>


### [261] [Beyond RGB and Events: Enhancing Object Detection under Adverse Lighting with Monocular Normal Maps](https://arxiv.org/abs/2508.02127)
*Mingjie Liu,Hanqing Liu,Chuang Zhu*

Main category: cs.CV

TL;DR: NRE-Net improves object detection in adverse lighting by fusing RGB, event data, and predicted normal maps, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Adverse lighting causes false detections in autonomous driving; existing methods (RGB or event data alone) are insufficient.

Method: Proposes NRE-Net, fusing RGB, event streams, and predicted normal maps via ADFM and EAFM modules.

Result: Achieves mAP50 improvements of 7.9% and 6.1% over frame-based methods, surpassing fusion-based approaches.

Conclusion: NRE-Net effectively addresses adverse lighting challenges by leveraging multi-modal fusion.

Abstract: Accurate object detection under adverse lighting conditions is critical for
real-world applications such as autonomous driving. Although neuromorphic event
cameras have been introduced to handle these scenarios, adverse lighting often
induces distracting reflections from tunnel walls or road surfaces, which
frequently lead to false obstacle detections. However, neither RGB nor event
data alone is robust enough to address these complexities, and mitigating these
issues without additional sensors remains underexplored. To overcome these
challenges, we propose leveraging normal maps, directly predicted from
monocular RGB images, as robust geometric cues to suppress false positives and
enhance detection accuracy. We introduce NRE-Net, a novel multi-modal detection
framework that effectively fuses three complementary modalities: monocularly
predicted surface normal maps, RGB images, and event streams. To optimize the
fusion process, our framework incorporates two key modules: the Adaptive
Dual-stream Fusion Module (ADFM), which integrates RGB and normal map features,
and the Event-modality Aware Fusion Module (EAFM), which adapts to the high
dynamic range characteristics of event data. Extensive evaluations on the
DSEC-Det-sub and PKU-DAVIS-SOD datasets demonstrate that NRE-Net significantly
outperforms state-of-the-art methods. Our approach achieves mAP50 improvements
of 7.9% and 6.1% over frame-based approaches (e.g., YOLOX), while surpassing
the fusion-based SFNet by 2.7% on the DSEC-Det-sub dataset and SODFormer by
7.1% on the PKU-DAVIS-SOD dataset.

</details>


### [262] [VDEGaussian: Video Diffusion Enhanced 4D Gaussian Splatting for Dynamic Urban Scenes Modeling](https://arxiv.org/abs/2508.02129)
*Yuru Xiao,Zihan Lin,Chao Lu,Deming Zhai,Kui Jiang,Wenbo Zhao,Wei Zhang,Junjun Jiang,Huanran Wang,Xianming Liu*

Main category: cs.CV

TL;DR: A novel video diffusion-enhanced 4D Gaussian Splatting framework improves dynamic urban scene modeling by addressing limitations in handling fast-moving objects and temporal discontinuities.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with pre-calibrated object tracks and undersampled capture, leading to inaccuracies in modeling fast-moving objects.

Method: The proposed framework uses a video diffusion model for temporally consistent priors, joint timestamp optimization for pose alignment, and uncertainty distillation for content integration.

Result: The method achieves a ~2 dB PSNR gain in novel view synthesis, especially for fast-moving objects.

Conclusion: The framework effectively enhances dynamic scene modeling by leveraging diffusion priors and innovative optimization techniques.

Abstract: Dynamic urban scene modeling is a rapidly evolving area with broad
applications. While current approaches leveraging neural radiance fields or
Gaussian Splatting have achieved fine-grained reconstruction and high-fidelity
novel view synthesis, they still face significant limitations. These often stem
from a dependence on pre-calibrated object tracks or difficulties in accurately
modeling fast-moving objects from undersampled capture, particularly due to
challenges in handling temporal discontinuities. To overcome these issues, we
propose a novel video diffusion-enhanced 4D Gaussian Splatting framework. Our
key insight is to distill robust, temporally consistent priors from a test-time
adapted video diffusion model. To ensure precise pose alignment and effective
integration of this denoised content, we introduce two core innovations: a
joint timestamp optimization strategy that refines interpolated frame poses,
and an uncertainty distillation method that adaptively extracts target content
while preserving well-reconstructed regions. Extensive experiments demonstrate
that our method significantly enhances dynamic modeling, especially for
fast-moving objects, achieving an approximate PSNR gain of 2 dB for novel view
synthesis over baseline approaches.

</details>


### [263] [A Neural Quality Metric for BRDF Models](https://arxiv.org/abs/2508.02131)
*Behnaz Kavoosighafi,Rafal K. Mantiuk,Saghi Hajisharif,Ehsan Miandji,Jonas Unger*

Main category: cs.CV

TL;DR: A neural quality metric for BRDF evaluation is introduced, operating directly in BRDF space without rendering, outperforming traditional metrics in correlating with human judgments.


<details>
  <summary>Details</summary>
Motivation: Traditional BRDF-space metrics fail to capture perceptual differences in rendered images, necessitating a perceptually informed alternative.

Method: A compact MLP is trained on measured and synthetic BRDF data, labeled using a perceptually validated image-space metric, to predict perceptual quality (JOD scores).

Result: The neural metric achieves higher correlation with human judgments than existing BRDF-space metrics.

Conclusion: While limited for BRDF fitting, the metric provides a perceptually grounded alternative for BRDF model evaluation.

Abstract: Accurately evaluating the quality of bidirectional reflectance distribution
function (BRDF) models is essential for photo-realistic rendering. Traditional
BRDF-space metrics often employ numerical error measures that fail to capture
perceptual differences evident in rendered images. In this paper, we introduce
the first perceptually informed neural quality metric for BRDF evaluation that
operates directly in BRDF space, eliminating the need for rendering during
quality assessment. Our metric is implemented as a compact multi-layer
perceptron (MLP), trained on a dataset of measured BRDFs supplemented with
synthetically generated data and labelled using a perceptually validated
image-space metric. The network takes as input paired samples of reference and
approximated BRDFs and predicts their perceptual quality in terms of
just-objectionable-difference (JOD) scores. We show that our neural metric
achieves significantly higher correlation with human judgments than existing
BRDF-space metrics. While its performance as a loss function for BRDF fitting
remains limited, the proposed metric offers a perceptually grounded alternative
for evaluating BRDF models.

</details>


### [264] [Free-MoRef: Instantly Multiplexing Context Perception Capabilities of Video-MLLMs within Single Inference](https://arxiv.org/abs/2508.02134)
*Kuo Wang,Quanlong Zheng,Junlin Xie,Yanhao Zhang,Jinguo Luo,Haonan Lu,Liang Lin,Fan Zhou,Guanbin Li*

Main category: cs.CV

TL;DR: Free-MoRef is a training-free approach for Video-MLLMs to handle long videos efficiently by multiplexing context perception without sacrificing performance or efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing Video-MLLMs struggle with long videos due to context length limitations, leading to suboptimal performance. Current solutions compromise granularity or efficiency.

Method: Free-MoRef splits vision tokens into multi-reference chunks, uses MoRef-attention for parallel clue gathering, and fuses references for unified reasoning.

Result: Free-MoRef achieves 2× to 8× longer input frame perception without compression, outperforming trained long-video-MLLMs on benchmarks like VideoMME and MLVU.

Conclusion: Free-MoRef offers an efficient, effective solution for long-video understanding, balancing performance and computational cost.

Abstract: Video Multimodal Large Language Models~(Video-MLLM) have achieved remarkable
advancements in video understanding tasks. However, constrained by the context
length limitation in the underlying LLMs, existing Video-MLLMs typically
exhibit suboptimal performance on long video scenarios. To understand extended
input frames, common solutions span token compression and streaming inference
techniques, which sacrifice feature granularity or inference efficiency.
Differently, to efficiently achieve comprehensive understanding of longer frame
inputs, we draw ideas from MoE and propose a training-free approach
\textbf{Free-MoRef}, which instantly multiplexes the context perception
capabilities of Video-MLLMs within one inference pass. Specifically, Free-MoRef
reconstructs the vision tokens into several short sequences as
multi-references. Subsequently, we introduce MoRef-attention, which gathers
clues from the multi-reference chunks in parallel to summarize unified query
activations. After the shadow layers in LLMs, a reference fusion step is
derived to compose a final mixed reasoning sequence with key tokens from
parallel chunks, which compensates the cross-reference vision interactions that
are neglected in MoRef-attention. By splitting and fusing the long vision token
sequences, Free-MoRef achieves improved performance under much lower computing
costs in reasoning multiplexed context length, demonstrating strong efficiency
and effectiveness. Experiments on VideoMME, MLVU, LongVideoBench show that
Free-MoRef achieves full perception of 2$\times$ to 8$\times$ longer input
frames without compression on a single A100 GPU while keeping instant
responses, thereby bringing significant performance gains, even surpassing
dedicatedly trained long-video-MLLMs. Codes are available at
https://github.com/wkfdb/Free-MoRef

</details>


### [265] [AID4AD: Aerial Image Data for Automated Driving Perception](https://arxiv.org/abs/2508.02140)
*Daniel Lengerer,Mathias Pechinger,Klaus Bogenberger,Carsten Markgraf*

Main category: cs.CV

TL;DR: The paper introduces AID4AD, a dataset combining aerial imagery with nuScenes for AV perception tasks, showing improved accuracy in map construction and motion prediction.


<details>
  <summary>Details</summary>
Motivation: To enhance automated vehicle perception by integrating spatially aligned aerial imagery, especially where high-definition maps are impractical.

Method: AID4AD dataset creation using SLAM-based alignment, manual quality control, and validation in map construction and motion prediction tasks.

Result: 15-23% improvement in map construction accuracy and 2% gain in trajectory prediction performance.

Conclusion: Aerial imagery is a scalable, adaptable environmental context source for AVs, with AID4AD released to support further research.

Abstract: This work investigates the integration of spatially aligned aerial imagery
into perception tasks for automated vehicles (AVs). As a central contribution,
we present AID4AD, a publicly available dataset that augments the nuScenes
dataset with high-resolution aerial imagery precisely aligned to its local
coordinate system. The alignment is performed using SLAM-based point cloud maps
provided by nuScenes, establishing a direct link between aerial data and
nuScenes local coordinate system. To ensure spatial fidelity, we propose an
alignment workflow that corrects for localization and projection distortions. A
manual quality control process further refines the dataset by identifying a set
of high-quality alignments, which we publish as ground truth to support future
research on automated registration. We demonstrate the practical value of
AID4AD in two representative tasks: in online map construction, aerial imagery
serves as a complementary input that improves the mapping process; in motion
prediction, it functions as a structured environmental representation that
replaces high-definition maps. Experiments show that aerial imagery leads to a
15-23% improvement in map construction accuracy and a 2% gain in trajectory
prediction performance. These results highlight the potential of aerial imagery
as a scalable and adaptable source of environmental context in automated
vehicle systems, particularly in scenarios where high-definition maps are
unavailable, outdated, or costly to maintain. AID4AD, along with evaluation
code and pretrained models, is publicly released to foster further research in
this direction: https://github.com/DriverlessMobility/AID4AD.

</details>


### [266] [TrackletGait: A Robust Framework for Gait Recognition in the Wild](https://arxiv.org/abs/2508.02143)
*Shaoxiong Zhang,Jinkai Zheng,Shangdong Zhu,Chenggang Yan*

Main category: cs.CV

TL;DR: TrackletGait improves gait recognition in real-world scenarios with novel sampling, downsampling, and loss techniques, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Address challenges in gait recognition like non-periodic and occluded sequences in uncontrolled environments.

Method: Proposes Random Tracklet Sampling, Haar Wavelet-based Downsampling, and Hardness Exclusion Triplet Loss.

Result: Achieves 77.8 and 80.4 rank-1 accuracy on Gait3D and GREW datasets with 10.3M parameters.

Conclusion: TrackletGait effectively handles real-world gait recognition challenges and outperforms existing methods.

Abstract: Gait recognition aims to identify individuals based on their body shape and
walking patterns. Though much progress has been achieved driven by deep
learning, gait recognition in real-world surveillance scenarios remains quite
challenging to current methods. Conventional approaches, which rely on periodic
gait cycles and controlled environments, struggle with the non-periodic and
occluded silhouette sequences encountered in the wild. In this paper, we
propose a novel framework, TrackletGait, designed to address these challenges
in the wild. We propose Random Tracklet Sampling, a generalization of existing
sampling methods, which strikes a balance between robustness and representation
in capturing diverse walking patterns. Next, we introduce Haar Wavelet-based
Downsampling to preserve information during spatial downsampling. Finally, we
present a Hardness Exclusion Triplet Loss, designed to exclude low-quality
silhouettes by discarding hard triplet samples. TrackletGait achieves
state-of-the-art results, with 77.8 and 80.4 rank-1 accuracy on the Gait3D and
GREW datasets, respectively, while using only 10.3M backbone parameters.
Extensive experiments are also conducted to further investigate the factors
affecting gait recognition in the wild.

</details>


### [267] [AURORA: Augmented Understanding via Structured Reasoning and Reinforcement Learning for Reference Audio-Visual Segmentation](https://arxiv.org/abs/2508.02149)
*Ziyang Luo,Nian Liu,Fahad Shahbaz Khan,Junwei Han*

Main category: cs.CV

TL;DR: AURORA improves Ref-AVS tasks by enhancing reasoning and segmentation via CoT prompting, feature distillation loss, and a two-stage training strategy, achieving top performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack semantic understanding and compromise precision; AURORA addresses these by improving reasoning and segmentation integration.

Method: Uses CoT prompting, segmentation feature distillation loss, and a two-stage training strategy (corrective reflective-style training and GRPO reinforcement learning).

Result: AURORA achieves state-of-the-art performance on Ref-AVS benchmarks and generalizes well to unreferenced segmentation.

Conclusion: AURORA effectively enhances reasoning and segmentation, outperforming existing methods and generalizing robustly.

Abstract: Reference Audio-Visual Segmentation (Ref-AVS) tasks challenge models to
precisely locate sounding objects by integrating visual, auditory, and textual
cues. Existing methods often lack genuine semantic understanding, tending to
memorize fixed reasoning patterns. Furthermore, jointly training for reasoning
and segmentation can compromise pixel-level precision. To address these issues,
we introduce AURORA, a novel framework designed to enhance genuine reasoning
and language comprehension in reference audio-visual segmentation. We employ a
structured Chain-of-Thought (CoT) prompting mechanism to guide the model
through a step-by-step reasoning process and introduce a novel segmentation
feature distillation loss to effectively integrate these reasoning abilities
without sacrificing segmentation performance. To further cultivate the model's
genuine reasoning capabilities, we devise a further two-stage training
strategy: first, a ``corrective reflective-style training" stage utilizes
self-correction to enhance the quality of reasoning paths, followed by
reinforcement learning via Group Reward Policy Optimization (GRPO) to bolster
robustness in challenging scenarios. Experiments demonstrate that AURORA
achieves state-of-the-art performance on Ref-AVS benchmarks and generalizes
effectively to unreferenced segmentation.

</details>


### [268] [AttriCtrl: Fine-Grained Control of Aesthetic Attribute Intensity in Diffusion Models](https://arxiv.org/abs/2508.02151)
*Die Chen,Zhongjie Duan,Zhiwen Li,Cen Chen,Daoyuan Chen,Yaliang Li,Yinda Chen*

Main category: cs.CV

TL;DR: AttriCtrl is a plug-and-play framework for precise, continuous control of aesthetic attributes in text-to-image diffusion models, addressing ambiguity in textual prompts and scalability issues of human preference data.


<details>
  <summary>Details</summary>
Motivation: Fine-grained control over aesthetic attributes in text-to-image models is challenging due to vague textual prompts and reliance on costly human preference data.

Method: AttriCtrl quantifies aesthetics using semantic similarity from vision-language models and employs a lightweight value encoder to map scalar intensities to embeddings in diffusion-based generation.

Result: The framework achieves accurate control over individual and multiple aesthetic attributes, with minimal training overhead and seamless integration into existing pipelines.

Conclusion: AttriCtrl offers intuitive, customizable aesthetic manipulation and strong compatibility with popular controllable generation frameworks, demonstrating practical utility.

Abstract: Recent breakthroughs in text-to-image diffusion models have significantly
enhanced both the visual fidelity and semantic controllability of generated
images. However, fine-grained control over aesthetic attributes remains
challenging, especially when users require continuous and intensity-specific
adjustments. Existing approaches often rely on vague textual prompts, which are
inherently ambiguous in expressing both the aesthetic semantics and the desired
intensity, or depend on costly human preference data for alignment, limiting
their scalability and practicality. To address these limitations, we propose
AttriCtrl, a plug-and-play framework for precise and continuous control of
aesthetic attributes. Specifically, we quantify abstract aesthetics by
leveraging semantic similarity from pre-trained vision-language models, and
employ a lightweight value encoder that maps scalar intensities in $[0,1]$ to
learnable embeddings within diffusion-based generation. This design enables
intuitive and customizable aesthetic manipulation, with minimal training
overhead and seamless integration into existing generation pipelines. Extensive
experiments demonstrate that AttriCtrl achieves accurate control over
individual attributes as well as flexible multi-attribute composition.
Moreover, it is fully compatible with popular open-source controllable
generation frameworks, showcasing strong integration capability and practical
utility across diverse generation scenarios.

</details>


### [269] [Efficient Chambolle-Pock based algorithms for Convoltional sparse representation](https://arxiv.org/abs/2508.02152)
*Yi Liu,Junjing Li,Yang Chen,Haowei Tang,Pengcheng Zhang,Tianling Lyu,Zhiguo Gui*

Main category: cs.CV

TL;DR: The paper proposes a fast and efficient method for convolutional sparse coding (CSC) and convolutional dictionary learning (CDL) using the Chambolle-Pock (CP) framework, eliminating the need for manual parameter selection and improving convergence speed.


<details>
  <summary>Details</summary>
Motivation: The ADMM-based approach for CSC requires careful parameter selection, which can lead to convergence issues. The paper aims to address this limitation by introducing a more efficient method.

Method: The authors use the Chambolle-Pock (CP) framework to solve CSC and CDL problems, avoiding manual parameter tuning. They also introduce an anisotropic total variation penalty for CSC.

Result: The proposed method achieves comparable results to ADMM for noise-free images and outperforms it in removing Gaussian noise.

Conclusion: The CP framework offers a faster and more efficient alternative to ADMM for solving CSC and CDL problems, with improved performance in noisy conditions.

Abstract: Recently convolutional sparse representation (CSR), as a sparse
representation technique, has attracted increasing attention in the field of
image processing, due to its good characteristic of translate-invariance. The
content of CSR usually consists of convolutional sparse coding (CSC) and
convolutional dictionary learning (CDL), and many studies focus on how to solve
the corresponding optimization problems. At present, the most efficient
optimization scheme for CSC is based on the alternating direction method of
multipliers (ADMM). However, the ADMM-based approach involves a penalty
parameter that needs to be carefully selected, and improper parameter selection
may result in either no convergence or very slow convergence. In this paper, a
novel fast and efficient method using Chambolle-Pock(CP) framework is proposed,
which does not require extra manual selection parameters in solving processing,
and has faster convergence speed. Furthermore, we propose an anisotropic total
variation penalty of the coefficient maps for CSC and apply the CP algorithm to
solve it. In addition, we also apply the CP framework to solve the
corresponding CDL problem. Experiments show that for noise-free image the
proposed CSC algorithms can achieve rival results of the latest ADMM-based
approach, while outperforms in removing noise from Gaussian noise pollution
image.

</details>


### [270] [DreamPainter: Image Background Inpainting for E-commerce Scenarios](https://arxiv.org/abs/2508.02155)
*Sijie Zhao,Jing Cheng,Yaoyao Wu,Hao Xu,Shaohui Jiao*

Main category: cs.CV

TL;DR: DreamPainter addresses e-commerce background generation challenges by combining text prompts and reference images, outperforming existing methods with its novel framework and high-quality dataset, DreamEcom-400K.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with product-background consistency and precise control in e-commerce image generation due to lack of domain-specific data and reliance on text-only prompts.

Method: Proposes DreamPainter, a framework using text prompts and reference images for control, leveraging the DreamEcom-400K dataset with product masks and background references.

Result: DreamPainter outperforms state-of-the-art methods, achieving high product consistency and effective integration of text and visual controls.

Conclusion: The approach successfully addresses e-commerce image generation challenges, offering improved consistency and control.

Abstract: Although diffusion-based image genenation has been widely explored and
applied, background generation tasks in e-commerce scenarios still face
significant challenges. The first challenge is to ensure that the generated
products are consistent with the given product inputs while maintaining a
reasonable spatial arrangement, harmonious shadows, and reflections between
foreground products and backgrounds. Existing inpainting methods fail to
address this due to the lack of domain-specific data. The second challenge
involves the limitation of relying solely on text prompts for image control, as
effective integrating visual information to achieve precise control in
inpainting tasks remains underexplored. To address these challenges, we
introduce DreamEcom-400K, a high-quality e-commerce dataset containing accurate
product instance masks, background reference images, text prompts, and
aesthetically pleasing product images. Based on this dataset, we propose
DreamPainter, a novel framework that not only utilizes text prompts for control
but also flexibly incorporates reference image information as an additional
control signal. Extensive experiments demonstrate that our approach
significantly outperforms state-of-the-art methods, maintaining high product
consistency while effectively integrating both text prompt and reference image
information.

</details>


### [271] [Unified Category-Level Object Detection and Pose Estimation from RGB Images using 3D Prototypes](https://arxiv.org/abs/2508.02157)
*Tom Fischer,Xiaojie Zhang,Eddy Ilg*

Main category: cs.CV

TL;DR: A unified model for object detection and 3D pose estimation in RGB images, outperforming state-of-the-art by 22.9%.


<details>
  <summary>Details</summary>
Motivation: Traditional methods rely on RGB-D inputs or separate models for detection and pose estimation, which are limiting.

Method: Integrates detection and pose estimation using neural mesh models with learned features and multi-model RANSAC.

Result: Achieves state-of-the-art results on REAL275, with 22.9% improvement in scale-agnostic metrics.

Conclusion: The unified method is more robust than single-stage baselines and works with RGB-only inputs.

Abstract: Recognizing objects in images is a fundamental problem in computer vision.
Although detecting objects in 2D images is common, many applications require
determining their pose in 3D space. Traditional category-level methods rely on
RGB-D inputs, which may not always be available, or employ two-stage approaches
that use separate models and representations for detection and pose estimation.
For the first time, we introduce a unified model that integrates detection and
pose estimation into a single framework for RGB images by leveraging neural
mesh models with learned features and multi-model RANSAC. Our approach achieves
state-of-the-art results for RGB category-level pose estimation on REAL275,
improving on the current state-of-the-art by 22.9% averaged across all
scale-agnostic metrics. Finally, we demonstrate that our unified method
exhibits greater robustness compared to single-stage baselines. Our code and
models are available at
https://github.com/Fischer-Tom/unified-detection-and-pose-estimation.

</details>


### [272] [After the Party: Navigating the Mapping From Color to Ambient Lighting](https://arxiv.org/abs/2508.02168)
*Florin-Alexandru Vasluianu,Tim Seizinger,Zongwei Wu,Radu Timofte*

Main category: cs.CV

TL;DR: CL3AN introduces a dataset and learning framework for restoring images under colored lighting, addressing limitations of existing methods by disentangling illumination and reflectance.


<details>
  <summary>Details</summary>
Motivation: Existing methods oversimplify complex illumination scenarios, leading to artifacts like color distortion and texture leakage.

Method: A novel learning framework leverages chromaticity and luminance guidance inspired by the Retinex model.

Result: The approach shows enhanced robustness under non-homogeneous lighting and material-specific reflectance, with competitive computational cost.

Conclusion: CL3AN provides an effective solution for illumination restoration, supported by a new dataset and benchmark.

Abstract: Illumination in practical scenarios is inherently complex, involving colored
light sources, occlusions, and diverse material interactions that produce
intricate reflectance and shading effects. However, existing methods often
oversimplify this challenge by assuming a single light source or uniform,
white-balanced lighting, leaving many of these complexities unaddressed.In this
paper, we introduce CL3AN, the first large-scale, high-resolution dataset of
its kind designed to facilitate the restoration of images captured under
multiple Colored Light sources to their Ambient-Normalized counterparts.
Through benchmarking, we find that leading approaches often produce artifacts,
such as illumination inconsistencies, texture leakage, and color distortion,
primarily due to their limited ability to precisely disentangle illumination
from reflectance. Motivated by this insight, we achieve such a desired
decomposition through a novel learning framework that leverages explicit
chromaticity and luminance components guidance, drawing inspiration from the
principles of the Retinex model. Extensive evaluations on existing benchmarks
and our dataset demonstrate the effectiveness of our approach, showcasing
enhanced robustness under non-homogeneous color lighting and material-specific
reflectance variations, all while maintaining a highly competitive
computational cost. The benchmark, codes, and models are available at
www.github.com/fvasluianu97/RLN2.

</details>


### [273] [GaussianCross: Cross-modal Self-supervised 3D Representation Learning via Gaussian Splatting](https://arxiv.org/abs/2508.02172)
*Lei Yao,Yi Wang,Yi Zhang,Moyun Liu,Lap-Pui Chau*

Main category: cs.CV

TL;DR: GaussianCross introduces a cross-modal self-supervised 3D representation learning method using 3D Gaussian Splatting to improve point cloud discrimination and structural information, achieving superior performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing model collapse and structural information deficiency in self-supervised 3D representation learning due to insufficient point discrimination difficulty.

Method: Integrates feed-forward 3D Gaussian Splatting (3DGS) to convert point clouds into a unified Gaussian representation, with a tri-attribute adaptive distillation splatting module for feature capturing.

Result: Achieves superior performance with high efficiency (0.1% parameters, 1% data) and improves fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ on ScanNet200.

Conclusion: GaussianCross effectively enhances 3D representation learning, demonstrating robustness, efficiency, and strong generalization capabilities.

Abstract: The significance of informative and robust point representations has been
widely acknowledged for 3D scene understanding. Despite existing
self-supervised pre-training counterparts demonstrating promising performance,
the model collapse and structural information deficiency remain prevalent due
to insufficient point discrimination difficulty, yielding unreliable
expressions and suboptimal performance. In this paper, we present
GaussianCross, a novel cross-modal self-supervised 3D representation learning
architecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniques
to address current challenges. GaussianCross seamlessly converts
scale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussian
representation without missing details, enabling stable and generalizable
pre-training. Subsequently, a tri-attribute adaptive distillation splatting
module is incorporated to construct a 3D feature field, facilitating synergetic
feature capturing of appearance, geometry, and semantic cues to maintain
cross-modal consistency. To validate GaussianCross, we perform extensive
evaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. In
particular, GaussianCross shows a prominent parameter and data efficiency,
achieving superior performance through linear probing (<0.1% parameters) and
limited data training (1% of scenes) compared to state-of-the-art methods.
Furthermore, GaussianCross demonstrates strong generalization capabilities,
improving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ on
ScanNet200 semantic and instance segmentation tasks, respectively, supporting
the effectiveness of our approach. The code, weights, and visualizations are
publicly available at
\href{https://rayyoh.github.io/GaussianCross/}{https://rayyoh.github.io/GaussianCross/}.

</details>


### [274] [Deep classification algorithm for De-identification of DICOM medical images](https://arxiv.org/abs/2508.02177)
*Bufano Michele,Kotter Elmar*

Main category: cs.CV

TL;DR: A Python algorithm was developed to de-identify PII and PHI in DICOM files using customizable parameters based on HIPAA's safe harbor method.


<details>
  <summary>Details</summary>
Motivation: Legal requirements (HIPAA) mandate de-identification of PII/PHI in medical images, including DICOM files, to protect patient privacy.

Method: Implemented an algorithm using HIPAA's safe harbor method with customizable input parameters to classify and de-identify DICOM tags.

Result: Sensitive information (names, history, personal data, institution) was successfully recognized and de-identified.

Conclusion: The flexible, customizable Python algorithm is promising for both research and daily use, with code publicly available.

Abstract: Background : De-identification of DICOM (Digital Imaging and Communi-cations
in Medicine) files is an essential component of medical image research.
Personal Identifiable Information (PII) and/or Personal Health Identifying
Information (PHI) need to be hidden or removed due to legal reasons. According
to the Health Insurance Portability and Accountability Act (HIPAA) and privacy
rules, also full-face photographic images and any compa-rable images are direct
identifiers and are considered protected health information that also need to
be de-identified. Objective : The study aimed to implement a method that permit
to de-identify the PII and PHI information present in the header and burned on
the pixel data of DICOM. Methods : To execute the de-identification, we
implemented an algorithm based on the safe harbor method, defined by HIPAA. Our
algorithm uses input customizable parameter to classify and then possibly
de-identify individual DICOM tags. Results : The most sensible information,
like names, history, personal data and institution were successfully
recognized. Conclusions : We developed a python algorithm that is able to
classify infor-mation present in a DICOM file. The flexibility provided by the
use of customi-zable input parameters, which allow the user to customize the
entire process de-pending on the case (e.g., the language), makes the entire
program very promis-ing for both everyday use and research purposes. Our code
is available at https://github.com/rtdicomexplorer/deep_deidentification.

</details>


### [275] [Weakly Supervised Multimodal Temporal Forgery Localization via Multitask Learning](https://arxiv.org/abs/2508.02179)
*Wenbo Xu,Wei Lu,Xiangyang Luo*

Main category: cs.CV

TL;DR: Proposes WMMT, a weakly supervised method for multimodal fine-grained temporal forgery localization using multitask learning and video-level annotations.


<details>
  <summary>Details</summary>
Motivation: Addresses the lack of systematic research on weakly supervised multimodal fine-grained temporal forgery localization (WS-MTFL) in Deepfake detection.

Method: Uses multitask learning for visual and audio modality detection, a Mixture-of-Experts structure for feature selection, and a feature enhancement module with attention. Introduces a deviation perceiving loss for temporal information.

Result: Achieves comparable results to fully supervised approaches in Deepfake detection and localization.

Conclusion: WMMT effectively addresses WS-MTFL, demonstrating the potential of weakly supervised learning in this domain.

Abstract: The spread of Deepfake videos has caused a trust crisis and impaired social
stability. Although numerous approaches have been proposed to address the
challenges of Deepfake detection and localization, there is still a lack of
systematic research on the weakly supervised multimodal fine-grained temporal
forgery localization (WS-MTFL). In this paper, we propose a novel weakly
supervised multimodal temporal forgery localization via multitask learning
(WMMT), which addresses the WS-MTFL under the multitask learning paradigm. WMMT
achieves multimodal fine-grained Deepfake detection and temporal partial
forgery localization using merely video-level annotations. Specifically, visual
and audio modality detection are formulated as two binary classification tasks.
The multitask learning paradigm is introduced to integrate these tasks into a
multimodal task. Furthermore, WMMT utilizes a Mixture-of-Experts structure to
adaptively select appropriate features and localization head, achieving
excellent flexibility and localization precision in WS-MTFL. A feature
enhancement module with temporal property preserving attention mechanism is
proposed to identify the intra- and inter-modality feature deviation and
construct comprehensive video features. To further explore the temporal
information for weakly supervised learning, an extensible deviation perceiving
loss has been proposed, which aims to enlarge the deviation of adjacent
segments of the forged samples and reduce the deviation of genuine samples.
Extensive experiments demonstrate the effectiveness of multitask learning for
WS-MTFL, and the WMMT achieves comparable results to fully supervised
approaches in several evaluation metrics.

</details>


### [276] [Test-Time Model Adaptation for Quantized Neural Networks](https://arxiv.org/abs/2508.02180)
*Zeshuai Deng,Guohao Chen,Shuaicheng Niu,Hui Luo,Shuhai Zhang,Yifan Yang,Renjie Chen,Wei Luo,Mingkui Tan*

Main category: cs.CV

TL;DR: The paper proposes a continual zeroth-order adaptation (ZOA) framework for quantized models to address performance degradation in dynamic environments, achieving efficient adaptation with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Quantized models suffer severe performance degradation in dynamic environments due to domain shifts, and existing test-time adaptation (TTA) methods are impractical for them.

Method: Introduces ZOA, a framework requiring only two forward passes for adaptation, and a domain knowledge management scheme to store and reuse knowledge efficiently.

Result: ZOA improves performance by 5.0% over state-of-the-art methods on quantized models, demonstrated on architectures like ViT-B and CNNs.

Conclusion: ZOA offers a practical and efficient solution for adapting quantized models, enhancing their robustness and generalization in dynamic settings.

Abstract: Quantizing deep models prior to deployment is a widely adopted technique to
speed up inference for various real-time applications, such as autonomous
driving. However, quantized models often suffer from severe performance
degradation in dynamic environments with potential domain shifts and this
degradation is significantly more pronounced compared with their full-precision
counterparts, as shown by our theoretical and empirical illustrations. To
address the domain shift problem, test-time adaptation (TTA) has emerged as an
effective solution by enabling models to learn adaptively from test data.
Unfortunately, existing TTA methods are often impractical for quantized models
as they typically rely on gradient backpropagation--an operation that is
unsupported on quantized models due to vanishing gradients, as well as memory
and latency constraints. In this paper, we focus on TTA for quantized models to
improve their robustness and generalization ability efficiently. We propose a
continual zeroth-order adaptation (ZOA) framework that enables efficient model
adaptation using only two forward passes, eliminating the computational burden
of existing methods. Moreover, we propose a domain knowledge management scheme
to store and reuse different domain knowledge with negligible memory
consumption, reducing the interference of different domain knowledge and
fostering the knowledge accumulation during long-term adaptation. Experimental
results on three classical architectures, including quantized transformer-based
and CNN-based models, demonstrate the superiority of our methods for quantized
model adaptation. On the quantized W6A6 ViT-B model, our ZOA is able to achieve
a 5.0\% improvement over the state-of-the-art FOA on ImageNet-C dataset. The
source code is available at https://github.com/DengZeshuai/ZOA.

</details>


### [277] [Failure Cases Are Better Learned But Boundary Says Sorry: Facilitating Smooth Perception Change for Accuracy-Robustness Trade-Off in Adversarial Training](https://arxiv.org/abs/2508.02186)
*Yanyun Wang,Li Liu*

Main category: cs.CV

TL;DR: The paper reveals that over-learning hard adversarial samples in Adversarial Training (AT) degrades decision boundaries, proposing Robust Perception Adversarial Training (RPAT) to mitigate the accuracy-robustness trade-off.


<details>
  <summary>Details</summary>
Motivation: The trade-off between clean accuracy and adversarial robustness in AT is attributed to over-learning hard adversarial samples, not insufficient learning.

Method: Proposes RPAT, a new AT objective encouraging smooth perception changes with input perturbations.

Result: RPAT outperforms four baselines and 12 SOTA methods on CIFAR-10, CIFAR-100, and Tiny-ImageNet.

Conclusion: RPAT effectively addresses the accuracy-robustness trade-off by balancing perception consistency and decision boundary smoothness.

Abstract: Adversarial Training (AT) is one of the most effective methods to train
robust Deep Neural Networks (DNNs). However, AT creates an inherent trade-off
between clean accuracy and adversarial robustness, which is commonly attributed
to the more complicated decision boundary caused by the insufficient learning
of hard adversarial samples. In this work, we reveal a counterintuitive fact
for the first time: From the perspective of perception consistency, hard
adversarial samples that can still attack the robust model after AT are already
learned better than those successfully defended. Thus, different from previous
views, we argue that it is rather the over-sufficient learning of hard
adversarial samples that degrades the decision boundary and contributes to the
trade-off problem. Specifically, the excessive pursuit of perception
consistency would force the model to view the perturbations as noise and ignore
the information within them, which should have been utilized to induce a
smoother perception transition towards the decision boundary to support its
establishment to an appropriate location. In response, we define a new AT
objective named Robust Perception, encouraging the model perception to change
smoothly with input perturbations, based on which we propose a novel Robust
Perception Adversarial Training (RPAT) method, effectively mitigating the
current accuracy-robustness trade-off. Experiments on CIFAR-10, CIFAR-100, and
Tiny-ImageNet with ResNet-18, PreActResNet-18, and WideResNet-34-10 demonstrate
the effectiveness of our method beyond four common baselines and 12
state-of-the-art (SOTA) works. The code is available at
https://github.com/FlaAI/RPAT.

</details>


### [278] [CMIC: Content-Adaptive Mamba for Learned Image Compression](https://arxiv.org/abs/2508.02192)
*Yunuo Chen,Zezheng Lyu,Bing He,Hongwei Hu,Qi Wang,Yuan Tian,Li Song,Wenjun Zhang,Guo Lu*

Main category: cs.CV

TL;DR: CAM introduces content-aware token reorganization and global priors in SSMs for improved LIC, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Vanilla Mamba's content-agnostic approach limits dynamic exploitation of content dependencies in LIC.

Method: CAM uses content-aware token clustering/reordering and integrates global priors via a prompt dictionary.

Result: CMIC outperforms VTM-21.0 by significant BD-rate reductions on multiple benchmarks.

Conclusion: CAM enhances global dependency capture in LIC while maintaining computational efficiency.

Abstract: Recent Learned image compression (LIC) leverages Mamba-style state-space
models (SSMs) for global receptive fields with linear complexity. However,
vanilla Mamba is content-agnostic, relying on fixed and predefined selective
scans, which restricts its ability to dynamically and fully exploit content
dependencies. We introduce Content-Adaptive Mamba (CAM), a dynamic SSM that
addresses two critical limitations. First, it employs content-aware token
reorganization, clustering and reordering tokens based on content similarity to
prioritize proximity in feature space over Euclidean space. Second, it
integrates global priors into SSM via a prompt dictionary, effectively
mitigating the strict causality and long-range decay in the token interactions
of Mamba. These innovations enable CAM to better capture global dependencies
while preserving computational efficiency. Leveraging CAM, our Content-Adaptive
Mamba-based LIC model (CMIC) achieves state-of-the-art rate-distortion
performance, surpassing VTM-21.0 by -15.91\%, -21.34\%, and -17.58\% BD-rate on
Kodak, Tecnick, and CLIC benchmarks, respectively.

</details>


### [279] [Welcome New Doctor: Continual Learning with Expert Consultation and Autoregressive Inference for Whole Slide Image Analysis](https://arxiv.org/abs/2508.02220)
*Doanh Cao Bui,Jin Tae Kwak*

Main category: cs.CV

TL;DR: COSFormer is a Transformer-based continual learning framework for efficient multi-task WSI analysis, outperforming existing methods without retraining on historical data.


<details>
  <summary>Details</summary>
Motivation: The need for resource-efficient continual learning systems in WSI analysis due to the growing volume of clinical data and computational demands.

Method: COSFormer, a Transformer-based framework, learns sequentially from new tasks without revisiting full historical datasets.

Result: Evaluated on seven WSI datasets, COSFormer shows superior generalizability and effectiveness in class- and task-incremental settings.

Conclusion: COSFormer is a robust solution for continual WSI analysis, balancing efficiency and performance in clinical applications.

Abstract: Whole Slide Image (WSI) analysis, with its ability to reveal detailed tissue
structures in magnified views, plays a crucial role in cancer diagnosis and
prognosis. Due to their giga-sized nature, WSIs require substantial storage and
computational resources for processing and training predictive models. With the
rapid increase in WSIs used in clinics and hospitals, there is a growing need
for a continual learning system that can efficiently process and adapt existing
models to new tasks without retraining or fine-tuning on previous tasks. Such a
system must balance resource efficiency with high performance. In this study,
we introduce COSFormer, a Transformer-based continual learning framework
tailored for multi-task WSI analysis. COSFormer is designed to learn
sequentially from new tasks wile avoiding the need to revisit full historical
datasets. We evaluate COSFormer on a sequence of seven WSI datasets covering
seven organs and six WSI-related tasks under both class-incremental and
task-incremental settings. The results demonstrate COSFormer's superior
generalizability and effectiveness compared to existing continual learning
frameworks, establishing it as a robust solution for continual WSI analysis in
clinical applications.

</details>


### [280] [An Event-based Fast Intensity Reconstruction Scheme for UAV Real-time Perception](https://arxiv.org/abs/2508.02238)
*Xin Dong,Yiwei Zhang,Yangjie Cui,Jinwu Xiang,Daochun Li,Zhan Tu*

Main category: cs.CV

TL;DR: The paper proposes ESI, a streamlined event-based intensity reconstruction method, ensuring real-time performance, high frame rates, and superior image quality for UAV applications.


<details>
  <summary>Details</summary>
Motivation: Event cameras excel in challenging visual conditions but require efficient onboard implementation. ESI aims to address this by enabling real-time intensity reconstruction.

Method: ESI performs single integration of event streams with an enhanced decay algorithm, ensuring high frame rates (100 FPS) and low computation.

Result: ESI outperforms state-of-the-art methods in runtime efficiency, reconstruction quality, and frame rate, proving effective in low-light UAV tracking.

Conclusion: ESI significantly enhances UAV onboard perception, especially in adverse visual conditions, outperforming existing algorithms in real-world tests.

Abstract: Event cameras offer significant advantages, including a wide dynamic range,
high temporal resolution, and immunity to motion blur, making them highly
promising for addressing challenging visual conditions. Extracting and
utilizing effective information from asynchronous event streams is essential
for the onboard implementation of event cameras. In this paper, we propose a
streamlined event-based intensity reconstruction scheme, event-based single
integration (ESI), to address such implementation challenges. This method
guarantees the portability of conventional frame-based vision methods to
event-based scenarios and maintains the intrinsic advantages of event cameras.
The ESI approach reconstructs intensity images by performing a single
integration of the event streams combined with an enhanced decay algorithm.
Such a method enables real-time intensity reconstruction at a high frame rate,
typically 100 FPS. Furthermore, the relatively low computation load of ESI fits
onboard implementation suitably, such as in UAV-based visual tracking
scenarios. Extensive experiments have been conducted to evaluate the
performance comparison of ESI and state-of-the-art algorithms. Compared to
state-of-the-art algorithms, ESI demonstrates remarkable runtime efficiency
improvements, superior reconstruction quality, and a high frame rate. As a
result, ESI enhances UAV onboard perception significantly under visual
adversary surroundings. In-flight tests, ESI demonstrates effective performance
for UAV onboard visual tracking under extremely low illumination
conditions(2-10lux), whereas other comparative algorithms fail due to
insufficient frame rate, poor image quality, or limited real-time performance.

</details>


### [281] [Forecasting When to Forecast: Accelerating Diffusion Models with Confidence-Gated Taylor](https://arxiv.org/abs/2508.02240)
*Xiaoliu Guan,Lielin Jiang,Hanqi Chen,Xu Zhang,Jiaxing Yan,Guanzhong Wang,Yi Liu,Zetao Zhang,Yu Wu*

Main category: cs.CV

TL;DR: The paper proposes a dynamic Taylor-based acceleration method for Diffusion Transformers (DiTs) to improve inference speed while maintaining quality, reducing memory overhead and adapting caching schedules.


<details>
  <summary>Details</summary>
Motivation: DiTs suffer from low inference speed, limiting their use in low-resource applications. Existing methods like TaylorSeer have memory and computation overheads and fixed caching schedules, leading to degraded outputs.

Method: The approach shifts Taylor prediction to the last block level, reducing cached features, and introduces a dynamic caching mechanism based on prediction reliability.

Result: Achieves 3.17x, 2.36x, and 4.14x acceleration on FLUX, DiT, and Wan Video with negligible quality drop.

Conclusion: The method effectively balances speed and quality, addressing limitations of prior work.

Abstract: Diffusion Transformers (DiTs) have demonstrated remarkable performance in
visual generation tasks. However, their low inference speed limits their
deployment in low-resource applications. Recent training-free approaches
exploit the redundancy of features across timesteps by caching and reusing past
representations to accelerate inference. Building on this idea, TaylorSeer
instead uses cached features to predict future ones via Taylor expansion.
However, its module-level prediction across all transformer blocks (e.g.,
attention or feedforward modules) requires storing fine-grained intermediate
features, leading to notable memory and computation overhead. Moreover, it
adopts a fixed caching schedule without considering the varying accuracy of
predictions across timesteps, which can lead to degraded outputs when
prediction fails. To address these limitations, we propose a novel approach to
better leverage Taylor-based acceleration. First, we shift the Taylor
prediction target from the module level to the last block level, significantly
reducing the number of cached features. Furthermore, observing strong
sequential dependencies among Transformer blocks, we propose to use the error
between the Taylor-estimated and actual outputs of the first block as an
indicator of prediction reliability. If the error is small, we trust the Taylor
prediction for the last block; otherwise, we fall back to full computation,
thereby enabling a dynamic caching mechanism. Empirical results show that our
method achieves a better balance between speed and quality, achieving a 3.17x
acceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible
quality drop. The Project Page is
\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}

</details>


### [282] [I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking](https://arxiv.org/abs/2508.02243)
*Ziyan Liu,Junwen Li,Kaiwen Li,Tong Ruan,Chao Wang,Xinyan He,Zongyu Wang,Xuezhi Cao,Jingping Liu*

Main category: cs.CV

TL;DR: A novel LLM-based framework, Intra- and Inter-modal Collaborative Reflections, improves multimodal entity linking by prioritizing text and iteratively integrating visual clues when needed, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Address challenges in current LLM-based methods: unnecessary image data use and one-time visual feature extraction, which reduce effectiveness and accuracy.

Method: Proposes a framework that first uses text, then iteratively integrates key visual clues via intra- and inter-modality evaluations for better accuracy.

Result: Outperforms state-of-the-art methods on three datasets with improvements of 3.2%, 5.1%, and 1.6%.

Conclusion: The framework effectively balances text and visual data, enhancing multimodal entity linking performance.

Abstract: Multimodal entity linking plays a crucial role in a wide range of
applications. Recent advances in large language model-based methods have become
the dominant paradigm for this task, effectively leveraging both textual and
visual modalities to enhance performance. Despite their success, these methods
still face two challenges, including unnecessary incorporation of image data in
certain scenarios and the reliance only on a one-time extraction of visual
features, which can undermine their effectiveness and accuracy. To address
these challenges, we propose a novel LLM-based framework for the multimodal
entity linking task, called Intra- and Inter-modal Collaborative Reflections.
This framework prioritizes leveraging text information to address the task.
When text alone is insufficient to link the correct entity through intra- and
inter-modality evaluations, it employs a multi-round iterative strategy that
integrates key visual clues from various aspects of the image to support
reasoning and enhance matching accuracy. Extensive experiments on three widely
used public datasets demonstrate that our framework consistently outperforms
current state-of-the-art methods in the task, achieving improvements of 3.2%,
5.1%, and 1.6%, respectively. Our code is available at
https://github.com/ziyan-xiaoyu/I2CR/.

</details>


### [283] [Semi-Supervised Semantic Segmentation via Derivative Label Propagation](https://arxiv.org/abs/2508.02254)
*Yuanbin Fu,Xiaojie Guo*

Main category: cs.CV

TL;DR: DerProp introduces a derivative label propagation method to improve pseudo-label reliability in semi-supervised semantic segmentation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the unreliability of pseudo-labels in semi-supervised semantic segmentation and reduce annotation burden.

Method: Uses derivative label propagation with discrete derivative operations on pixel-wise features for regularization, constraining the solution space.

Result: Demonstrates superior performance over other methods in experiments.

Conclusion: DerProp effectively enhances pseudo-label reliability and segmentation accuracy.

Abstract: Semi-supervised semantic segmentation, which leverages a limited set of
labeled images, helps to relieve the heavy annotation burden. While
pseudo-labeling strategies yield promising results, there is still room for
enhancing the reliability of pseudo-labels. Hence, we develop a semi-supervised
framework, namely DerProp, equipped with a novel derivative label propagation
to rectify imperfect pseudo-labels. Our label propagation method imposes
discrete derivative operations on pixel-wise feature vectors as additional
regularization, thereby generating strictly regularized similarity metrics.
Doing so effectively alleviates the ill-posed problem that identical
similarities correspond to different features, through constraining the
solution space. Extensive experiments are conducted to verify the rationality
of our design, and demonstrate our superiority over other methods. Codes are
available at https://github.com/ForawardStar/DerProp/.

</details>


### [284] [Patho-AgenticRAG: Towards Multimodal Agentic Retrieval-Augmented Generation for Pathology VLMs via Reinforcement Learning](https://arxiv.org/abs/2508.02258)
*Wenchuan Zhang,Jingru Guo,Hengzhe Zhang,Penghao Zhang,Jie Chen,Shuwan Zhang,Zhang Zhang,Yuhao Yi,Hong Bu*

Main category: cs.CV

TL;DR: Patho-AgenticRAG is a multimodal RAG framework for pathology, addressing VLMs' hallucinations by leveraging text-image retrieval from authoritative textbooks, improving diagnostic accuracy.


<details>
  <summary>Details</summary>
Motivation: Pathology VLMs face challenges like ultra-high resolution and complex semantics, leading to hallucinations. Existing RAG methods rely on text-only knowledge, missing visual cues.

Method: Proposes Patho-AgenticRAG, a multimodal RAG framework with page-level embeddings from pathology textbooks, enabling joint text-image search and reasoning.

Result: Outperforms existing models in tasks like multiple-choice diagnosis and visual question answering.

Conclusion: Patho-AgenticRAG enhances diagnostic accuracy by integrating visual and textual information, addressing VLMs' limitations in pathology.

Abstract: Although Vision Language Models (VLMs) have shown strong generalization in
medical imaging, pathology presents unique challenges due to ultra-high
resolution, complex tissue structures, and nuanced clinical semantics. These
factors make pathology VLMs prone to hallucinations, i.e., generating outputs
inconsistent with visual evidence, which undermines clinical trust. Existing
RAG approaches in this domain largely depend on text-based knowledge bases,
limiting their ability to leverage diagnostic visual cues. To address this, we
propose Patho-AgenticRAG, a multimodal RAG framework with a database built on
page-level embeddings from authoritative pathology textbooks. Unlike
traditional text-only retrieval systems, it supports joint text-image search,
enabling direct retrieval of textbook pages that contain both the queried text
and relevant visual cues, thus avoiding the loss of critical image-based
information. Patho-AgenticRAG also supports reasoning, task decomposition, and
multi-turn search interactions, improving accuracy in complex diagnostic
scenarios. Experiments show that Patho-AgenticRAG significantly outperforms
existing multimodal models in complex pathology tasks like multiple-choice
diagnosis and visual question answering. Our project is available at the
Patho-AgenticRAG repository:
https://github.com/Wenchuan-Zhang/Patho-AgenticRAG.

</details>


### [285] [SplatSSC: Decoupled Depth-Guided Gaussian Splatting for Semantic Scene Completion](https://arxiv.org/abs/2508.02261)
*Rui Qian,Haozhi Cao,Tianchen Deng,Shenghai Yuan,Lihua Xie*

Main category: cs.CV

TL;DR: SplatSSC improves monocular 3D semantic scene completion by using depth-guided initialization and a Gaussian aggregator, achieving state-of-the-art results with reduced latency and memory.


<details>
  <summary>Details</summary>
Motivation: Current object-centric methods for 3D semantic scene completion rely on inefficient random initialization of primitives, leading to artifacts and inefficiency.

Method: SplatSSC introduces a depth-guided initialization strategy (GMF module) and a Decoupled Gaussian Aggregator (DGA) to enhance robustness and efficiency.

Result: Outperforms prior methods by 6.3% in IoU and 4.1% in mIoU, with 9.3% reductions in latency and memory.

Conclusion: SplatSSC offers a more efficient and accurate solution for monocular 3D semantic scene completion.

Abstract: Monocular 3D Semantic Scene Completion (SSC) is a challenging yet promising
task that aims to infer dense geometric and semantic descriptions of a scene
from a single image. While recent object-centric paradigms significantly
improve efficiency by leveraging flexible 3D Gaussian primitives, they still
rely heavily on a large number of randomly initialized primitives, which
inevitably leads to 1) inefficient primitive initialization and 2) outlier
primitives that introduce erroneous artifacts. In this paper, we propose
SplatSSC, a novel framework that resolves these limitations with a depth-guided
initialization strategy and a principled Gaussian aggregator. Instead of random
initialization, SplatSSC utilizes a dedicated depth branch composed of a
Group-wise Multi-scale Fusion (GMF) module, which integrates multi-scale image
and depth features to generate a sparse yet representative set of initial
Gaussian primitives. To mitigate noise from outlier primitives, we develop the
Decoupled Gaussian Aggregator (DGA), which enhances robustness by decomposing
geometric and semantic predictions during the Gaussian-to-voxel splatting
process. Complemented with a specialized Probability Scale Loss, our method
achieves state-of-the-art performance on the Occ-ScanNet dataset, outperforming
prior approaches by over 6.3% in IoU and 4.1% in mIoU, while reducing both
latency and memory consumption by more than 9.3%. The code will be released
upon acceptance.

</details>


### [286] [Semi-Supervised Dual-Threshold Contrastive Learning for Ultrasound Image Classification and Segmentation](https://arxiv.org/abs/2508.02265)
*Peng Zhang,Zhihui Lai,Heng Kong*

Main category: cs.CV

TL;DR: The paper introduces Hermes, a semi-supervised dual-threshold contrastive learning strategy for ultrasound image classification and segmentation, addressing issues like overconfident pseudo-labels and lack of task affinity.


<details>
  <summary>Details</summary>
Motivation: Current methods suffer from incorrect pseudo-labels and independent task treatment, degrading performance in semi-supervised contrastive learning.

Method: Hermes combines contrastive and semi-supervised learning, uses pseudo-labels for guidance, and includes inter-task attention, saliency, and consistency learning.

Result: Hermes outperforms state-of-the-art methods on public and private ultrasound datasets.

Conclusion: The proposed strategy effectively improves performance by integrating tasks and refining pseudo-label selection.

Abstract: Confidence-based pseudo-label selection usually generates overly confident
yet incorrect predictions, due to the early misleadingness of model and
overfitting inaccurate pseudo-labels in the learning process, which heavily
degrades the performance of semi-supervised contrastive learning. Moreover,
segmentation and classification tasks are treated independently and the
affinity fails to be fully explored. To address these issues, we propose a
novel semi-supervised dual-threshold contrastive learning strategy for
ultrasound image classification and segmentation, named Hermes. This strategy
combines the strengths of contrastive learning with semi-supervised learning,
where the pseudo-labels assist contrastive learning by providing additional
guidance. Specifically, an inter-task attention and saliency module is also
developed to facilitate information sharing between the segmentation and
classification tasks. Furthermore, an inter-task consistency learning strategy
is designed to align tumor features across both tasks, avoiding negative
transfer for reducing features discrepancy. To solve the lack of publicly
available ultrasound datasets, we have collected the SZ-TUS dataset, a thyroid
ultrasound image dataset. Extensive experiments on two public ultrasound
datasets and one private dataset demonstrate that Hermes consistently
outperforms several state-of-the-art methods across various semi-supervised
settings.

</details>


### [287] [SGAD: Semantic and Geometric-aware Descriptor for Local Feature Matching](https://arxiv.org/abs/2508.02278)
*Xiangzeng Liu,Chi Wang,Guanglu Shi,Xiaodong Zhang,Qiguang Miao,Miao Fan*

Main category: cs.CV

TL;DR: SGAD introduces a novel area descriptor network for efficient and accurate local feature matching, outperforming existing methods in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing A2PM methods rely on inefficient pixel-level comparisons and complex graph matching, limiting scalability.

Method: SGAD generates discriminative area descriptors for direct matching, uses a novel supervision strategy for classification and ranking, and employs HCRF to eliminate overlapping areas.

Result: SGAD reduces runtime by 60x (0.82s vs. 60.23s) compared to MESA and improves accuracy in pose estimation tasks.

Conclusion: SGAD sets a new state-of-the-art in area-based matching, offering significant efficiency and accuracy gains.

Abstract: Local feature matching remains a fundamental challenge in computer vision.
Recent Area to Point Matching (A2PM) methods have improved matching accuracy.
However, existing research based on this framework relies on inefficient
pixel-level comparisons and complex graph matching that limit scalability. In
this work, we introduce the Semantic and Geometric-aware Descriptor Network
(SGAD), which fundamentally rethinks area-based matching by generating highly
discriminative area descriptors that enable direct matching without complex
graph optimization. This approach significantly improves both accuracy and
efficiency of area matching. We further improve the performance of area
matching through a novel supervision strategy that decomposes the area matching
task into classification and ranking subtasks. Finally, we introduce the
Hierarchical Containment Redundancy Filter (HCRF) to eliminate overlapping
areas by analyzing containment graphs. SGAD demonstrates remarkable performance
gains, reducing runtime by 60x (0.82s vs. 60.23s) compared to MESA. Extensive
evaluations show consistent improvements across multiple point matchers:
SGAD+LoFTR reduces runtime compared to DKM, while achieving higher accuracy
(0.82s vs. 1.51s, 65.98 vs. 61.11) in outdoor pose estimation, and SGAD+ROMA
delivers +7.39% AUC@5{\deg} in indoor pose estimation, establishing a new
state-of-the-art.

</details>


### [288] [Do Edges Matter? Investigating Edge-Enhanced Pre-Training for Medical Image Segmentation](https://arxiv.org/abs/2508.02281)
*Paul Zaha,Lars Böcking,Simeon Allmendinger,Leopold Müller,Niklas Kühl*

Main category: cs.CV

TL;DR: The paper explores how edge-focused pre-training of foundation models affects medical image segmentation across modalities, proposing a meta-learning strategy for optimal performance.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding how edge preprocessing influences segmentation performance in medical imaging and improve cross-modality capabilities.

Method: Pre-train foundation models on raw or edge-enhanced data, fine-tune on modality-specific subsets, and use meta-learning to select the best pre-training approach.

Result: Edge-focused pre-training shows mixed results; meta-learning improves overall segmentation performance by 16.42% over edge-only and 19.30% over raw-only models.

Conclusion: Selective application of edge-focused pre-training via meta-learning enhances segmentation performance across diverse medical imaging tasks.

Abstract: Medical image segmentation is crucial for disease diagnosis and treatment
planning, yet developing robust segmentation models often requires substantial
computational resources and large datasets. Existing research shows that
pre-trained and finetuned foundation models can boost segmentation performance.
However, questions remain about how particular image preprocessing steps may
influence segmentation performance across different medical imaging modalities.
In particular, edges-abrupt transitions in pixel intensity-are widely
acknowledged as vital cues for object boundaries but have not been
systematically examined in the pre-training of foundation models. We address
this gap by investigating to which extend pre-training with data processed
using computationally efficient edge kernels, such as kirsch, can improve
cross-modality segmentation capabilities of a foundation model. Two versions of
a foundation model are first trained on either raw or edge-enhanced data across
multiple medical imaging modalities, then finetuned on selected raw subsets
tailored to specific medical modalities. After systematic investigation using
the medical domains Dermoscopy, Fundus, Mammography, Microscopy, OCT, US, and
XRay, we discover both increased and reduced segmentation performance across
modalities using edge-focused pre-training, indicating the need for a selective
application of this approach. To guide such selective applications, we propose
a meta-learning strategy. It uses standard deviation and image entropy of the
raw image to choose between a model pre-trained on edge-enhanced or on raw data
for optimal performance. Our experiments show that integrating this
meta-learning layer yields an overall segmentation performance improvement
across diverse medical imaging tasks by 16.42% compared to models pre-trained
on edge-enhanced data only and 19.30% compared to models pre-trained on raw
data only.

</details>


### [289] [Unleashing the Temporal Potential of Stereo Event Cameras for Continuous-Time 3D Object Detection](https://arxiv.org/abs/2508.02288)
*Jae-Young Kang,Hoonhee Cho,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: A novel stereo 3D object detection framework using only event cameras, outperforming prior methods in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Event cameras address perception gaps in high-speed scenarios, unlike LiDAR and RGB cameras with fixed frame rates.

Method: Proposes a dual filter mechanism to extract semantic and geometric information from event data and aligns bounding boxes with object-centric info.

Result: Outperforms prior approaches in dynamic environments, showcasing robust, continuous-time 3D perception.

Conclusion: Event cameras hold potential for robust 3D detection without relying on conventional sensors.

Abstract: 3D object detection is essential for autonomous systems, enabling precise
localization and dimension estimation. While LiDAR and RGB cameras are widely
used, their fixed frame rates create perception gaps in high-speed scenarios.
Event cameras, with their asynchronous nature and high temporal resolution,
offer a solution by capturing motion continuously. The recent approach, which
integrates event cameras with conventional sensors for continuous-time
detection, struggles in fast-motion scenarios due to its dependency on
synchronized sensors. We propose a novel stereo 3D object detection framework
that relies solely on event cameras, eliminating the need for conventional 3D
sensors. To compensate for the lack of semantic and geometric information in
event data, we introduce a dual filter mechanism that extracts both.
Additionally, we enhance regression by aligning bounding boxes with
object-centric information. Experiments show that our method outperforms prior
approaches in dynamic environments, demonstrating the potential of event
cameras for robust, continuous-time 3D perception. The code is available at
https://github.com/mickeykang16/Ev-Stereo3D.

</details>


### [290] [Towards Real Unsupervised Anomaly Detection Via Confident Meta-Learning](https://arxiv.org/abs/2508.02293)
*Muhammad Aqeel,Shakiba Sharifi,Marco Cristani,Francesco Setti*

Main category: cs.CV

TL;DR: CoMet introduces a meta-learning strategy for anomaly detection, enabling training on uncurated datasets without manual filtering, improving robustness and performance.


<details>
  <summary>Details</summary>
Motivation: Current unsupervised anomaly detection assumes all training data are nominal, requiring manual curation and introducing bias. CoMet addresses this by allowing training on mixed datasets.

Method: CoMet combines Soft Confident Learning (weighting low-confidence samples less) and Meta-Learning (regularizing updates via validation loss covariance) for stable, robust training.

Result: CoMet outperforms baselines on MVTec-AD, VIADUCT, and KSDD2, showing insensitivity to training anomalies and achieving state-of-the-art results.

Conclusion: CoMet eliminates the need for manual data curation, enhances adaptability, and improves anomaly detection performance across diverse datasets.

Abstract: So-called unsupervised anomaly detection is better described as
semi-supervised, as it assumes all training data are nominal. This assumption
simplifies training but requires manual data curation, introducing bias and
limiting adaptability. We propose Confident Meta-learning (CoMet), a novel
training strategy that enables deep anomaly detection models to learn from
uncurated datasets where nominal and anomalous samples coexist, eliminating the
need for explicit filtering. Our approach integrates Soft Confident Learning,
which assigns lower weights to low-confidence samples, and Meta-Learning, which
stabilizes training by regularizing updates based on training validation loss
covariance. This prevents overfitting and enhances robustness to noisy data.
CoMet is model-agnostic and can be applied to any anomaly detection method
trainable via gradient descent. Experiments on MVTec-AD, VIADUCT, and KSDD2
with two state-of-the-art models demonstrate the effectiveness of our approach,
consistently improving over the baseline methods, remaining insensitive to
anomalies in the training set, and setting a new state-of-the-art across all
datasets.

</details>


### [291] [Whole-body Representation Learning For Competing Preclinical Disease Risk Assessment](https://arxiv.org/abs/2508.02307)
*Dmitrii Seletkov,Sophie Starck,Ayhan Can Erdur,Yundi Zhang,Daniel Rueckert,Rickmer Braren*

Main category: cs.CV

TL;DR: A self-supervised whole-body representation learning method improves preclinical disease risk assessment, outperforming traditional radiomics in multiple diseases and enhancing CVD subgroup predictions when combined with cardiac MRI.


<details>
  <summary>Details</summary>
Motivation: Current image-based risk prediction methods are limited by single-condition focus and reliance on hand-crafted features, necessitating a more comprehensive approach.

Method: Proposes a whole-body self-supervised representation learning method under competing risk modeling, validated in diseases like CVD, T2D, COPD, and CKD. Combined with cardiac MRI for CVD subgroups.

Result: Outperforms whole-body radiomics in multiple diseases and improves CVD subgroup predictions (IHD, HD, stroke) in preclinical screening scenarios.

Conclusion: Demonstrates translational potential for standalone screening and multi-modal clinical workflows, enabling early personalized risk stratification.

Abstract: Reliable preclinical disease risk assessment is essential to move public
healthcare from reactive treatment to proactive identification and prevention.
However, image-based risk prediction algorithms often consider one condition at
a time and depend on hand-crafted features obtained through segmentation tools.
We propose a whole-body self-supervised representation learning method for the
preclinical disease risk assessment under a competing risk modeling. This
approach outperforms whole-body radiomics in multiple diseases, including
cardiovascular disease (CVD), type 2 diabetes (T2D), chronic obstructive
pulmonary disease (COPD), and chronic kidney disease (CKD). Simulating a
preclinical screening scenario and subsequently combining with cardiac MRI, it
sharpens further the prediction for CVD subgroups: ischemic heart disease
(IHD), hypertensive diseases (HD), and stroke. The results indicate the
translational potential of whole-body representations as a standalone screening
modality and as part of a multi-modal framework within clinical workflows for
early personalized risk stratification. The code is available at
https://github.com/yayapa/WBRLforCR/

</details>


### [292] [Is Uncertainty Quantification a Viable Alternative to Learned Deferral?](https://arxiv.org/abs/2508.02319)
*Anna M. Wundram,Christian F. Baumgartner*

Main category: cs.CV

TL;DR: AI models for patient care need human-AI collaboration to ensure safety. This study compares learned deferral models and uncertainty quantification methods for robustness to data shifts, finding the latter more promising.


<details>
  <summary>Details</summary>
Motivation: To ensure safe AI implementation in healthcare by improving models' ability to defer decisions to humans when uncertain, especially under data shifts.

Method: Evaluated learned deferral models and uncertainty quantification methods on an ophthalmology dataset for glaucoma classification, testing in- and out-of-distribution performance.

Result: Uncertainty quantification methods showed better robustness to out-of-distribution inputs compared to learned deferral models.

Conclusion: Uncertainty quantification is a promising approach for AI deferral in clinical settings, particularly under data shifts.

Abstract: Artificial Intelligence (AI) holds the potential to dramatically improve
patient care. However, it is not infallible, necessitating
human-AI-collaboration to ensure safe implementation. One aspect of AI safety
is the models' ability to defer decisions to a human expert when they are
likely to misclassify autonomously. Recent research has focused on methods that
learn to defer by optimising a surrogate loss function that finds the optimal
trade-off between predicting a class label or deferring. However, during
clinical translation, models often face challenges such as data shift.
Uncertainty quantification methods aim to estimate a model's confidence in its
predictions. However, they may also be used as a deferral strategy which does
not rely on learning from specific training distribution. We hypothesise that
models developed to quantify uncertainty are more robust to out-of-distribution
(OOD) input than learned deferral models that have been trained in a supervised
fashion. To investigate this hypothesis, we constructed an extensive evaluation
study on a large ophthalmology dataset, examining both learned deferral models
and established uncertainty quantification methods, assessing their performance
in- and out-of-distribution. Specifically, we evaluate their ability to
accurately classify glaucoma from fundus images while deferring cases with a
high likelihood of error. We find that uncertainty quantification methods may
be a promising choice for AI deferral.

</details>


### [293] [Zero-shot Compositional Action Recognition with Neural Logic Constraints](https://arxiv.org/abs/2508.02320)
*Gefan Ye,Lin Li,Kexin Li,Jun Xiao,Long chen*

Main category: cs.CV

TL;DR: LogicCAR introduces symbolic reasoning to address spurious correlations and semantic ambiguity in zero-shot compositional action recognition, outperforming baselines on Sth-com dataset.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in ZS-CAR: missing compositional structure and semantic hierarchy constraints, which lead to spurious correlations and ambiguity.

Method: Proposes LogicCAR with dual symbolic constraints: Explicit Compositional Logic and Hierarchical Primitive Logic, formalized in first-order logic and embedded into neural networks.

Result: Outperforms existing baselines on the Sth-com dataset.

Conclusion: LogicCAR effectively bridges symbolic abstraction and neural models, improving compositional reasoning in ZS-CAR.

Abstract: Zero-shot compositional action recognition (ZS-CAR) aims to identify unseen
verb-object compositions in the videos by exploiting the learned knowledge of
verb and object primitives during training. Despite compositional learning's
progress in ZS-CAR, two critical challenges persist: 1) Missing compositional
structure constraint, leading to spurious correlations between primitives; 2)
Neglecting semantic hierarchy constraint, leading to semantic ambiguity and
impairing the training process. In this paper, we argue that human-like
symbolic reasoning offers a principled solution to these challenges by
explicitly modeling compositional and hierarchical structured abstraction. To
this end, we propose a logic-driven ZS-CAR framework LogicCAR that integrates
dual symbolic constraints: Explicit Compositional Logic and Hierarchical
Primitive Logic. Specifically, the former models the restrictions within the
compositions, enhancing the compositional reasoning ability of our model. The
latter investigates the semantical dependencies among different primitives,
empowering the models with fine-to-coarse reasoning capacity. By formalizing
these constraints in first-order logic and embedding them into neural network
architectures, LogicCAR systematically bridges the gap between symbolic
abstraction and existing models. Extensive experiments on the Sth-com dataset
demonstrate that our LogicCAR outperforms existing baseline methods, proving
the effectiveness of our logic-driven constraints.

</details>


### [294] [Dream-to-Recon: Monocular 3D Reconstruction with Diffusion-Depth Distillation from Single Images](https://arxiv.org/abs/2508.02323)
*Philipp Wulff,Felix Wimbauer,Dominik Muhle,Daniel Cremers*

Main category: cs.CV

TL;DR: A method for single-image volumetric scene reconstruction using pre-trained 2D diffusion and depth models, outperforming multi-view supervised baselines.


<details>
  <summary>Details</summary>
Motivation: Single-image volumetric reconstruction is vital for applications like autonomous driving and robotics, but existing methods rely on costly 3D ground truth or multi-view data.

Method: Leverages pre-trained 2D diffusion and depth models to generate synthetic scene geometry from a single image, distilling a feed-forward reconstruction model.

Result: Matches or outperforms state-of-the-art multi-view supervised baselines on KITTI-360 and Waymo datasets, excelling in dynamic scenes.

Conclusion: The approach offers a cost-effective alternative to multi-view supervision, with superior performance in dynamic scenarios.

Abstract: Volumetric scene reconstruction from a single image is crucial for a broad
range of applications like autonomous driving and robotics. Recent volumetric
reconstruction methods achieve impressive results, but generally require
expensive 3D ground truth or multi-view supervision. We propose to leverage
pre-trained 2D diffusion models and depth prediction models to generate
synthetic scene geometry from a single image. This can then be used to distill
a feed-forward scene reconstruction model. Our experiments on the challenging
KITTI-360 and Waymo datasets demonstrate that our method matches or outperforms
state-of-the-art baselines that use multi-view supervision, and offers unique
advantages, for example regarding dynamic scenes.

</details>


### [295] [Qwen-Image Technical Report](https://arxiv.org/abs/2508.02324)
*Chenfei Wu,Jiahao Li,Jingren Zhou,Junyang Lin,Kaiyuan Gao,Kun Yan,Sheng-ming Yin,Shuai Bai,Xiao Xu,Yilei Chen,Yuxiang Chen,Zecheng Tang,Zekai Zhang,Zhengyi Wang,An Yang,Bowen Yu,Chen Cheng,Dayiheng Liu,Deqing Li,Hang Zhang,Hao Meng,Hu Wei,Jingyuan Ni,Kai Chen,Kuan Cao,Liang Peng,Lin Qu,Minggang Wu,Peng Wang,Shuting Yu,Tingkun Wen,Wensen Feng,Xiaoxiao Xu,Yi Wang,Yichang Zhang,Yongqiang Zhu,Yujia Wu,Yuxuan Cai,Zenan Liu*

Main category: cs.CV

TL;DR: Qwen-Image is an advanced image generation model excelling in complex text rendering and precise editing, using a progressive training strategy and dual-encoding for consistency.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in complex text rendering and enhance image editing consistency.

Method: Uses a comprehensive data pipeline, progressive training, and a dual-encoding mechanism with Qwen2.5-VL and VAE.

Result: Achieves state-of-the-art performance in text rendering and image editing across benchmarks.

Conclusion: Qwen-Image demonstrates strong capabilities in generation and editing, with notable progress in logographic languages like Chinese.

Abstract: We present Qwen-Image, an image generation foundation model in the Qwen
series that achieves significant advances in complex text rendering and precise
image editing. To address the challenges of complex text rendering, we design a
comprehensive data pipeline that includes large-scale data collection,
filtering, annotation, synthesis, and balancing. Moreover, we adopt a
progressive training strategy that starts with non-text-to-text rendering,
evolves from simple to complex textual inputs, and gradually scales up to
paragraph-level descriptions. This curriculum learning approach substantially
enhances the model's native text rendering capabilities. As a result,
Qwen-Image not only performs exceptionally well in alphabetic languages such as
English, but also achieves remarkable progress on more challenging logographic
languages like Chinese. To enhance image editing consistency, we introduce an
improved multi-task training paradigm that incorporates not only traditional
text-to-image (T2I) and text-image-to-image (TI2I) tasks but also
image-to-image (I2I) reconstruction, effectively aligning the latent
representations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed
the original image into Qwen2.5-VL and the VAE encoder to obtain semantic and
reconstructive representations, respectively. This dual-encoding mechanism
enables the editing module to strike a balance between preserving semantic
consistency and maintaining visual fidelity. Qwen-Image achieves
state-of-the-art performance, demonstrating its strong capabilities in both
image generation and editing across multiple benchmarks.

</details>


### [296] [CLIP-IN: Enhancing Fine-Grained Visual Understanding in CLIP via Instruction Editing Data and Long Captions](https://arxiv.org/abs/2508.02329)
*Ziteng Wang,Siqi Yang,Limeng Qiao,Lin Ma*

Main category: cs.CV

TL;DR: CLIP-IN enhances CLIP's fine-grained perception using hard negative contrastive learning and long descriptive captions, improving performance on fine-grained tasks without losing zero-shot capabilities.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitation of VLMs like CLIP in detailed visual comprehension by improving fine-grained perception.

Method: Uses instruction-editing datasets for hard negative pairs and a symmetric contrastive loss, along with long captions and rotary positional encodings.

Result: Achieves gains on MMVP and fine-grained tasks, reduces visual hallucinations in MLLMs, and maintains zero-shot performance.

Conclusion: Combining targeted contrastive learning with descriptive information enhances VLM fine-grained understanding.

Abstract: Despite the success of Vision-Language Models (VLMs) like CLIP in aligning
vision and language, their proficiency in detailed, fine-grained visual
comprehension remains a key challenge. We present CLIP-IN, a novel framework
that bolsters CLIP's fine-grained perception through two core innovations.
Firstly, we leverage instruction-editing datasets, originally designed for
image manipulation, as a unique source of hard negative image-text pairs.
Coupled with a symmetric hard negative contrastive loss, this enables the model
to effectively distinguish subtle visual-semantic differences. Secondly,
CLIP-IN incorporates long descriptive captions, utilizing rotary positional
encodings to capture rich semantic context often missed by standard CLIP. Our
experiments demonstrate that CLIP-IN achieves substantial gains on the MMVP
benchmark and various fine-grained visual recognition tasks, without
compromising robust zero-shot performance on broader classification and
retrieval tasks. Critically, integrating CLIP-IN's visual representations into
Multimodal Large Language Models significantly reduces visual hallucinations
and enhances reasoning abilities. This work underscores the considerable
potential of synergizing targeted, instruction-based contrastive learning with
comprehensive descriptive information to elevate the fine-grained understanding
of VLMs.

</details>


### [297] [Correspondence-Free Fast and Robust Spherical Point Pattern Registration](https://arxiv.org/abs/2508.02339)
*Anik Sarker,Alan T. Asbeck*

Main category: cs.CV

TL;DR: A novel rotation estimation algorithm for spherical patterns reduces complexity to linear time (O(n)) and outperforms existing methods in speed and accuracy, validated on synthetic and real-world tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for rotation estimation on spherical patterns are computationally expensive (O(n^3)) and lack robustness to outliers, prompting the need for a more efficient and accurate solution.

Method: The paper introduces three algorithms: SPMC (Spherical Pattern Matching by Correlation), FRS (Fast Rotation Search), and a hybrid SPMC+FRS approach, reformulating the problem as a spherical point-set alignment (Wahba problem for 3D unit vectors).

Result: The proposed algorithms are over 10x faster and more accurate than state-of-the-art methods, validated on synthetic data and adapted to real-world tasks like Point Cloud Registration and spherical image rotation estimation.

Conclusion: The new approach significantly improves efficiency and accuracy for rotation estimation in spherical patterns, with practical applications in PCR and spherical imaging.

Abstract: Existing methods for rotation estimation between two spherical
($\mathbb{S}^2$) patterns typically rely on spherical cross-correlation
maximization between two spherical function. However, these approaches exhibit
computational complexities greater than cubic $O(n^3)$ with respect to rotation
space discretization and lack extensive evaluation under significant outlier
contamination. To this end, we propose a rotation estimation algorithm between
two spherical patterns with linear time complexity $O(n)$. Unlike existing
spherical-function-based methods, we explicitly represent spherical patterns as
discrete 3D point sets on the unit sphere, reformulating rotation estimation as
a spherical point-set alignment (i.e., Wahba problem for 3D unit vectors).
Given the geometric nature of our formulation, our spherical pattern alignment
algorithm naturally aligns with the Wahba problem framework for 3D unit
vectors. Specifically, we introduce three novel algorithms: (1) SPMC (Spherical
Pattern Matching by Correlation), (2) FRS (Fast Rotation Search), and (3) a
hybrid approach (SPMC+FRS) that combines the advantages of the previous two
methods. Our experiments demonstrate that in the $\mathbb{S}^2$ domain and in
correspondence-free settings, our algorithms are over 10x faster and over 10x
more accurate than current state-of-the-art methods for the Wahba problem with
outliers. We validate our approach through extensive simulations on a new
dataset of spherical patterns, the ``Robust Vector Alignment Dataset.
"Furthermore, we adapt our methods to two real-world tasks: (i) Point Cloud
Registration (PCR) and (ii) rotation estimation for spherical images.

</details>


### [298] [Learning Partially-Decorrelated Common Spaces for Ad-hoc Video Search](https://arxiv.org/abs/2508.02340)
*Fan Hu,Zijie Xin,Xirong Li*

Main category: cs.CV

TL;DR: The paper proposes LPD (Learning Partially Decorrelated common spaces) to address the challenge of visual diversity in Ad-hoc Video Search (AVS) by constructing feature-specific common spaces and using de-correlation loss.


<details>
  <summary>Details</summary>
Motivation: The visual diversity in AVS makes it hard to retrieve all relevant videos comprehensively, as current methods overlook the need for diverse spaces.

Method: LPD introduces feature-specific common space construction and de-correlation loss, along with an entropy-based fair multi-space triplet ranking loss for consistency.

Result: Experiments on TRECVID AVS benchmarks (2016-2023) show LPD's effectiveness, and visualizations confirm its ability to enhance result diversity.

Conclusion: LPD successfully addresses AVS challenges by leveraging diverse spaces and de-correlation, improving retrieval comprehensiveness and diversity.

Abstract: Ad-hoc Video Search (AVS) involves using a textual query to search for
multiple relevant videos in a large collection of unlabeled short videos. The
main challenge of AVS is the visual diversity of relevant videos. A simple
query such as "Find shots of a man and a woman dancing together indoors" can
span a multitude of environments, from brightly lit halls and shadowy bars to
dance scenes in black-and-white animations. It is therefore essential to
retrieve relevant videos as comprehensively as possible. Current solutions for
the AVS task primarily fuse multiple features into one or more common spaces,
yet overlook the need for diverse spaces. To fully exploit the expressive
capability of individual features, we propose LPD, short for Learning Partially
Decorrelated common spaces. LPD incorporates two key innovations:
feature-specific common space construction and the de-correlation loss.
Specifically, LPD learns a separate common space for each video and text
feature, and employs de-correlation loss to diversify the ordering of negative
samples across different spaces. To enhance the consistency of multi-space
convergence, we designed an entropy-based fair multi-space triplet ranking
loss. Extensive experiments on the TRECVID AVS benchmarks (2016-2023) justify
the effectiveness of LPD. Moreover, diversity visualizations of LPD's spaces
highlight its ability to enhance result diversity.

</details>


### [299] [mmWave Radar-Based Non-Line-of-Sight Pedestrian Localization at T-Junctions Utilizing Road Layout Extraction via Camera](https://arxiv.org/abs/2508.02348)
*Byeonggyu Park,Hee-Yeun Kim,Byonghyok Choi,Hansang Cho,Byungkwan Kim,Soomok Lee,Mingu Jeon,Seong-Woo Kim*

Main category: cs.CV

TL;DR: A novel framework combines radar and camera data to localize pedestrians in NLoS urban environments, overcoming limitations of each sensor.


<details>
  <summary>Details</summary>
Motivation: Accurate localization of pedestrians in NLoS regions is challenging due to radar distortions and camera limitations.

Method: Uses camera-derived road layout to interpret 2D radar PCD for spatial scene reconstruction.

Result: Validated with real-vehicle experiments, showing practical applicability in outdoor NLoS scenarios.

Conclusion: The framework effectively localizes NLoS pedestrians by fusing radar and camera data.

Abstract: Pedestrians Localization in Non-Line-of-Sight (NLoS) regions within urban
environments poses a significant challenge for autonomous driving systems.
While mmWave radar has demonstrated potential for detecting objects in such
scenarios, the 2D radar point cloud (PCD) data is susceptible to distortions
caused by multipath reflections, making accurate spatial inference difficult.
Additionally, although camera images provide high-resolution visual
information, they lack depth perception and cannot directly observe objects in
NLoS regions. In this paper, we propose a novel framework that interprets radar
PCD through road layout inferred from camera for localization of NLoS
pedestrians. The proposed method leverages visual information from the camera
to interpret 2D radar PCD, enabling spatial scene reconstruction. The
effectiveness of the proposed approach is validated through experiments
conducted using a radar-camera system mounted on a real vehicle. The
localization performance is evaluated using a dataset collected in outdoor NLoS
driving environments, demonstrating the practical applicability of the method.

</details>


### [300] [Text2Lip: Progressive Lip-Synced Talking Face Generation from Text via Viseme-Guided Rendering](https://arxiv.org/abs/2508.02362)
*Xu Wang,Shengeng Tang,Fei Wang,Lechao Cheng,Dan Guo,Feng Xue,Richang Hong*

Main category: cs.CV

TL;DR: Text2Lip is a viseme-centric framework for generating talking faces using textual input, outperforming audio-driven methods in semantic fidelity and visual realism.


<details>
  <summary>Details</summary>
Motivation: Overcoming challenges of audio-driven methods, such as reliance on paired audio-visual data and ambiguous acoustic-lip mapping, to improve scalability and robustness.

Method: Uses structured viseme sequences as a phonetic-visual bridge, employs a progressive viseme-audio replacement strategy, and a landmark-guided renderer for photorealistic synthesis.

Result: Outperforms existing methods in semantic fidelity, visual realism, and modality robustness.

Conclusion: Text2Lip establishes a new paradigm for controllable and flexible talking face generation.

Abstract: Generating semantically coherent and visually accurate talking faces requires
bridging the gap between linguistic meaning and facial articulation. Although
audio-driven methods remain prevalent, their reliance on high-quality paired
audio visual data and the inherent ambiguity in mapping acoustics to lip motion
pose significant challenges in terms of scalability and robustness. To address
these issues, we propose Text2Lip, a viseme-centric framework that constructs
an interpretable phonetic-visual bridge by embedding textual input into
structured viseme sequences. These mid-level units serve as a linguistically
grounded prior for lip motion prediction. Furthermore, we design a progressive
viseme-audio replacement strategy based on curriculum learning, enabling the
model to gradually transition from real audio to pseudo-audio reconstructed
from enhanced viseme features via cross-modal attention. This allows for robust
generation in both audio-present and audio-free scenarios. Finally, a
landmark-guided renderer synthesizes photorealistic facial videos with accurate
lip synchronization. Extensive evaluations show that Text2Lip outperforms
existing approaches in semantic fidelity, visual realism, and modality
robustness, establishing a new paradigm for controllable and flexible talking
face generation. Our project homepage is https://plyon1.github.io/Text2Lip/.

</details>


### [301] [Transport-Guided Rectified Flow Inversion: Improved Image Editing Using Optimal Transport Theory](https://arxiv.org/abs/2508.02363)
*Marian Lupascu,Mihai-Sorin Stupariu*

Main category: cs.CV

TL;DR: OTIP is a zero-shot framework using optimal transport theory to balance reconstruction fidelity and editing flexibility in rectified flow models, achieving high-fidelity results and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The challenge of balancing reconstruction fidelity and editing flexibility in image inversion for practical image editing applications.

Method: Leverages optimal transport theory to guide inversion, computing optimal transport paths between image and noise distributions.

Result: Achieves high-fidelity reconstruction (LPIPS 0.001, SSIM 0.992) and outperforms baselines in reconstruction loss (7.8%-12.9% improvement) and semantic editing (11.2% identity preservation, 1.6% perceptual quality).

Conclusion: OTIP provides superior semantic consistency and detail preservation in diverse editing tasks while maintaining computational efficiency.

Abstract: Effective image inversion in rectified flow models - mapping real images to
editable latent representations - is crucial for practical image editing
applications; however, achieving optimal balance between reconstruction
fidelity and editing flexibility remains a fundamental challenge. In this work,
we introduce the Optimal Transport Inversion Pipeline (OTIP), a zero-shot
framework that leverages optimal transport theory to guide the inversion
process in rectified flow models. Our underlying hypothesis is that
incorporating transport-based guidance during the reverse diffusion process can
effectively balance reconstruction accuracy and editing controllability through
principled trajectory optimization. The method computes optimal transport paths
between image and noise distributions while maintaining computational
efficiency. Our approach achieves high-fidelity reconstruction with LPIPS
scores of 0.001 and SSIM of 0.992 on face editing benchmarks, demonstrating
superior preservation of fine-grained details compared to existing methods. We
evaluate the framework across multiple editing tasks, observing 7.8% to 12.9%
improvements in reconstruction loss over RF-Inversion on the LSUN-Bedroom and
LSUN-Church datasets, respectively. For semantic face editing, our method
achieves an 11.2% improvement in identity preservation and a 1.6% enhancement
in perceptual quality, while maintaining computational efficiency comparable to
baseline approaches. Qualitatively, our method produces visually compelling
edits with superior semantic consistency and fine-grained detail preservation
across diverse editing scenarios. Code is available at:
https://github.com/marianlupascu/OT-Inversion

</details>


### [302] [TRUDI and TITUS: A Multi-Perspective Dataset and A Three-Stage Recognition System for Transportation Unit Identification](https://arxiv.org/abs/2508.02372)
*Emre Gülsoylu,André Kelm,Lennart Bengtson,Matthias Hirsch,Christian Wilms,Tim Rolff,Janick Edinger,Simone Frintrop*

Main category: cs.CV

TL;DR: The paper introduces the TRUDI dataset for identifying transportation units (TUs) in port logistics and presents TITUS, a pipeline for TU ID recognition, demonstrating robustness across diverse conditions.


<details>
  <summary>Details</summary>
Motivation: Progress in TU identification is hindered by the lack of diverse, real-world benchmark datasets. This work aims to fill that gap and support logistics efficiency.

Method: The TRUDI dataset includes 35,034 annotated instances. TITUS, a three-stage pipeline, segments TUs, detects ID text locations, and recognizes/validates IDs.

Result: TITUS reliably identifies TUs under varying conditions, outperforming systems requiring specific setups.

Conclusion: The TRUDI dataset and TITUS pipeline advance TU identification, supporting digital transformation in port logistics.

Abstract: Identifying transportation units (TUs) is essential for improving the
efficiency of port logistics. However, progress in this field has been hindered
by the lack of publicly available benchmark datasets that capture the diversity
and dynamics of real-world port environments. To address this gap, we present
the TRUDI dataset-a comprehensive collection comprising 35,034 annotated
instances across five categories: container, tank container, trailer, ID text,
and logo. The images were captured at operational ports using both ground-based
and aerial cameras, under a wide variety of lighting and weather conditions.
For the identification of TUs-which involves reading the 11-digit alphanumeric
ID typically painted on each unit-we introduce TITUS, a dedicated pipeline that
operates in three stages: (1) segmenting the TU instances, (2) detecting the
location of the ID text, and (3) recognising and validating the extracted ID.
Unlike alternative systems, which often require similar scenes, specific camera
angles or gate setups, our evaluation demonstrates that TITUS reliably
identifies TUs from a range of camera perspectives and in varying lighting and
weather conditions. By making the TRUDI dataset publicly available, we provide
a robust benchmark that enables the development and comparison of new
approaches. This contribution supports digital transformation efforts in
multipurpose ports and helps to increase the efficiency of entire logistics
chains.

</details>


### [303] [Uni-Layout: Integrating Human Feedback in Unified Layout Generation and Evaluation](https://arxiv.org/abs/2508.02374)
*Shuo Lu,Yanyin Chen,Wei Feng,Jiahao Fan,Fengheng Li,Zheng Zhang,Jingjing Lv,Junjie Shen,Ching Law,Jian Liang*

Main category: cs.CV

TL;DR: Uni-Layout is a unified framework for layout generation and evaluation, incorporating human feedback and dynamic alignment for superior performance.


<details>
  <summary>Details</summary>
Motivation: Current layout generation methods are task-specific and use misaligned metrics, limiting applicability and effectiveness.

Method: Uni-Layout integrates a unified generator, a human-mimicking evaluator (using Layout-HF100k dataset), and Dynamic-Margin Preference Optimization for alignment.

Result: Uni-Layout outperforms task-specific and general-purpose methods in experiments.

Conclusion: The framework offers a scalable, human-aligned solution for layout generation and evaluation.

Abstract: Layout generation plays a crucial role in enhancing both user experience and
design efficiency. However, current approaches suffer from task-specific
generation capabilities and perceptually misaligned evaluation metrics, leading
to limited applicability and ineffective measurement. In this paper, we propose
\textit{Uni-Layout}, a novel framework that achieves unified generation,
human-mimicking evaluation and alignment between the two. For universal
generation, we incorporate various layout tasks into a single taxonomy and
develop a unified generator that handles background or element contents
constrained tasks via natural language prompts. To introduce human feedback for
the effective evaluation of layouts, we build \textit{Layout-HF100k}, the first
large-scale human feedback dataset with 100,000 expertly annotated layouts.
Based on \textit{Layout-HF100k}, we introduce a human-mimicking evaluator that
integrates visual and geometric information, employing a Chain-of-Thought
mechanism to conduct qualitative assessments alongside a confidence estimation
module to yield quantitative measurements. For better alignment between the
generator and the evaluator, we integrate them into a cohesive system by
adopting Dynamic-Margin Preference Optimization (DMPO), which dynamically
adjusts margins based on preference strength to better align with human
judgments. Extensive experiments show that \textit{Uni-Layout} significantly
outperforms both task-specific and general-purpose methods. Our code is
publicly available at https://github.com/JD-GenX/Uni-Layout.

</details>


### [304] [SMART-Ship: A Comprehensive Synchronized Multi-modal Aligned Remote Sensing Targets Dataset and Benchmark for Berthed Ships Analysis](https://arxiv.org/abs/2508.02384)
*Chen-Chen Fan,Peiyao Guo,Linping Zhang,Kehan Qi,Haolin Huang,Yong-Qiang Mao,Yuxi Suo,Zhizhuo Jiang,Yu Liu,You He*

Main category: cs.CV

TL;DR: The paper introduces SMART-Ship, a multi-modal dataset for maritime surveillance, featuring spatiotemporal registered images from five modalities with fine-grained annotations for ships. It supports diverse tasks and benchmarks.


<details>
  <summary>Details</summary>
Motivation: Maritime surveillance is challenging due to multi-scale targets and dynamic environments. Existing datasets lack spatiotemporal consistency and multi-modal alignment.

Method: The SMART-Ship dataset includes 1092 multi-modal image sets (38,838 ships) with polygonal annotations, fine-grained categories, and change masks. Images are spatiotemporally registered and acquired within one week.

Result: The dataset supports five fundamental tasks and benchmarks. Experiments show its effectiveness for multi-modal remote sensing interpretation.

Conclusion: SMART-Ship bridges a critical gap in maritime surveillance, enabling diverse RS tasks and revealing future research directions.

Abstract: Given the limitations of satellite orbits and imaging conditions, multi-modal
remote sensing (RS) data is crucial in enabling long-term earth observation.
However, maritime surveillance remains challenging due to the complexity of
multi-scale targets and the dynamic environments. To bridge this critical gap,
we propose a Synchronized Multi-modal Aligned Remote sensing Targets dataset
for berthed ships analysis (SMART-Ship), containing spatiotemporal registered
images with fine-grained annotation for maritime targets from five modalities:
visible-light, synthetic aperture radar (SAR), panchromatic, multi-spectral,
and near-infrared. Specifically, our dataset consists of 1092 multi-modal image
sets, covering 38,838 ships. Each image set is acquired within one week and
registered to ensure spatiotemporal consistency. Ship instances in each set are
annotated with polygonal location information, fine-grained categories,
instance-level identifiers, and change region masks, organized hierarchically
to support diverse multi-modal RS tasks. Furthermore, we define standardized
benchmarks on five fundamental tasks and comprehensively compare representative
methods across the dataset. Thorough experiment evaluations validate that the
proposed SMART-Ship dataset could support various multi-modal RS interpretation
tasks and reveal the promising directions for further exploration.

</details>


### [305] [Enhancing Object Discovery for Unsupervised Instance Segmentation and Object Detection](https://arxiv.org/abs/2508.02386)
*Xingyu Feng,Hebei Gao,Hong Li*

Main category: cs.CV

TL;DR: COLER is a zero-shot unsupervised model for instance segmentation and object detection, using CutOnce for pseudo labels and self-training for improved performance.


<details>
  <summary>Details</summary>
Motivation: To advance unsupervised object localization by simplifying the process and eliminating reliance on clustering or post-processing.

Method: Uses CutOnce to generate coarse pseudo labels, then trains the detector with these masks. Includes novel modules for leveraging self-supervised models and avoiding post-processing.

Result: Outperforms state-of-the-art methods on multiple benchmarks without needing specialized loss functions.

Conclusion: COLER is a simple yet effective approach that advances unsupervised object localization.

Abstract: We propose Cut-Once-and-LEaRn (COLER), a simple approach for unsupervised
instance segmentation and object detection. COLER first uses our developed
CutOnce to generate coarse pseudo labels, then enables the detector to learn
from these masks. CutOnce applies Normalized Cut only once and does not rely on
any clustering methods, but it can generate multiple object masks in an image.
We have designed several novel yet simple modules that not only allow CutOnce
to fully leverage the object discovery capabilities of self-supervised models,
but also free it from reliance on mask post-processing. During training, COLER
achieves strong performance without requiring specially designed loss functions
for pseudo labels, and its performance is further improved through
self-training. COLER is a zero-shot unsupervised model that outperforms
previous state-of-the-art methods on multiple benchmarks.We believe our method
can help advance the field of unsupervised object localization.

</details>


### [306] [Hydra: Accurate Multi-Modal Leaf Wetness Sensing with mm-Wave and Camera Fusion](https://arxiv.org/abs/2508.02409)
*Yimeng Liu,Maolin Gan,Huaili Zeng,Li Liu,Younsuk Dong,Zhichao Cao*

Main category: cs.CV

TL;DR: Hydra integrates mm-Wave radar and camera tech to detect leaf wetness, achieving high accuracy (96%) in varied conditions, addressing gaps in current LWD measurement methods.


<details>
  <summary>Details</summary>
Motivation: Current LWD detection lacks standardization and robustness, with prior methods failing in real-world conditions. Hydra aims to improve precision and applicability in agriculture.

Method: Combines mm-Wave radar and camera data using a CNN for feature fusion and a transformer-based encoder for feature mapping, followed by a classifier. Dataset augmentation enhances generalization.

Result: Achieves up to 96% accuracy in lab tests and ~90% in real-world farm conditions (e.g., rain, low light).

Conclusion: Hydra offers a robust, accurate solution for LWD detection, bridging the gap between research and practical agricultural applications.

Abstract: Leaf Wetness Duration (LWD), the time that water remains on leaf surfaces, is
crucial in the development of plant diseases. Existing LWD detection lacks
standardized measurement techniques, and variations across different plant
characteristics limit its effectiveness. Prior research proposes diverse
approaches, but they fail to measure real natural leaves directly and lack
resilience in various environmental conditions. This reduces the precision and
robustness, revealing a notable practical application and effectiveness gap in
real-world agricultural settings. This paper presents Hydra, an innovative
approach that integrates millimeter-wave (mm-Wave) radar with camera technology
to detect leaf wetness by determining if there is water on the leaf. We can
measure the time to determine the LWD based on this detection. Firstly, we
design a Convolutional Neural Network (CNN) to selectively fuse multiple
mm-Wave depth images with an RGB image to generate multiple feature images.
Then, we develop a transformer-based encoder to capture the inherent connection
among the multiple feature images to generate a feature map, which is further
fed to a classifier for detection. Moreover, we augment the dataset during
training to generalize our model. Implemented using a frequency-modulated
continuous-wave (FMCW) radar within the 76 to 81 GHz band, Hydra's performance
is meticulously evaluated on plants, demonstrating the potential to classify
leaf wetness with up to 96% accuracy across varying scenarios. Deploying Hydra
in the farm, including rainy, dawn, or poorly light nights, it still achieves
an accuracy rate of around 90%.

</details>


### [307] [HGTS-Former: Hierarchical HyperGraph Transformer for Multivariate Time Series Analysis](https://arxiv.org/abs/2508.02411)
*Xiao Wang,Hao Si,Fan Zhang,Xiaoya Zhou,Dengdi Sun,Wanli Lyu,Qingquan Yang,Jin Tang*

Main category: cs.CV

TL;DR: The paper introduces HGTS-Former, a hypergraph-based transformer for multivariate time series analysis, addressing challenges like high dimensionality and complex interactions.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with high dimensionality and dynamic interactions in multivariate time series. Hypergraphs offer strong structural modeling potential.

Method: HGTS-Former normalizes and embeds time series patches, uses multi-head self-attention for temporal representation, constructs hierarchical hypergraphs for pattern aggregation, and employs an EdgeToNode module with a feed-forward network.

Result: Experiments on two tasks and eight datasets confirm HGTS-Former's effectiveness.

Conclusion: HGTS-Former successfully models complex multivariate time series interactions, with code available for reproducibility.

Abstract: Multivariate time series analysis has long been one of the key research
topics in the field of artificial intelligence. However, analyzing complex time
series data remains a challenging and unresolved problem due to its high
dimensionality, dynamic nature, and complex interactions among variables.
Inspired by the strong structural modeling capability of hypergraphs, this
paper proposes a novel hypergraph-based time series transformer backbone
network, termed HGTS-Former, to address the multivariate coupling in time
series data. Specifically, given the multivariate time series signal, we first
normalize and embed each patch into tokens. Then, we adopt the multi-head
self-attention to enhance the temporal representation of each patch. The
hierarchical hypergraphs are constructed to aggregate the temporal patterns
within each channel and fine-grained relations between different variables.
After that, we convert the hyperedge into node features through the EdgeToNode
module and adopt the feed-forward network to further enhance the output
features. Extensive experiments conducted on two multivariate time series tasks
and eight datasets fully validated the effectiveness of our proposed
HGTS-Former. The source code will be released on
https://github.com/Event-AHU/Time_Series_Analysis.

</details>


### [308] [Glioblastoma Overall Survival Prediction With Vision Transformers](https://arxiv.org/abs/2508.02439)
*Yin Lin,iccardo Barbieri,Domenico Aquino,Giuseppe Lauria,Marina Grisoli,Elena De Momi,Alberto Redaelli,Simona Ferrante*

Main category: cs.CV

TL;DR: An AI approach using Vision Transformers (ViTs) predicts glioblastoma survival from MRI images without tumor segmentation, achieving 62.5% accuracy on the BRATS dataset.


<details>
  <summary>Details</summary>
Motivation: Predicting Overall Survival (OS) for glioblastoma patients is crucial for personalized treatment, but traditional methods rely on tumor segmentation, which is resource-intensive.

Method: The study uses Vision Transformers (ViTs) to extract features directly from MRI images, bypassing segmentation, and evaluates performance on the BRATS dataset.

Result: The model achieved 62.5% accuracy, with balanced precision, recall, and F1 scores, outperforming other methods in these metrics.

Conclusion: ViTs show promise for efficient OS prediction in medical imaging, though dataset size limits generalization. The method is computationally efficient and segmentation-free.

Abstract: Glioblastoma is one of the most aggressive and common brain tumors, with a
median survival of 10-15 months. Predicting Overall Survival (OS) is critical
for personalizing treatment strategies and aligning clinical decisions with
patient outcomes. In this study, we propose a novel Artificial Intelligence
(AI) approach for OS prediction using Magnetic Resonance Imaging (MRI) images,
exploiting Vision Transformers (ViTs) to extract hidden features directly from
MRI images, eliminating the need of tumor segmentation. Unlike traditional
approaches, our method simplifies the workflow and reduces computational
resource requirements.
  The proposed model was evaluated on the BRATS dataset, reaching an accuracy
of 62.5% on the test set, comparable to the top-performing methods.
Additionally, it demonstrated balanced performance across precision, recall,
and F1 score, overcoming the best model in these metrics. The dataset size
limits the generalization of the ViT which typically requires larger datasets
compared to convolutional neural networks. This limitation in generalization is
observed across all the cited studies. This work highlights the applicability
of ViTs for downsampled medical imaging tasks and establishes a foundation for
OS prediction models that are computationally efficient and do not rely on
segmentation.

</details>


### [309] [InfoSyncNet: Information Synchronization Temporal Convolutional Network for Visual Speech Recognition](https://arxiv.org/abs/2508.02460)
*Junxiao Xue,Xiaozhen Liu,Xuecheng Wu,Fei Yu,Jun Wang*

Main category: cs.CV

TL;DR: InfoSyncNet improves visual speech recognition by dynamically adjusting focus and handling data inconsistencies, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Accurate lip-reading from silent videos is vital for AT and AR, but variability in sequences and uneven data distribution make it challenging.

Method: Introduces InfoSyncNet with a non-uniform quantization module and tailored data augmentation to handle visual speech inconsistencies.

Result: Achieves 92.0% and 60.7% Top-1 ACC on LRW and LRW1000 datasets, setting new benchmarks.

Conclusion: InfoSyncNet effectively addresses challenges in visual speech recognition, offering superior performance and practical utility.

Abstract: Estimating spoken content from silent videos is crucial for applications in
Assistive Technology (AT) and Augmented Reality (AR). However, accurately
mapping lip movement sequences in videos to words poses significant challenges
due to variability across sequences and the uneven distribution of information
within each sequence. To tackle this, we introduce InfoSyncNet, a non-uniform
sequence modeling network enhanced by tailored data augmentation techniques.
Central to InfoSyncNet is a non-uniform quantization module positioned between
the encoder and decoder, enabling dynamic adjustment to the network's focus and
effectively handling the natural inconsistencies in visual speech data.
Additionally, multiple training strategies are incorporated to enhance the
model's capability to handle variations in lighting and the speaker's
orientation. Comprehensive experiments on the LRW and LRW1000 datasets confirm
the superiority of InfoSyncNet, achieving new state-of-the-art accuracies of
92.0% and 60.7% Top-1 ACC. The code is available for download (see comments).

</details>


### [310] [SAMPO: Visual Preference Optimization for Intent-Aware Segmentation with Vision Foundation Models](https://arxiv.org/abs/2508.02464)
*Yonghuang Wu,Wenwen Zeng,Xuan Xie,Chengqian Zhao,Guoqing Wu,Jinhua Yu*

Main category: cs.CV

TL;DR: SAMPO bridges the intent gap in SAM by optimizing for high-level categorical intent from sparse prompts, achieving state-of-the-art performance in medical segmentation with superior data efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the limitation of SAM, which segments only explicitly prompted objects, failing to generalize to semantically related instances desired by users, especially in dense homogeneous domains like biomedical nuclei segmentation.

Method: Introduces SAMPO, a framework that uses preference optimization to teach visual foundation models to infer categorical intent from sparse visual interactions, without relying on language models.

Result: SAMPO outperforms existing methods, achieving a 9+ percentage point improvement on PanNuke-T2 with only 10% of training data.

Conclusion: SAMPO establishes a new paradigm for intent-aware alignment in visual foundation models, eliminating dependencies on auxiliary prompt generators or language models.

Abstract: Foundation models like Segment Anything Model (SAM) excel in promptable
segmentation but suffer from an intent gap: they segment only explicitly
prompted objects, failing to generalize to semantically related instances
implicitly desired by users. This limitation is critical in domains with dense
homogeneous objects (e.g., biomedical nuclei segmentation), where sparse visual
prompts typically yield incomplete results, rendering dense annotations
impractical due to prohibitive cost. To bridge this gap, we introduce SAMPO
(Segment Anything Model with Preference Optimization), a novel framework that
teaches visual foundation models to infer high-level categorical intent from
sparse visual interactions. Unlike conventional pixel-level fine-tuning, SAMPO
optimizes models to implicitly capture target-class characteristics through
preference optimization. This approach, which operates without dependency on
language models, enables robust multi-object segmentation even under sparse
prompting and demonstrates superior data efficiency during fine-tuning.
Validated on three medical segmentation tasks, SAMPO achieves state-of-the-art
performance: on challenging tasks like PanNuke-T2, our method, when fine-tuned
with only 10% of the training data, significantly outperforms all existing
methods trained on the full 100% dataset, achieving an improvement of over 9
percentage points compared to the best baseline. Our work establishes a new
paradigm for intent-aware alignment in visual foundation models, removing
dependencies on auxiliary prompt generators or language-model-assisted
preference learning.

</details>


### [311] [Multi-class Image Anomaly Detection for Practical Applications: Requirements and Robust Solutions](https://arxiv.org/abs/2508.02477)
*Jaehyuk Heo,Pilsung Kang*

Main category: cs.CV

TL;DR: The paper introduces Hierarchical Coreset (HierCore), a framework for multi-class image anomaly detection, addressing performance gaps and class information usage. It validates HierCore's robustness across scenarios with/without class labels.


<details>
  <summary>Details</summary>
Motivation: To improve multi-class anomaly detection by addressing underperformance of single models compared to class-specific ones and exploring the impact of class information on detection thresholds.

Method: Proposes HierCore, a hierarchical memory bank framework, to estimate class-wise decision criteria without needing class labels. Evaluates existing methods and HierCore under four label-availability scenarios.

Result: HierCore consistently meets all requirements and performs robustly across all settings, outperforming existing methods.

Conclusion: HierCore is a practical solution for real-world multi-class anomaly detection, adaptable to varying label availability.

Abstract: Recent advances in image anomaly detection have extended unsupervised
learning-based models from single-class settings to multi-class frameworks,
aiming to improve efficiency in training time and model storage. When a single
model is trained to handle multiple classes, it often underperforms compared to
class-specific models in terms of per-class detection accuracy. Accordingly,
previous studies have primarily focused on narrowing this performance gap.
However, the way class information is used, or not used, remains a relatively
understudied factor that could influence how detection thresholds are defined
in multi-class image anomaly detection. These thresholds, whether
class-specific or class-agnostic, significantly affect detection outcomes. In
this study, we identify and formalize the requirements that a multi-class image
anomaly detection model must satisfy under different conditions, depending on
whether class labels are available during training and evaluation. We then
re-examine existing methods under these criteria. To meet these challenges, we
propose Hierarchical Coreset (HierCore), a novel framework designed to satisfy
all defined requirements. HierCore operates effectively even without class
labels, leveraging a hierarchical memory bank to estimate class-wise decision
criteria for anomaly detection. We empirically validate the applicability and
robustness of existing methods and HierCore under four distinct scenarios,
determined by the presence or absence of class labels in the training and
evaluation phases. The experimental results demonstrate that HierCore
consistently meets all requirements and maintains strong, stable performance
across all settings, highlighting its practical potential for real-world
multi-class anomaly detection tasks.

</details>


### [312] [Fine-grained Multiple Supervisory Network for Multi-modal Manipulation Detecting and Grounding](https://arxiv.org/abs/2508.02479)
*Xinquan Yu,Wei Lu,Xiangyang Luo*

Main category: cs.CV

TL;DR: The paper introduces the FMS network for detecting and grounding multi-modal media manipulation, addressing limitations of existing methods by incorporating modality reliability, unimodal internal, and cross-modal supervision.


<details>
  <summary>Details</summary>
Motivation: Existing methods for detecting multi-modal media manipulation underperform due to unreliable unimodal data and lack of comprehensive forgery supervision.

Method: The FMS network includes three modules: MDSC for modality reliability supervision, UFMR for unimodal internal supervision, and MFAR for cross-modal supervision.

Result: Extensive experiments show FMS outperforms state-of-the-art methods.

Conclusion: FMS provides a robust solution for detecting and grounding multi-modal media manipulation by leveraging comprehensive supervision.

Abstract: The task of Detecting and Grounding Multi-Modal Media Manipulation (DGM$^4$)
is a branch of misinformation detection. Unlike traditional binary
classification, it includes complex subtasks such as forgery content
localization and forgery method classification. Consider that existing methods
are often limited in performance due to neglecting the erroneous interference
caused by unreliable unimodal data and failing to establish comprehensive
forgery supervision for mining fine-grained tampering traces. In this paper, we
present a Fine-grained Multiple Supervisory (FMS) network, which incorporates
modality reliability supervision, unimodal internal supervision and cross-modal
supervision to provide comprehensive guidance for DGM$^4$ detection. For
modality reliability supervision, we propose the Multimodal Decision Supervised
Correction (MDSC) module. It leverages unimodal weak supervision to correct the
multi-modal decision-making process. For unimodal internal supervision, we
propose the Unimodal Forgery Mining Reinforcement (UFMR) module. It amplifies
the disparity between real and fake information within unimodal modality from
both feature-level and sample-level perspectives. For cross-modal supervision,
we propose the Multimodal Forgery Alignment Reasoning (MFAR) module. It
utilizes soft-attention interactions to achieve cross-modal feature perception
from both consistency and inconsistency perspectives, where we also design the
interaction constraints to ensure the interaction quality. Extensive
experiments demonstrate the superior performance of our FMS compared to
state-of-the-art methods.

</details>


### [313] [MindShot: Multi-Shot Video Reconstruction from fMRI with LLM Decoding](https://arxiv.org/abs/2508.02480)
*Wenwen Zeng,Yonghuang Wu,Yifan Chen,Xuan Xie,Chengqian Zhao,Feiyu Yin,Guoqing Wu,Jinhua Yu*

Main category: cs.CV

TL;DR: A novel divide-and-decode framework for multi-shot fMRI video reconstruction, addressing signal mixing, temporal resolution mismatch, and dataset limitations.


<details>
  <summary>Details</summary>
Motivation: Understanding visual cognition and enabling brain-computer interfaces require reconstructing dynamic videos from fMRI, but current methods are limited to single-shot clips.

Method: Proposes a shot boundary predictor, generative keyframe captioning with LLMs, and large-scale data synthesis.

Result: Outperforms state-of-the-art methods in multi-shot reconstruction fidelity, with a 71.8% improvement in decoded caption CLIP similarity.

Conclusion: Establishes a new paradigm for multi-shot fMRI reconstruction by combining explicit decomposition and semantic prompting.

Abstract: Reconstructing dynamic videos from fMRI is important for understanding visual
cognition and enabling vivid brain-computer interfaces. However, current
methods are critically limited to single-shot clips, failing to address the
multi-shot nature of real-world experiences. Multi-shot reconstruction faces
fundamental challenges: fMRI signal mixing across shots, the temporal
resolution mismatch between fMRI and video obscuring rapid scene changes, and
the lack of dedicated multi-shot fMRI-video datasets. To overcome these
limitations, we propose a novel divide-and-decode framework for multi-shot fMRI
video reconstruction. Our core innovations are: (1) A shot boundary predictor
module explicitly decomposing mixed fMRI signals into shot-specific segments.
(2) Generative keyframe captioning using LLMs, which decodes robust textual
descriptions from each segment, overcoming temporal blur by leveraging
high-level semantics. (3) Novel large-scale data synthesis (20k samples) from
existing datasets. Experimental results demonstrate our framework outperforms
state-of-the-art methods in multi-shot reconstruction fidelity. Ablation
studies confirm the critical role of fMRI decomposition and semantic
captioning, with decomposition significantly improving decoded caption CLIP
similarity by 71.8%. This work establishes a new paradigm for multi-shot fMRI
reconstruction, enabling accurate recovery of complex visual narratives through
explicit decomposition and semantic prompting.

</details>


### [314] [Low-Frequency First: Eliminating Floating Artifacts in 3D Gaussian Splatting](https://arxiv.org/abs/2508.02493)
*Jianchao Wang,Peng Zhou,Cen Li,Rong Quan,Jie Qin*

Main category: cs.CV

TL;DR: EFA-GS reduces floating artifacts in 3D Gaussian Splatting by expanding under-optimized Gaussians and using depth/scale strategies, improving PSNR by 1.68 dB.


<details>
  <summary>Details</summary>
Motivation: Floating artifacts in 3DGS degrade visual fidelity, and their causes in low-quality initialization are not fully understood.

Method: Analyzes artifacts from a frequency-domain perspective, proposes EFA-GS to expand under-optimized Gaussians, and introduces depth/scale strategies for refinement.

Result: EFA-GS reduces artifacts while preserving details, achieving a 1.68 dB PSNR improvement on the RWLQ dataset.

Conclusion: EFA-GS effectively mitigates floating artifacts and enhances 3D reconstruction quality, with potential for downstream editing tasks.

Abstract: 3D Gaussian Splatting (3DGS) is a powerful and computationally efficient
representation for 3D reconstruction. Despite its strengths, 3DGS often
produces floating artifacts, which are erroneous structures detached from the
actual geometry and significantly degrade visual fidelity. The underlying
mechanisms causing these artifacts, particularly in low-quality initialization
scenarios, have not been fully explored. In this paper, we investigate the
origins of floating artifacts from a frequency-domain perspective and identify
under-optimized Gaussians as the primary source. Based on our analysis, we
propose \textit{Eliminating-Floating-Artifacts} Gaussian Splatting (EFA-GS),
which selectively expands under-optimized Gaussians to prioritize accurate
low-frequency learning. Additionally, we introduce complementary depth-based
and scale-based strategies to dynamically refine Gaussian expansion,
effectively mitigating detail erosion. Extensive experiments on both synthetic
and real-world datasets demonstrate that EFA-GS substantially reduces floating
artifacts while preserving high-frequency details, achieving an improvement of
1.68 dB in PSNR over baseline method on our RWLQ dataset. Furthermore, we
validate the effectiveness of our approach in downstream 3D editing tasks. Our
implementation will be released on GitHub.

</details>


### [315] [Rethinking Transparent Object Grasping: Depth Completion with Monocular Depth Estimation and Instance Mask](https://arxiv.org/abs/2508.02507)
*Yaofeng Cheng,Xinkai Gao,Sen Zhang,Chao Zeng,Fusheng Zha,Lining Sun,Chenguang Yang*

Main category: cs.CV

TL;DR: ReMake improves depth completion for transparent objects by using instance masks and monocular depth estimation, enhancing accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Depth cameras often fail with transparent objects, leading to incomplete data and unreliable robotic grasping. Existing methods struggle in real-world scenarios due to complex light interactions.

Method: ReMake uses an instance mask to distinguish transparent regions and monocular depth estimation for context, focusing learning on accurate depth estimation in transparent areas.

Result: The method outperforms existing approaches in benchmarks and real-world scenarios, showing better accuracy and generalization.

Conclusion: ReMake effectively addresses depth completion for transparent objects, improving robotic grasping reliability.

Abstract: Due to the optical properties, transparent objects often lead depth cameras
to generate incomplete or invalid depth data, which in turn reduces the
accuracy and reliability of robotic grasping. Existing approaches typically
input the RGB-D image directly into the network to output the complete depth,
expecting the model to implicitly infer the reliability of depth values.
However, while effective in training datasets, such methods often fail to
generalize to real-world scenarios, where complex light interactions lead to
highly variable distributions of valid and invalid depth data. To address this,
we propose ReMake, a novel depth completion framework guided by an instance
mask and monocular depth estimation. By explicitly distinguishing transparent
regions from non-transparent ones, the mask enables the model to concentrate on
learning accurate depth estimation in these areas from RGB-D input during
training. This targeted supervision reduces reliance on implicit reasoning and
improves generalization to real-world scenarios. Additionally, monocular depth
estimation provides depth context between the transparent object and its
surroundings, enhancing depth prediction accuracy. Extensive experiments show
that our method outperforms existing approaches on both benchmark datasets and
real-world scenarios, demonstrating superior accuracy and generalization
capability. Code and videos are available at
https://chengyaofeng.github.io/ReMake.github.io/.

</details>


### [316] [Engagement Prediction of Short Videos with Large Multimodal Models](https://arxiv.org/abs/2508.02516)
*Wei Sun,Linhan Cao,Yuqin Cao,Weixia Zhang,Wen Wen,Kaiwei Zhang,Zijian Chen,Fangfang Lu,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: The paper explores using large multimodal models (LMMs) for video engagement prediction, comparing VideoLLaMA2 (audio, visual, language) and Qwen2.5-VL (visual, language). VideoLLaMA2 outperforms Qwen2.5-VL, emphasizing audio's role. Ensembling both models achieved top performance in the ICCV VQualA 2025 challenge.


<details>
  <summary>Details</summary>
Motivation: Predicting video engagement is crucial for recommendation systems and content creation but is complex due to multimodal factors like semantics, visuals, and audio. Existing methods struggle with cross-modality interactions.

Method: Two LMMs—VideoLLaMA2 (audio, visual, language) and Qwen2.5-VL (visual, language)—are trained on the SnapUGC dataset. VideoLLaMA2 processes frames, metadata, and sound, while Qwen2.5-VL uses frames and metadata.

Result: Both models perform competitively, with VideoLLaMA2 outperforming Qwen2.5-VL, showing audio's importance. Ensembling them won the ICCV VQualA 2025 challenge.

Conclusion: LMMs are effective for engagement prediction, with audio features playing a key role. Ensembling multimodal models can enhance performance.

Abstract: The rapid proliferation of user-generated content (UGC) on short-form video
platforms has made video engagement prediction increasingly important for
optimizing recommendation systems and guiding content creation. However, this
task remains challenging due to the complex interplay of factors such as
semantic content, visual quality, audio characteristics, and user background.
Prior studies have leveraged various types of features from different
modalities, such as visual quality, semantic content, background sound, etc.,
but often struggle to effectively model their cross-feature and cross-modality
interactions. In this work, we empirically investigate the potential of large
multimodal models (LMMs) for video engagement prediction. We adopt two
representative LMMs: VideoLLaMA2, which integrates audio, visual, and language
modalities, and Qwen2.5-VL, which models only visual and language modalities.
Specifically, VideoLLaMA2 jointly processes key video frames, text-based
metadata, and background sound, while Qwen2.5-VL utilizes only key video frames
and text-based metadata. Trained on the SnapUGC dataset, both models
demonstrate competitive performance against state-of-the-art baselines,
showcasing the effectiveness of LMMs in engagement prediction. Notably,
VideoLLaMA2 consistently outperforms Qwen2.5-VL, highlighting the importance of
audio features in engagement prediction. By ensembling two types of models, our
method achieves first place in the ICCV VQualA 2025 EVQA-SnapUGC Challenge on
short-form video engagement prediction. The code is available at
https://github.com/sunwei925/LMM-EVQA.git.

</details>


### [317] [Understanding the Risks of Asphalt Art on the Reliability of Surveillance Perception Systems](https://arxiv.org/abs/2508.02530)
*Jin Ma,Abyad Enan,Long Cheng,Mashrur Chowdhury*

Main category: cs.CV

TL;DR: The study examines how artistic crosswalks (asphalt art) affect pedestrian detection in vision-based surveillance systems, finding that complex designs degrade performance and adversarial art can manipulate detections.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of artistic crosswalks on pedestrian detection in surveillance systems, especially under benign and adversarial conditions.

Method: Constructed realistic crosswalk scenarios by compositing street art into surveillance scenes, evaluating a pretrained model's pedestrian detection performance.

Result: Complex artistic patterns degrade detection, and adversarial art can obscure pedestrians or create false detections.

Conclusion: Artistic crosswalks pose a vulnerability for surveillance systems, emphasizing the need for robust models that account for visual variations.

Abstract: Artistic crosswalks featuring asphalt art, introduced by different
organizations in recent years, aim to enhance the visibility and safety of
pedestrians. However, their visual complexity may interfere with surveillance
systems that rely on vision-based object detection models. In this study, we
investigate the impact of asphalt art on pedestrian detection performance of a
pretrained vision-based object detection model. We construct realistic
crosswalk scenarios by compositing various street art patterns into a fixed
surveillance scene and evaluate the model's performance in detecting
pedestrians on asphalt-arted crosswalks under both benign and adversarial
conditions. A benign case refers to pedestrian crosswalks painted with existing
normal asphalt art, whereas an adversarial case involves digitally crafted or
altered asphalt art perpetrated by an attacker. Our results show that while
simple, color-based designs have minimal effect, complex artistic patterns,
particularly those with high visual salience, can significantly degrade
pedestrian detection performance. Furthermore, we demonstrate that
adversarially crafted asphalt art can be exploited to deliberately obscure real
pedestrians or generate non-existent pedestrian detections. These findings
highlight a potential vulnerability in urban vision-based pedestrian
surveillance systems and underscore the importance of accounting for
environmental visual variations when designing robust pedestrian perception
models.

</details>


### [318] [Precision-Aware Video Compression for Reducing Bandwidth Requirements in Video Communication for Vehicle Detection-Based Applications](https://arxiv.org/abs/2508.02533)
*Abyad Enan,Jon C Calhoun,Mashrur Chowdhury*

Main category: cs.CV

TL;DR: PAVC dynamically adjusts video compression based on weather/lighting to balance bandwidth usage and vehicle detection accuracy in ITS.


<details>
  <summary>Details</summary>
Motivation: Limited bandwidth in ITS can hinder real-time performance, and lossy compression degrades video quality, impacting vehicle detection accuracy. Environmental factors further complicate this.

Method: Uses Precision-Aware Video Compression (PAVC) to dynamically adjust compression levels based on weather and lighting conditions, optimizing bandwidth and detection accuracy.

Result: PAVC improves vehicle detection accuracy by up to 13% and reduces bandwidth usage by up to 8.23x (moderate bandwidth) or 72x (limited bandwidth).

Conclusion: PAVC effectively balances bandwidth efficiency and detection accuracy in ITS, adapting to environmental conditions for optimal performance.

Abstract: Computer vision has become a popular tool in intelligent transportation
systems (ITS), enabling various applications through roadside traffic cameras
that capture video and transmit it in real time to computing devices within the
same network. The efficiency of this video transmission largely depends on the
available bandwidth of the communication system. However, limited bandwidth can
lead to communication bottlenecks, hindering the real-time performance of ITS
applications. To mitigate this issue, lossy video compression techniques can be
used to reduce bandwidth requirements, at the cost of degrading video quality.
This degradation can negatively impact the accuracy of applications that rely
on real-time vehicle detection. Additionally, vehicle detection accuracy is
influenced by environmental factors such as weather and lighting conditions,
suggesting that compression levels should be dynamically adjusted in response
to these variations. In this work, we utilize a framework called
Precision-Aware Video Compression (PAVC), where a roadside video camera
captures footage of vehicles on roadways, compresses videos, and then transmits
them to a processing unit, running a vehicle detection algorithm for
safety-critical applications, such as real-time collision risk assessment. The
system dynamically adjusts the video compression level based on current weather
and lighting conditions to maintain vehicle detection accuracy while minimizing
bandwidth usage. Our results demonstrate that PAVC improves vehicle detection
accuracy by up to 13% and reduces communication bandwidth requirements by up to
8.23x in areas with moderate bandwidth availability. Moreover, in locations
with severely limited bandwidth, PAVC reduces bandwidth requirements by up to
72x while preserving vehicle detection performance.

</details>


### [319] [MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming](https://arxiv.org/abs/2508.02549)
*Shuo Wang,Yongcai Wang,Wanting Li,Yucheng Wang,Maiyue Chen,Kaihui Wang,Zhizhong Su,Xudong Cai,Yeying Jin,Deying Li,Zhaoxin Fan*

Main category: cs.CV

TL;DR: MonoDream is a lightweight Vision-Language Action (VLA) framework that enhances monocular navigation by learning a Unified Navigation Representation (UNR), narrowing the performance gap with panoramic-based agents.


<details>
  <summary>Details</summary>
Motivation: To address the cost and accessibility limitations of panoramic RGB-D sensors in Vision-Language Navigation (VLN) tasks, while improving monocular input performance.

Method: Introduces MonoDream, which learns a UNR aligning visual semantics and language-grounded action intent. Uses Latent Panoramic Dreaming (LPD) tasks to predict latent features of panoramic RGB-D from monocular input.

Result: Improves monocular navigation performance on VLN benchmarks, significantly reducing the gap with panoramic-based agents.

Conclusion: MonoDream demonstrates the feasibility of effective monocular navigation by leveraging shared feature representations and latent panoramic predictions.

Abstract: Vision-Language Navigation (VLN) tasks often leverage panoramic RGB and depth
inputs to provide rich spatial cues for action planning, but these sensors can
be costly or less accessible in real-world deployments. Recent approaches based
on Vision-Language Action (VLA) models achieve strong results with monocular
input, yet they still lag behind methods using panoramic RGB-D information. We
present MonoDream, a lightweight VLA framework that enables monocular agents to
learn a Unified Navigation Representation (UNR). This shared feature
representation jointly aligns navigation-relevant visual semantics (e.g.,
global layout, depth, and future cues) and language-grounded action intent,
enabling more reliable action prediction. MonoDream further introduces Latent
Panoramic Dreaming (LPD) tasks to supervise the UNR, which train the model to
predict latent features of panoramic RGB and depth observations at both current
and future steps based on only monocular input. Experiments on multiple VLN
benchmarks show that MonoDream consistently improves monocular navigation
performance and significantly narrows the gap with panoramic-based agents.

</details>


### [320] [ReMoMask: Retrieval-Augmented Masked Motion Generation](https://arxiv.org/abs/2508.02605)
*Zhengdao Li,Siheng Wang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: ReMoMask is a unified framework for Text-to-Motion generation, addressing limitations of current methods with innovations like Bidirectional Momentum, Semantic Spatio-temporal Attention, and RAG-Classier-Free Guidance, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current Text-to-Motion (T2M) methods face issues like limited diversity, error accumulation, and physical implausibility in generative models, and diffusion inertia, partial-mode collapse, and asynchronous artifacts in Retrieval-Augmented Generation (RAG) methods.

Method: ReMoMask integrates three innovations: 1) Bidirectional Momentum Text-Motion Model for improved retrieval precision, 2) Semantic Spatio-temporal Attention for biomechanical constraints, and 3) RAG-Classier-Free Guidance for enhanced generalization.

Result: ReMoMask outperforms previous methods, improving FID scores by 3.88% on HumanML3D and 10.97% on KIT-ML.

Conclusion: ReMoMask sets a new benchmark for T2M generation, offering efficient, coherent motion synthesis with superior performance.

Abstract: Text-to-Motion (T2M) generation aims to synthesize realistic and semantically
aligned human motion sequences from natural language descriptions. However,
current approaches face dual challenges: Generative models (e.g., diffusion
models) suffer from limited diversity, error accumulation, and physical
implausibility, while Retrieval-Augmented Generation (RAG) methods exhibit
diffusion inertia, partial-mode collapse, and asynchronous artifacts. To
address these limitations, we propose ReMoMask, a unified framework integrating
three key innovations: 1) A Bidirectional Momentum Text-Motion Model decouples
negative sample scale from batch size via momentum queues, substantially
improving cross-modal retrieval precision; 2) A Semantic Spatio-temporal
Attention mechanism enforces biomechanical constraints during part-level fusion
to eliminate asynchronous artifacts; 3) RAG-Classier-Free Guidance incorporates
minor unconditional generation to enhance generalization. Built upon MoMask's
RVQ-VAE, ReMoMask efficiently generates temporally coherent motions in minimal
steps. Extensive experiments on standard benchmarks demonstrate the
state-of-the-art performance of ReMoMask, achieving a 3.88% and 10.97%
improvement in FID scores on HumanML3D and KIT-ML, respectively, compared to
the previous SOTA method RAG-T2M. Code:
https://github.com/AIGeeksGroup/ReMoMask. Website:
https://aigeeksgroup.github.io/ReMoMask.

</details>


### [321] [Evaluating Variance in Visual Question Answering Benchmarks](https://arxiv.org/abs/2508.02645)
*Nikitha SR*

Main category: cs.CV

TL;DR: The paper examines performance variance in MLLMs for VQA, analyzing factors like training seed and hyperparameters, and proposes Cloze-style evaluation for reliability.


<details>
  <summary>Details</summary>
Motivation: Current MLLM evaluations overlook performance variance caused by stochastic outputs, training seeds, and hyperparameters, limiting reliability.

Method: Analyzes variance across 14 VQA benchmarks, studying training seed, framework non-determinism, model scale, and instruction finetuning. Explores Cloze-style evaluation.

Result: Highlights significant performance variability and the limitations of current evaluation practices.

Conclusion: Advocates for variance-aware methodologies to improve MLLM robustness and reliability.

Abstract: Multimodal large language models (MLLMs) have emerged as powerful tools for
visual question answering (VQA), enabling reasoning and contextual
understanding across visual and textual modalities. Despite their advancements,
the evaluation of MLLMs on VQA benchmarks often relies on point estimates,
overlooking the significant variance in performance caused by factors such as
stochastic model outputs, training seed sensitivity, and hyperparameter
configurations. This paper critically examines these issues by analyzing
variance across 14 widely used VQA benchmarks, covering diverse tasks such as
visual reasoning, text understanding, and commonsense reasoning. We
systematically study the impact of training seed, framework non-determinism,
model scale, and extended instruction finetuning on performance variability.
Additionally, we explore Cloze-style evaluation as an alternate assessment
strategy, studying its effectiveness in reducing stochasticity and improving
reliability across benchmarks. Our findings highlight the limitations of
current evaluation practices and advocate for variance-aware methodologies to
foster more robust and reliable development of MLLMs.

</details>


### [322] [PMGS: Reconstruction of Projectile Motion across Large Spatiotemporal Spans via 3D Gaussian Splatting](https://arxiv.org/abs/2508.02660)
*Yijun Xu,Jingrui Zhang,Yuhan Chen,Dingwen Wang,Lei Yu,Chu He*

Main category: cs.CV

TL;DR: PMGS reconstructs projectile motion using 3D Gaussian splatting, combining dynamic scene decomposition, SE(3) pose learning, and physics-based constraints for accurate, high-speed rigid motion recovery.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with long-term, large-scale rigid motion reconstruction and lack physical consistency. PMGS addresses these gaps.

Method: Two-stage workflow: 1) Target Modeling (dynamic scene decomposition, point density control); 2) Motion Recovery (SE(3) pose learning, acceleration consistency, dynamic annealing, Kalman fusion).

Result: PMGS outperforms mainstream dynamic methods in reconstructing high-speed nonlinear rigid motion.

Conclusion: PMGS effectively bridges physics and pose estimation, offering robust reconstruction of complex rigid motion.

Abstract: Modeling complex rigid motion across large spatiotemporal spans remains an
unresolved challenge in dynamic reconstruction. Existing paradigms are mainly
confined to short-term, small-scale deformation and offer limited consideration
for physical consistency. This study proposes PMGS, focusing on reconstructing
Projectile Motion via 3D Gaussian Splatting. The workflow comprises two stages:
1) Target Modeling: achieving object-centralized reconstruction through dynamic
scene decomposition and an improved point density control; 2) Motion Recovery:
restoring full motion sequences by learning per-frame SE(3) poses. We introduce
an acceleration consistency constraint to bridge Newtonian mechanics and pose
estimation, and design a dynamic simulated annealing strategy that adaptively
schedules learning rates based on motion states. Futhermore, we devise a Kalman
fusion scheme to optimize error accumulation from multi-source observations to
mitigate disturbances. Experiments show PMGS's superior performance in
reconstructing high-speed nonlinear rigid motion compared to mainstream dynamic
methods.

</details>


### [323] [MedVLThinker: Simple Baselines for Multimodal Medical Reasoning](https://arxiv.org/abs/2508.02669)
*Xiaoke Huang,Juncheng Wu,Hui Liu,Xianfeng Tang,Yuyin Zhou*

Main category: cs.CV

TL;DR: MedVLThinker introduces open, reproducible baselines for building reasoning-centric medical LMMs, outperforming SFT with RLVR and achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The lack of open recipes for medical LMMs hinders research and comparison.

Method: Systematic data curation and two training paradigms: SFT and RLVR.

Result: RLVR outperforms SFT; text-only data boosts performance more than multimodal data.

Conclusion: MedVLThinker provides a strong open foundation for future medical reasoning research.

Abstract: Large Reasoning Models (LRMs) have introduced a new paradigm in AI by
enabling models to ``think before responding" via chain-of-thought reasoning.
However, the absence of open and reproducible recipes for building
reasoning-centric medical LMMs hinders community-wide research, analysis, and
comparison. In this paper, we present MedVLThinker, a suite of simple yet
strong baselines. Our fully open recipe consists of: (1) systematic data
curation for both text-only and image-text medical data, filtered according to
varying levels of reasoning difficulty, and (2) two training paradigms:
Supervised Fine-Tuning (SFT) on distilled reasoning traces and Reinforcement
Learning with Verifiable Rewards (RLVR) based on final answer correctness.
Across extensive experiments on the Qwen2.5-VL model family (3B, 7B) and six
medical QA benchmarks, we find that RLVR consistently and significantly
outperforms SFT. Additionally, under the RLVR framework, a key,
counter-intuitive finding is that training on our curated text-only reasoning
data provides a more substantial performance boost than training on multimodal
image-text data. Our best open 7B model, trained using the RLVR recipe on
text-only data, establishes a new state-of-the-art on existing public VQA
benchmarks, surpassing all previous open-source medical LMMs. Furthermore,
scaling our model to 32B achieves performance on par with the proprietary
GPT-4o. We release all curated data, models, and code to provide the community
with a strong, open foundation for future research in multimodal medical
reasoning.

</details>


### [324] [Raw Data Matters: Enhancing Prompt Tuning by Internal Augmentation on Vision-Language Models](https://arxiv.org/abs/2508.02671)
*Haoyang Li,Liang Wang,Chao Wang,Siyu Zhou,Jing Jiang,Yan Peng,Guodong Long*

Main category: cs.CV

TL;DR: AugPT is a self-contained prompt tuning method using internal augmentation to enhance model performance without external knowledge.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on costly external knowledge and ignore image features. AugPT aims to exploit known features efficiently.

Method: Uses self-supervised augmentation on unlabeled images and a gating mechanism to filter noisy samples.

Result: Improves performance and generalization without external knowledge.

Conclusion: AugPT is effective and resource-efficient for CLIP-based prompt tuning.

Abstract: For CLIP-based prompt tuning, introducing more data as additional knowledge
for enhancing fine-tuning process is proved to be an effective approach.
Existing data amplification strategies for prompt tuning typically rely on
external knowledge (e.g., large language models or pre-structured knowledge
bases), resulting in higher costs for data collection and processing, while
generally ignoring further utilization of features in image modality. To
address this, we propose Augmentation-driven Prompt Tuning (AugPT), a
self-contained distillation-based prompt tuning approach using only internal
augmentation on raw dataset to better exploit known features. Specifically,
AugPT employs self-supervised augmentation on unlabeled images in the training
set, and introduces a novel gating mechanism based on consensus test, reusing
the pre-trained prompt tuning backbone model to spontaneously filter noisy
samples, further enhancing the quality of augmented views. Extensive
experiments validate that AugPT simultaneously enhances model performance and
generalization capability without using appended external knowledge. The code
of AugPT is available at: https://github.com/JREion/AugPT .

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [325] [HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and Decision in Embodied Agents](https://arxiv.org/abs/2508.02629)
*Yibin Liu,Zhixuan Liang,Zanxin Chen,Tianxing Chen,Mengkang Hu,Wanxi Dong,Congsheng Xu,Zhaoming Han,Yusen Qin,Yao Mu*

Main category: cs.RO

TL;DR: HyCodePolicy is a hybrid control framework for embodied agents, combining code synthesis, geometric grounding, perceptual monitoring, and iterative repair to improve policy robustness and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs lack adaptive monitoring and repair mechanisms for code policy execution in embodied agents.

Method: HyCodePolicy decomposes instructions into subgoals, generates executable programs, monitors execution with a VLM, and repairs failures using dual feedback.

Result: The framework enhances robot manipulation policies' robustness and sample efficiency.

Conclusion: HyCodePolicy offers a scalable solution for integrating multimodal reasoning into autonomous decision-making.

Abstract: Recent advances in multimodal large language models (MLLMs) have enabled
richer perceptual grounding for code policy generation in embodied agents.
However, most existing systems lack effective mechanisms to adaptively monitor
policy execution and repair codes during task completion. In this work, we
introduce HyCodePolicy, a hybrid language-based control framework that
systematically integrates code synthesis, geometric grounding, perceptual
monitoring, and iterative repair into a closed-loop programming cycle for
embodied agents. Technically, given a natural language instruction, our system
first decomposes it into subgoals and generates an initial executable program
grounded in object-centric geometric primitives. The program is then executed
in simulation, while a vision-language model (VLM) observes selected
checkpoints to detect and localize execution failures and infer failure
reasons. By fusing structured execution traces capturing program-level events
with VLM-based perceptual feedback, HyCodePolicy infers failure causes and
repairs programs. This hybrid dual feedback mechanism enables self-correcting
program synthesis with minimal human supervision. Our results demonstrate that
HyCodePolicy significantly improves the robustness and sample efficiency of
robot manipulation policies, offering a scalable strategy for integrating
multimodal reasoning into autonomous decision-making pipelines.

</details>


### [326] [Sparse 3D Perception for Rose Harvesting Robots: A Two-Stage Approach Bridging Simulation and Real-World Applications](https://arxiv.org/abs/2508.00900)
*Taha Samavati,Mohsen Soryani,Sina Mansouri*

Main category: cs.RO

TL;DR: A 3D perception pipeline for flower-harvesting robots is proposed, using synthetic data to train a lightweight deep neural network for rose center localization, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The labor-intensive harvesting of medicinal plants like Damask roses limits scalability, prompting the need for automated solutions.

Method: A two-stage algorithm combines 2D point-based detection on stereo images with depth estimation via a lightweight deep neural network, trained on a photorealistic synthetic dataset.

Result: The method achieves an F1 score of 95.6% (synthetic) and 74.4% (real) in 2D detection, with 3% depth estimation error at 2 meters.

Conclusion: The pipeline bridges the synthetic-real data gap, offering a scalable solution for precision harvesting in agricultural automation.

Abstract: The global demand for medicinal plants, such as Damask roses, has surged with
population growth, yet labor-intensive harvesting remains a bottleneck for
scalability. To address this, we propose a novel 3D perception pipeline
tailored for flower-harvesting robots, focusing on sparse 3D localization of
rose centers. Our two-stage algorithm first performs 2D point-based detection
on stereo images, followed by depth estimation using a lightweight deep neural
network. To overcome the challenge of scarce real-world labeled data, we
introduce a photorealistic synthetic dataset generated via Blender, simulating
a dynamic rose farm environment with precise 3D annotations. This approach
minimizes manual labeling costs while enabling robust model training. We
evaluate two depth estimation paradigms: a traditional triangulation-based
method and our proposed deep learning framework. Results demonstrate the
superiority of our method, achieving an F1 score of 95.6% (synthetic) and 74.4%
(real) in 2D detection, with a depth estimation error of 3% at a 2-meter range
on synthetic data. The pipeline is optimized for computational efficiency,
ensuring compatibility with resource-constrained robotic systems. By bridging
the domain gap between synthetic and real-world data, this work advances
agricultural automation for specialty crops, offering a scalable solution for
precision harvesting.

</details>


### [327] [A Survey on Deep Multi-Task Learning in Connected Autonomous Vehicles](https://arxiv.org/abs/2508.00917)
*Jiayuan Wang,Farhad Pourpanah,Q. M. Jonathan Wu,Ning Zhang*

Main category: cs.RO

TL;DR: A survey on multi-task learning (MTL) for connected autonomous vehicles (CAVs), covering its applications in perception, prediction, planning, control, and multi-agent collaboration, and highlighting research gaps.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for CAVs use separate models for tasks like object detection and trajectory prediction, leading to inefficiencies. MTL offers a unified solution for improved efficiency and resource utilization.

Method: The paper reviews MTL applications in CAVs, focusing on key functional modules like perception and prediction, and analyzes existing methods.

Result: MTL shows promise in enhancing CAV performance by reducing computational overhead and improving real-time capabilities.

Conclusion: The survey identifies research gaps and suggests future directions to advance MTL methodologies for CAV systems.

Abstract: Connected autonomous vehicles (CAVs) must simultaneously perform multiple
tasks, such as object detection, semantic segmentation, depth estimation,
trajectory prediction, motion prediction, and behaviour prediction, to ensure
safe and reliable navigation in complex environments. Vehicle-to-everything
(V2X) communication enables cooperative driving among CAVs, thereby mitigating
the limitations of individual sensors, reducing occlusions, and improving
perception over long distances. Traditionally, these tasks are addressed using
distinct models, which leads to high deployment costs, increased computational
overhead, and challenges in achieving real-time performance. Multi-task
learning (MTL) has recently emerged as a promising solution that enables the
joint learning of multiple tasks within a single unified model. This offers
improved efficiency and resource utilization. To the best of our knowledge,
this survey is the first comprehensive review focused on MTL in the context of
CAVs. We begin with an overview of CAVs and MTL to provide foundational
background. We then explore the application of MTL across key functional
modules, including perception, prediction, planning, control, and multi-agent
collaboration. Finally, we discuss the strengths and limitations of existing
methods, identify key research gaps, and provide directions for future research
aimed at advancing MTL methodologies for CAV systems.

</details>


### [328] [Hestia: Hierarchical Next-Best-View Exploration for Systematic Intelligent Autonomous Data Collection](https://arxiv.org/abs/2508.01014)
*Cheng-You Lu,Zhuoli Zhuang,Nguyen Thanh Trung Le,Da Xiao,Yu-Cheng Chang,Thomas Do,Srinath Sridhar,Chin-teng Lin*

Main category: cs.RO

TL;DR: Hestia introduces a reinforcement learning-based method for autonomous 5-DoF next-best-viewpoint prediction, improving efficiency in 3D data collection.


<details>
  <summary>Details</summary>
Motivation: Manual data collection for 3D reconstruction is time-consuming and labor-intensive, necessitating an automated solution.

Method: Hestia uses reinforcement learning to predict next-best viewpoints, incorporating dataset choice, observation design, action space, reward calculation, and learning schemes.

Result: Hestia performs robustly across datasets and real-world drone deployments, proving feasible for autonomous scene exploration.

Conclusion: Hestia offers a systematic and generalizable solution for intelligent autonomous data collection, outperforming prior methods.

Abstract: Advances in 3D reconstruction and novel view synthesis have enabled
efficient, photorealistic rendering, but the data collection process remains
largely manual, making it time-consuming and labor-intensive. To address the
challenges, this study introduces Hierarchical Next-Best-View Exploration for
Systematic Intelligent Autonomous Data Collection (Hestia), which leverages
reinforcement learning to learn a generalizable policy for 5-DoF next-best
viewpoint prediction. Unlike prior approaches, Hestia systematically defines
the next-best-view task by proposing core components such as dataset choice,
observation design, action space, reward calculation, and learning schemes,
forming a foundation for the planner. Hestia goes beyond prior next-best-view
approaches and traditional capture systems through integration and validation
in a real-world setup, where a drone serves as a mobile sensor for active scene
exploration. Experimental results show that Hestia performs robustly across
three datasets and translated object settings in the NVIDIA IsaacLab
environment, and proves feasible for real-world deployment.

</details>


### [329] [From Photons to Physics: Autonomous Indoor Drones and the Future of Objective Property Assessment](https://arxiv.org/abs/2508.01965)
*Petteri Teikari,Mike Jarrell,Irene Bandera Moreno,Harri Pesola*

Main category: cs.RO

TL;DR: The paper reviews how autonomous indoor drones with physics-aware sensing can revolutionize property assessment by enabling objective, quantitative measurements. It covers platform architectures, advanced sensing, intelligent autonomy, and workflow integration.


<details>
  <summary>Details</summary>
Motivation: To shift property assessment from subjective visual inspection to objective, quantitative measurement using autonomous drones and advanced sensing technologies.

Method: The review examines four domains: platform architectures for indoor navigation, advanced sensing modalities, intelligent autonomy algorithms, and integration with property workflows.

Result: Identifies key innovations like heterogeneous computing, hyperspectral imaging, and 3D Gaussian Splatting that enable drones to perform detailed property assessments.

Conclusion: Autonomous indoor drones with physics-aware sensing can transform property assessment, offering precise, efficient, and standardized evaluations.

Abstract: The convergence of autonomous indoor drones with physics-aware sensing
technologies promises to transform property assessment from subjective visual
inspection to objective, quantitative measurement. This comprehensive review
examines the technical foundations enabling this paradigm shift across four
critical domains: (1) platform architectures optimized for indoor navigation,
where weight constraints drive innovations in heterogeneous computing,
collision-tolerant design, and hierarchical control systems; (2) advanced
sensing modalities that extend perception beyond human vision, including
hyperspectral imaging for material identification, polarimetric sensing for
surface characterization, and computational imaging with metaphotonics enabling
radical miniaturization; (3) intelligent autonomy through active reconstruction
algorithms, where drones equipped with 3D Gaussian Splatting make strategic
decisions about viewpoint selection to maximize information gain within battery
constraints; and (4) integration pathways with existing property workflows,
including Building Information Modeling (BIM) systems and industry standards
like Uniform Appraisal Dataset (UAD) 3.6.

</details>


### [330] [ScrewSplat: An End-to-End Method for Articulated Object Recognition](https://arxiv.org/abs/2508.02146)
*Seungyeon Kim,Junsu Ha,Young Hun Kim,Yonghyeon Lee,Frank C. Park*

Main category: cs.RO

TL;DR: ScrewSplat is an end-to-end RGB-based method for articulated object recognition, optimizing screw axes to recover kinematic structure and integrating Gaussian Splatting for 3D geometry and part segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing methods for articulated object recognition rely on strong assumptions, additional inputs, or complex steps, limiting practicality.

Method: Randomly initializes screw axes, iteratively optimizes them, and integrates Gaussian Splatting for 3D geometry and part segmentation.

Result: Achieves state-of-the-art recognition accuracy and enables zero-shot, text-guided manipulation.

Conclusion: ScrewSplat offers a simple, effective solution for articulated object recognition using only RGB inputs.

Abstract: Articulated object recognition -- the task of identifying both the geometry
and kinematic joints of objects with movable parts -- is essential for enabling
robots to interact with everyday objects such as doors and laptops. However,
existing approaches often rely on strong assumptions, such as a known number of
articulated parts; require additional inputs, such as depth images; or involve
complex intermediate steps that can introduce potential errors -- limiting
their practicality in real-world settings. In this paper, we introduce
ScrewSplat, a simple end-to-end method that operates solely on RGB
observations. Our approach begins by randomly initializing screw axes, which
are then iteratively optimized to recover the object's underlying kinematic
structure. By integrating with Gaussian Splatting, we simultaneously
reconstruct the 3D geometry and segment the object into rigid, movable parts.
We demonstrate that our method achieves state-of-the-art recognition accuracy
across a diverse set of articulated objects, and further enables zero-shot,
text-guided manipulation using the recovered kinematic model.

</details>


### [331] [A Moment Matching-Based Method for Sparse and Noisy Point Cloud Registration](https://arxiv.org/abs/2508.02187)
*Xingyi Li,Han Zhang,Ziliang Wang,Yukai Yang,Weidong Chen*

Main category: cs.RO

TL;DR: A moment matching-based framework for robust point cloud registration in sparse and noisy conditions, outperforming traditional methods like ICP and NDT.


<details>
  <summary>Details</summary>
Motivation: Challenges in point cloud registration under sparse and noisy conditions, where traditional methods like ICP and NDT struggle.

Method: Treats point clouds as i.i.d. samples, matches generalized Gaussian Radial Basis moments to estimate rigid transformation without point-to-point correspondences.

Result: Higher accuracy and robustness than existing methods, validated on synthetic and real-world datasets; improves 4D Radar SLAM performance.

Conclusion: Moment matching shows promise for robust registration in sparse, noisy scenarios, achieving LiDAR-like performance.

Abstract: Point cloud registration is a key step in robotic perception tasks, such as
Simultaneous Localization and Mapping (SLAM). It is especially challenging in
conditions with sparse points and heavy noise. Traditional registration
methods, such as Iterative Closest Point (ICP) and Normal Distributions
Transform (NDT), often have difficulties in achieving a robust and accurate
alignment under these conditions. In this paper, we propose a registration
framework based on moment matching. In particular, the point clouds are
regarded as i.i.d. samples drawn from the same distribution observed in the
source and target frames. We then match the generalized Gaussian Radial Basis
moments calculated from the point clouds to estimate the rigid transformation
between two frames. Moreover, such method does not require explicit
point-to-point correspondences among the point clouds. We further show the
consistency of the proposed method. Experiments on synthetic and real-world
datasets show that our approach achieves higher accuracy and robustness than
existing methods. In addition, we integrate our framework into a 4D Radar SLAM
system. The proposed method significantly improves the localization performance
and achieves results comparable to LiDAR-based systems. These findings
demonstrate the potential of moment matching technique for robust point cloud
registration in sparse and noisy scenarios.

</details>


### [332] [Improving Generalization of Language-Conditioned Robot Manipulation](https://arxiv.org/abs/2508.02405)
*Chenglin Cui,Chaoran Zhu,Changjae Oh,Andrea Cavallaro*

Main category: cs.RO

TL;DR: A framework for learning object-arrangement tasks from few demonstrations, using a two-stage approach for target localization and region determination, validated in simulation and real-world robots.


<details>
  <summary>Details</summary>
Motivation: Existing methods require large data for fine-tuning VLMs in unseen environments; this work aims to reduce data dependency.

Method: Two-stage framework: target localization for picking and region determination for placing, with an instance-level semantic fusion module aligning image crops and text embeddings.

Result: Improved generalization and zero-shot ability in real-robot manipulation, validated in simulations and real-world scenarios.

Conclusion: The framework efficiently learns from few demonstrations, enhancing robotic manipulation with natural language instructions.

Abstract: The control of robots for manipulation tasks generally relies on visual
input. Recent advances in vision-language models (VLMs) enable the use of
natural language instructions to condition visual input and control robots in a
wider range of environments. However, existing methods require a large amount
of data to fine-tune VLMs for operating in unseen environments. In this paper,
we present a framework that learns object-arrangement tasks from just a few
demonstrations. We propose a two-stage framework that divides
object-arrangement tasks into a target localization stage, for picking the
object, and a region determination stage for placing the object. We present an
instance-level semantic fusion module that aligns the instance-level image
crops with the text embedding, enabling the model to identify the target
objects defined by the natural language instructions. We validate our method on
both simulation and real-world robotic environments. Our method, fine-tuned
with a few demonstrations, improves generalization capability and demonstrates
zero-shot ability in real-robot manipulation scenarios.

</details>


### [333] [QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots](https://arxiv.org/abs/2508.02512)
*Sheng Wu,Fei Teng,Hao Shi,Qi Jiang,Kai Luo,Kaiwei Wang,Kailun Yang*

Main category: cs.RO

TL;DR: QuaDreamer is a panoramic data generation engine for quadruped robots, addressing the lack of high-quality training data by mimicking robot motion and enhancing video realism with novel techniques like VJE, SOC, and PE.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-quality panoramic training data for quadruped robots due to kinematic constraints and calibration challenges limits robust perception system development.

Method: QuaDreamer uses Vertical Jitter Encoding (VJE) for motion mimicry, Scene-Object Controller (SOC) for motion management, and Panoramic Enhancer (PE) for distortion correction.

Result: Generated videos improve training for panoramic visual perception models, enhancing multi-object tracking in 360-degree scenes.

Conclusion: QuaDreamer provides a viable solution for generating realistic panoramic data, advancing perception systems for quadruped robots.

Abstract: Panoramic cameras, capturing comprehensive 360-degree environmental data, are
suitable for quadruped robots in surrounding perception and interaction with
complex environments. However, the scarcity of high-quality panoramic training
data-caused by inherent kinematic constraints and complex sensor calibration
challenges-fundamentally limits the development of robust perception systems
tailored to these embodied platforms. To address this issue, we propose
QuaDreamer-the first panoramic data generation engine specifically designed for
quadruped robots. QuaDreamer focuses on mimicking the motion paradigm of
quadruped robots to generate highly controllable, realistic panoramic videos,
providing a data source for downstream tasks. Specifically, to effectively
capture the unique vertical vibration characteristics exhibited during
quadruped locomotion, we introduce Vertical Jitter Encoding (VJE). VJE extracts
controllable vertical signals through frequency-domain feature filtering and
provides high-quality prompts. To facilitate high-quality panoramic video
generation under jitter signal control, we propose a Scene-Object Controller
(SOC) that effectively manages object motion and boosts background jitter
control through the attention mechanism. To address panoramic distortions in
wide-FoV video generation, we propose the Panoramic Enhancer (PE)-a dual-stream
architecture that synergizes frequency-texture refinement for local detail
enhancement with spatial-structure correction for global geometric consistency.
We further demonstrate that the generated video sequences can serve as training
data for the quadruped robot's panoramic visual perception model, enhancing the
performance of multi-object tracking in 360-degree scenes. The source code and
model weights will be publicly available at
https://github.com/losehu/QuaDreamer.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [334] [Cyber-Zero: Training Cybersecurity Agents without Runtime](https://arxiv.org/abs/2508.00910)
*Terry Yue Zhuo,Dingmin Wang,Hantian Ding,Varun Kumar,Zijian Wang*

Main category: cs.CR

TL;DR: Cyber-Zero is a runtime-free framework for training cybersecurity LLMs using synthesized trajectories from CTF writeups, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Runtime environments are often unavailable in cybersecurity, limiting LLM training. Cyber-Zero addresses this gap by synthesizing trajectories without actual execution contexts.

Method: Leverages CTF writeups and persona-driven LLM simulation to reverse-engineer runtime behaviors and generate realistic interaction sequences.

Result: Achieves up to 13.1% performance gains on CTF benchmarks, with Cyber-Zero-32B matching proprietary models like DeepSeek-V3-0324 and Claude-3.5-Sonnet.

Conclusion: Runtime-free trajectory synthesis democratizes development of high-performance cybersecurity agents, offering cost-effective state-of-the-art solutions.

Abstract: Large Language Models (LLMs) have achieved remarkable success in software
engineering tasks when trained with executable runtime environments,
particularly in resolving GitHub issues. However, such runtime environments are
often unavailable in other domains, especially cybersecurity, where challenge
configurations and execution contexts are ephemeral or restricted. We present
Cyber-Zero, the first runtime-free framework for synthesizing high-quality
agent trajectories to train cybersecurity LLMs. Cyber-Zero leverages publicly
available CTF writeups and employs persona-driven LLM simulation to
reverse-engineer runtime behaviors and generate realistic, long-horizon
interaction sequences without actual environments. Using trajectories
synthesized by Cyber-Zero, we train LLM-based agents that achieve up to 13.1%
absolute performance gains over baseline models on three prominent CTF
benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best model,
Cyber-Zero-32B, establishes new state-of-the-art performance among open-weight
models, matching the capabilities of proprietary systems like DeepSeek-V3-0324
and Claude-3.5-Sonnet while offering superior cost-effectiveness, and
demonstrating that runtime-free trajectory synthesis can effectively
democratize the development of state-of-the-art cybersecurity agents.

</details>


### [335] [AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection](https://arxiv.org/abs/2508.01249)
*Peiran Wang,Yang Liu,Yunfei Lu,Yifeng Cai,Hongbo Chen,Qingyou Yang,Jie Zhang,Jue Hong,Ye Wu*

Main category: cs.CR

TL;DR: AgentArmor is a program analysis framework that converts LLM agent traces into structured programs to detect security risks like prompt injection attacks, achieving high accuracy in detection.


<details>
  <summary>Details</summary>
Motivation: The dynamic and non-transparent behavior of LLM agents introduces security risks, especially prompt injection attacks, necessitating a structured approach to analyze and mitigate these risks.

Method: AgentArmor reconstructs agent traces as graph-based intermediate representations (CFG, DFG, PDG), enforces security policies via a type system, and includes components for graph construction, property registration, and static checking.

Result: AgentArmor achieves 95.75% true positive rate (TPR) and only 3.66% false positive rate (FPR) on the AgentDojo benchmark, effectively detecting vulnerabilities.

Conclusion: AgentArmor successfully enables program analysis for LLM agent security, detecting prompt injection and enforcing fine-grained security constraints.

Abstract: Large Language Model (LLM) agents offer a powerful new paradigm for solving
various problems by combining natural language reasoning with the execution of
external tools. However, their dynamic and non-transparent behavior introduces
critical security risks, particularly in the presence of prompt injection
attacks. In this work, we propose a novel insight that treats the agent runtime
traces as structured programs with analyzable semantics. Thus, we present
AgentArmor, a program analysis framework that converts agent traces into graph
intermediate representation-based structured program dependency representations
(e.g., CFG, DFG, and PDG) and enforces security policies via a type system.
AgentArmor consists of three key components: (1) a graph constructor that
reconstructs the agent's working traces as graph-based intermediate
representations with control flow and data flow described within; (2) a
property registry that attaches security-relevant metadata of interacted tools
& data, and (3) a type system that performs static inference and checking over
the intermediate representation. By representing agent behavior as structured
programs, AgentArmor enables program analysis over sensitive data flow, trust
boundaries, and policy violations. We evaluate AgentArmor on the AgentDojo
benchmark, the results show that AgentArmor can achieve 95.75% of TPR, with
only 3.66% of FPR. Our results demonstrate AgentArmor's ability to detect
prompt injection vulnerabilities and enforce fine-grained security constraints.

</details>


### [336] [ConfGuard: A Simple and Effective Backdoor Detection for Large Language Models](https://arxiv.org/abs/2508.01365)
*Zihan Wang,Rui Zhang,Hongwei Li,Wenshu Fan,Wenbo Jiang,Qingchuan Zhao,Guowen Xu*

Main category: cs.CR

TL;DR: ConfGuard detects backdoor attacks in LLMs by identifying "sequence lock"—abnormally high confidence in target sequences—enabling real-time, high-accuracy defense with minimal latency.


<details>
  <summary>Details</summary>
Motivation: Existing defenses for LLMs are ineffective due to their autoregressive nature and vast output space, leading to poor performance and high latency.

Method: ConfGuard monitors token confidences in a sliding window to detect sequence lock, a phenomenon where backdoored models generate target sequences with abnormally high confidence.

Result: ConfGuard achieves near 100% true positive rate and negligible false positive rate, with almost no additional latency.

Conclusion: ConfGuard is a practical, lightweight solution for real-time backdoor defense in LLMs.

Abstract: Backdoor attacks pose a significant threat to Large Language Models (LLMs),
where adversaries can embed hidden triggers to manipulate LLM's outputs. Most
existing defense methods, primarily designed for classification tasks, are
ineffective against the autoregressive nature and vast output space of LLMs,
thereby suffering from poor performance and high latency. To address these
limitations, we investigate the behavioral discrepancies between benign and
backdoored LLMs in output space. We identify a critical phenomenon which we
term sequence lock: a backdoored model generates the target sequence with
abnormally high and consistent confidence compared to benign generation.
Building on this insight, we propose ConfGuard, a lightweight and effective
detection method that monitors a sliding window of token confidences to
identify sequence lock. Extensive experiments demonstrate ConfGuard achieves a
near 100\% true positive rate (TPR) and a negligible false positive rate (FPR)
in the vast majority of cases. Crucially, the ConfGuard enables real-time
detection almost without additional latency, making it a practical backdoor
defense for real-world LLM deployments.

</details>


### [337] [DUP: Detection-guided Unlearning for Backdoor Purification in Language Models](https://arxiv.org/abs/2508.01647)
*Man Hu,Yahui Ding,Yatao Yang,Liangyu Chen,Yanhao Jia,Shuai Zhao*

Main category: cs.CR

TL;DR: DUP is a unified framework combining backdoor detection and purification, using feature-level anomalies and parameter-efficient unlearning to defend against stealthy attacks without full retraining.


<details>
  <summary>Details</summary>
Motivation: Current backdoor defense strategies are limited by coarse-grained detection and the need for full retraining or clean models.

Method: DUP integrates detection (using class-agnostic distances and inter-layer transitions) with purification (via knowledge distillation to unlearn backdoors).

Result: DUP outperforms existing methods in detection accuracy and purification efficacy across diverse attacks and models.

Conclusion: DUP provides a robust and efficient defense against advanced backdoor attacks without requiring full retraining or external clean models.

Abstract: As backdoor attacks become more stealthy and robust, they reveal critical
weaknesses in current defense strategies: detection methods often rely on
coarse-grained feature statistics, and purification methods typically require
full retraining or additional clean models. To address these challenges, we
propose DUP (Detection-guided Unlearning for Purification), a unified framework
that integrates backdoor detection with unlearning-based purification. The
detector captures feature-level anomalies by jointly leveraging class-agnostic
distances and inter-layer transitions. These deviations are integrated through
a weighted scheme to identify poisoned inputs, enabling more fine-grained
analysis. Based on the detection results, we purify the model through a
parameter-efficient unlearning mechanism that avoids full retraining and does
not require any external clean model. Specifically, we innovatively repurpose
knowledge distillation to guide the student model toward increasing its output
divergence from the teacher on detected poisoned samples, effectively forcing
it to unlearn the backdoor behavior. Extensive experiments across diverse
attack methods and language model architectures demonstrate that DUP achieves
superior defense performance in detection accuracy and purification efficacy.
Our code is available at https://github.com/ManHu2025/DUP.

</details>


### [338] [Complete Evasion, Zero Modification: PDF Attacks on AI Text Detection](https://arxiv.org/abs/2508.01887)
*Aldan Creo*

Main category: cs.CR

TL;DR: PDFuzz is a novel attack exploiting PDF text layout discrepancies to evade AI-generated text detectors, reducing detector performance to random levels while preserving visual fidelity.


<details>
  <summary>Details</summary>
Motivation: The robustness of AI-generated text detectors against evasion attacks is questionable, prompting the need to explore vulnerabilities in current systems.

Method: PDFuzz manipulates character positioning in PDFs to scramble extraction sequences, preserving textual content but altering extraction order.

Result: The attack reduces detector accuracy from 93.6% to 50.4% and F1 score from 0.938 to 0.0, achieving complete evasion.

Conclusion: PDFuzz exposes a vulnerability in detection systems tied to PDF structures, highlighting the need for stronger safeguards.

Abstract: AI-generated text detectors have become essential tools for maintaining
content authenticity, yet their robustness against evasion attacks remains
questionable. We present PDFuzz, a novel attack that exploits the discrepancy
between visual text layout and extraction order in PDF documents. Our method
preserves exact textual content while manipulating character positioning to
scramble extraction sequences. We evaluate this approach against the ArguGPT
detector using a dataset of human and AI-generated text. Our results
demonstrate complete evasion: detector performance drops from (93.6 $\pm$ 1.4)
% accuracy and 0.938 $\pm$ 0.014 F1 score to random-level performance ((50.4
$\pm$ 3.2) % accuracy, 0.0 F1 score) while maintaining perfect visual fidelity.
Our work reveals a vulnerability in current detection systems that is inherent
to PDF document structures and underscores the need for implementing sturdy
safeguards against such attacks. We make our code publicly available at
https://github.com/ACMCMC/PDFuzz.

</details>


### [339] [A Decentralized Framework for Ethical Authorship Validation in Academic Publishing: Leveraging Self-Sovereign Identity and Blockchain Technology](https://arxiv.org/abs/2508.01913)
*Kamal Al-Sabahi,Yousuf Khamis Al Mabsali*

Main category: cs.CR

TL;DR: A decentralized framework using SSI and blockchain is proposed to address unethical practices in academic publishing by ensuring authorship consent, accurate attribution, and conflict-of-interest detection.


<details>
  <summary>Details</summary>
Motivation: To combat unethical practices like unconsented authorship and undisclosed conflicts of interest in academic publishing, which current systems like ORCID fail to fully address.

Method: Leverages Self-Sovereign Identity (SSI) and blockchain, using Decentralized Identifiers (DIDs), Verifiable Credentials (VCs), and Zero-Knowledge Proofs (ZKPs) for secure verification and privacy-preserving conflict detection.

Result: The framework improves ethical compliance and confidence in scholarly communication, as indicated by stakeholder surveys.

Conclusion: The proposed model advances transparency, accountability, and trust in academic publishing.

Abstract: Academic publishing, integral to knowledge dissemination and scientific
advancement, increasingly faces threats from unethical practices such as
unconsented authorship, gift authorship, author ambiguity, and undisclosed
conflicts of interest. While existing infrastructures like ORCID effectively
disambiguate researcher identities, they fall short in enforcing explicit
authorship consent, accurately verifying contributor roles, and robustly
detecting conflicts of interest during peer review. To address these
shortcomings, this paper introduces a decentralized framework leveraging
Self-Sovereign Identity (SSI) and blockchain technology. The proposed model
uses Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs) to
securely verify author identities and contributions, reducing ambiguity and
ensuring accurate attribution. A blockchain-based trust registry records
authorship consent and peer-review activity immutably. Privacy-preserving
cryptographic techniques, especially Zero-Knowledge Proofs (ZKPs), support
conflict-of-interest detection without revealing sensitive data. Verified
authorship metadata and consent records are embedded in publications,
increasing transparency. A stakeholder survey of researchers, editors, and
reviewers suggests the framework improves ethical compliance and confidence in
scholarly communication. This work represents a step toward a more transparent,
accountable, and trustworthy academic publishing ecosystem.

</details>


### [340] [CP-FREEZER: Latency Attacks against Vehicular Cooperative Perception](https://arxiv.org/abs/2508.01062)
*Chenyi Wang,Ruoyu Song,Raymond Muller,Jean-Philippe Monteuuis,Z. Berkay Celik,Jonathan Petit,Ryan Gerdes,Ming Li*

Main category: cs.CR

TL;DR: CP-FREEZER is a latency attack on cooperative perception (CP) systems, increasing computation delay by 90x, exposing critical vulnerabilities in CP timeliness.


<details>
  <summary>Details</summary>
Motivation: Prior work focused on perceptual accuracy attacks, but CP's robustness against timeliness attacks, crucial for autonomous driving safety, was unexplored.

Method: CP-FREEZER injects adversarial perturbations via V2V messages, overcoming challenges like non-differentiable preprocessing and asynchronous victim input.

Result: The attack increases CP latency by 90x, exceeding 3 seconds per frame with 100% success in real-world tests.

Conclusion: CP-FREEZER reveals a severe threat to CP availability, underscoring the need for robust defenses.

Abstract: Cooperative perception (CP) enhances situational awareness of connected and
autonomous vehicles by exchanging and combining messages from multiple agents.
While prior work has explored adversarial integrity attacks that degrade
perceptual accuracy, little is known about CP's robustness against attacks on
timeliness (or availability), a safety-critical requirement for autonomous
driving. In this paper, we present CP-FREEZER, the first latency attack that
maximizes the computation delay of CP algorithms by injecting adversarial
perturbation via V2V messages. Our attack resolves several unique challenges,
including the non-differentiability of point cloud preprocessing, asynchronous
knowledge of the victim's input due to transmission delays, and uses a novel
loss function that effectively maximizes the execution time of the CP pipeline.
Extensive experiments show that CP-FREEZER increases end-to-end CP latency by
over $90\times$, pushing per-frame processing time beyond 3 seconds with a 100%
success rate on our real-world vehicle testbed. Our findings reveal a critical
threat to the availability of CP systems, highlighting the urgent need for
robust defenses.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [341] [Toward a reliable PWM-based light-emitting diode visual stimulus for improved SSVEP response with minimal visual fatigue](https://arxiv.org/abs/2508.02359)
*Surej Mouli,Ramaswamy Palaniappan*

Main category: eess.SP

TL;DR: The paper explores using high duty-cycle visual stimuli to reduce eye fatigue in SSVEP applications, finding an optimal 85% duty-cycle for peak response and reduced strain.


<details>
  <summary>Details</summary>
Motivation: SSVEP is useful for brain-computer interfaces but causes eye fatigue and has accuracy issues due to PWM limitations. High-frequency stimuli reduce fatigue but degrade performance, prompting the study of high duty-cycles.

Method: EEG data was recorded using custom LED hardware with duty-cycles ranging from 50% to 95%, tested on ten subjects to measure visual strain and SSVEP response.

Result: Higher duty-cycles reduced visual strain, with an 85% duty-cycle yielding a subject-independent peak SSVEP response.

Conclusion: An 85% duty-cycle optimizes SSVEP performance while minimizing fatigue, enhancing practical applications.

Abstract: Steady state visual evoked response (SSVEP) is widely used in visual-based
diagnosis and applications such as brain computer interfacing due to its high
information transfer rate and the capability to activate commands through
simple gaze control. However, one major impediment in using flashing visual
stimulus to obtain SSVEP is eye fatigue that prevents continued long term use
preventing practical deployment. This combined with the difficulty in
establishing precise pulse-width modulation (PWM) that results in poorer
accuracy warrants the development of appropriate approach to solve these
issues. Various studies have suggested the usage of high frequencies of visual
stimulus to reduce the visual fatigue for the user but this results in poor
response performance. Here, the authors study the use of extremely high
duty-cycles in the stimulus in the hope of solving these constraints.
Electroencephalogram data was recorded with PWM duty-cycles of 50 to 95%
generated by a precise custom-made light-emitting diode hardware and tested ten
subjects responded that increasing duty-cycles had less visual strain for all
the frequency values and the SSVEP exhibited a subject-independent peak
response for duty-cycle of 85%. This could pave the way for increased usage of
SSVEP for practical applications.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [342] [The Attribution Crisis in LLM Search Results](https://arxiv.org/abs/2508.00838)
*Ilan Strauss,Jangho Yang,Tim O'Reilly,Sruly Rosenblat,Isobel Moure*

Main category: cs.DL

TL;DR: The paper highlights an "attribution gap" in web-enabled LLMs, where models often fail to cite sources despite consuming web content. Analyzing 14,000 logs, it reveals exploitation patterns like no search, no citation, and high-volume, low-credit behavior. Recommendations include transparent search architectures.


<details>
  <summary>Details</summary>
Motivation: To address the lack of proper attribution in LLMs when answering queries using web content, which undermines trust and transparency.

Method: Analysis of ~14,000 real-world LMArena conversation logs from search-enabled LLMs (Gemini, GPT-4o, Perplexity's Sonar) using a negative binomial hurdle model.

Result: Key findings: 34% of Gemini and 24% of GPT-4o responses lack explicit web fetches; 92% of Gemini answers have no citations; Sonar visits ~10 pages but cites only 3-4. GPT-4o's small gap is due to selective logging. Citation efficiency varies (0.19-0.45).

Conclusion: Retrieval design, not technical limits, drives attribution gaps. The paper advocates for standardized telemetry and full disclosure of search traces and citation logs to improve transparency.

Abstract: Web-enabled LLMs frequently answer queries without crediting the web pages
they consume, creating an "attribution gap" - the difference between relevant
URLs read and those actually cited. Drawing on approximately 14,000 real-world
LMArena conversation logs with search-enabled LLM systems, we document three
exploitation patterns: 1) No Search: 34% of Google Gemini and 24% of OpenAI
GPT-4o responses are generated without explicitly fetching any online content;
2) No citation: Gemini provides no clickable citation source in 92% of answers;
3) High-volume, low-credit: Perplexity's Sonar visits approximately 10 relevant
pages per query but cites only three to four. A negative binomial hurdle model
shows that the average query answered by Gemini or Sonar leaves about 3
relevant websites uncited, whereas GPT-4o's tiny uncited gap is best explained
by its selective log disclosures rather than by better attribution. Citation
efficiency - extra citations provided per additional relevant web page visited
- varies widely across models, from 0.19 to 0.45 on identical queries,
underscoring that retrieval design, not technical limits, shapes ecosystem
impact. We recommend a transparent LLM search architecture based on
standardized telemetry and full disclosure of search traces and citation logs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [343] [AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks](https://arxiv.org/abs/2508.00890)
*Fali Wang,Hui Liu,Zhenwei Dai,Jingying Zeng,Zhiwei Zhang,Zongyu Wu,Chen Luo,Zhen Li,Xianfeng Tang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: AgentTTS is a framework for optimizing compute resources in multi-stage LLM tasks, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing test-time scaling (TTS) research focuses on single-stage tasks, but real-world problems are multi-stage, requiring tailored compute allocation.

Method: AgentTTS uses an LLM-agent to autonomously search for optimal model and budget allocations through iterative feedback.

Result: AgentTTS outperforms baselines in efficiency, robustness, and interpretability across six datasets.

Conclusion: AgentTTS effectively addresses the challenges of compute-optimal scaling in multi-stage tasks.

Abstract: Test-time scaling (TTS) enhances the performance of large language models
(LLMs) by allocating additional compute resources during inference. However,
existing research primarily investigates TTS in single-stage tasks; while many
real-world problems are multi-stage complex tasks, composed of a sequence of
heterogeneous subtasks with each subtask requires LLM of specific capability.
Therefore, we study a novel problem: the test-time compute-optimal scaling in
multi-stage complex tasks, aiming to select suitable models and allocate
budgets per subtask to maximize overall performance. TTS in multi-stage tasks
introduces two fundamental challenges: (i) The combinatorial search space of
model and budget allocations, combined with the high cost of inference, makes
brute-force search impractical. (ii) The optimal model and budget allocations
across subtasks are interdependent, increasing the complexity of the
compute-optimal search. To address this gap, we conduct extensive pilot
experiments on four tasks across six datasets, deriving three empirical
insights characterizing the behavior of LLMs in multi-stage complex tasks.
Informed by these insights, we propose AgentTTS, an LLM-agent-based framework
that autonomously searches for compute-optimal allocations through iterative
feedback-driven interactions with the execution environment. Experimental
results demonstrate that AgentTTS significantly outperforms traditional and
other LLM-based baselines in search efficiency, and shows improved robustness
to varying training set sizes and enhanced interpretability.

</details>


### [344] [An analysis of AI Decision under Risk: Prospect theory emerges in Large Language Models](https://arxiv.org/abs/2508.00902)
*Kenneth Payne*

Main category: cs.AI

TL;DR: The paper tests Kahneman and Tversky's prospect theory on Large Language Models (LLMs), finding similarities to human risk judgment, with context and framing effects playing key roles.


<details>
  <summary>Details</summary>
Motivation: To explore whether LLMs exhibit human-like risk judgment biases, as described in prospect theory, and how context influences these biases.

Method: Conducted experiments with LLMs, including chain-of-thought reasoners, across various scenarios (e.g., military vs. civilian) to analyze risk appetite and framing effects.

Result: LLMs often align with prospect theory, showing context-dependent biases. Military scenarios exhibit stronger framing effects than civilian ones.

Conclusion: LLMs mirror human heuristics and biases, with framing effects varying by context. The findings also contribute to the debate on reasoning vs. memorisation in LLMs.

Abstract: Judgment of risk is key to decision-making under uncertainty. As Daniel
Kahneman and Amos Tversky famously discovered, humans do so in a distinctive
way that departs from mathematical rationalism. Specifically, they demonstrated
experimentally that humans accept more risk when they feel themselves at risk
of losing something than when they might gain. I report the first tests of
Kahneman and Tversky's landmark 'prospect theory' with Large Language Models,
including today's state of the art chain-of-thought 'reasoners'.
  In common with humans, I find that prospect theory often anticipates how
these models approach risky decisions across a range of scenarios. I also
demonstrate that context is key to explaining much of the variance in risk
appetite. The 'frame' through which risk is apprehended appears to be embedded
within the language of the scenarios tackled by the models. Specifically, I
find that military scenarios generate far larger 'framing effects' than do
civilian settings, ceteris paribus. My research suggests, therefore, that
language models the world, capturing our human heuristics and biases. But also
that these biases are uneven - the idea of a 'frame' is richer than simple
gains and losses. Wittgenstein's notion of 'language games' explains the
contingent, localised biases activated by these scenarios. Finally, I use my
findings to reframe the ongoing debate about reasoning and memorisation in
LLMs.

</details>


### [345] [CADDesigner: Conceptual Design of CAD Models Based on General-Purpose Agent](https://arxiv.org/abs/2508.01031)
*Jingzhe Ni,Xiaolong Yin,Xintong Li,Xingyu Lu,Ji Wei,Ruofeng Tong,Min Tang,Peng Du*

Main category: cs.AI

TL;DR: An LLM-powered CAD design agent simplifies conceptual design by accepting text and sketches, refining requirements interactively, and generating high-quality CAD code using a novel CIP paradigm.


<details>
  <summary>Details</summary>
Motivation: To lower the expertise barrier in CAD design and improve efficiency by leveraging LLMs for interactive and intuitive design assistance.

Method: Uses a Context-Independent Imperative Paradigm (CIP) to generate CAD code, incorporates iterative visual feedback, and stores designs in a knowledge base for continuous improvement.

Result: Achieves state-of-the-art performance in CAD code generation.

Conclusion: The agent effectively bridges the gap between novice users and CAD design, enhancing accessibility and quality.

Abstract: Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing
but typically requires a high level of expertise from designers. To lower the
entry barrier and improve design efficiency, we present an agent for CAD
conceptual design powered by large language models (LLMs). The agent accepts
both abstract textual descriptions and freehand sketches as input, engaging in
interactive dialogue with users to refine and clarify design requirements
through comprehensive requirement analysis. Built upon a novel
Context-Independent Imperative Paradigm (CIP), the agent generates high-quality
CAD modeling code. During the generation process, the agent incorporates
iterative visual feedback to improve model quality. Generated design cases are
stored in a structured knowledge base, enabling continuous improvement of the
agent's code generation capabilities. Experimental results demonstrate that our
method achieves state-of-the-art performance in CAD code generation.

</details>


### [346] [Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens](https://arxiv.org/abs/2508.01191)
*Chengshuai Zhao,Zhen Tan,Pingchuan Ma,Dawei Li,Bohan Jiang,Yancheng Wang,Yingzhen Yang,Huan Liu*

Main category: cs.AI

TL;DR: CoT reasoning in LLMs is superficial and fails beyond training distributions, revealing its brittleness.


<details>
  <summary>Details</summary>
Motivation: To explore if CoT reasoning is a learned inductive bias from training data and its limitations due to distribution discrepancies.

Method: Study CoT reasoning via task, length, and format dimensions using DataAlchemy, a controlled environment for training and probing LLMs.

Result: CoT reasoning is brittle and ineffective when pushed beyond training distributions.

Conclusion: Highlights the challenge of achieving genuine, generalizable reasoning in LLMs.

Abstract: Chain-of-Thought (CoT) prompting has been shown to improve Large Language
Model (LLM) performance on various tasks. With this approach, LLMs appear to
produce human-like reasoning steps before providing answers (a.k.a., CoT
reasoning), which often leads to the perception that they engage in deliberate
inferential processes. However, some initial findings suggest that CoT
reasoning may be more superficial than it appears, motivating us to explore
further. In this paper, we study CoT reasoning via a data distribution lens and
investigate if CoT reasoning reflects a structured inductive bias learned from
in-distribution data, allowing the model to conditionally generate reasoning
paths that approximate those seen during training. Thus, its effectiveness is
fundamentally bounded by the degree of distribution discrepancy between the
training data and the test queries. With this lens, we dissect CoT reasoning
via three dimensions: task, length, and format. To investigate each dimension,
we design DataAlchemy, an isolated and controlled environment to train LLMs
from scratch and systematically probe them under various distribution
conditions. Our results reveal that CoT reasoning is a brittle mirage that
vanishes when it is pushed beyond training distributions. This work offers a
deeper understanding of why and when CoT reasoning fails, emphasizing the
ongoing challenge of achieving genuine and generalizable reasoning.

</details>


### [347] [Multi-TW: Benchmarking Multimodal Models on Traditional Chinese Question Answering in Taiwan](https://arxiv.org/abs/2508.01274)
*Jui-Ming Yao,Bing-Cheng Xie,Sheng-Wei Peng,Hao-Yuan Chen,He-Rong Zheng,Bing-Jia Tan,Peter Shaojui Wang,Shun-Feng Su*

Main category: cs.AI

TL;DR: Multi-TW is the first Traditional Chinese benchmark for evaluating any-to-any multimodal models, focusing on performance and latency. It includes 900 questions and shows closed-source models outperform open-source ones, though open-source models excel in audio tasks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack tri-modal evaluation in Traditional Chinese and ignore inference latency, limiting comprehensive assessment of multimodal models.

Method: Multi-TW uses 900 multiple-choice questions (image-text and audio-text pairs) from official proficiency tests. It evaluates any-to-any models and VLMs with audio transcription.

Result: Closed-source models outperform open-source ones across modalities, but open-source models perform well in audio tasks. End-to-end pipelines have lower latency than VLMs with separate transcription.

Conclusion: Multi-TW highlights the need for Traditional Chinese fine-tuning and efficient multimodal architectures, providing a comprehensive evaluation framework.

Abstract: Multimodal Large Language Models (MLLMs) process visual, acoustic, and
textual inputs, addressing the limitations of single-modality LLMs. However,
existing benchmarks often overlook tri-modal evaluation in Traditional Chinese
and do not consider inference latency. To address this, we introduce Multi-TW,
the first Traditional Chinese benchmark for evaluating the performance and
latency of any-to-any multimodal models. Multi-TW includes 900 multiple-choice
questions (image and text, audio and text pairs) sourced from official
proficiency tests developed with the Steering Committee for the Test of
Proficiency-Huayu (SC-TOP). We evaluated various any-to-any models and
vision-language models (VLMs) with audio transcription. Our results show that
closed-source models generally outperform open-source ones across modalities,
although open-source models can perform well in audio tasks. End-to-end
any-to-any pipelines offer clear latency advantages compared to VLMs using
separate audio transcription. Multi-TW presents a comprehensive view of model
capabilities and highlights the need for Traditional Chinese fine-tuning and
efficient multimodal architectures.

</details>


### [348] [Uncertainty-Based Methods for Automated Process Reward Data Construction and Output Aggregation in Mathematical Reasoning](https://arxiv.org/abs/2508.01773)
*Jiuzhou Han,Wray Buntine,Ehsan Shareghi*

Main category: cs.AI

TL;DR: Proposes an uncertainty-driven framework for automated process reward data construction for PRMs, introduces two output aggregation methods, and demonstrates their effectiveness in improving mathematical reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing methods for constructing high-quality process reward data for PRMs are labor-intensive or inefficient.

Method: Uncertainty-driven framework for automated PRM data construction, including generation and annotation. Introduces Hybrid Majority Reward Vote and Weighted Reward Frequency Vote for output aggregation.

Result: Effective and efficient PRM data construction; improved mathematical reasoning across diverse PRMs.

Conclusion: The proposed framework and aggregation methods enhance PRM performance and reasoning abilities.

Abstract: Large language models have demonstrated remarkable capabilities in complex
mathematical reasoning tasks, but they inevitably generate errors throughout
multi-step solutions. Process-level Reward Models (PRMs) have shown great
promise by providing supervision and evaluation at each intermediate step,
thereby effectively improving the models' reasoning abilities. However,
training effective PRMs requires high-quality process reward data, yet existing
methods for constructing such data are often labour-intensive or inefficient.
In this paper, we propose an uncertainty-driven framework for automated process
reward data construction, encompassing both data generation and annotation
processes for PRMs. Additionally, we identify the limitations of both majority
vote and PRMs, and introduce two generic uncertainty-aware output aggregation
methods: Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, which
combine the strengths of majority vote with PRMs. Extensive experiments on
ProcessBench, MATH, and GSMPlus show the effectiveness and efficiency of the
proposed PRM data construction framework, and demonstrate that the two output
aggregation methods further improve the mathematical reasoning abilities across
diverse PRMs. The code and data will be publicly available at
https://github.com/Jiuzhouh/UnPRM.

</details>


### [349] [LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?](https://arxiv.org/abs/2508.01780)
*Guozhao Mo,Wenliang Zhong,Jiawei Chen,Xuanang Chen,Yaojie Lu,Hongyu Lin,Ben He,Xianpei Han,Le Sun*

Main category: cs.AI

TL;DR: LiveMCPBench is a comprehensive benchmark for evaluating LLM agents in large-scale MCP environments, featuring 95 real-world tasks, 70 MCP servers, and 527 tools. It includes LiveMCPEval for automated evaluation and introduces the MCP Copilot Agent, achieving 78.95% success with Claude-Sonnet-4.


<details>
  <summary>Details</summary>
Motivation: Existing MCP benchmarks are limited to single-server settings, failing to evaluate agent capabilities in large-scale, real-world scenarios.

Method: Developed LiveMCPBench with 95 tasks, LiveMCPTool (70 servers, 527 tools), and LiveMCPEval (LLM-as-a-Judge framework). Introduced MCP Copilot Agent for dynamic planning and API interaction.

Result: Best-performing model (Claude-Sonnet-4) achieved 78.95% success rate, but performance varied widely among models. LiveMCPEval agreed with human reviewers 81% of the time.

Conclusion: LiveMCPBench provides a unified framework for scalable, reproducible evaluation of LLM agents in realistic, tool-rich MCP environments.

Abstract: With the rapid development of Model Context Protocol (MCP), the number of MCP
servers has surpassed 10,000. However, existing MCP benchmarks are limited to
single-server settings with only a few tools, hindering effective evaluation of
agent capabilities in large-scale, real-world scenarios. To address this
limitation, we present LiveMCPBench, the first comprehensive benchmark
comprising 95 real-world tasks grounded in the MCP ecosystem, designed to
evaluate LLM agents at scale across diverse servers. To support a scalable and
reproducible evaluation pipeline in large-scale MCP environments, we curate
LiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and
527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework
that enables automated and adaptive evaluation in dynamic, time-varying task
environments, achieving 81% agreement with human reviewers. Finally, we propose
the MCP Copilot Agent, a multi-step agent that routes tools for dynamic
planning and executes tools for API interaction across the entire LiveMCPTool
suite. Our evaluation covers 10 leading models, with the best-performing model
(Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large
performance variance across models, and several widely-used models perform
poorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench
offers the first unified framework for benchmarking LLM agents in realistic,
tool-rich, and dynamic MCP environments, laying a solid foundation for scalable
and reproducible research on agent capabilities. Our code and data will be
publicly available at https://icip-cas.github.io/LiveMCPBench.

</details>


### [350] [Trainable Dynamic Mask Sparse Attention](https://arxiv.org/abs/2508.02124)
*Jingze Shi,Yifan Wu,Bingheng Wu,Yiran Peng,Liangdong Wang,Guang Liu,Yuyu Luo*

Main category: cs.AI

TL;DR: Dynamic Mask Attention (DMA) introduces a trainable sparse attention mechanism with content-aware and position-aware sparsity, improving efficiency and performance in long-context modeling.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity of standard self-attention in large language models is a bottleneck for long-context modeling. Existing sparse attention methods may suffer from static patterns or information loss.

Method: DMA dynamically generates content-aware sparse masks from value representations and implements position-aware sparse attention to skip unnecessary calculations.

Result: DMA outperforms other attention mechanisms in perplexity, multi-query associative recall, and needle-in-a-haystack tasks, balancing efficiency and performance.

Conclusion: DMA effectively reduces computational complexity while maintaining information fidelity, making it a promising solution for long-context modeling.

Abstract: In large language models, the demand for modeling long contexts is constantly
increasing, but the quadratic complexity of the standard self-attention
mechanism often becomes a bottleneck. Although existing sparse attention
mechanisms have improved efficiency, they may still encounter issues such as
static patterns or information loss. We introduce a trainable dynamic mask
sparse attention mechanism, Dynamic Mask Attention, which effectively utilizes
content-aware and position-aware sparsity. DMA achieves this through two key
innovations: First, it dynamically generates content-aware sparse masks from
value representations, enabling the model to identify and focus on critical
information adaptively. Second, it implements position-aware sparse attention
computation that effectively skips unnecessary calculation regions. This
dual-sparsity design allows the model to significantly reduce the computational
complexity of important information while retaining complete information,
achieving an excellent balance between information fidelity and computational
efficiency. We have verified the performance of DMA through comprehensive
experiments. Comparative studies show that DMA outperforms multi-head
attention, sliding window attention, multi-head latent attention, and native
sparse attention in terms of perplexity under Chinchilla Scaling Law settings.
Moreover, in challenging multi-query associative recall tasks, DMA also
demonstrates superior performance and efficiency compared to these methods.
Crucially, in the evaluation of a 1.7B parameter model, DMA significantly
outperforms multi-head attention in both standard benchmark performance and the
challenging needle-in-a-haystack task. These experimental results highlight its
capability to balance model efficiency and long-context modeling ability
effectively.

</details>


### [351] [OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical Modeling](https://arxiv.org/abs/2508.02503)
*Maxime Bouscary,Saurabh Amin*

Main category: cs.AI

TL;DR: OptiHive is an LLM-based framework that generates high-quality solvers for optimization problems from natural-language descriptions without iterative self-correction, outperforming baselines significantly.


<details>
  <summary>Details</summary>
Motivation: LLM-based solvers are unreliable and slow due to iterative repair loops. OptiHive aims to automate problem-solving without these drawbacks.

Method: OptiHive uses a single batched LLM query to generate diverse components (solvers, problem instances, validation tests) and filters out errors. A statistical model quantifies uncertainty and selects solvers.

Result: OptiHive increases the optimality rate from 5% to 92% on complex problems like the Multi-Depot Vehicle Routing Problem.

Conclusion: OptiHive provides a reliable, interpretable, and efficient alternative to traditional LLM-based solvers, significantly improving performance.

Abstract: LLM-based solvers have emerged as a promising means of automating problem
modeling and solving. However, they remain unreliable and often depend on
iterative repair loops that result in significant latency. We introduce
OptiHive, an LLM-based framework that produces high-quality solvers for
optimization problems from natural-language descriptions without iterative
self-correction. OptiHive uses a single batched LLM query to generate diverse
components (solvers, problem instances, and validation tests) and filters out
erroneous components to ensure fully interpretable outputs. Taking into account
the imperfection of the generated components, we employ a statistical model to
infer their true performance, enabling principled uncertainty quantification
and solver selection. On tasks ranging from traditional optimization problems
to challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive
significantly outperforms baselines, increasing the optimality rate from 5\% to
92\% on the most complex problems.

</details>


### [352] [Test-time Prompt Intervention](https://arxiv.org/abs/2508.02511)
*Chenxu Yang,Qingyi Si,Mz Dai,Dingyu Yao,Mingyu Zheng,Minghui Chen,Zheng Lin,Weiping Wang*

Main category: cs.AI

TL;DR: The paper introduces PI, a framework for Test-time Prompt Intervention to reduce redundancy in LLM reasoning by dynamically guiding and regulating reasoning paths during inference.


<details>
  <summary>Details</summary>
Motivation: Existing reasoning models in LLMs produce redundant chains of thought due to over-reliance on outcome reward paradigms, lacking process reward data.

Method: PI integrates human expertise and cognitive principles via three modules: When (timely intervention), How (proper intervention), and Which (post-intervention sampling).

Result: PI significantly shortens chains of thought while reducing hallucination, leading to more concise and reliable reasoning.

Conclusion: PI enhances controllability and interpretability in LLM reasoning, addressing redundancy and improving efficiency.

Abstract: Test-time compute has led to remarkable success in the large language model
(LLM) community, particularly for complex tasks, where longer chains of thought
(CoTs) are generated to enhance reasoning capabilities. However, growing
evidence reveals that such reasoning models often produce CoTs plagued by
excessive redundancy, including unnecessary verification steps and repetitive
reasoning shifts. The root cause lies in post-training of them that overly rely
on outcome reward paradigms, as the data of process reward paradigms, which
regulate intermediate reasoning steps, is difficult to construct at scale. To
address this, we propose PI, a novel framework for Test-time Prompt
Intervention. PI provides an interface to dynamically guide and regulate
reasoning paths during inference through timely (When module) and proper (How
module) interventions and post-intervention sampling (Which module). This
allows human problem-solving expertise and cognitive science principles to be
seamlessly integrated into LLMs' reasoning processes, enhancing controllability
and interpretability. Extensive experiments across multiple models and datasets
demonstrate that PI significantly shortens CoTs while reducing hallucination,
yielding more concise and reliable reasoning.

</details>


### [353] [HealthFlow: A Self-Evolving AI Agent with Meta Planning for Autonomous Healthcare Research](https://arxiv.org/abs/2508.02621)
*Yinghao Zhu,Yifan Qi,Zixiang Wang,Lei Gu,Dehao Sui,Haoran Hu,Xichen Zhang,Ziyi He,Liantao Ma,Lequan Yu*

Main category: cs.AI

TL;DR: HealthFlow is a self-evolving AI agent for healthcare that improves strategic planning through meta-level evolution, outperforming existing frameworks.


<details>
  <summary>Details</summary>
Motivation: Current AI agents in healthcare rely on static strategies, limiting their ability to improve strategic planning, which is crucial for complex domains.

Method: HealthFlow uses a meta-level evolution mechanism to refine high-level problem-solving policies by learning from successes and failures, and introduces EHRFlowBench for evaluation.

Result: HealthFlow significantly outperforms state-of-the-art agent frameworks in experiments.

Conclusion: This work advances AI in healthcare by shifting focus from tool-users to self-evolving task-managers, enabling more autonomous scientific discovery.

Abstract: The efficacy of AI agents in healthcare research is hindered by their
reliance on static, predefined strategies. This creates a critical limitation:
agents can become better tool-users but cannot learn to become better strategic
planners, a crucial skill for complex domains like healthcare. We introduce
HealthFlow, a self-evolving AI agent that overcomes this limitation through a
novel meta-level evolution mechanism. HealthFlow autonomously refines its own
high-level problem-solving policies by distilling procedural successes and
failures into a durable, strategic knowledge base. To anchor our research and
facilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark
featuring complex, realistic health data analysis tasks derived from
peer-reviewed clinical research. Our comprehensive experiments demonstrate that
HealthFlow's self-evolving approach significantly outperforms state-of-the-art
agent frameworks. This work marks a necessary shift from building better
tool-users to designing smarter, self-evolving task-managers, paving the way
for more autonomous and effective AI for scientific discovery.

</details>


### [354] [Noosemia: toward a Cognitive and Phenomenological Account of Intentionality Attribution in Human-Generative AI Interaction](https://arxiv.org/abs/2508.02622)
*Enrico De Santis,Antonello Rizzi*

Main category: cs.AI

TL;DR: The paper introduces 'Noosemia,' a phenomenon where humans attribute intentionality and agency to AI systems due to linguistic performance and complexity, not physical resemblance. It proposes a framework to explain this and links it to existing concepts like pareidolia and animism.


<details>
  <summary>Details</summary>
Motivation: To understand and formalize the cognitive-phenomenological phenomenon of humans attributing human-like traits to AI systems, driven by linguistic and epistemic factors.

Method: A multidisciplinary framework is proposed, linking LLM meaning holism to the 'LLM Contextual Cognitive Field' to explain coherence and agency in human-AI interactions.

Result: Noosemia is distinguished from similar phenomena (e.g., pareidolia, animism) and 'a-noosemia' is introduced to describe the withdrawal of such attributions.

Conclusion: The paper highlights the philosophical, epistemological, and social implications of noosemia and suggests future research directions.

Abstract: This paper introduces and formalizes Noosemia, a novel
cognitive-phenomenological phenomenon emerging from human interaction with
generative AI systems, particularly those enabling dialogic or multimodal
exchanges. We propose a multidisciplinary framework to explain how, under
certain conditions, users attribute intentionality, agency, and even
interiority to these systems - a process grounded not in physical resemblance,
but in linguistic performance, epistemic opacity, and emergent technological
complexity. By linking an LLM declination of meaning holism to our technical
notion of the LLM Contextual Cognitive Field, we clarify how LLMs construct
meaning relationally and how coherence and a simulacrum of agency arise at the
human-AI interface. The analysis situates noosemia alongside pareidolia,
animism, the intentional stance and the uncanny valley, distinguishing its
unique characteristics. We also introduce a-noosemia to describe the
phenomenological withdrawal of such projections. The paper concludes with
reflections on the broader philosophical, epistemological, and social
implications of noosemic dynamics and directions for future research.

</details>


### [355] [SURE-Med: Systematic Uncertainty Reduction for Enhanced Reliability in Medical Report Generation](https://arxiv.org/abs/2508.01693)
*Yuhang Gu,Xingyu Hu,Yuyu Fan,Xulin Yan,Longhuan Xu,Peng peng*

Main category: cs.AI

TL;DR: SURE-Med addresses visual, distributional, and contextual uncertainties in automated medical report generation, improving reliability and clinical trustworthiness.


<details>
  <summary>Details</summary>
Motivation: The heavy workload of radiologists and the unreliability of current MRG systems due to uncertainties in visual, label distribution, and contextual data.

Method: SURE-Med uses Frontal-Aware View Repair Resampling, Token Sensitive Learning, and Contextual Evidence Filter to mitigate uncertainties.

Result: Achieves state-of-the-art performance on MIMIC-CXR and IU-Xray benchmarks.

Conclusion: SURE-Med enhances reliability in medical report generation, advancing trustworthy clinical decision support.

Abstract: Automated medical report generation (MRG) holds great promise for reducing
the heavy workload of radiologists. However, its clinical deployment is
hindered by three major sources of uncertainty. First, visual uncertainty,
caused by noisy or incorrect view annotations, compromises feature extraction.
Second, label distribution uncertainty, stemming from long-tailed disease
prevalence, biases models against rare but clinically critical conditions.
Third, contextual uncertainty, introduced by unverified historical reports,
often leads to factual hallucinations. These challenges collectively limit the
reliability and clinical trustworthiness of MRG systems. To address these
issues, we propose SURE-Med, a unified framework that systematically reduces
uncertainty across three critical dimensions: visual, distributional, and
contextual. To mitigate visual uncertainty, a Frontal-Aware View Repair
Resampling module corrects view annotation errors and adaptively selects
informative features from supplementary views. To tackle label distribution
uncertainty, we introduce a Token Sensitive Learning objective that enhances
the modeling of critical diagnostic sentences while reweighting
underrepresented diagnostic terms, thereby improving sensitivity to infrequent
conditions. To reduce contextual uncertainty, our Contextual Evidence Filter
validates and selectively incorporates prior information that aligns with the
current image, effectively suppressing hallucinations. Extensive experiments on
the MIMIC-CXR and IU-Xray benchmarks demonstrate that SURE-Med achieves
state-of-the-art performance. By holistically reducing uncertainty across
multiple input modalities, SURE-Med sets a new benchmark for reliability in
medical report generation and offers a robust step toward trustworthy clinical
decision support.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [356] [MeshLLM: Empowering Large Language Models to Progressively Understand and Generate 3D Mesh](https://arxiv.org/abs/2508.01242)
*Shuangkang Fang,I-Chao Shen,Yufeng Wang,Yi-Hsuan Tsai,Yi Yang,Shuchang Zhou,Wenrui Ding,Takeo Igarashi,Ming-Hsuan Yang*

Main category: cs.GR

TL;DR: MeshLLM is a framework using LLMs to process 3D meshes via Primitive-Mesh decomposition and large-scale dataset creation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations in dataset scale and 3D structural information loss in existing mesh serialization methods.

Method: Introduces Primitive-Mesh decomposition and local mesh assembly training to enhance LLMs' mesh understanding.

Result: Creates a 1500k+ sample dataset and outperforms LLaMA-Mesh in generation quality and shape understanding.

Conclusion: MeshLLM shows strong potential for text-serialized 3D mesh processing.

Abstract: We present MeshLLM, a novel framework that leverages large language models
(LLMs) to understand and generate text-serialized 3D meshes. Our approach
addresses key limitations in existing methods, including the limited dataset
scale when catering to LLMs' token length and the loss of 3D structural
information during mesh serialization. We introduce a Primitive-Mesh
decomposition strategy, which divides 3D meshes into structurally meaningful
subunits. This enables the creation of a large-scale dataset with 1500k+
samples, almost 50 times larger than previous methods, which aligns better with
the LLM scaling law principles. Furthermore, we propose inferring face
connectivity from vertices and local mesh assembly training strategies,
significantly enhancing the LLMs' ability to capture mesh topology and spatial
structures. Experiments show that MeshLLM outperforms the state-of-the-art
LLaMA-Mesh in both mesh generation quality and shape understanding,
highlighting its great potential in processing text-serialized 3D meshes.

</details>


### [357] [ReMu: Reconstructing Multi-layer 3D Clothed Human from Image Layers](https://arxiv.org/abs/2508.01381)
*Onat Vuran,Hsuan-I Ho*

Main category: cs.GR

TL;DR: ReMu reconstructs multi-layer 3D garments from single RGB camera images, using a unified 3D representation and collision-aware optimization for realistic results.


<details>
  <summary>Details</summary>
Motivation: To simplify the creation of life-like clothed human avatars without expensive multi-view setups or specialized editing.

Method: Uses a single RGB camera (Image Layers setup), reconstructs and aligns garments in a shared coordinate system, and refines with collision-aware optimization and implicit neural fields.

Result: Produces nearly penetration-free 3D clothed humans, competitive with category-specific methods.

Conclusion: ReMu is a template-free, category-agnostic solution for diverse clothing styles, demonstrating practical viability.

Abstract: The reconstruction of multi-layer 3D garments typically requires expensive
multi-view capture setups and specialized 3D editing efforts. To support the
creation of life-like clothed human avatars, we introduce ReMu for
reconstructing multi-layer clothed humans in a new setup, Image Layers, which
captures a subject wearing different layers of clothing with a single RGB
camera. To reconstruct physically plausible multi-layer 3D garments, a unified
3D representation is necessary to model these garments in a layered manner.
Thus, we first reconstruct and align each garment layer in a shared coordinate
system defined by the canonical body pose. Afterwards, we introduce a
collision-aware optimization process to address interpenetration and further
refine the garment boundaries leveraging implicit neural fields. It is worth
noting that our method is template-free and category-agnostic, which enables
the reconstruction of 3D garments in diverse clothing styles. Through our
experiments, we show that our method reconstructs nearly penetration-free 3D
clothed humans and achieves competitive performance compared to
category-specific methods. Project page: https://eth-ait.github.io/ReMu/

</details>


### [358] [A Plug-and-Play Multi-Criteria Guidance for Diverse In-Betweening Human Motion Generation](https://arxiv.org/abs/2508.01590)
*Hua Yu,Jiao Liu,Xu Gui,Melvin Wong,Yaqing Hou,Yew-Soon Ong*

Main category: cs.GR

TL;DR: MCG-IMM is a plug-and-play method for diverse in-betweening human motion generation, enhancing pretrained models without extra parameters via multi-criteria guidance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of maintaining diversity in generated motion sequences while ensuring smooth transitions between keyframes.

Method: Reformulates sampling as a multi-criteria optimization problem, guiding pretrained models (e.g., diffusion models, VAEs, GANs) to explore sequences meeting diversity and smoothness criteria.

Result: Outperforms state-of-the-art methods on four human motion datasets.

Conclusion: MCG-IMM effectively enhances motion diversity and smoothness in a plug-and-play manner, compatible with various generative models.

Abstract: In-betweening human motion generation aims to synthesize intermediate motions
that transition between user-specified keyframes. In addition to maintaining
smooth transitions, a crucial requirement of this task is to generate diverse
motion sequences. It is still challenging to maintain diversity, particularly
when it is necessary for the motions within a generated batch sampling to
differ meaningfully from one another due to complex motion dynamics. In this
paper, we propose a novel method, termed the Multi-Criteria Guidance with
In-Betweening Motion Model (MCG-IMM), for in-betweening human motion
generation. A key strength of MCG-IMM lies in its plug-and-play nature: it
enhances the diversity of motions generated by pretrained models without
introducing additional parameters This is achieved by providing a sampling
process of pretrained generative models with multi-criteria guidance.
Specifically, MCG-IMM reformulates the sampling process of pretrained
generative model as a multi-criteria optimization problem, and introduces an
optimization process to explore motion sequences that satisfy multiple
criteria, e.g., diversity and smoothness. Moreover, our proposed plug-and-play
multi-criteria guidance is compatible with different families of generative
models, including denoised diffusion probabilistic models, variational
autoencoders, and generative adversarial networks. Experiments on four popular
human motion datasets demonstrate that MCG-IMM consistently state-of-the-art
methods in in-betweening motion generation task.

</details>


### [359] [Uncertainty Estimation for Novel Views in Gaussian Splatting from Primitive-Based Representations of Error and Visibility](https://arxiv.org/abs/2508.02443)
*Thomas Gottwald,Edgar Heinert,Matthias Rottmann*

Main category: cs.GR

TL;DR: A novel method for uncertainty estimation in Gaussian Splatting is introduced, focusing on error and visibility representations of primitives, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Uncertainty estimation is vital for critical applications like robotics and medicine, but existing methods rely on variance estimation, lacking meaningful uncertainty information.

Method: The method projects training error and visibility onto primitives, rendering uncertainty feature maps for novel views and aggregating them via pixel-wise regression on holdout data.

Result: The method outperforms state-of-the-art techniques, showing high correlation to true errors, especially on foreground objects, and generalizes to new scenes without holdout data.

Conclusion: The proposed method effectively estimates uncertainty in Gaussian Splatting, offering improved accuracy and generalization for critical applications.

Abstract: In this work, we present a novel method for uncertainty estimation (UE) in
Gaussian Splatting. UE is crucial for using Gaussian Splatting in critical
applications such as robotics and medicine. Previous methods typically estimate
the variance of Gaussian primitives and use the rendering process to obtain
pixel-wise uncertainties. Our method establishes primitive representations of
error and visibility of trainings views, which carries meaningful uncertainty
information. This representation is obtained by projection of training error
and visibility onto the primitives. Uncertainties of novel views are obtained
by rendering the primitive representations of uncertainty for those novel
views, yielding uncertainty feature maps. To aggregate these uncertainty
feature maps of novel views, we perform a pixel-wise regression on holdout
data. In our experiments, we analyze the different components of our method,
investigating various combinations of uncertainty feature maps and regression
models. Furthermore, we considered the effect of separating splatting into
foreground and background. Our UEs show high correlations to true errors,
outperforming state-of-the-art methods, especially on foreground objects. The
trained regression models show generalization capabilities to new scenes,
allowing uncertainty estimation without the need for holdout data.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [360] [FairFedMed: Benchmarking Group Fairness in Federated Medical Imaging with FairLoRA](https://arxiv.org/abs/2508.00873)
*Minghan Li,Congcong Wen,Yu Tian,Min Shi,Yan Luo,Hao Huang,Yi Fang,Mengyu Wang*

Main category: cs.CY

TL;DR: The paper introduces FairFedMed, a benchmark for fairness in medical Federated Learning (FL), and proposes FairLoRA, a fairness-aware FL framework that improves both performance and fairness in medical image classification.


<details>
  <summary>Details</summary>
Motivation: Addressing fairness in healthcare FL is critical due to unequal access and outcomes, yet current FL research lacks focus on medical applications and demographic fairness.

Method: The authors create FairFedMed, a dataset for fairness in medical FL, and propose FairLoRA, a framework using SVD-based low-rank approximation to customize models per demographic group while maintaining efficiency.

Result: FairLoRA achieves state-of-the-art performance in medical image classification and significantly improves fairness across diverse populations.

Conclusion: The work fills a gap in fairness-aware FL for healthcare, providing a benchmark and framework that enhances both accuracy and equity in medical applications.

Abstract: Fairness remains a critical concern in healthcare, where unequal access to
services and treatment outcomes can adversely affect patient health. While
Federated Learning (FL) presents a collaborative and privacy-preserving
approach to model training, ensuring fairness is challenging due to
heterogeneous data across institutions, and current research primarily
addresses non-medical applications. To fill this gap, we establish the first
experimental benchmark for fairness in medical FL, evaluating six
representative FL methods across diverse demographic attributes and imaging
modalities. We introduce FairFedMed, the first medical FL dataset specifically
designed to study group fairness (i.e., demographics). It comprises two parts:
FairFedMed-Oph, featuring 2D fundus and 3D OCT ophthalmology samples with six
demographic attributes; and FairFedMed-Chest, which simulates real
cross-institutional FL using subsets of CheXpert and MIMIC-CXR. Together, they
support both simulated and real-world FL across diverse medical modalities and
demographic groups. Existing FL models often underperform on medical images and
overlook fairness across demographic groups. To address this, we propose
FairLoRA, a fairness-aware FL framework based on SVD-based low-rank
approximation. It customizes singular value matrices per demographic group
while sharing singular vectors, ensuring both fairness and efficiency.
Experimental results on the FairFedMed dataset demonstrate that FairLoRA not
only achieves state-of-the-art performance in medical image classification but
also significantly improves fairness across diverse populations. Our code and
dataset can be accessible via link: https://wang.hms.harvard.edu/fairfedmed/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [361] [Hallucination Detection and Mitigation with Diffusion in Multi-Variate Time-Series Foundation Models](https://arxiv.org/abs/2508.00881)
*Vijja Wichitwechkarn,Charles Fox,Ruchi Choudhary*

Main category: cs.LG

TL;DR: The paper introduces definitions and methods for detecting and mitigating hallucinations in multi-variate time-series (MVTS) foundation models, achieving significant reduction in hallucination levels.


<details>
  <summary>Details</summary>
Motivation: Existing definitions and methods for hallucination in natural language processing do not extend to MVTS foundation models, necessitating new approaches.

Method: Proposes new definitions for MVTS hallucination and uses a diffusion model for detection and mitigation, benchmarking on relational datasets.

Result: Pre-trained MVTS models hallucinate up to 59.5% compared to a baseline; mitigation reduces this by up to 47.7%.

Conclusion: The new definitions and methods can enhance the adoption and safe use of MVTS foundation models.

Abstract: Foundation models for natural language processing have many coherent
definitions of hallucination and methods for its detection and mitigation.
However, analogous definitions and methods do not exist for multi-variate
time-series (MVTS) foundation models. We propose new definitions for MVTS
hallucination, along with new detection and mitigation methods using a
diffusion model to estimate hallucination levels. We derive relational datasets
from popular time-series datasets to benchmark these relational hallucination
levels. Using these definitions and models, we find that open-source
pre-trained MVTS imputation foundation models relationally hallucinate on
average up to 59.5% as much as a weak baseline. The proposed mitigation method
reduces this by up to 47.7% for these models. The definition and methods may
improve adoption and safe usage of MVTS foundation models.

</details>


### [362] [Filtering with Self-Attention and Storing with MLP: One-Layer Transformers Can Provably Acquire and Extract Knowledge](https://arxiv.org/abs/2508.00901)
*Ruichen Xu,Kexin Chen*

Main category: cs.LG

TL;DR: The paper investigates how transformers acquire and extract knowledge, introducing a one-layer framework with self-attention and MLPs to analyze training dynamics and prove convergence and generalization guarantees.


<details>
  <summary>Details</summary>
Motivation: To understand the opaque mechanisms of knowledge acquisition and extraction in transformers, especially given prior limitations of simplified models.

Method: A tractable one-layer transformer framework with self-attention and MLPs, analyzed through gradient dynamics for convergence and generalization guarantees.

Result: Transformers achieve near-optimal training loss (knowledge acquisition) and low generalization error under specific conditions (knowledge extraction), but fail otherwise, leading to hallucinations.

Conclusion: The framework provides theoretical insights into transformer behavior, validated by experiments on synthetic and real-world datasets.

Abstract: Modern large language models excel in knowledge-intensive tasks, yet how
transformers acquire (store) knowledge during pre-training and extract
(retrieve) it during post-fine-tuning inference remains theoretically opaque.
While prior theoretical work has begun to investigate these questions through
the analysis of training dynamics, such studies are limited to single-layer,
attention-only architectures. However, most existing studies suggest that MLPs
are the most contributing components for storing knowledge in transformer-based
language models. Meanwhile, our empirical investigations reveal that such
simplified models, when trained using standard next-token prediction
objectives, may be incapable of acquiring or extracting factual knowledge. To
overcome this limitation, we introduce a tractable one-layer transformer
framework that crucially incorporates both self-attention and MLP modules. By
tracking its gradient dynamics, we establish convergence and generalization
guarantees that illuminate the ability of knowledge acquisition and extraction.
We prove that 1) Transformers can achieve near-optimal training loss during
pre-training, signifying effective knowledge acquisition; 2) With a large
fine-tuning dataset and specific data multiplicity conditions met, transformers
can achieve low generalization error when tested on factual knowledge learned
during pre-training but not reinforced during the fine-tuning, indicating
successful knowledge extraction; 3) When the conditions are not satisfied,
transformers exhibit high generalization loss, resulting in hallucinations. Our
analysis includes both full fine-tuning and low-rank fine-tuning. Furthermore,
our analysis offers theoretical insights into several pertinent empirical
phenomena, such as the role of learning rate schedules. Experiments on
synthetic and real-world PopQA datasets with GPT-2 and Llama-3.2-1B validate
our results.

</details>


### [363] [Small sample-based adaptive text classification through iterative and contrastive description refinement](https://arxiv.org/abs/2508.00957)
*Amrit Rajeev,Udayaadithya Avadhanam,Harshula Tulapurkar,SaiBarath Sundar*

Main category: cs.LG

TL;DR: A framework combining iterative topic refinement, contrastive prompting, and active learning improves zero-shot text classification in dynamic domains with evolving knowledge.


<details>
  <summary>Details</summary>
Motivation: Zero-shot classification is challenging in domains like ticketing systems due to evolving knowledge and ambiguous categories. LLMs struggle with generalization, and few-shot methods lack data diversity.

Method: The framework uses iterative contrastive prompting to refine category distinctions, starting with labeled samples and involving human-in-the-loop for category updates.

Result: Achieves 91% accuracy on AGNews (3 seen, 1 unseen class) and 84% on DBpedia (8 seen, 1 unseen), with minimal accuracy drop for unseen classes.

Conclusion: Prompt-based semantic reasoning is effective for fine-grained classification with limited supervision, suitable for dynamic environments.

Abstract: Zero-shot text classification remains a difficult task in domains with
evolving knowledge and ambiguous category boundaries, such as ticketing
systems. Large language models (LLMs) often struggle to generalize in these
scenarios due to limited topic separability, while few-shot methods are
constrained by insufficient data diversity. We propose a classification
framework that combines iterative topic refinement, contrastive prompting, and
active learning. Starting with a small set of labeled samples, the model
generates initial topic labels. Misclassified or ambiguous samples are then
used in an iterative contrastive prompting process to refine category
distinctions by explicitly teaching the model to differentiate between closely
related classes. The framework features a human-in-the-loop component, allowing
users to introduce or revise category definitions in natural language. This
enables seamless integration of new, unseen categories without retraining,
making the system well-suited for real-world, dynamic environments. The
evaluations on AGNews and DBpedia demonstrate strong performance: 91% accuracy
on AGNews (3 seen, 1 unseen class) and 84% on DBpedia (8 seen, 1 unseen), with
minimal accuracy shift after introducing unseen classes (82% and 87%,
respectively). The results highlight the effectiveness of prompt-based semantic
reasoning for fine-grained classification with limited supervision.

</details>


### [364] [Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models](https://arxiv.org/abs/2508.01908)
*Istabrak Abbes,Gopeshh Subbaraj,Matthew Riemer,Nizar Islah,Benjamin Therien,Tsuguchika Tabaru,Hiroaki Kingetsu,Sarath Chandar,Irina Rish*

Main category: cs.LG

TL;DR: The paper explores continual pre-training for LLMs to avoid retraining from scratch, evaluating experience replay and gradient alignment to mitigate distribution shifts. Both methods stabilize learning without forgetting, with gradient alignment being newly effective in LLM pre-training. Efficient MER is proposed, and scaling analysis suggests replaying old examples is more valuable than increasing model size.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of retraining LLMs from scratch when new data arrives, the paper investigates continual pre-training methods to prevent performance degradation due to distribution shifts.

Method: The study evaluates experience replay and gradient alignment in continual pre-training of Llama-family models at scale (100B tokens per language). It also proposes an efficient MER implementation combining replay with gradient alignment.

Result: Both replay and gradient alignment stabilize learning without forgetting. Gradient alignment is newly effective for LLMs. Scaling analysis shows replaying old examples is more compute-efficient than increasing model size.

Conclusion: Continual pre-training with replay and gradient alignment is effective for LLMs, with replay being more valuable than model scaling. Efficient MER combines their benefits with minimal overhead.

Abstract: Training large language models (LLMs) typically involves pre-training on
massive corpora, only to restart the process entirely when new data becomes
available. A more efficient and resource-conserving approach would be continual
pre-training, where models are updated with new data rather than retraining
from scratch. However, the introduction of new data often causes distribution
shifts, leading to performance degradation on previously learned tasks. In this
paper, we take a deeper look at two popular proposals for addressing this
distribution shift within the continual learning literature: experience replay
and gradient alignment. We consider continual pre-training of models within the
Llama family of architectures at a large scale across languages with 100
billion tokens of training data in each language, finding that both replay and
gradient alignment lead to more stable learning without forgetting. This
conclusion holds both as we vary the model scale and as we vary the number and
diversity of tasks. Moreover, we are the first to demonstrate the effectiveness
of gradient alignment techniques in the context of LLM pre-training and propose
an efficient implementation of meta-experience replay (MER) that imbues
experience replay with the benefits of gradient alignment despite negligible
compute and memory overhead. Our scaling analysis across model sizes and replay
rates indicates that small rates of replaying old examples are definitely a
more valuable use of compute than investing in model size, but that it is more
compute efficient to scale the size of the model than invest in high rates of
replaying old examples.

</details>


### [365] [Decomposing Representation Space into Interpretable Subspaces with Unsupervised Learning](https://arxiv.org/abs/2508.01916)
*Xinting Huang,Michael Hahn*

Main category: cs.LG

TL;DR: The paper introduces Neighbor Distance Minimization (NDM), an unsupervised method to find interpretable subspaces in neural models, revealing organized representations of abstract concepts.


<details>
  <summary>Details</summary>
Motivation: To explore how neural models organize and encode different aspects of inputs in separate subspaces, and whether these subspaces can be identified unsupervised.

Method: Proposes Neighbor Distance Minimization (NDM) to learn non-basis-aligned subspaces without supervision.

Result: NDM finds interpretable subspaces, often corresponding to abstract concepts, and shows strong alignment with known circuit variables in GPT-2.

Conclusion: NDM provides a scalable, unsupervised approach to understanding model internals and circuit construction.

Abstract: Understanding internal representations of neural models is a core interest of
mechanistic interpretability. Due to its large dimensionality, the
representation space can encode various aspects about inputs. To what extent
are different aspects organized and encoded in separate subspaces? Is it
possible to find these ``natural'' subspaces in a purely unsupervised way?
Somewhat surprisingly, we can indeed achieve this and find interpretable
subspaces by a seemingly unrelated training objective. Our method, neighbor
distance minimization (NDM), learns non-basis-aligned subspaces in an
unsupervised manner. Qualitative analysis shows subspaces are interpretable in
many cases, and encoded information in obtained subspaces tends to share the
same abstract concept across different inputs, making such subspaces similar to
``variables'' used by the model. We also conduct quantitative experiments using
known circuits in GPT-2; results show a strong connection between subspaces and
circuit variables. We also provide evidence showing scalability to 2B models by
finding separate subspaces mediating context and parametric knowledge routing.
Viewed more broadly, our findings offer a new perspective on understanding
model internals and building circuits.

</details>


### [366] [MolReasoner: Toward Effective and Interpretable Reasoning for Molecular LLMs](https://arxiv.org/abs/2508.02066)
*Guojiang Zhao,Sihang Li,Zixiang Lu,Zheng Cheng,Haitao Lin,Lirong Wu,Hanchen Xia,Hengxing Cai,Wentao Guo,Hongshuai Wang,Mingjun Xu,Siyu Zhu,Guolin Ke,Linfeng Zhang,Zhifeng Gao*

Main category: cs.LG

TL;DR: MolReasoner is a two-stage framework (Mol-SFT and Mol-RL) that enhances LLMs' molecular reasoning by combining synthetic CoT samples and reinforcement learning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current LLM approaches for molecular reasoning lack domain-specific semantics and struggle with interpretability and reasoning depth.

Method: 1. Mol-SFT: Uses GPT-4o-generated synthetic CoT samples to initialize reasoning. 2. Mol-RL: Applies reinforcement learning with chemical-specific rewards.

Result: MolReasoner improves interpretability, molecular understanding, and generalization, outperforming existing methods.

Conclusion: MolReasoner shifts LLMs from memorization to robust chemical reasoning, marking a significant advancement.

Abstract: Large Language Models(LLMs) have demonstrated remarkable performance across
various domains, yet their capabilities in molecular reasoning remain
insufficiently explored. Current approaches tend to rely heavily on
general-purpose prompting, which lacks domain-specific molecular semantics,
while those that use fine-tuning strategies often face challenges with
interpretability and reasoning depth. To address these issues, we introduce
MolReasoner, a two-stage framework designed to transition LLMs from
memorization towards chemical reasoning. First, we propose Mol-SFT, which
initializes the model's reasoning abilities via synthetic Chain-of-Thought(CoT)
samples generated by GPT-4o and verified for chemical accuracy. Subsequently,
Mol-RL applies reinforcement learning with specialized reward functions
designed explicitly to align chemical structures with linguistic descriptions,
thereby enhancing molecular reasoning capabilities. Our approach notably
enhances interpretability, improving the model 's molecular understanding and
enabling better generalization. Extensive experiments demonstrate that
MolReasoner outperforms existing methods, and marking a significant shift from
memorization-based outputs to robust chemical reasoning.

</details>


### [367] [CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search](https://arxiv.org/abs/2508.02091)
*Xiaoya Li,Xiaofei Sun,Albert Wang,Chris Shum,Jiwei Li*

Main category: cs.LG

TL;DR: CRINN introduces a reinforcement learning-based approach for optimizing ANNS algorithms, achieving top performance on several benchmarks.


<details>
  <summary>Details</summary>
Motivation: To automate the optimization of ANNS algorithms, which are crucial for AI applications like RAG and agent-based LLMs, by leveraging reinforcement learning.

Method: CRINN treats ANNS optimization as a reinforcement learning problem, using execution speed as the reward signal to generate faster implementations while maintaining accuracy.

Result: CRINN outperforms state-of-the-art ANNS algorithms on three benchmarks and ties on two others.

Conclusion: CRINN demonstrates that LLMs augmented with reinforcement learning can automate complex algorithmic optimizations, extending beyond ANNS.

Abstract: Approximate nearest-neighbor search (ANNS) algorithms have become
increasingly critical for recent AI applications, particularly in
retrieval-augmented generation (RAG) and agent-based LLM applications. In this
paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS
optimization as a reinforcement learning problem where execution speed serves
as the reward signal. This approach enables the automatic generation of
progressively faster ANNS implementations while maintaining accuracy
constraints. Our experimental evaluation demonstrates CRINN's effectiveness
across six widely-used NNS benchmark datasets. When compared against
state-of-the-art open-source ANNS algorithms, CRINN achieves best performance
on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and
GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean
and GloVe-25-angular). The implications of CRINN's success reach well beyond
ANNS optimization: It validates that LLMs augmented with reinforcement learning
can function as an effective tool for automating sophisticated algorithmic
optimizations that demand specialized knowledge and labor-intensive manual
refinement.Code can be found at https://github.com/deepreinforce-ai/CRINN

</details>


### [368] [LeanK: Learnable K Cache Channel Pruning for Efficient Decoding](https://arxiv.org/abs/2508.02215)
*Yike Zhang,Zhiyuan He,Huiqiang Jiang,Chengruidong Zhang,Yuqing Yang,Jianyong Wang,Lili Qiu*

Main category: cs.LG

TL;DR: LeanK is a learning-based method to prune unimportant key (K) cache channels in LLMs, reducing memory and speeding up decoding without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) face efficiency challenges due to growing key-value (KV) cache, prompting the need for optimization.

Method: LeanK uses a two-stage training process to learn static channel masks, pruning K cache channels while meeting sparsity and hardware alignment requirements.

Result: Experiments show 70% K cache and 16%-18% V cache memory reduction, with a 1.3x speedup in attention computation.

Conclusion: LeanK effectively optimizes LLM efficiency, offering insights into model channels and attention heads during long-context inference.

Abstract: Large language models (LLMs) enable long-context tasks but face efficiency
challenges due to the growing key-value (KV) cache. We propose LeanK, a
learning-based method that prunes unimportant key (K) cache channels by
leveraging static channel sparsity. With a novel two-stage training process,
LeanK learns channel-wise static mask that could satisfy specific sparsity
ratio and hardware alignment requirement. LeanK reduces GPU memory and
accelerates decoding without sacrificing accuracy. Experiments demonstrate up
to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel
enables 1.3x speedup for attention computation. We also provide insights into
model channels and attention heads during long-context inference by analyzing
the learned importance distribution. Our code is available at
https://aka.ms/LeanK.

</details>


### [369] [CellForge: Agentic Design of Virtual Cell Models](https://arxiv.org/abs/2508.02276)
*Xiangru Tang,Zhuoyun Yu,Jiapeng Chen,Yan Cui,Daniel Shao,Weixu Wang,Fang Wu,Yuchen Zhuang,Wenqi Shi,Zhi Huang,Arman Cohan,Xihong Lin,Fabian Theis,Smita Krishnaswamy,Mark Gerstein*

Main category: cs.LG

TL;DR: CellForge is an AI-driven system that autonomously builds optimized computational models for virtual cells using multi-agent collaboration, outperforming state-of-the-art methods in single-cell perturbation prediction.


<details>
  <summary>Details</summary>
Motivation: The complexity of biological systems and the need for interdisciplinary expertise make autonomous virtual cell modeling challenging. CellForge aims to address this by leveraging multi-agent AI.

Method: CellForge uses a multi-agent framework with three modules: Task Analysis, Method Design (with specialized agents and a moderator), and Experiment Execution to transform raw data into optimized models.

Result: CellForge outperforms state-of-the-art methods in single-cell perturbation prediction across diverse datasets, demonstrating the effectiveness of multi-agent collaboration.

Conclusion: CellForge shows that iterative interaction among AI agents with diverse perspectives yields superior solutions for virtual cell modeling.

Abstract: Virtual cell modeling represents an emerging frontier at the intersection of
artificial intelligence and biology, aiming to predict quantities such as
responses to diverse perturbations quantitatively. However, autonomously
building computational models for virtual cells is challenging due to the
complexity of biological systems, the heterogeneity of data modalities, and the
need for domain-specific expertise across multiple disciplines. Here, we
introduce CellForge, an agentic system that leverages a multi-agent framework
that transforms presented biological datasets and research objectives directly
into optimized computational models for virtual cells. More specifically, given
only raw single-cell multi-omics data and task descriptions as input, CellForge
outputs both an optimized model architecture and executable code for training
virtual cell models and inference. The framework integrates three core modules:
Task Analysis for presented dataset characterization and relevant literature
retrieval, Method Design, where specialized agents collaboratively develop
optimized modeling strategies, and Experiment Execution for automated
generation of code. The agents in the Design module are separated into experts
with differing perspectives and a central moderator, and have to
collaboratively exchange solutions until they achieve a reasonable consensus.
We demonstrate CellForge's capabilities in single-cell perturbation prediction,
using six diverse datasets that encompass gene knockouts, drug treatments, and
cytokine stimulations across multiple modalities. CellForge consistently
outperforms task-specific state-of-the-art methods. Overall, CellForge
demonstrates how iterative interaction between LLM agents with differing
perspectives provides better solutions than directly addressing a modeling
challenge. Our code is publicly available at
https://github.com/gersteinlab/CellForge.

</details>


### [370] [CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative Credit Assignment](https://arxiv.org/abs/2508.02298)
*Guofu Xie,Yunsheng Shi,Hongtao Tian,Ting Yao,Xiao Zhang*

Main category: cs.LG

TL;DR: CAPO introduces fine-grained credit assignment in RLVR by using an LLM as a Generative Process Reward Model, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current RLVR methods assign coarse-grained rewards, hindering precise credit assignment and leading to suboptimal policies.

Method: CAPO leverages an LLM (LLM-as-GenPRM) to generate step-wise critiques for token-level rewards, enhancing credit assignment.

Result: CAPO outperforms supervised and RL-based methods across multiple benchmarks, including mathematical and out-of-domain tasks.

Conclusion: CAPO provides efficient, verifiable, and fine-grained credit assignment, improving reasoning in LLMs.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has improved the
reasoning abilities of Large Language Models (LLMs) by using rule-based binary
feedback, helping to mitigate reward hacking. However, current RLVR methods
typically treat whole responses as single actions, assigning the same reward to
every token. This coarse-grained feedback hampers precise credit assignment,
making it hard for models to identify which reasoning steps lead to success or
failure, and often results in suboptimal policies and inefficient learning.
Methods like PPO provide credit assignment through value estimation, but often
yield inaccurate and unverifiable signals due to limited sampling. On the other
hand, methods using Process Reward Models can provide step-by-step judgments
for each reasoning step, but they require high-quality process supervision
labels and are time-consuming when applied in online reinforcement learning
(RL). To overcome these limitations, we introduce a simple but efficient method
Credit Assignment Policy Optimization (CAPO). Given a reasoning response
rollout from the policy model, CAPO directly leverages an off-the-shelf,
general-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to
generate all step-wise critique by one pass, thereby providing verifiable
token-level rewards to refine the tokens that were originally assigned
identical rule-based rewards. This enables more fine-grained credit assignment
in an effective way. Furthermore, to enhance the accuracy and robustness of
CAPO, we employ voting mechanisms that scale with the number of generated
critiques. Extensive experiments using different backbones like Llama and Qwen
models and in different sizes show that CAPO consistently outperforms
supervised learning-based and RL-based fine-tuning methods across six
challenging mathematical benchmarks and three out-of-domain benchmarks.

</details>


### [371] [Language Model Guided Reinforcement Learning in Quantitative Trading](https://arxiv.org/abs/2508.02366)
*Adam Darmanin,Vince Vella*

Main category: cs.LG

TL;DR: A hybrid system combining LLMs and RL improves trading performance by leveraging LLMs for strategic guidance and RL for tactical execution.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of RL in algorithmic trading, such as myopic behavior and lack of transparency, by integrating LLMs for strategic reasoning.

Method: Use LLMs to generate high-level trading strategies and guide RL agents. Evaluate strategy rationale via expert review and performance via Sharpe Ratio and Maximum Drawdown.

Result: LLM-guided agents outperform standard RL in return and risk metrics.

Conclusion: The hybrid approach enhances trading performance by combining the strengths of LLMs and RL.

Abstract: Algorithmic trading requires short-term decisions aligned with long-term
financial goals. While reinforcement learning (RL) has been explored for such
tactical decisions, its adoption remains limited by myopic behavior and opaque
policy rationale. In contrast, large language models (LLMs) have recently
demonstrated strategic reasoning and multi-modal financial signal
interpretation when guided by well-designed prompts.
  We propose a hybrid system where LLMs generate high-level trading strategies
to guide RL agents in their actions. We evaluate (i) the rationale of
LLM-generated strategies via expert review, and (ii) the Sharpe Ratio (SR) and
Maximum Drawdown (MDD) of LLM-guided agents versus unguided baselines. Results
show improved return and risk metrics over standard RL.

</details>


### [372] [What are you sinking? A geometric approach on attention sink](https://arxiv.org/abs/2508.02546)
*Valeria Ruscio,Umberto Nanni,Fabrizio Silvestri*

Main category: cs.LG

TL;DR: Attention sink (AS) in transformers is a fundamental geometric principle for establishing reference frames, not just an artifact. It emerges early in training and is influenced by architecture components like position encoding.


<details>
  <summary>Details</summary>
Motivation: To understand the underlying geometric principles behind the attention sink phenomenon in transformers and its role in establishing reference frames.

Method: Analyzed various transformer architectures to identify three types of reference frames (centralized, distributed, bidirectional) and studied their emergence during training.

Result: Attention sink is a manifestation of optimal solutions for stable coordinate systems in high-dimensional spaces, influenced by architecture components like position encoding.

Conclusion: This study redefines transformer attention mechanisms, offering insights for architecture design and understanding the relationship with attention sink.

Abstract: Attention sink (AS) is a consistent pattern in transformer attention maps
where certain tokens (often special tokens or positional anchors)
disproportionately attract attention from other tokens. We show that in
transformers, AS is not an architectural artifact, but it is the manifestation
of a fundamental geometric principle: the establishment of reference frames
that anchor representational spaces. We analyze several architectures and
identify three distinct reference frame types, centralized, distributed, and
bidirectional, that correlate with the attention sink phenomenon. We show that
they emerge during the earliest stages of training as optimal solutions to the
problem of establishing stable coordinate systems in high-dimensional spaces.
We show the influence of architecture components, particularly position
encoding implementations, on the specific type of reference frame. This
perspective transforms our understanding of transformer attention mechanisms
and provides insights for both architecture design and the relationship with
AS.

</details>


### [373] [Parameter-Efficient Routed Fine-Tuning: Mixture-of-Experts Demands Mixture of Adaptation Modules](https://arxiv.org/abs/2508.02587)
*Yilun Liu,Yunpu Ma,Yuetian Lu,Shuo Chen,Zifeng Ding,Volker Tresp*

Main category: cs.LG

TL;DR: The paper explores integrating routing mechanisms into Parameter-Efficient Fine-Tuning (PEFT) for Mixture-of-Experts (MoE) models, showing improved performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT strategies do not leverage MoE's dynamic routing, prompting investigation into routed adaptation modules.

Method: Analyzed PEFT components in MoE models, tested routing strategies, and experimented with OLMoE-1B-7B and Mixtral-8x7B on reasoning tasks.

Result: Validated the effectiveness of routed PEFT, identified optimal configurations, and provided practical insights.

Conclusion: Routed PEFT aligns better with MoE architecture, enhancing performance and efficiency, with empirical support for practical applications.

Abstract: Mixture-of-Experts (MoE) benefits from a dynamic routing mechanism among
their specialized experts, which existing Parameter- Efficient Fine-Tuning
(PEFT) strategies fail to leverage. This motivates us to investigate whether
adaptation modules themselves should incorporate routing mechanisms to align
with MoE's multi-expert architecture. We analyze dynamics of core components
when applying PEFT to MoE language models and examine how different routing
strategies affect adaptation effectiveness. Extensive experiments adapting
OLMoE-1B-7B and Mixtral-8x7B on various commonsense and math reasoning tasks
validate the performance and efficiency of our routed approach. We identify the
optimal configurations for different scenarios and provide empirical analyses
with practical insights to facilitate better PEFT and MoE applications.

</details>


### [374] [FRAM: Frobenius-Regularized Assignment Matching with Mixed-Precision Computing](https://arxiv.org/abs/2508.00887)
*Binrui Shen,Yuan Liang,Shengxin Zhu*

Main category: cs.LG

TL;DR: The paper proposes a novel relaxation framework (FRA) for graph matching, addressing errors in existing methods by reformulating the problem as a Frobenius-regularized Linear Assignment problem. The SDSN algorithm and mixed-precision architecture enhance efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing projection-based relaxations for graph matching introduce errors due to numerical scale sensitivity and geometric misalignment. The goal is to mitigate these issues while maintaining accuracy.

Method: The paper introduces the FRA framework, which reformulates the problem as a Frobenius-regularized Linear Assignment problem. The SDSN algorithm is proposed for efficient solving, alongside a mixed-precision architecture for acceleration.

Result: FRAM outperforms baseline methods in CPU benchmarks and achieves up to 370X speedup with GPU-based mixed-precision, with minimal accuracy loss.

Conclusion: The FRA framework and SDSN algorithm effectively address errors in existing methods, offering improved accuracy and significant computational speedup.

Abstract: Graph matching, typically formulated as a Quadratic Assignment Problem (QAP),
seeks to establish node correspondences between two graphs. To address the
NP-hardness of QAP, some existing methods adopt projection-based relaxations
that embed the problem into the convex hull of the discrete domain. However,
these relaxations inevitably enlarge the feasible set, introducing two sources
of error: numerical scale sensitivity and geometric misalignment between the
relaxed and original domains. To alleviate these errors, we propose a novel
relaxation framework by reformulating the projection step as a
Frobenius-regularized Linear Assignment (FRA) problem, where a tunable
regularization term mitigates feasible region inflation. This formulation
enables normalization-based operations to preserve numerical scale invariance
without compromising accuracy. To efficiently solve FRA, we propose the Scaling
Doubly Stochastic Normalization (SDSN) algorithm. Building on its favorable
computational properties, we develop a theoretically grounded mixed-precision
architecture to achieve substantial acceleration. Comprehensive CPU-based
benchmarks demonstrate that FRAM consistently outperforms all baseline methods
under identical precision settings. When combined with a GPU-based
mixed-precision architecture, FRAM achieves up to 370X speedup over its
CPU-FP64 counterpart, with negligible loss in solution accuracy.

</details>


### [375] [IMU: Influence-guided Machine Unlearning](https://arxiv.org/abs/2508.01620)
*Xindi Fan,Jing Wu,Mingyi Zhou,Pengwei Liang,Dinh Phung*

Main category: cs.LG

TL;DR: The paper introduces Influence-guided Machine Unlearning (IMU), a method for selective data forgetting in deep learning models without needing the original training data.


<details>
  <summary>Details</summary>
Motivation: Deep learning models' vulnerability to attacks and data memorization raises privacy concerns, necessitating effective machine unlearning (MU) methods that don't rely on the original data.

Method: IMU uses gradient ascent and dynamically allocates unlearning intensities based on data point influences, requiring only the forget set.

Result: IMU outperforms existing retain-data-free MU methods in vision and language tasks.

Conclusion: IMU is a practical and effective solution for machine unlearning, enhancing privacy without compromising model utility.

Abstract: Recent studies have shown that deep learning models are vulnerable to attacks
and tend to memorize training data points, raising significant concerns about
privacy leakage. This motivates the development of machine unlearning (MU),
i.e., a paradigm that enables models to selectively forget specific data points
upon request. However, most existing MU algorithms require partial or full
fine-tuning on the retain set. This necessitates continued access to the
original training data, which is often impractical due to privacy concerns and
storage constraints. A few retain-data-free MU methods have been proposed, but
some rely on access to auxiliary data and precomputed statistics of the retain
set, while others scale poorly when forgetting larger portions of data. In this
paper, we propose Influence-guided Machine Unlearning (IMU), a simple yet
effective method that conducts MU using only the forget set. Specifically, IMU
employs gradient ascent and innovatively introduces dynamic allocation of
unlearning intensities across different data points based on their influences.
This adaptive strategy significantly enhances unlearning effectiveness while
maintaining model utility. Results across vision and language tasks demonstrate
that IMU consistently outperforms existing retain-data-free MU methods.

</details>


### [376] [Imbalance-Robust and Sampling-Efficient Continuous Conditional GANs via Adaptive Vicinity and Auxiliary Regularization](https://arxiv.org/abs/2508.01725)
*Xin Ding,Yun Chen,Yongwei Wang,Kao Zhang,Sen Zhang,Peibei Cao,Xiangxue Wang*

Main category: cs.LG

TL;DR: CcGAN-AVAR improves CcGAN by addressing data imbalance and sampling inefficiency, outperforming CCDM in speed and quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods (CcGAN and CCDM) have limitations: CcGAN suffers from data imbalance, and CCDM is computationally expensive.

Method: CcGAN-AVAR introduces adaptive vicinity and a multi-task discriminator for better training, leveraging GAN's one-step generation.

Result: Achieves 300x-2000x faster inference than CCDM and superior generation quality in imbalanced settings.

Conclusion: CcGAN-AVAR sets a new benchmark for conditional generative models in efficiency and quality.

Abstract: Recent advances in conditional generative modeling have introduced Continuous
conditional Generative Adversarial Network (CcGAN) and Continuous Conditional
Diffusion Model (CCDM) for estimating high-dimensional data distributions
conditioned on scalar, continuous regression labels (e.g., angles, ages, or
temperatures). However, these approaches face fundamental limitations: CcGAN
suffers from data imbalance due to fixed-size vicinity constraints, while CCDM
requires computationally expensive iterative sampling. We present CcGAN-AVAR,
an enhanced CcGAN framework that addresses both challenges: (1) leveraging the
GAN framework's native one-step generation to overcome CCDMs' sampling
bottleneck (achieving 300x-2000x faster inference), while (2) two novel
components specifically target data imbalance - an adaptive vicinity mechanism
that dynamically adjusts vicinity's size, and a multi-task discriminator that
constructs two regularization terms (through auxiliary regression and density
ratio estimation) to significantly improve generator training. Extensive
experiments on four benchmark datasets (64x64 to 192x192 resolution) across
eight challenging imbalanced settings demonstrate that CcGAN-AVAR achieves
state-of-the-art generation quality while maintaining sampling efficiency.

</details>


### [377] [OccamVTS: Distilling Vision Models to 1% Parameters for Time Series Forecasting](https://arxiv.org/abs/2508.01727)
*Sisuo Lyu,Siru Zhong,Weilin Ruan,Qingxiang Liu,Qingsong Wen,Hui Xiong,Yuxuan Liang*

Main category: cs.LG

TL;DR: OccamVTS distills only 1% of essential parameters from large vision models (LVMs) for time series forecasting, improving accuracy by avoiding irrelevant visual features.


<details>
  <summary>Details</summary>
Motivation: Current LVMs for time series forecasting are inefficient, as 99% of their parameters are unnecessary and can impair accuracy by capturing irrelevant high-level semantics.

Method: Proposes OccamVTS, a knowledge distillation framework using pyramid-style feature alignment and correlation distillation to transfer only useful low-level features from LVMs to lightweight networks.

Result: Achieves state-of-the-art performance with 1% of original parameters, excelling in few-shot and zero-shot scenarios.

Conclusion: Aggressive parameter reduction in LVMs enhances forecasting accuracy by focusing on essential temporal patterns, validated across multiple benchmarks.

Abstract: Time series forecasting is fundamental to diverse applications, with recent
approaches leverage large vision models (LVMs) to capture temporal patterns
through visual representations. We reveal that while vision models enhance
forecasting performance, 99% of their parameters are unnecessary for time
series tasks. Through cross-modal analysis, we find that time series align with
low-level textural features but not high-level semantics, which can impair
forecasting accuracy. We propose OccamVTS, a knowledge distillation framework
that extracts only the essential 1% of predictive information from LVMs into
lightweight networks. Using pre-trained LVMs as privileged teachers, OccamVTS
employs pyramid-style feature alignment combined with correlation and feature
distillation to transfer beneficial patterns while filtering out semantic
noise. Counterintuitively, this aggressive parameter reduction improves
accuracy by eliminating overfitting to irrelevant visual features while
preserving essential temporal patterns. Extensive experiments across multiple
benchmark datasets demonstrate that OccamVTS consistently achieves
state-of-the-art performance with only 1% of the original parameters,
particularly excelling in few-shot and zero-shot scenarios.

</details>


### [378] [$ε$-Softmax: Approximating One-Hot Vectors for Mitigating Label Noise](https://arxiv.org/abs/2508.02387)
*Jialiang Wang,Xiong Zhou,Deming Zhai,Junjun Jiang,Xiangyang Ji,Xianming Liu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Noisy labels pose a common challenge for training accurate deep neural
networks. To mitigate label noise, prior studies have proposed various robust
loss functions to achieve noise tolerance in the presence of label noise,
particularly symmetric losses. However, they usually suffer from the
underfitting issue due to the overly strict symmetric condition. In this work,
we propose a simple yet effective approach for relaxing the symmetric
condition, namely $\epsilon$-softmax, which simply modifies the outputs of the
softmax layer to approximate one-hot vectors with a controllable error
$\epsilon$. Essentially, $\epsilon$-softmax not only acts as an alternative for
the softmax layer, but also implicitly plays the crucial role in modifying the
loss function. We prove theoretically that $\epsilon$-softmax can achieve
noise-tolerant learning with controllable excess risk bound for almost any loss
function. Recognizing that $\epsilon$-softmax-enhanced losses may slightly
reduce fitting ability on clean datasets, we further incorporate them with one
symmetric loss, thereby achieving a better trade-off between robustness and
effective learning. Extensive experiments demonstrate the superiority of our
method in mitigating synthetic and real-world label noise. The code is
available at https://github.com/cswjl/eps-softmax.

</details>


### [379] [Clinical Expert Uncertainty Guided Generalized Label Smoothing for Medical Noisy Label Learning](https://arxiv.org/abs/2508.02495)
*Kunyu Zhang,Lin Gu,Liangchen Liu,Yingke Chen,Bingyang Wang,Jin Yan,Yingying Zhu*

Main category: cs.LG

TL;DR: The paper addresses label noise in medical image datasets caused by expert uncertainty in clinical notes, proposing a benchmark and label smoothing method to improve performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods ignore expert-driven uncertainty in clinical notes, leading to noisy labels in medical image datasets.

Method: The study examines the impact of expert uncertainty on label noise and introduces a clinical expert uncertainty-aware benchmark with a label smoothing technique.

Result: The proposed method outperforms current state-of-the-art approaches by better handling expert-written uncertainty.

Conclusion: Incorporating expert uncertainty into medical image analysis improves label quality and model performance.

Abstract: Many previous studies have proposed extracting image labels from clinical
notes to create large-scale medical image datasets at a low cost. However,
these approaches inherently suffer from label noise due to uncertainty from the
clinical experts. When radiologists and physicians analyze medical images to
make diagnoses, they often include uncertainty-aware notes such as ``maybe'' or
``not excluded''. Unfortunately, current text-mining methods overlook these
nuances, resulting in the creation of noisy labels. Existing methods for
handling noisy labels in medical image analysis, which typically address the
problem through post-processing techniques, have largely ignored the important
issue of expert-driven uncertainty contributing to label noise. To better
incorporate the expert-written uncertainty in clinical notes into medical image
analysis and address the label noise issue, we first examine the impact of
clinical expert uncertainty on label noise. We then propose a clinical expert
uncertainty-aware benchmark, along with a label smoothing method, which
significantly improves performance compared to current state-of-the-art
approaches.

</details>


### [380] [Explainable AI Methods for Neuroimaging: Systematic Failures of Common Tools, the Need for Domain-Specific Validation, and a Proposal for Safe Application](https://arxiv.org/abs/2508.02560)
*Nys Tjade Siegel,James H. Cole,Mohamad Habes,Stefan Haufe,Kerstin Ritter,Marc-André Schulz*

Main category: cs.LG

TL;DR: A large-scale validation of XAI methods in neuroimaging reveals failures in GradCAM and Layer-wise Relevance Propagation, while SmoothGrad performs robustly, emphasizing the need for domain-specific XAI adaptation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of rigorous validation for XAI methods in neuroimaging, risking misinterpretation of deep learning models.

Method: A novel XAI validation framework was used to systematically compare methods on ~45,000 structural brain MRIs, with verifiable ground truth tasks.

Result: GradCAM failed to localize features, and Layer-wise Relevance Propagation produced artifactual explanations, while SmoothGrad was consistently accurate.

Conclusion: Domain-specific adaptation of XAI methods is crucial, and prior neuroimaging studies using standard XAI may need re-evaluation.

Abstract: Trustworthy interpretation of deep learning models is critical for
neuroimaging applications, yet commonly used Explainable AI (XAI) methods lack
rigorous validation, risking misinterpretation. We performed the first
large-scale, systematic comparison of XAI methods on ~45,000 structural brain
MRIs using a novel XAI validation framework. This framework establishes
verifiable ground truth by constructing prediction tasks with known signal
sources - from localized anatomical features to subject-specific clinical
lesions - without artificially altering input images. Our analysis reveals
systematic failures in two of the most widely used methods: GradCAM
consistently failed to localize predictive features, while Layer-wise Relevance
Propagation generated extensive, artifactual explanations that suggest
incompatibility with neuroimaging data characteristics. Our results indicate
that these failures stem from a domain mismatch, where methods with design
principles tailored to natural images require substantial adaptation for
neuroimaging data. In contrast, the simpler, gradient-based method SmoothGrad,
which makes fewer assumptions about data structure, proved consistently
accurate, suggesting its conceptual simplicity makes it more robust to this
domain shift. These findings highlight the need for domain-specific adaptation
and validation of XAI methods, suggest that interpretations from prior
neuroimaging studies using standard XAI methodology warrant re-evaluation, and
provide urgent guidance for practical application of XAI in neuroimaging.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [381] [Dialogue Systems Engineering: A Survey and Future Directions](https://arxiv.org/abs/2508.02279)
*Mikio Nakano,Hironori Takeuchi,Sadahiro Yoshikawa,Yoichi Matsuyama,Kazunori Komatani*

Main category: cs.SE

TL;DR: The paper introduces Dialogue Systems Engineering (DSE) as a subfield of software engineering for dialogue systems, surveys its current state, and outlines future directions.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of large language models has expanded the potential applications of dialogue systems, necessitating specialized software engineering approaches for their development and maintenance.

Method: The authors survey DSE by aligning it with the knowledge areas of SWEBOK Version 4.0, identifying gaps and future research directions.

Result: The paper maps DSE to SWEBOK, highlights unexplored topics, and suggests future directions for the field.

Conclusion: Dialogue Systems Engineering is an emerging field requiring tailored software engineering practices, with significant opportunities for future research and development.

Abstract: This paper proposes to refer to the field of software engineering related to
the life cycle of dialogue systems as Dialogue Systems Engineering, and surveys
this field while also discussing its future directions. With the advancement of
large language models, the core technologies underlying dialogue systems have
significantly progressed. As a result, dialogue system technology is now
expected to be applied to solving various societal issues and in business
contexts. To achieve this, it is important to build, operate, and continuously
improve dialogue systems correctly and efficiently. Accordingly, in addition to
applying existing software engineering knowledge, it is becoming increasingly
important to evolve software engineering tailored specifically to dialogue
systems. In this paper, we enumerate the knowledge areas of dialogue systems
engineering based on those of software engineering, as defined in the Software
Engineering Body of Knowledge (SWEBOK) Version 4.0, and survey each area. Based
on this survey, we identify unexplored topics in each area and discuss the
future direction of dialogue systems engineering.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [382] [Contextual Phenotyping of Pediatric Sepsis Cohort Using Large Language Models](https://arxiv.org/abs/2505.09805)
*Aditya Nagori,Ayush Gautam,Matthew O. Wiens,Vuong Nguyen,Nathan Kenya Mugisha,Jerome Kabakyenga,Niranjan Kissoon,John Mark Ansermino,Rishikesan Kamaleswaran*

Main category: q-bio.QM

TL;DR: LLM-based clustering outperforms classical methods in patient subgroup analysis, achieving higher Silhouette Scores and better contextual understanding.


<details>
  <summary>Details</summary>
Motivation: Traditional clustering methods struggle with high-dimensional, heterogeneous healthcare data and lack contextual understanding, necessitating better approaches for personalized care.

Method: Used LLMs (LLAMA 3.1 8B, DeepSeek-R1-Distill-Llama-8B, Stella-En-400M-V5) to generate embeddings from serialized patient records, applied K-means clustering, and compared with classical methods (K-Medoids on UMAP and FAMD-reduced data).

Result: Stella-En-400M-V5 achieved the highest Silhouette Score (0.86). LLM-based methods identified distinct subgroups and outperformed classical techniques.

Conclusion: LLMs show promise for contextual phenotyping and informed decision-making in resource-limited settings.

Abstract: Clustering patient subgroups is essential for personalized care and efficient
resource use. Traditional clustering methods struggle with high-dimensional,
heterogeneous healthcare data and lack contextual understanding. This study
evaluates Large Language Model (LLM) based clustering against classical methods
using a pediatric sepsis dataset from a low-income country (LIC), containing
2,686 records with 28 numerical and 119 categorical variables. Patient records
were serialized into text with and without a clustering objective. Embeddings
were generated using quantized LLAMA 3.1 8B, DeepSeek-R1-Distill-Llama-8B with
low-rank adaptation(LoRA), and Stella-En-400M-V5 models. K-means clustering was
applied to these embeddings. Classical comparisons included K-Medoids
clustering on UMAP and FAMD-reduced mixed data. Silhouette scores and
statistical tests evaluated cluster quality and distinctiveness.
Stella-En-400M-V5 achieved the highest Silhouette Score (0.86). LLAMA 3.1 8B
with the clustering objective performed better with higher number of clusters,
identifying subgroups with distinct nutritional, clinical, and socioeconomic
profiles. LLM-based methods outperformed classical techniques by capturing
richer context and prioritizing key features. These results highlight potential
of LLMs for contextual phenotyping and informed decision-making in
resource-limited settings.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [383] [Point-wise Diffusion Models for Physical Systems with Shape Variations: Application to Spatio-temporal and Large-scale system](https://arxiv.org/abs/2508.01230)
*Jiyong Kim,Sunwoong Yang,Namwoo Kang*

Main category: physics.comp-ph

TL;DR: A novel point-wise diffusion model for predicting complex physical systems with shape variations, offering efficiency and accuracy improvements over traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional image-based diffusion models by enabling direct processing of unstructured data (e.g., meshes, point clouds) while preserving geometric fidelity.

Method: Uses forward and backward diffusion processes at individual spatio-temporal points, coupled with a point-wise diffusion transformer for denoising. Employs DDIM for efficient deterministic sampling (5-10 steps).

Result: Achieves 100-200x computational speedup, 94.4% reduction in training time, 89.0% fewer parameters, and 28% better accuracy compared to image-based models. Outperforms DeepONet and Meshgraphnet.

Conclusion: The point-wise approach is superior for real-time prediction, offering efficiency, accuracy, and flexibility across diverse physical systems.

Abstract: This study introduces a novel point-wise diffusion model that processes
spatio-temporal points independently to efficiently predict complex physical
systems with shape variations. This methodological contribution lies in
applying forward and backward diffusion processes at individual spatio-temporal
points, coupled with a point-wise diffusion transformer architecture for
denoising. Unlike conventional image-based diffusion models that operate on
structured data representations, this framework enables direct processing of
any data formats including meshes and point clouds while preserving geometric
fidelity. We validate our approach across three distinct physical domains with
complex geometric configurations: 2D spatio-temporal systems including cylinder
fluid flow and OLED drop impact test, and 3D large-scale system for road-car
external aerodynamics. To justify the necessity of our point-wise approach for
real-time prediction applications, we employ denoising diffusion implicit
models (DDIM) for efficient deterministic sampling, requiring only 5-10 steps
compared to traditional 1000-step and providing computational speedup of 100 to
200 times during inference without compromising accuracy. In addition, our
proposed model achieves superior performance compared to image-based diffusion
model: reducing training time by 94.4% and requiring 89.0% fewer parameters
while achieving over 28% improvement in prediction accuracy. Comprehensive
comparisons against data-flexible surrogate models including DeepONet and
Meshgraphnet demonstrate consistent superiority of our approach across all
three physical systems. To further refine the proposed model, we investigate
two key aspects: 1) comparison of final physical states prediction or
incremental change prediction, and 2) computational efficiency evaluation
across varying subsampling ratios (10%-100%).

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [384] [DBAIOps: A Reasoning LLM-Enhanced Database Operation and Maintenance System using Knowledge Graphs](https://arxiv.org/abs/2508.01136)
*Wei Zhou,Peng Sun,Xuanhe Zhou,Qianglei Zang,Ji Xu,Tieying Zhang,Guoliang Li,Fan Wu*

Main category: cs.DB

TL;DR: DBAIOps is a hybrid system combining reasoning LLMs and knowledge graphs to enhance database O&M by leveraging expert experience, outperforming existing methods in accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing automatic database O&M methods fail to effectively utilize expert experience, leading to inaccurate or generic results.

Method: DBAIOps uses a heterogeneous graph model for diagnosis experience, reusable anomaly models, and a two-stage graph evolution mechanism with reasoning LLMs.

Result: DBAIOps achieves 34.85% and 47.22% higher accuracy in root cause and human evaluations compared to baselines.

Conclusion: DBAIOps effectively bridges the gap between expert experience and automated O&M, improving diagnosis accuracy and usability.

Abstract: The operation and maintenance (O&M) of database systems is critical to
ensuring system availability and performance, typically requiring expert
experience (e.g., identifying metric-to-anomaly relations) for effective
diagnosis and recovery. However, existing automatic database O&M methods,
including commercial products, cannot effectively utilize expert experience. On
the one hand, rule-based methods only support basic O&M tasks (e.g.,
metric-based anomaly detection), which are mostly numerical equations and
cannot effectively incorporate literal O&M experience (e.g., troubleshooting
guidance in manuals). On the other hand, LLM-based methods, which retrieve
fragmented information (e.g., standard documents + RAG), often generate
inaccurate or generic results. To address these limitations, we present
DBAIOps, a novel hybrid database O&M system that combines reasoning LLMs with
knowledge graphs to achieve DBA-style diagnosis. First, DBAIOps introduces a
heterogeneous graph model for representing the diagnosis experience, and
proposes a semi-automatic graph construction algorithm to build that graph from
thousands of documents. Second, DBAIOps develops a collection of (800+)
reusable anomaly models that identify both directly alerted metrics and
implicitly correlated experience and metrics. Third, for each anomaly, DBAIOps
proposes a two-stage graph evolution mechanism to explore relevant diagnosis
paths and identify missing relations automatically. It then leverages a
reasoning LLM (e.g., DeepSeek-R1) to infer root causes and generate clear
diagnosis reports for both DBAs and common users. Our evaluation over four
mainstream database systems (Oracle, MySQL, PostgreSQL, and DM8) demonstrates
that DBAIOps outperforms state-of-the-art baselines, 34.85% and 47.22% higher
in root cause and human evaluation accuracy, respectively.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [385] [A Large-Scale Benchmark of Cross-Modal Learning for Histology and Gene Expression in Spatial Transcriptomics](https://arxiv.org/abs/2508.01490)
*Rushin H. Gindra,Giovanni Palla,Mathias Nguyen,Sophia J. Wagner,Manuel Tran,Fabian J Theis,Dieter Saur,Lorin Crawford,Tingying Peng*

Main category: q-bio.GN

TL;DR: HESCAPE is a benchmark for cross-modal contrastive pretraining in spatial transcriptomics, showing gene expression encoders are key for alignment but batch effects hinder performance.


<details>
  <summary>Details</summary>
Motivation: The lack of benchmarks for multimodal learning in spatial transcriptomics motivates the creation of HESCAPE to evaluate methods combining histology images and gene expression data.

Method: HESCAPE uses a curated pan-organ dataset with 6 gene panels and 54 donors, evaluating image and gene expression encoders across pretraining strategies for downstream tasks like gene mutation classification and expression prediction.

Result: Gene expression encoders drive alignment, outperforming baselines, but contrastive pretraining improves mutation classification while degrading expression prediction due to batch effects.

Conclusion: Batch effects challenge cross-modal alignment, necessitating robust multimodal learning approaches. HESCAPE provides tools to advance research in spatial transcriptomics.

Abstract: Spatial transcriptomics enables simultaneous measurement of gene expression
and tissue morphology, offering unprecedented insights into cellular
organization and disease mechanisms. However, the field lacks comprehensive
benchmarks for evaluating multimodal learning methods that leverage both
histology images and gene expression data. Here, we present HESCAPE, a
large-scale benchmark for cross-modal contrastive pretraining in spatial
transcriptomics, built on a curated pan-organ dataset spanning 6 different gene
panels and 54 donors. We systematically evaluated state-of-the-art image and
gene expression encoders across multiple pretraining strategies and assessed
their effectiveness on two downstream tasks: gene mutation classification and
gene expression prediction. Our benchmark demonstrates that gene expression
encoders are the primary determinant of strong representational alignment, and
that gene models pretrained on spatial transcriptomics data outperform both
those trained without spatial data and simple baseline approaches. However,
downstream task evaluation reveals a striking contradiction: while contrastive
pretraining consistently improves gene mutation classification performance, it
degrades direct gene expression prediction compared to baseline encoders
trained without cross-modal objectives. We identify batch effects as a key
factor that interferes with effective cross-modal alignment. Our findings
highlight the critical need for batch-robust multimodal learning approaches in
spatial transcriptomics. To accelerate progress in this direction, we release
HESCAPE, providing standardized datasets, evaluation protocols, and
benchmarking tools for the community

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [386] [ReCoSeg++:Extended Residual-Guided Cross-Modal Diffusion for Brain Tumor Segmentation](https://arxiv.org/abs/2508.01058)
*Sara Yavari,Rahul Nitin Pandya,Jacob Furst*

Main category: eess.IV

TL;DR: A semi-supervised, two-stage framework for brain tumor segmentation in MRI scans, using residual-guided DDPM and a lightweight U-Net, achieves superior performance on BraTS 2021.


<details>
  <summary>Details</summary>
Motivation: Accurate brain tumor segmentation is crucial for clinical diagnosis and treatment planning, but ground-truth masks are often unavailable.

Method: A two-stage approach: 1) Cross-modal synthesis via DDPM to generate T1ce images and residual maps; 2) Segmentation using a U-Net with residual maps and other modalities. Slice-level filtering and thresholding optimize performance.

Result: Achieves Dice score of 93.02% and IoU of 86.7% on BraTS 2021, outperforming the ReCoSeg baseline.

Conclusion: The method improves accuracy and scalability for real-world, multi-center MRI datasets.

Abstract: Accurate segmentation of brain tumors in MRI scans is critical for clinical
diagnosis and treatment planning. We propose a semi-supervised, two-stage
framework that extends the ReCoSeg approach to the larger and more
heterogeneous BraTS 2021 dataset, while eliminating the need for ground-truth
masks for the segmentation objective. In the first stage, a residual-guided
denoising diffusion probabilistic model (DDPM) performs cross-modal synthesis
by reconstructing the T1ce modality from FLAIR, T1, and T2 scans. The residual
maps, capturing differences between predicted and actual T1ce images, serve as
spatial priors to enhance downstream segmentation. In the second stage, a
lightweight U-Net takes as input the concatenation of residual maps, computed
as the difference between real T1ce and synthesized T1ce, with T1, T2, and
FLAIR modalities to improve whole tumor segmentation. To address the increased
scale and variability of BraTS 2021, we apply slice-level filtering to exclude
non-informative samples and optimize thresholding strategies to balance
precision and recall. Our method achieves a Dice score of $93.02\%$ and an IoU
of $86.7\%$ for whole tumor segmentation on the BraTS 2021 dataset,
outperforming the ReCoSeg baseline on BraTS 2020 (Dice: $91.7\%$, IoU:
$85.3\%$), and demonstrating improved accuracy and scalability for real-world,
multi-center MRI datasets.

</details>


### [387] [Mobile U-ViT: Revisiting large kernel and U-shaped ViT for efficient medical image segmentation](https://arxiv.org/abs/2508.01064)
*Fenghe Tang,Bingkun Nian,Jianrui Ding,Wenxin Ma,Quan Quan,Chengqi Dong,Jie Yang,Wei Liu,S. Kevin Zhou*

Main category: eess.IV

TL;DR: Mobile U-ViT is a lightweight, efficient model for medical image segmentation, combining ConvUtr and LGL blocks for local-global information exchange, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing mobile models for natural images perform poorly on medical tasks due to the information density gap. A lightweight, universal, and high-performing network for medical images is needed.

Method: Uses ConvUtr for hierarchical patch embedding, LGL blocks for local-global exchange, and a lightweight transformer bottleneck with a cascaded decoder.

Result: Achieves state-of-the-art performance on eight public 2D/3D datasets, including zero-shot testing on four unseen datasets.

Conclusion: Mobile U-ViT is an efficient, powerful, and generalizable solution for mobile medical image analysis.

Abstract: In clinical practice, medical image analysis often requires efficient
execution on resource-constrained mobile devices. However, existing mobile
models-primarily optimized for natural images-tend to perform poorly on medical
tasks due to the significant information density gap between natural and
medical domains. Combining computational efficiency with medical
imaging-specific architectural advantages remains a challenge when developing
lightweight, universal, and high-performing networks. To address this, we
propose a mobile model called Mobile U-shaped Vision Transformer (Mobile U-ViT)
tailored for medical image segmentation. Specifically, we employ the newly
purposed ConvUtr as a hierarchical patch embedding, featuring a
parameter-efficient large-kernel CNN with inverted bottleneck fusion. This
design exhibits transformer-like representation learning capacity while being
lighter and faster. To enable efficient local-global information exchange, we
introduce a novel Large-kernel Local-Global-Local (LGL) block that effectively
balances the low information density and high-level semantic discrepancy of
medical images. Finally, we incorporate a shallow and lightweight transformer
bottleneck for long-range modeling and employ a cascaded decoder with
downsample skip connections for dense prediction. Despite its reduced
computational demands, our medical-optimized architecture achieves
state-of-the-art performance across eight public 2D and 3D datasets covering
diverse imaging modalities, including zero-shot testing on four unseen
datasets. These results establish it as an efficient yet powerful and
generalization solution for mobile medical image analysis. Code is available at
https://github.com/FengheTan9/Mobile-U-ViT.

</details>


### [388] [Diagnostic Accuracy of Open-Source Vision-Language Models on Diverse Medical Imaging Tasks](https://arxiv.org/abs/2508.01016)
*Gustav Müller-Franzes,Debora Jutz,Jakob Nikolas Kather,Christiane Kuhl,Sven Nebelung,Daniel Truhn*

Main category: eess.IV

TL;DR: The study compared five VLMs (Qwen2.5, Phi-4, Gemma3, Llama3.2, Mistral3.1) on medical image tasks using the MedFMC dataset. Qwen2.5 excelled in chest radiographs and endoscopy, while all models struggled with retinal fundoscopy. Multimodal input and chain-of-thought reasoning did not improve accuracy.


<details>
  <summary>Details</summary>
Motivation: To evaluate the diagnostic accuracy of open-source VLMs across diverse medical imaging tasks and identify their strengths and limitations for clinical use.

Method: Retrospective study using the MedFMC dataset (22,349 images from 7,461 patients) across five medical tasks. Models were tested in three settings: visual input only, multimodal input, and chain-of-thought reasoning. Accuracy was compared using bootstrapped confidence intervals.

Result: Qwen2.5 performed best in chest radiographs (90.4%) and endoscopy (84.2%). For colon pathology and neonatal jaundice, Qwen2.5 and Phi-4 were comparable. All models struggled with retinal fundoscopy (highest accuracy 18.6%). Multimodal input and chain-of-thought reasoning did not improve accuracy.

Conclusion: Open-source VLMs show promise in certain medical tasks but require further development for complex domains like retinal fundoscopy before clinical adoption.

Abstract: This retrospective study evaluated five VLMs (Qwen2.5, Phi-4, Gemma3,
Llama3.2, and Mistral3.1) using the MedFMC dataset. This dataset includes
22,349 images from 7,461 patients encompassing chest radiography (19 disease
multi-label classifications), colon pathology (tumor detection), endoscopy
(colorectal lesion identification), neonatal jaundice assessment (skin
color-based treatment necessity), and retinal fundoscopy (5-point diabetic
retinopathy grading). Diagnostic accuracy was compared in three experimental
settings: visual input only, multimodal input, and chain-of-thought reasoning.
Model accuracy was assessed against ground truth labels, with statistical
comparisons using bootstrapped confidence intervals (p<.05). Qwen2.5 achieved
the highest accuracy for chest radiographs (90.4%) and endoscopy images
(84.2%), significantly outperforming the other models (p<.001). In colon
pathology, Qwen2.5 (69.0%) and Phi-4 (69.6%) performed comparably (p=.41), both
significantly exceeding other VLMs (p<.001). Similarly, for neonatal jaundice
assessment, Qwen2.5 (58.3%) and Phi-4 (58.1%) showed comparable leading
accuracies (p=.93) significantly exceeding their counterparts (p<.001). All
models struggled with retinal fundoscopy; Qwen2.5 and Gemma3 achieved the
highest, albeit modest, accuracies at 18.6% (comparable, p=.99), significantly
better than other tested models (p<.001). Unexpectedly, multimodal input
reduced accuracy for some models and modalities, and chain-of-thought reasoning
prompts also failed to improve accuracy. The open-source VLMs demonstrated
promising diagnostic capabilities, particularly in chest radiograph
interpretation. However, performance in complex domains such as retinal
fundoscopy was limited, underscoring the need for further development and
domain-specific adaptation before widespread clinical application.

</details>


### [389] [CoCoLIT: ControlNet-Conditioned Latent Image Translation for MRI to Amyloid PET Synthesis](https://arxiv.org/abs/2508.01292)
*Alec Sargood,Lemuel Puglisi,James H. Cole,Neil P. Oxtoby,Daniele Ravì,Daniel C. Alexander*

Main category: eess.IV

TL;DR: CoCoLIT is a diffusion-based latent generative framework for synthesizing amyloid PET scans from structural MRI, outperforming state-of-the-art methods in Alzheimer's Disease screening.


<details>
  <summary>Details</summary>
Motivation: To provide a cost-effective, large-scale Alzheimer's Disease screening method by leveraging MRI data, which may indirectly encode amyloid deposition information.

Method: Introduces CoCoLIT, featuring Weighted Image Space Loss (WISL), Latent Average Stabilization (LAS) analysis, and ControlNet-based conditioning for MRI-to-PET translation.

Result: Significantly outperforms existing methods, with +10.5% and +23.7% improvements in amyloid-positivity classification on internal and external datasets.

Conclusion: CoCoLIT offers an effective solution for amyloid PET synthesis from MRI, enhancing Alzheimer's Disease screening accessibility.

Abstract: Synthesizing amyloid PET scans from the more widely available and accessible
structural MRI modality offers a promising, cost-effective approach for
large-scale Alzheimer's Disease (AD) screening. This is motivated by evidence
that, while MRI does not directly detect amyloid pathology, it may nonetheless
encode information correlated with amyloid deposition that can be uncovered
through advanced modeling. However, the high dimensionality and structural
complexity of 3D neuroimaging data pose significant challenges for existing
MRI-to-PET translation methods. Modeling the cross-modality relationship in a
lower-dimensional latent space can simplify the learning task and enable more
effective translation. As such, we present CoCoLIT (ControlNet-Conditioned
Latent Image Translation), a diffusion-based latent generative framework that
incorporates three main innovations: (1) a novel Weighted Image Space Loss
(WISL) that improves latent representation learning and synthesis quality; (2)
a theoretical and empirical analysis of Latent Average Stabilization (LAS), an
existing technique used in similar generative models to enhance inference
consistency; and (3) the introduction of ControlNet-based conditioning for
MRI-to-PET translation. We evaluate CoCoLIT's performance on publicly available
datasets and find that our model significantly outperforms state-of-the-art
methods on both image-based and amyloid-related metrics. Notably, in
amyloid-positivity classification, CoCoLIT outperforms the second-best method
with improvements of +10.5% on the internal dataset and +23.7% on the external
dataset. The code and models of our approach are available at
https://github.com/brAIn-science/CoCoLIT.

</details>


### [390] [SWAN: Synergistic Wavelet-Attention Network for Infrared Small Target Detection](https://arxiv.org/abs/2508.01322)
*Yuxin Jing,Jufeng Zhao,Tianpei Zhang,Yiming Zhu*

Main category: eess.IV

TL;DR: SWAN, a novel framework for IRSTD, combines wavelet convolution and attention mechanisms to improve detection accuracy in complex backgrounds.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with distinguishing small targets from clutter due to reliance on local spatial patterns.

Method: Proposes SWAN with Haar Wavelet Convolution, Shifted Spatial Attention, and Residual Dual-Channel Attention for cross-domain fusion and feature calibration.

Result: Outperforms state-of-the-art methods in accuracy and robustness, especially in complex scenarios.

Conclusion: SWAN effectively addresses IRSTD challenges by integrating spatial and frequency domain insights.

Abstract: Infrared small target detection (IRSTD) is thus critical in both civilian and
military applications. This study addresses the challenge of precisely IRSTD in
complex backgrounds. Recent methods focus fundamental reliance on conventional
convolution operations, which primarily capture local spatial patterns and
struggle to distinguish the unique frequency-domain characteristics of small
targets from intricate background clutter. To overcome these limitations, we
proposed the Synergistic Wavelet-Attention Network (SWAN), a novel framework
designed to perceive targets from both spatial and frequency domains. SWAN
leverages a Haar Wavelet Convolution (HWConv) for a deep, cross-domain fusion
of the frequency energy and spatial details of small target. Furthermore, a
Shifted Spatial Attention (SSA) mechanism efficiently models long-range spatial
dependencies with linear computational complexity, enhancing contextual
awareness. Finally, a Residual Dual-Channel Attention (RDCA) module adaptively
calibrates channel-wise feature responses to suppress background interference
while amplifying target-pertinent signals. Extensive experiments on benchmark
datasets demonstrate that SWAN surpasses existing state-of-the-art methods,
showing significant improvements in detection accuracy and robustness,
particularly in complex challenging scenarios.

</details>


### [391] [Classification of Brain Tumors using Hybrid Deep Learning Models](https://arxiv.org/abs/2508.01350)
*Neerav Nemchand Gala*

Main category: eess.IV

TL;DR: The study applied transfer learning with EfficientNetV2 to classify brain tumors, outperforming EfficientNet and ResNet50 but requiring more training time.


<details>
  <summary>Details</summary>
Motivation: To address the high computational and data demands of CNNs in medical image interpretation.

Method: Compared EfficientNetV2, EfficientNet, and ResNet50 for classifying brain tumors (glioma, meningioma, pituitary) using transfer learning.

Result: EfficientNetV2 achieved superior performance but with increased training time due to its complexity.

Conclusion: EfficientNetV2 is effective for medical image classification but trades off performance for higher computational cost.

Abstract: The use of Convolutional Neural Networks (CNNs) has greatly improved the
interpretation of medical images. However, conventional CNNs typically demand
extensive computational resources and large training datasets. To address these
limitations, this study applied transfer learning to achieve strong
classification performance using fewer training samples. Specifically, the
study compared EfficientNetV2 with its predecessor, EfficientNet, and with
ResNet50 in classifying brain tumors into three types: glioma, meningioma, and
pituitary tumors. Results showed that EfficientNetV2 delivered superior
performance compared to the other models. However, this improvement came at the
cost of increased training time, likely due to the model's greater complexity.

</details>


### [392] [Predicting EGFR Mutation in LUAD from Histopathological Whole-Slide Images Using Pretrained Foundation Model and Transfer Learning: An Indian Cohort Study](https://arxiv.org/abs/2508.01352)
*Sagar Singh Gwal,Rajan,Suyash Devgan,Shraddhanjali Satapathy,Abhishek Goyal,Nuruddin Mohammad Iqbal,Vivaan Jain,Prabhat Singh Mallik,Deepali Jain,Ishaan Gupta*

Main category: eess.IV

TL;DR: A deep learning framework using vision transformers and attention-based multiple instance learning predicts EGFR mutation status in lung adenocarcinoma from H&E-stained slides, showing high accuracy across datasets.


<details>
  <summary>Details</summary>
Motivation: Predicting EGFR mutation status in lung adenocarcinoma is crucial for targeted therapy, especially in Southeast Asian populations with higher mutation rates.

Method: A DL framework combining vision transformers and ABMIL was trained on an Indian cohort (170 WSI) and tested on internal (30 WSI) and external (86 WSI from TCGA) datasets.

Result: The model achieved AUCs of 0.933 (internal) and 0.965 (external), outperforming prior studies.

Conclusion: The framework accurately predicts EGFR mutations from routine pathology slides, even in resource-limited settings.

Abstract: Lung adenocarcinoma (LUAD) is a subtype of non-small cell lung cancer
(NSCLC). LUAD with mutation in the EGFR gene accounts for approximately 46% of
LUAD cases. Patients carrying EGFR mutations can be treated with specific
tyrosine kinase inhibitors (TKIs). Hence, predicting EGFR mutation status can
help in clinical decision making. H&E-stained whole slide imaging (WSI) is a
routinely performed screening procedure for cancer staging and subtyping,
especially affecting the Southeast Asian populations with significantly higher
incidence of the mutation when compared to Caucasians (39-64% vs 7-22%). Recent
progress in AI models has shown promising results in cancer detection and
classification. In this study, we propose a deep learning (DL) framework built
on vision transformers (ViT) based pathology foundation model and
attention-based multiple instance learning (ABMIL) architecture to predict EGFR
mutation status from H&E WSI. The developed pipeline was trained using data
from an Indian cohort (170 WSI) and evaluated across two independent datasets:
Internal test (30 WSI from Indian cohort) set, and an external test set from
TCGA (86 WSI). The model shows consistent performance across both datasets,
with AUCs of 0.933 (+/-0.010), and 0.965 (+/-0.015) for the internal and
external test sets respectively. This proposed framework can be efficiently
trained on small datasets, achieving superior performance as compared to
several prior studies irrespective of training domain. The current study
demonstrates the feasibility of accurately predicting EGFR mutation status
using routine pathology slides, particularly in resource-limited settings using
foundation models and attention-based multiple instance learning.

</details>


### [393] [Deeply Supervised Multi-Task Autoencoder for Biological Brain Age estimation using three dimensional T$_1$-weighted magnetic resonance imaging](https://arxiv.org/abs/2508.01565)
*Mehreen Kanwal,Yunsik Son*

Main category: eess.IV

TL;DR: A Deeply Supervised Multitask Autoencoder (DSMT-AE) framework is proposed for accurate brain age estimation from 3D MRI scans, addressing challenges like vanishing gradients and sex-based structural differences. It achieves state-of-the-art performance by combining deep supervision and multitask learning.


<details>
  <summary>Details</summary>
Motivation: Accurate brain age estimation is crucial for identifying neurodegenerative diseases. Challenges include vanishing gradients in 3D models and sex-based structural differences, necessitating improved methods.

Method: The DSMT-AE framework uses deep supervision and multitask learning, optimizing brain age prediction alongside sex classification and image reconstruction to enhance feature representation.

Result: DSMT-AE achieves state-of-the-art performance on the OpenBHB dataset, demonstrating robustness across age and sex subgroups. Ablation studies confirm the contributions of each component.

Conclusion: The DSMT-AE framework effectively addresses key challenges in brain age estimation, improving accuracy and generalizability through deep supervision and multitask learning.

Abstract: Accurate estimation of biological brain age from three dimensional (3D)
T$_1$-weighted magnetic resonance imaging (MRI) is a critical imaging biomarker
for identifying accelerated aging associated with neurodegenerative diseases.
Effective brain age prediction necessitates training 3D models to leverage
comprehensive insights from volumetric MRI scans, thereby fully capturing
spatial anatomical context. However, optimizing deep 3D models remains
challenging due to problems such as vanishing gradients. Furthermore, brain
structural patterns differ significantly between sexes, which impacts aging
trajectories and vulnerability to neurodegenerative diseases, thereby making
sex classification crucial for enhancing the accuracy and generalizability of
predictive models. To address these challenges, we propose a Deeply Supervised
Multitask Autoencoder (DSMT-AE) framework for brain age estimation. DSMT-AE
employs deep supervision, which involves applying supervisory signals at
intermediate layers during training, to stabilize model optimization, and
multitask learning to enhance feature representation. Specifically, our
framework simultaneously optimizes brain age prediction alongside auxiliary
tasks of sex classification and image reconstruction, thus effectively
capturing anatomical and demographic variability to improve prediction
accuracy. We extensively evaluate DSMT-AE on the Open Brain Health Benchmark
(OpenBHB) dataset, the largest multisite neuroimaging cohort combining ten
publicly available datasets. The results demonstrate that DSMT-AE achieves
state-of-the-art performance and robustness across age and sex subgroups.
Additionally, our ablation study confirms that each proposed component
substantially contributes to the improved predictive accuracy and robustness of
the overall architecture.

</details>


### [394] [Tractography-Guided Dual-Label Collaborative Learning for Multi-Modal Cranial Nerves Parcellation](https://arxiv.org/abs/2508.01577)
*Lei Xie,Junxiong Huang,Yuanjing Feng,Qingrun Zeng*

Main category: eess.IV

TL;DR: Proposes DCLNet, a tractography-guided dual-label network for multi-modal CNs parcellation, improving performance by combining coarse and precise labels and soft information swapping between MRI types.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal CNs parcellation methods underutilize diffusion MRI, leading to suboptimal fusion performance.

Method: Introduces DCLNet with coarse labels from tractography and precise expert labels, plus a Modality-adaptive Encoder Module for MRI fusion.

Result: Shows improved performance on the HCP dataset compared to single-label networks.

Conclusion: Dual-label strategies effectively address ambiguities in CNs parcellation.

Abstract: The parcellation of Cranial Nerves (CNs) serves as a crucial quantitative
methodology for evaluating the morphological characteristics and anatomical
pathways of specific CNs. Multi-modal CNs parcellation networks have achieved
promising segmentation performance, which combine structural Magnetic Resonance
Imaging (MRI) and diffusion MRI. However, insufficient exploration of diffusion
MRI information has led to low performance of existing multi-modal fusion. In
this work, we propose a tractography-guided Dual-label Collaborative Learning
Network (DCLNet) for multi-modal CNs parcellation. The key contribution of our
DCLNet is the introduction of coarse labels of CNs obtained from fiber
tractography through CN atlas, and collaborative learning with precise labels
annotated by experts. Meanwhile, we introduce a Modality-adaptive Encoder
Module (MEM) to achieve soft information swapping between structural MRI and
diffusion MRI. Extensive experiments conducted on the publicly available Human
Connectome Project (HCP) dataset demonstrate performance improvements compared
to single-label network. This systematic validation underscores the
effectiveness of dual-label strategies in addressing inherent ambiguities in
CNs parcellation tasks.

</details>


### [395] [Measuring and Predicting Where and When Pathologists Focus their Visual Attention while Grading Whole Slide Images of Cancer](https://arxiv.org/abs/2508.01668)
*Souradeep Chakraborty,Ruoyu Xue,Rajarsi Gupta,Oksana Yaskiv,Constantin Friedman,Natallia Sheuka,Dana Perez,Paul Friedman,Won-Tak Choi,Waqas Mahmud,Beatrice Knudsen,Gregory Zelinsky,Joel Saltz,Dimitris Samaras*

Main category: eess.IV

TL;DR: The paper presents a method to predict pathologists' attention trajectories in prostate cancer WSIs using a two-stage transformer-based model, outperforming baselines and aiding training.


<details>
  <summary>Details</summary>
Motivation: To improve pathology training by predicting expert pathologists' attention movements in WSIs, enabling better decision support systems.

Method: A two-stage model: (1) predicts static attention heatmaps using transformers, (2) autoregressively predicts dynamic scanpaths from heatmaps.

Result: The model outperforms chance and baseline models in predicting attention scanpaths.

Conclusion: The method can assist trainees in learning expert-like attention allocation during WSI reading.

Abstract: The ability to predict the attention of expert pathologists could lead to
decision support systems for better pathology training. We developed methods to
predict the spatio-temporal (where and when) movements of pathologists'
attention as they grade whole slide images (WSIs) of prostate cancer. We
characterize a pathologist's attention trajectory by their x, y, and m
(magnification) movements of a viewport as they navigate WSIs using a digital
microscope. This information was obtained from 43 pathologists across 123 WSIs,
and we consider the task of predicting the pathologist attention scanpaths
constructed from the viewport centers. We introduce a fixation extraction
algorithm that simplifies an attention trajectory by extracting fixations in
the pathologist's viewing while preserving semantic information, and we use
these pre-processed data to train and test a two-stage model to predict the
dynamic (scanpath) allocation of attention during WSI reading via intermediate
attention heatmap prediction. In the first stage, a transformer-based
sub-network predicts the attention heatmaps (static attention) across different
magnifications. In the second stage, we predict the attention scanpath by
sequentially modeling the next fixation points in an autoregressive manner
using a transformer-based approach, starting at the WSI center and leveraging
multi-magnification feature representations from the first stage. Experimental
results show that our scanpath prediction model outperforms chance and baseline
models. Tools developed from this model could assist pathology trainees in
learning to allocate their attention during WSI reading like an expert.

</details>


### [396] [LoRA-based methods on Unet for transfer learning in Subarachnoid Hematoma Segmentation](https://arxiv.org/abs/2508.01772)
*Cristian Minoccheri,Matthew Hodgman,Haoyuan Ma,Rameez Merchant,Emily Wittrup,Craig Williamson,Kayvan Najarian*

Main category: eess.IV

TL;DR: The paper explores transfer learning for aneurysmal SAH segmentation, introducing CP-LoRA and DoRA variants, showing they outperform standard Unet fine-tuning with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Aneurysmal SAH is highly fatal; transfer learning from related hematoma types is underexplored. LoRA methods are rarely used in medical imaging despite their potential.

Method: Pre-trained Unet on TBI scans, fine-tuned on SAH data. Introduced CP-LoRA and DoRA variants, comparing them to existing LoRA methods and standard fine-tuning.

Result: LoRA-based methods outperformed standard fine-tuning, with CP-LoRA achieving comparable results using fewer parameters. Performance improved with larger hemorrhage volumes.

Conclusion: Transfer learning between hematoma types is feasible; LoRA methods significantly enhance SAH segmentation over conventional Unet fine-tuning.

Abstract: Aneurysmal subarachnoid hemorrhage (SAH) is a life-threatening neurological
emergency with mortality rates exceeding 30%. Transfer learning from related
hematoma types represents a potentially valuable but underexplored approach.
Although Unet architectures remain the gold standard for medical image
segmentation due to their effectiveness on limited datasets, Low-Rank
Adaptation (LoRA) methods for parameter-efficient transfer learning have been
rarely applied to convolutional neural networks in medical imaging contexts. We
implemented a Unet architecture pre-trained on computed tomography scans from
124 traumatic brain injury patients across multiple institutions, then
fine-tuned on 30 aneurysmal SAH patients from the University of Michigan Health
System using 3-fold cross-validation. We developed a novel CP-LoRA method based
on tensor CP-decomposition and introduced DoRA variants (DoRA-C, convDoRA,
CP-DoRA) that decompose weight matrices into magnitude and directional
components. We compared these approaches against existing LoRA methods (LoRA-C,
convLoRA) and standard fine-tuning strategies across different modules on a
multi-view Unet model. LoRA-based methods consistently outperformed standard
Unet fine-tuning. Performance varied by hemorrhage volume, with all methods
showing improved accuracy for larger volumes. CP-LoRA achieved comparable
performance to existing methods while using significantly fewer parameters.
Over-parameterization with higher ranks consistently yielded better performance
than strictly low-rank adaptations. This study demonstrates that transfer
learning between hematoma types is feasible and that LoRA-based methods
significantly outperform conventional Unet fine-tuning for aneurysmal SAH
segmentation.

</details>


### [397] [Joint Lossless Compression and Steganography for Medical Images via Large Language Models](https://arxiv.org/abs/2508.01782)
*Pengcheng Zheng,Xiaorong Pu,Kecheng Chen,Jiaxin Huang,Meng Yang,Bai Feng,Yazhou Ren,Jianan Jiang*

Main category: eess.IV

TL;DR: A novel framework combines lossless compression and steganography for medical images, improving performance, efficiency, and security using adaptive modalities decomposition and A-LoRA fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based compressors for medical images lack efficiency and security, which are critical in medical scenarios.

Method: Proposes a joint framework with adaptive modalities decomposition, dual-path compression, and segmented message steganography, enhanced by A-LoRA fine-tuning.

Result: Demonstrates superior compression ratios, efficiency, and security in experiments.

Conclusion: The method effectively addresses the trade-off and security issues in medical image compression.

Abstract: Recently, large language models (LLMs) have driven promis ing progress in
lossless image compression. However, di rectly adopting existing paradigms for
medical images suf fers from an unsatisfactory trade-off between compression
  performance and efficiency. Moreover, existing LLM-based
  compressors often overlook the security of the compres sion process, which is
critical in modern medical scenarios.
  To this end, we propose a novel joint lossless compression
  and steganography framework. Inspired by bit plane slicing
  (BPS), we find it feasible to securely embed privacy messages
  into medical images in an invisible manner. Based on this in sight, an
adaptive modalities decomposition strategy is first
  devised to partition the entire image into two segments, pro viding global
and local modalities for subsequent dual-path
  lossless compression. During this dual-path stage, we inno vatively propose a
segmented message steganography algo rithm within the local modality path to
ensure the security of
  the compression process. Coupled with the proposed anatom ical priors-based
low-rank adaptation (A-LoRA) fine-tuning
  strategy, extensive experimental results demonstrate the su periority of our
proposed method in terms of compression ra tios, efficiency, and security. The
source code will be made
  publicly available.

</details>


### [398] [Large Kernel MedNeXt for Breast Tumor Segmentation and Self-Normalizing Network for pCR Classification in Magnetic Resonance Images](https://arxiv.org/abs/2508.01831)
*Toufiq Musah*

Main category: eess.IV

TL;DR: The paper proposes a two-stage training strategy with a large-kernel MedNeXt architecture for breast tumor segmentation in DCE-MRI, achieving improved performance. It also uses radiomic features for pCR classification, with promising results in subgroups.


<details>
  <summary>Details</summary>
Motivation: Accurate breast tumor segmentation in DCE-MRI is crucial for tasks like pCR assessment, motivating the development of a robust segmentation and classification method.

Method: A large-kernel MedNeXt architecture with a two-stage training strategy (UpKern algorithm) is used for segmentation. For pCR classification, a self-normalizing network (SNN) is trained on radiomic features.

Result: Segmentation achieved a Dice score of 0.67 and NormHD of 0.24. pCR classification reached 57% balanced accuracy, with up to 75% in subgroups.

Conclusion: Combining larger receptive fields and radiomics-driven classification shows promise, with future work suggested for advanced ensembling and clinical variable integration.

Abstract: Accurate breast tumor segmentation in dynamic contrast-enhanced magnetic
resonance imaging (DCE-MRI) is important for downstream tasks such as
pathological complete response (pCR) assessment. In this work, we address both
segmentation and pCR classification using the large-scale MAMA-MIA DCE-MRI
dataset. We employ a large-kernel MedNeXt architecture with a two-stage
training strategy that expands the receptive field from 3x3x3 to 5x5x5 kernels
using the UpKern algorithm. This approach allows stable transfer of learned
features to larger kernels, improving segmentation performance on the unseen
validation set. An ensemble of large-kernel models achieved a Dice score of
0.67 and a normalized Hausdorff Distance (NormHD) of 0.24. For pCR
classification, we trained a self-normalizing network (SNN) on radiomic
features extracted from the predicted segmentations and first post-contrast
DCE-MRI, reaching an average balanced accuracy of 57\%, and up to 75\% in some
subgroups. Our findings highlight the benefits of combining larger receptive
fields and radiomics-driven classification while motivating future work on
advanced ensembling and the integration of clinical variables to further
improve performance and generalization. Code:
https://github.com/toufiqmusah/caladan-mama-mia.git

</details>


### [399] [Less is More: AMBER-AFNO -- a New Benchmark for Lightweight 3D Medical Image Segmentation](https://arxiv.org/abs/2508.01941)
*Andrea Dosi,Semanto Mondal,Rajib Chandra Ghosh,Massimo Brescia,Giuseppe Longo*

Main category: eess.IV

TL;DR: AMBER-AFNO, a transformer-based model adapted for 3D medical datacube segmentation, reduces complexity by 80% while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency in 3D medical segmentation by adapting a remote sensing model (AMBER) with AFNO for frequency-domain mixing.

Method: Replaced multi-head self-attention with AFNO in AMBER, reducing parameters and FLOPs. Evaluated on ACDC and Synapse datasets.

Result: Achieved competitive/superior accuracy with 80% fewer parameters, better training efficiency, and memory usage.

Conclusion: AMBER-AFNO offers a lightweight, efficient alternative for 3D medical segmentation without sacrificing performance.

Abstract: This work presents the results of a methodological transfer from remote
sensing to healthcare, adapting AMBER -- a transformer-based model originally
designed for multiband images, such as hyperspectral data -- to the task of 3D
medical datacube segmentation. In this study, we use the AMBER architecture
with Adaptive Fourier Neural Operators (AFNO) in place of the multi-head
self-attention mechanism. While existing models rely on various forms of
attention to capture global context, AMBER-AFNO achieves this through
frequency-domain mixing, enabling a drastic reduction in model complexity. This
design reduces the number of trainable parameters by over 80% compared to
UNETR++, while maintaining a FLOPs count comparable to other state-of-the-art
architectures. Model performance is evaluated on two benchmark 3D medical
datasets -- ACDC and Synapse -- using standard metrics such as Dice Similarity
Coefficient (DSC) and Hausdorff Distance (HD), demonstrating that AMBER-AFNO
achieves competitive or superior accuracy with significant gains in training
efficiency, inference speed, and memory usage.

</details>


### [400] [REACT-KD: Region-Aware Cross-modal Topological Knowledge Distillation for Interpretable Medical Image Classification](https://arxiv.org/abs/2508.02104)
*Hongzhao Chen,Hexiao Ding,Yufeng Jiang,Jing Lan,Ka Chun Li,Gerald W. Y. Cheng,Sam Ng,Chi Lai Ho,Jing Cai,Liang-ting Lin,Jung Sun Yoo*

Main category: eess.IV

TL;DR: REACT-KD is a framework for reliable tumor classification using CT imaging, leveraging multi-modal knowledge distillation and region-aware guidance to improve accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: The challenge of heterogeneous imaging quality, limited annotations, and lack of anatomical guidance in clinical tumor classification motivates the need for a robust and interpretable solution.

Method: REACT-KD uses a dual-teacher design (PET/CT and low-dose CT) to guide a lightweight CT-based student model via semantic alignment and anatomical topology modeling. It employs modality dropout for reliability.

Result: Achieves 93.4% AUC on PET/CT and 76.6%-81.5% AUC on varying CT dose levels, with high clinical benefit in decision curve analysis.

Conclusion: REACT-KD demonstrates strong performance and reliability, making it promising for real-world tumor diagnostics.

Abstract: Reliable and interpretable tumor classification from clinical imaging remains
a core challenge due to heterogeneous modality quality, limited annotations,
and the lack of structured anatomical guidance. We introduce REACT-KD, a
Region-Aware Cross-modal Topological Knowledge Distillation framework that
transfers rich supervision from high-fidelity multi-modal sources into a
lightweight CT-based student model. The framework uses a dual teacher design:
one branch captures structure-function relationships using dual-tracer PET/CT,
and the other models dose-aware features through synthetically degraded
low-dose CT data. These branches jointly guide the student model through two
complementary objectives. The first focuses on semantic alignment via logits
distillation, while the second models anatomical topology using region graph
distillation. A shared CBAM-3D module is employed to maintain consistent
attention across modalities. To improve reliability for deployment, REACT-KD
introduces modality dropout during training, allowing inference under partial
or noisy inputs. The staging task for hepatocellular carcinoma (HCC) is
conducted as a case study. REACT-KD achieves an average AUC of 93.4% on an
internal PET/CT cohort and maintains 76.6% to 81.5% AUC across varying dose
levels in external CT testing. Decision curve analysis shows that REACT-KD
consistently provides the highest clinical benefit across decision thresholds,
supporting its potential in real-world diagnostics. Code is available at
https://github.com/Kinetics-JOJO/REACT-KD.

</details>


### [401] [Tackling Ill-posedness of Reversible Image Conversion with Well-posed Invertible Network](https://arxiv.org/abs/2508.02111)
*Yuanfei Huang,Hua Huang*

Main category: eess.IV

TL;DR: The paper addresses the ill-posedness in reversible image conversion (RIC) by proposing a well-posed invertible 1x1 convolution (WIC) and two networks, WIN-Naïve and WIN, to eliminate reliance on random variables and improve performance.


<details>
  <summary>Details</summary>
Motivation: Existing RIC methods remain ill-posed due to uncertainty from random variables, limiting their reliability.

Method: Develops WIC to create a well-posed system, introduces WIN-Naïve and WIN networks with skip-connections for long-term memory.

Result: Achieves state-of-the-art performance in tasks like image hiding, rescaling, and decolorization.

Conclusion: The proposed approach overcomes RIC bottlenecks and sets a new benchmark, validated by extensive experiments.

Abstract: Reversible image conversion (RIC) suffers from ill-posedness issues due to
its forward conversion process being considered an underdetermined system.
Despite employing invertible neural networks (INN), existing RIC methods
intrinsically remain ill-posed as inevitably introducing uncertainty by
incorporating randomly sampled variables. To tackle the ill-posedness dilemma,
we focus on developing a reliable approximate left inverse for the
underdetermined system by constructing an overdetermined system with a non-zero
Gram determinant, thus ensuring a well-posed solution. Based on this principle,
we propose a well-posed invertible $1\times1$ convolution (WIC), which
eliminates the reliance on random variable sampling and enables the development
of well-posed invertible networks. Furthermore, we design two innovative
networks, WIN-Na\"ive and WIN, with the latter incorporating advanced
skip-connections to enhance long-term memory. Our methods are evaluated across
diverse RIC tasks, including reversible image hiding, image rescaling, and
image decolorization, consistently achieving state-of-the-art performance.
Extensive experiments validate the effectiveness of our approach, demonstrating
its ability to overcome the bottlenecks of existing RIC solutions and setting a
new benchmark in the field. Codes are available in
https://github.com/BNU-ERC-ITEA/WIN.

</details>


### [402] [GR-Gaussian: Graph-Based Radiative Gaussian Splatting for Sparse-View CT Reconstruction](https://arxiv.org/abs/2508.02408)
*Yikuang Yuluo,Yue Ma,Kuan Shen,Tongtong Jin,Wang Liao,Yangpu Ma,Fuquan Wang*

Main category: eess.IV

TL;DR: GR-Gaussian improves CT reconstruction under sparse-view conditions by reducing needle-like artifacts with a graph-based 3D Gaussian Splatting framework.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D Gaussian Splatting in CT reconstruction suffer from needle-like artifacts under sparse-view conditions due to reliance on average gradient magnitude.

Method: GR-Gaussian introduces a Denoised Point Cloud Initialization Strategy and a Pixel-Graph-Aware Gradient Strategy to refine gradient computation and improve density representation.

Result: Experiments show PSNR improvements of 0.67 dB and 0.92 dB, and SSIM gains of 0.011 and 0.021 on X-3D and real-world datasets.

Conclusion: GR-Gaussian is effective for accurate CT reconstruction in sparse-view conditions, outperforming existing methods.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a promising approach for CT
reconstruction. However, existing methods rely on the average gradient
magnitude of points within the view, often leading to severe needle-like
artifacts under sparse-view conditions. To address this challenge, we propose
GR-Gaussian, a graph-based 3D Gaussian Splatting framework that suppresses
needle-like artifacts and improves reconstruction accuracy under sparse-view
conditions. Our framework introduces two key innovations: (1) a Denoised Point
Cloud Initialization Strategy that reduces initialization errors and
accelerates convergence; and (2) a Pixel-Graph-Aware Gradient Strategy that
refines gradient computation using graph-based density differences, improving
splitting accuracy and density representation. Experiments on X-3D and
real-world datasets validate the effectiveness of GR-Gaussian, achieving PSNR
improvements of 0.67 dB and 0.92 dB, and SSIM gains of 0.011 and 0.021. These
results highlight the applicability of GR-Gaussian for accurate CT
reconstruction under challenging sparse-view conditions.

</details>


### [403] [Identifying actionable driver mutations in lung cancer using an efficient Asymmetric Transformer Decoder](https://arxiv.org/abs/2508.02431)
*Biagio Brattoli,Jack Shi,Jongchan Park,Taebum Lee,Donggeun Yoo,Sergio Pereira*

Main category: eess.IV

TL;DR: The paper evaluates MIL techniques to detect six key NSCLC driver mutations, introduces an Asymmetric Transformer Decoder, and outperforms existing models by 3-4%.


<details>
  <summary>Details</summary>
Motivation: To address limited genetic testing adoption in NSCLC by improving ML-based mutation detection for broader clinical impact.

Method: Uses Multiple Instance Learning (MIL) and an Asymmetric Transformer Decoder to analyze patch embeddings and incorporate tissue type.

Result: Outperforms top MIL models by 3% on average and over 4% for rare mutations like ERBB2 and BRAF.

Conclusion: The approach advances ML-based tests as practical alternatives to standard genetic testing, especially for rare mutations.

Abstract: Identifying actionable driver mutations in non-small cell lung cancer (NSCLC)
can impact treatment decisions and significantly improve patient outcomes.
Despite guideline recommendations, broader adoption of genetic testing remains
challenging due to limited availability and lengthy turnaround times. Machine
Learning (ML) methods for Computational Pathology (CPath) offer a potential
solution; however, research often focuses on only one or two common mutations,
limiting the clinical value of these tools and the pool of patients who can
benefit from them. This study evaluates various Multiple Instance Learning
(MIL) techniques to detect six key actionable NSCLC driver mutations: ALK,
BRAF, EGFR, ERBB2, KRAS, and MET ex14. Additionally, we introduce an Asymmetric
Transformer Decoder model that employs queries and key-values of varying
dimensions to maintain a low query dimensionality. This approach efficiently
extracts information from patch embeddings and minimizes overfitting risks,
proving highly adaptable to the MIL setting. Moreover, we present a method to
directly utilize tissue type in the model, addressing a typical MIL limitation
where either all regions or only some specific regions are analyzed, neglecting
biological relevance. Our method outperforms top MIL models by an average of
3%, and over 4% when predicting rare mutations such as ERBB2 and BRAF, moving
ML-based tests closer to being practical alternatives to standard genetic
testing.

</details>


### [404] [From Pixels to Pathology: Restoration Diffusion for Diagnostic-Consistent Virtual IHC](https://arxiv.org/abs/2508.02528)
*Jingsong Liu,Xiaofeng Deng,Han Li,Azar Kazemi,Christian Grashei,Gesa Wilkens,Xin You,Tanja Groll,Nassir Navab,Carolin Mogler,Peter J. Schüffler*

Main category: eess.IV

TL;DR: The paper introduces Star-Diff, a diffusion model for virtual staining from H&E to IHC, addressing challenges like misaligned ground truths and structural integrity. It proposes the Semantic Fidelity Score (SFS) for evaluation and achieves SOTA performance.


<details>
  <summary>Details</summary>
Motivation: H&E lacks molecular diagnostics, while IHC is costly and slow. Virtual staining bridges this gap but faces evaluation and structural challenges.

Method: Star-Diff combines residual and noise-based pathways for structure-aware staining. SFS evaluates diagnostic consistency.

Result: Star-Diff achieves SOTA in visual fidelity and diagnostic relevance on the BCI dataset.

Conclusion: Star-Diff offers a practical solution for clinical workflows, like intraoperative IHC synthesis, with rapid inference and strong alignment.

Abstract: Hematoxylin and eosin (H&E) staining is the clinical standard for assessing
tissue morphology, but it lacks molecular-level diagnostic information. In
contrast, immunohistochemistry (IHC) provides crucial insights into biomarker
expression, such as HER2 status for breast cancer grading, but remains costly
and time-consuming, limiting its use in time-sensitive clinical workflows. To
address this gap, virtual staining from H&E to IHC has emerged as a promising
alternative, yet faces two core challenges: (1) Lack of fair evaluation of
synthetic images against misaligned IHC ground truths, and (2) preserving
structural integrity and biological variability during translation. To this
end, we present an end-to-end framework encompassing both generation and
evaluation in this work. We introduce Star-Diff, a structure-aware staining
restoration diffusion model that reformulates virtual staining as an image
restoration task. By combining residual and noise-based generation pathways,
Star-Diff maintains tissue structure while modeling realistic biomarker
variability. To evaluate the diagnostic consistency of the generated IHC
patches, we propose the Semantic Fidelity Score (SFS), a
clinical-grading-task-driven metric that quantifies class-wise semantic
degradation based on biomarker classification accuracy. Unlike pixel-level
metrics such as SSIM and PSNR, SFS remains robust under spatial misalignment
and classifier uncertainty. Experiments on the BCI dataset demonstrate that
Star-Diff achieves state-of-the-art (SOTA) performance in both visual fidelity
and diagnostic relevance. With rapid inference and strong clinical alignment,it
presents a practical solution for applications such as intraoperative virtual
IHC synthesis.

</details>


### [405] [RL-U$^2$Net: A Dual-Branch UNet with Reinforcement Learning-Assisted Multimodal Feature Fusion for Accurate 3D Whole-Heart Segmentation](https://arxiv.org/abs/2508.02557)
*Jierui Qu,Jianchun Zhao*

Main category: eess.IV

TL;DR: Proposes RL-U²Net, a dual-branch U-Net with reinforcement learning for multi-modal 3D heart segmentation, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Improving multi-modal segmentation accuracy for cardiovascular diagnosis by addressing spatial inconsistency and static fusion in existing methods.

Method: Uses a dual-branch U-Net with an RL-XAlign module for feature alignment and cross-modal attention, followed by ensemble-based decision-making.

Result: Achieves Dice coefficients of 93.1% (CT) and 87.0% (MRI) on the MM-WHS 2017 dataset, surpassing existing methods.

Conclusion: RL-U²Net is effective and superior for multi-modal whole-heart segmentation, enhancing accuracy and robustness.

Abstract: Accurate whole-heart segmentation is a critical component in the precise
diagnosis and interventional planning of cardiovascular diseases. Integrating
complementary information from modalities such as computed tomography (CT) and
magnetic resonance imaging (MRI) can significantly enhance segmentation
accuracy and robustness. However, existing multi-modal segmentation methods
face several limitations: severe spatial inconsistency between modalities
hinders effective feature fusion; fusion strategies are often static and lack
adaptability; and the processes of feature alignment and segmentation are
decoupled and inefficient. To address these challenges, we propose a
dual-branch U-Net architecture enhanced by reinforcement learning for feature
alignment, termed RL-U$^2$Net, designed for precise and efficient multi-modal
3D whole-heart segmentation. The model employs a dual-branch U-shaped network
to process CT and MRI patches in parallel, and introduces a novel RL-XAlign
module between the encoders. The module employs a cross-modal attention
mechanism to capture semantic correspondences between modalities and a
reinforcement-learning agent learns an optimal rotation strategy that
consistently aligns anatomical pose and texture features. The aligned features
are then reconstructed through their respective decoders. Finally, an
ensemble-learning-based decision module integrates the predictions from
individual patches to produce the final segmentation result. Experimental
results on the publicly available MM-WHS 2017 dataset demonstrate that the
proposed RL-U$^2$Net outperforms existing state-of-the-art methods, achieving
Dice coefficients of 93.1% on CT and 87.0% on MRI, thereby validating the
effectiveness and superiority of the proposed approach.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [406] [ChEmbed: Enhancing Chemical Literature Search Through Domain-Specific Text Embeddings](https://arxiv.org/abs/2508.01643)
*Ali Shiraee Kasmaee,Mohammad Khodadad,Mehdi Astaraki,Mohammad Arshi Saloot,Nicholas Sherck,Hamidreza Mahyar,Soheila Samiee*

Main category: cs.IR

TL;DR: ChEmbed is a specialized text embedding model for chemistry, improving retrieval quality by fine-tuning on chemical text and adding specialized tokens.


<details>
  <summary>Details</summary>
Motivation: General-purpose text embedding models fail to represent complex chemical terminologies, leading to poor retrieval quality in chemistry literature.

Method: Developed ChEmbed by fine-tuning on chemistry-specific text (PubChem, Semantic Scholar, ChemRxiv), generating synthetic queries, and adding 900 chemical tokens to the tokenizer.

Result: ChEmbed outperforms state-of-the-art models, increasing nDCG@10 from 0.82 to 0.91 (+9 pp) on the ChemRxiv Retrieval benchmark.

Conclusion: ChEmbed is a lightweight, reproducible solution that significantly enhances chemical literature retrieval.

Abstract: Retrieval-Augmented Generation (RAG) systems in chemistry heavily depend on
accurate and relevant retrieval of chemical literature. However,
general-purpose text embedding models frequently fail to adequately represent
complex chemical terminologies, resulting in suboptimal retrieval quality.
Specialized embedding models tailored to chemical literature retrieval have not
yet been developed, leaving a substantial performance gap. To address this
challenge, we introduce ChEmbed, a domain-adapted family of text embedding
models fine-tuned on a dataset comprising chemistry-specific text from the
PubChem, Semantic Scholar, and ChemRxiv corpora. To create effective training
data, we employ large language models to synthetically generate queries,
resulting in approximately 1.7 million high-quality query-passage pairs.
Additionally, we augment the tokenizer by adding 900 chemically specialized
tokens to previously unused slots, which significantly reduces the
fragmentation of chemical entities, such as IUPAC names. ChEmbed also maintains
a 8192-token context length, enabling the efficient retrieval of longer
passages compared to many other open-source embedding models, which typically
have a context length of 512 or 2048 tokens. Evaluated on our newly introduced
ChemRxiv Retrieval benchmark, ChEmbed outperforms state-of-the-art general
embedding models, raising nDCG@10 from 0.82 to 0.91 (+9 pp). ChEmbed represents
a practical, lightweight, and reproducible embedding solution that effectively
improves retrieval for chemical literature search.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [407] [Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and Regional Languages Around the Globe](https://arxiv.org/abs/2508.01691)
*Tiantian Feng,Kevin Huang,Anfeng Xu,Xuan Shi,Thanathai Lertpetchpun,Jihwan Lee,Yoonjeong Lee,Dani Byrd,Shrikanth Narayanan*

Main category: cs.SD

TL;DR: Voxlect is a benchmark for modeling dialects using speech foundation models, evaluating performance across multiple languages and dialects, and enabling downstream applications like ASR augmentation and speech generation evaluation.


<details>
  <summary>Details</summary>
Motivation: To address the need for comprehensive dialect modeling and evaluation in speech foundation models, leveraging diverse datasets for robust analysis.

Method: Utilizes over 2 million training utterances from 30 speech corpora with dialectal information, evaluating widely used speech foundation models for dialect classification and robustness under noise.

Result: Demonstrates dialect classification performance aligned with geographic continuity and enables applications like ASR augmentation and speech generation evaluation.

Conclusion: Voxlect provides a valuable tool for dialect modeling and downstream applications, publicly available for further research.

Abstract: We present Voxlect, a novel benchmark for modeling dialects and regional
languages worldwide using speech foundation models. Specifically, we report
comprehensive benchmark evaluations on dialects and regional language varieties
in English, Arabic, Mandarin and Cantonese, Tibetan, Indic languages, Thai,
Spanish, French, German, Brazilian Portuguese, and Italian. Our study used over
2 million training utterances from 30 publicly available speech corpora that
are provided with dialectal information. We evaluate the performance of several
widely used speech foundation models in classifying speech dialects. We assess
the robustness of the dialectal models under noisy conditions and present an
error analysis that highlights modeling results aligned with geographic
continuity. In addition to benchmarking dialect classification, we demonstrate
several downstream applications enabled by Voxlect. Specifically, we show that
Voxlect can be applied to augment existing speech recognition datasets with
dialect information, enabling a more detailed analysis of ASR performance
across dialectal variations. Voxlect is also used as a tool to evaluate the
performance of speech generation systems. Voxlect is publicly available with
the license of the RAIL family at: https://github.com/tiantiaf0627/voxlect.

</details>


### [408] [Hidden in the Noise: Unveiling Backdoors in Audio LLMs Alignment through Latent Acoustic Pattern Triggers](https://arxiv.org/abs/2508.02175)
*Liang Lin,Miao Yu,Kaiwen Luo,Yibo Zhang,Lilan Peng,Dexian Wang,Xuehai Tang,Yuanhe Zhang,Xikang Yang,Zhenhong Zhou,Kun Wang,Yang Liu*

Main category: cs.SD

TL;DR: The paper investigates vulnerabilities of Audio Large Language Models (ALLMs) to backdoor attacks using acoustic triggers, introducing the Hidden in the Noise (HIN) framework. It evaluates risks via the AudioSafe benchmark, revealing high attack success rates and stealthy impacts.


<details>
  <summary>Details</summary>
Motivation: The rise of ALLMs necessitates understanding their safety risks, particularly unique audio-specific vulnerabilities like backdoor attacks.

Method: The HIN framework modifies audio waveforms with acoustic changes (e.g., temporal dynamics, spectrally tailored noise) to embed triggers. The AudioSafe benchmark assesses nine risk types.

Result: Experiments show high attack success rates (over 90%) for certain audio features, varying sensitivity across features, and stealthy impacts on ALLMs.

Conclusion: ALLMs are vulnerable to audio-specific backdoor attacks, highlighting the need for improved safety measures.

Abstract: As Audio Large Language Models (ALLMs) emerge as powerful tools for speech
processing, their safety implications demand urgent attention. While
considerable research has explored textual and vision safety, audio's distinct
characteristics present significant challenges. This paper first investigates:
Is ALLM vulnerable to backdoor attacks exploiting acoustic triggers? In
response to this issue, we introduce Hidden in the Noise (HIN), a novel
backdoor attack framework designed to exploit subtle, audio-specific features.
HIN applies acoustic modifications to raw audio waveforms, such as alterations
to temporal dynamics and strategic injection of spectrally tailored noise.
These changes introduce consistent patterns that an ALLM's acoustic feature
encoder captures, embedding robust triggers within the audio stream. To
evaluate ALLM robustness against audio-feature-based triggers, we develop the
AudioSafe benchmark, assessing nine distinct risk types. Extensive experiments
on AudioSafe and three established safety datasets reveal critical
vulnerabilities in existing ALLMs: (I) audio features like environment noise
and speech rate variations achieve over 90% average attack success rate. (II)
ALLMs exhibit significant sensitivity differences across acoustic features,
particularly showing minimal response to volume as a trigger, and (III)
poisoned sample inclusion causes only marginal loss curve fluctuations,
highlighting the attack's stealth.

</details>


### [409] [Localizing Audio-Visual Deepfakes via Hierarchical Boundary Modeling](https://arxiv.org/abs/2508.02000)
*Xuanjun Chen,Shih-Peng Cheng,Jiawei Du,Lin Zhang,Xiaoxiao Miao,Chung-Che Wang,Haibin Wu,Hung-yi Lee,Jyh-Shing Roger Jang*

Main category: cs.SD

TL;DR: HBMNet, a Hierarchical Boundary Modeling Network, improves audio-visual temporal deepfake localization by integrating audio-visual feature encoding, coarse-fine boundary modeling, and multi-scale temporal cues.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of localizing deepfake regions in audio-visual content where manipulations are sparse and partial.

Method: Proposes HBMNet with three modules: Audio-Visual Feature Encoder, Coarse Proposal Generator, and Fine-grained Probabilities Generator, enhanced by modality-specific encoding and frame-level supervision.

Result: HBMNet outperforms BA-TFD and UMMAFormer, with improved precision and recall, and shows scalability potential.

Conclusion: HBMNet effectively tackles partial manipulation localization by combining modality and temporal enhancements, demonstrating superior performance.

Abstract: Audio-visual temporal deepfake localization under the content-driven partial
manipulation remains a highly challenging task. In this scenario, the deepfake
regions are usually only spanning a few frames, with the majority of the rest
remaining identical to the original. To tackle this, we propose a Hierarchical
Boundary Modeling Network (HBMNet), which includes three modules: an
Audio-Visual Feature Encoder that extracts discriminative frame-level
representations, a Coarse Proposal Generator that predicts candidate boundary
regions, and a Fine-grained Probabilities Generator that refines these
proposals using bidirectional boundary-content probabilities. From the modality
perspective, we enhance audio-visual learning through dedicated encoding and
fusion, reinforced by frame-level supervision to boost discriminability. From
the temporal perspective, HBMNet integrates multi-scale cues and bidirectional
boundary-content relationships. Experiments show that encoding and fusion
primarily improve precision, while frame-level supervision boosts recall. Each
module (audio-visual fusion, temporal scales, bi-directionality) contributes
complementary benefits, collectively enhancing localization performance. HBMNet
outperforms BA-TFD and UMMAFormer and shows improved potential scalability with
more training data.

</details>


### [410] [Towards Reliable Audio Deepfake Attribution and Model Recognition: A Multi-Level Autoencoder-Based Framework](https://arxiv.org/abs/2508.02521)
*Andrea Di Pierno,Luca Guarnera,Dario Allegra,Sebastiano Battiato*

Main category: cs.SD

TL;DR: LAVA is a hierarchical framework for detecting and attributing audio deepfakes, achieving high accuracy in identifying generation technologies and specific models.


<details>
  <summary>Details</summary>
Motivation: Addressing the underexplored challenge of attributing audio deepfakes to their source models to enhance trust in digital communications.

Method: Uses a convolutional autoencoder for latent representations and two classifiers (ADA and ADMR) with confidence-based rejection for robustness.

Result: Achieves F1-scores over 95% for ADA and 96.31% for ADMR, with robustness confirmed on unseen attacks.

Conclusion: LAVA advances deepfake attribution and model recognition under open-set conditions, validated on public benchmarks.

Abstract: The proliferation of audio deepfakes poses a growing threat to trust in
digital communications. While detection methods have advanced, attributing
audio deepfakes to their source models remains an underexplored yet crucial
challenge. In this paper we introduce LAVA (Layered Architecture for Voice
Attribution), a hierarchical framework for audio deepfake detection and model
recognition that leverages attention-enhanced latent representations extracted
by a convolutional autoencoder trained solely on fake audio. Two specialized
classifiers operate on these features: Audio Deepfake Attribution (ADA), which
identifies the generation technology, and Audio Deepfake Model Recognition
(ADMR), which recognize the specific generative model instance. To improve
robustness under open-set conditions, we incorporate confidence-based rejection
thresholds. Experiments on ASVspoof2021, FakeOrReal, and CodecFake show strong
performance: the ADA classifier achieves F1-scores over 95% across all
datasets, and the ADMR module reaches 96.31% macro F1 across six classes.
Additional tests on unseen attacks from ASVpoof2019 LA and error propagation
analysis confirm LAVA's robustness and reliability. The framework advances the
field by introducing a supervised approach to deepfake attribution and model
recognition under open-set conditions, validated on public benchmarks and
accompanied by publicly released models and code. Models and code are available
at https://www.github.com/adipiz99/lava-framework.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [411] [Human Capital Visualization using Speech Amount during Meetings](https://arxiv.org/abs/2508.02075)
*Ekai Hashimoto,Takeshi Mizumoto,Kohei Nagira,Shun Shiramatsu*

Main category: cs.HC

TL;DR: The study proposes a method to visualize human capital by analyzing speech in routine meetings, addressing gaps in conventional quantification methods.


<details>
  <summary>Details</summary>
Motivation: To highlight the role of conversations in human capital, which conventional methods overlook, and to foster innovation through better internal communication.

Method: Uses conversation visualization technology to quantify speech, analyzing differences by attributes (e.g., gender, job post), participant presence, and correlations with continuous attributes.

Result: Analyzed speech amounts in weekly meetings at SMEs, demonstrating the method's effectiveness in visualizing human capital.

Conclusion: The proposed method effectively quantifies and visualizes human capital through speech analysis, offering insights for organizational improvement.

Abstract: In recent years, many companies have recognized the importance of human
resources and are investing in human capital to revitalize their organizations
and enhance internal communication, thereby fostering innovation. However,
conventional quantification methods have mainly focused on readily measurable
indicators without addressing the fundamental role of conversations in human
capital. This study focuses on routine meetings and proposes strategies to
visualize human capital by analyzing speech amount during these meetings. We
employ conversation visualization technology, which operates effectively, to
quantify speech. We then measure differences in speech amount by attributes
such as gender and job post, changes in speech amount depending on whether
certain participants are present, and correlations between speech amount and
continuous attributes. To verify the effectiveness of our proposed methods, we
analyzed speech amounts by departmental affiliation during weekly meetings at
small to medium enterprises.

</details>


### [412] [Understanding User Preferences for Interaction Styles in Conversational Recommender Systems: The Predictive Role of System Qualities, User Experience, and Traits](https://arxiv.org/abs/2508.02328)
*Raj Mahmud,Shlomo Berkovsky,Mukesh Prasad,A. Baki Kocaballi*

Main category: cs.HC

TL;DR: The study explores user interaction preferences in Conversational Recommender Systems (CRSs), identifying enjoyment, usefulness, novelty, and conversational quality as predictors of exploratory interaction preference. It also uncovers latent user profiles and moderating effects of age, gender, and control preference.


<details>
  <summary>Details</summary>
Motivation: To understand the factors shaping user interaction preferences in CRSs, which support both task-oriented and exploratory interactions.

Method: A within-subjects study with 139 participants experiencing scripted CRS dialogues, followed by ratings and logistic regression analysis. Clustering and moderation analyses were also conducted.

Result: Exploratory interaction preference was predicted by enjoyment, usefulness, novelty, and conversational quality. Five latent user profiles emerged, with age, gender, and control preference moderating choices.

Conclusion: The findings enhance CRS user modelling by integrating affective, cognitive, and trait-level predictors, informing adaptive dialogue design for conversational AI systems.

Abstract: Conversational Recommender Systems (CRSs) deliver personalised
recommendations through multi-turn natural language dialogue and increasingly
support both task-oriented and exploratory interactions. Yet, the factors
shaping user interaction preferences remain underexplored. In this
within-subjects study (\(N = 139\)), participants experienced two scripted CRS
dialogues, rated their experiences, and indicated the importance of eight
system qualities. Logistic regression revealed that preference for the
exploratory interaction was predicted by enjoyment, usefulness, novelty, and
conversational quality. Unexpectedly, perceived effectiveness was also
associated with exploratory preference. Clustering uncovered five latent user
profiles with distinct dialogue style preferences. Moderation analyses
indicated that age, gender, and control preference significantly influenced
these choices. These findings integrate affective, cognitive, and trait-level
predictors into CRS user modelling and inform autonomy-sensitive,
value-adaptive dialogue design. The proposed predictive and adaptive framework
applies broadly to conversational AI systems seeking to align dynamically with
evolving user needs.

</details>


### [413] [Six Guidelines for Trustworthy, Ethical and Responsible Automation Design](https://arxiv.org/abs/2508.02371)
*Matouš Jelínek,Nadine Schlicker,Ewart de Visser*

Main category: cs.HC

TL;DR: The paper proposes six design guidelines for fostering accurate trustworthiness assessments in automated systems, derived from interdisciplinary literature, to promote ethical and safe human-automation interactions.


<details>
  <summary>Details</summary>
Motivation: Calibrated trust in automated systems is essential for their safe integration into society, requiring alignment between user perception and actual system trustworthiness.

Method: The guidelines are derived from literature in human-computer interaction, cognitive psychology, automation research, user-experience design, and ethics, incorporating principles from pragmatics like common ground and Gricean maxims.

Result: The guidelines aim to help designers create systems with relevant trustworthiness cues, fostering calibrated trust and improving human-automation interactions.

Conclusion: The proposed guidelines offer actionable insights for designers and can also serve as a tool to evaluate existing systems' effectiveness in enabling accurate trustworthiness assessments.

Abstract: Calibrated trust in automated systems (Lee and See 2004) is critical for
their safe and seamless integration into society. Users should only rely on a
system recommendation when it is actually correct and reject it when it is
factually wrong. One requirement to achieve this goal is an accurate
trustworthiness assessment, ensuring that the user's perception of the system's
trustworthiness aligns with its actual trustworthiness, allowing users to make
informed decisions about the extent to which they can rely on the system
(Schlicker et al. 2022). We propose six design guidelines to help designers
optimize for accurate trustworthiness assessments, thus fostering ethical and
responsible human-automation interactions. The proposed guidelines are derived
from existing literature in various fields, such as human-computer interaction,
cognitive psychology, automation research, user-experience design, and ethics.
We are incorporating key principles from the field of pragmatics, specifically
the cultivation of common ground (H. H. Clark 1996) and Gricean communication
maxims (Grice 1975). These principles are essential for the design of automated
systems because the user's perception of the system's trustworthiness is shaped
by both environmental contexts, such as organizational culture or societal
norms, and by situational context, including the specific circumstances or
scenarios in which the interaction occurs (Hoff and Bashir 2015). Our proposed
guidelines provide actionable insights for designers to create automated
systems that make relevant trustworthiness cues available. This would ideally
foster calibrated trust and more satisfactory, productive, and safe
interactions between humans and automated systems. Furthermore, the proposed
heuristics might work as a tool for evaluating to what extent existing systems
enable users to accurately assess a system's trustworthiness.

</details>


### [414] [AIAP: A No-Code Workflow Builder for Non-Experts with Natural Language and Multi-Agent Collaboration](https://arxiv.org/abs/2508.02470)
*Hyunjn An,Yongwon Kim,Wonduk Seo,Joonil Park,Daye Kang,Changhoon Oh,Dokyun Kim,Seunghyun Lee*

Main category: cs.HC

TL;DR: AIAP is a no-code platform using natural language and visual workflows to simplify AI design for non-experts, improving user experience and reducing barriers.


<details>
  <summary>Details</summary>
Motivation: Non-experts struggle with expressing intent and managing AI system complexity, necessitating a more intuitive design tool.

Method: AIAP integrates natural language input with visual workflows, using a multi-agent system to decompose user instructions into modular steps.

Result: A user study with 32 participants showed improved ability to develop services intuitively due to AI-generated suggestions and modular workflows.

Conclusion: Natural language-based visual programming reduces barriers and enhances user experience in AI service design.

Abstract: While many tools are available for designing AI, non-experts still face
challenges in clearly expressing their intent and managing system complexity.
We introduce AIAP, a no-code platform that integrates natural language input
with visual workflows. AIAP leverages a coordinated multi-agent system to
decompose ambiguous user instructions into modular, actionable steps, hidden
from users behind a unified interface. A user study involving 32 participants
showed that AIAP's AI-generated suggestions, modular workflows, and automatic
identification of data, actions, and context significantly improved
participants' ability to develop services intuitively. These findings highlight
that natural language-based visual programming significantly reduces barriers
and enhances user experience in AI service design.

</details>


### [415] [Visuo-Acoustic Hand Pose and Contact Estimation](https://arxiv.org/abs/2508.00852)
*Yuemin Ma,Uksang Yoo,Yunchao Yao,Shahram Najam Syed,Luca Bondi,Jonathan Francis,Jean Oh,Jeffrey Ichnowski*

Main category: cs.HC

TL;DR: VibeMesh is a wearable system combining vision and acoustic sensing for accurate hand pose and contact estimation, outperforming vision-only methods.


<details>
  <summary>Details</summary>
Motivation: Hand pose and contact estimation is crucial for robotics, VR, and biomechanics but faces challenges like occlusion and limited sensing.

Method: VibeMesh uses bone-conduction speakers and piezoelectric microphones to capture acoustic signals, paired with a graph-based attention network for cross-modal signal processing.

Result: VibeMesh achieves higher accuracy and robustness than vision-only baselines, especially in occluded or static-contact scenarios.

Conclusion: VibeMesh offers a lightweight, non-intrusive solution for dense hand contact and pose estimation, validated by empirical results.

Abstract: Accurately estimating hand pose and hand-object contact events is essential
for robot data-collection, immersive virtual environments, and biomechanical
analysis, yet remains challenging due to visual occlusion, subtle contact cues,
limitations in vision-only sensing, and the lack of accessible and flexible
tactile sensing. We therefore introduce VibeMesh, a novel wearable system that
fuses vision with active acoustic sensing for dense, per-vertex hand contact
and pose estimation. VibeMesh integrates a bone-conduction speaker and sparse
piezoelectric microphones, distributed on a human hand, emitting structured
acoustic signals and capturing their propagation to infer changes induced by
contact. To interpret these cross-modal signals, we propose a graph-based
attention network that processes synchronized audio spectra and RGB-D-derived
hand meshes to predict contact with high spatial resolution. We contribute: (i)
a lightweight, non-intrusive visuo-acoustic sensing platform; (ii) a
cross-modal graph network for joint pose and contact inference; (iii) a dataset
of synchronized RGB-D, acoustic, and ground-truth contact annotations across
diverse manipulation scenarios; and (iv) empirical results showing that
VibeMesh outperforms vision-only baselines in accuracy and robustness,
particularly in occluded or static-contact settings.

</details>


### [416] [Sonify Anything: Towards Context-Aware Sonic Interactions in AR](https://arxiv.org/abs/2508.01789)
*Laura Schütz,Sasan Matinfar,Ulrich Eck,Daniel Roth,Nassir Navab*

Main category: cs.HC

TL;DR: The paper proposes a framework for generating context-aware, material-based sounds in AR to enhance realism and interaction by using computer vision and physical modeling synthesis.


<details>
  <summary>Details</summary>
Motivation: Current AR lacks natural sonic interactions, leading to incongruent multisensory experiences. The goal is to improve realism and user perception by dynamically generating sounds based on real object materials and impact dynamics.

Method: The framework uses computer vision to recognize and segment real object materials, then employs physical modeling synthesis to generate material-based sounds in real-time.

Result: A user study showed material-based sounds significantly improved realism and enabled better material distinction compared to generic sounds.

Conclusion: Context-aware, material-based sounds in AR enhance realism and user perception of real-world surroundings.

Abstract: In Augmented Reality (AR), virtual objects interact with real objects.
However, the lack of physicality of virtual objects leads to the absence of
natural sonic interactions. When virtual and real objects collide, either no
sound or a generic sound is played. Both lead to an incongruent multisensory
experience, reducing interaction and object realism. Unlike in Virtual Reality
(VR) and games, where predefined scenes and interactions allow for the playback
of pre-recorded sound samples, AR requires real-time sound synthesis that
dynamically adapts to novel contexts and objects to provide audiovisual
congruence during interaction. To enhance real-virtual object interactions in
AR, we propose a framework for context-aware sounds using methods from computer
vision to recognize and segment the materials of real objects. The material's
physical properties and the impact dynamics of the interaction are used to
generate material-based sounds in real-time using physical modelling synthesis.
In a user study with 24 participants, we compared our congruent material-based
sounds to a generic sound effect, mirroring the current standard of
non-context-aware sounds in AR applications. The results showed that
material-based sounds led to significantly more realistic sonic interactions.
Material-based sounds also enabled participants to distinguish visually similar
materials with significantly greater accuracy and confidence. These findings
show that context-aware, material-based sonic interactions in AR foster a
stronger sense of realism and enhance our perception of real-world
surroundings.

</details>


### [417] [Implicit Search Intent Recognition using EEG and Eye Tracking: Novel Dataset and Cross-User Prediction](https://arxiv.org/abs/2508.01860)
*Mansi Sharma,Shuang Chen,Philipp Müller,Maurice Rekrut,Antonio Krüger*

Main category: cs.HC

TL;DR: The paper addresses limitations in recognizing human visual search intents (navigational vs. informational) by introducing a publicly available EEG and eye-tracking dataset and a cross-user prediction method, achieving 84.5% accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve real-world applicability of intent recognition by overcoming fixed search times and user-specific training requirements.

Method: Combines EEG and eye-tracking data with a cross-user prediction approach, allowing user-determined search times.

Result: Achieves 84.5% accuracy in cross-user evaluations, comparable to within-user accuracy (85.5%).

Conclusion: The method enhances flexibility and practicality for real-world visual search assistance.

Abstract: For machines to effectively assist humans in challenging visual search tasks,
they must differentiate whether a human is simply glancing into a scene
(navigational intent) or searching for a target object (informational intent).
Previous research proposed combining electroencephalography (EEG) and
eye-tracking measurements to recognize such search intents implicitly, i.e.,
without explicit user input. However, the applicability of these approaches to
real-world scenarios suffers from two key limitations. First, previous work
used fixed search times in the informational intent condition -- a stark
contrast to visual search, which naturally terminates when the target is found.
Second, methods incorporating EEG measurements addressed prediction scenarios
that require ground truth training data from the target user, which is
impractical in many use cases. We address these limitations by making the first
publicly available EEG and eye-tracking dataset for navigational vs.
informational intent recognition, where the user determines search times. We
present the first method for cross-user prediction of search intents from EEG
and eye-tracking recordings and reach 84.5% accuracy in leave-one-user-out
evaluations -- comparable to within-user prediction accuracy (85.5%) but
offering much greater flexibility

</details>

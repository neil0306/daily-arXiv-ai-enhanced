<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 67]
- [cs.CV](#cs.CV) [Total: 204]
- [cs.CG](#cs.CG) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 2]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [physics.optics](#physics.optics) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.CY](#cs.CY) [Total: 4]
- [cs.RO](#cs.RO) [Total: 9]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.AI](#cs.AI) [Total: 9]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.LG](#cs.LG) [Total: 18]
- [cs.MM](#cs.MM) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Enhancing Urban Visual Place Recognition for Crowdsourced Flood Imagery via LLM-Guided Attention](https://arxiv.org/abs/2512.11811)
*Fengyi Xu,Jun Ma,Waishan Qiu,Cui Guo*

Main category: cs.CL

TL;DR: VPR-AttLLM is a model-agnostic framework that integrates LLM reasoning into Visual Place Recognition to improve geo-localization of crowdsourced crisis imagery without retraining.


<details>
  <summary>Details</summary>
Motivation: Crowdsourced street-view imagery from social media lacks reliable geographic metadata for emergency response, and existing VPR models degrade with visual distortions and domain shifts in cross-source scenarios.

Method: Integrates LLM semantic reasoning and geospatial knowledge into VPR pipelines through attention-guided descriptor enhancement, identifying location-informative regions and suppressing transient visual noise without model retraining.

Result: Consistently improves recall performance across three SOTA VPR models (CosPlace, EigenPlaces, SALAD) with relative gains of 1-3% typically, reaching up to 8% on challenging real flood imagery across multiple benchmarks.

Conclusion: Establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval, bridging human-like spatial reasoning with modern VPR architectures for scalable urban monitoring and rapid crisis imagery geo-localization.

Abstract: Crowdsourced street-view imagery from social media provides valuable real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing Visual Place Recognition (VPR) models exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts inherent in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geospatial knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress transient visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.

</details>


### [2] [Reinforcement Learning for Latent-Space Thinking in LLMs](https://arxiv.org/abs/2512.11816)
*Enes Özeren,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: RL-trained latent-space thinking models still underperform traditional language-space CoT in math reasoning despite attempts to improve efficiency.


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought reasoning in discrete language space is inefficient due to many tokens enforcing linguistic rules rather than reasoning. Latent-space thinking in continuous embedding space could be more efficient, but existing methods fail on complex tasks like mathematical reasoning.

Method: Investigated reinforcement learning techniques (GRPO and novel Latent RL method) for directly optimizing latent thinking steps, as an alternative to supervised fine-tuning approaches like Coconut that have inherent limitations.

Result: RL-trained latent-space thinking models still lag behind traditional language-space CoT models in mathematical reasoning domain, despite attempts to improve efficiency through continuous embedding space.

Conclusion: Current RL approaches for latent-space thinking are not yet competitive with traditional language-space CoT for complex reasoning tasks like mathematics, highlighting the need for further research in this direction.

Abstract: Chain-of-Thought (CoT) reasoning typically utilizes the discrete language space for thinking, which is inherently inefficient, as many generated tokens only enforce linguistic rules that are not required for reasoning. To bypass this, latent-space thinking allows models to think using the continuous embedding space. While existing methods for training those models show domain-specific gains, they fail to maintain performance in complex tasks, such as mathematical reasoning. We experimentally demonstrate that the Coconut approach, a form of supervised fine-tuning for latent-space thinking, is highly sensitive to design choices and exhibits several inherent limitations. To address these issues, we investigate reinforcement learning (RL) techniques -- an underexplored direction in latent-space thinking -- including GRPO and design a novel Latent RL method for directly optimizing the latent thinking steps. Our experimental results reveal that these RL-trained models still lag behind traditional language-space CoT models in the mathematical reasoning domain. We make our codebase publicly available.

</details>


### [3] [KH-FUNSD: A Hierarchical and Fine-Grained Layout Analysis Dataset for Low-Resource Khmer Business Document](https://arxiv.org/abs/2512.11849)
*Nimol Thuon,Jun Du*

Main category: cs.CL

TL;DR: First hierarchical dataset (KH-FUNSD) for Khmer business document understanding, featuring three-level annotation for layout analysis and information extraction, with baseline model benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address the lack of document AI tools for Khmer language (17M speakers), particularly for business documents critical for public administration and private enterprise in Cambodia.

Method: Created KH-FUNSD dataset with three-level annotation: 1) region detection (header, form field, footer zones), 2) FUNSD-style annotation (questions, answers, headers, relationships), 3) fine-grained semantic classification (field labels, values, headers, footers, symbols).

Result: First publicly available hierarchical dataset for Khmer form document understanding (receipts, invoices, quotations), with benchmark results from leading models establishing baselines for non-Latin, low-resource scripts.

Conclusion: KH-FUNSD fills critical resource gap for Khmer document AI, enabling both layout analysis and information extraction, while highlighting unique challenges of non-Latin scripts in document understanding.

Abstract: Automated document layout analysis remains a major challenge for low-resource, non-Latin scripts. Khmer is a language spoken daily by over 17 million people in Cambodia, receiving little attention in the development of document AI tools. The lack of dedicated resources is particularly acute for business documents, which are critical for both public administration and private enterprise. To address this gap, we present \textbf{KH-FUNSD}, the first publicly available, hierarchically annotated dataset for Khmer form document understanding, including receipts, invoices, and quotations. Our annotation framework features a three-level design: (1) region detection that divides each document into core zones such as header, form field, and footer; (2) FUNSD-style annotation that distinguishes questions, answers, headers, and other key entities, together with their relationships; and (3) fine-grained classification that assigns specific semantic roles, such as field labels, values, headers, footers, and symbols. This multi-level approach supports both comprehensive layout analysis and precise information extraction. We benchmark several leading models, providing the first set of baseline results for Khmer business documents, and discuss the distinct challenges posed by non-Latin, low-resource scripts. The KH-FUNSD dataset and documentation will be available at URL.

</details>


### [4] [Direct Confidence Alignment: Aligning Verbalized Confidence with Internal Confidence In Large Language Models](https://arxiv.org/abs/2512.11998)
*Glenn Zhang,Treasure Mayowa,Jason Fan,Yicheng Fu,Aaron Sandoval,Sean O'Brien,Kevin Zhu*

Main category: cs.CL

TL;DR: DCA uses Direct Preference Optimization to align LLM's verbalized confidence with internal confidence, improving transparency but effectiveness varies by model architecture.


<details>
  <summary>Details</summary>
Motivation: LLMs need trustworthy calibration, but internal confidence (from token probabilities) doesn't align well with verbalized confidence, causing misleading calibration results.

Method: Direct Confidence Alignment (DCA) uses Direct Preference Optimization to align LLM's verbalized confidence with its internal confidence rather than ground-truth accuracy.

Result: DCA improves alignment metrics on certain model architectures but is ineffective on others, showing need for model-aware approaches.

Conclusion: While DCA enhances confidence alignment for some LLMs, its varying effectiveness across architectures highlights the need for more tailored approaches for trustworthy LLMs.

Abstract: Producing trustworthy and reliable Large Language Models (LLMs) has become increasingly important as their usage becomes more widespread. Calibration seeks to achieve this by improving the alignment between the model's confidence and the actual likelihood of its responses being correct or desirable. However, it has been observed that the internal confidence of a model, derived from token probabilities, is not well aligned with its verbalized confidence, leading to misleading results with different calibration methods. In this paper, we propose Direct Confidence Alignment (DCA), a method using Direct Preference Optimization to align an LLM's verbalized confidence with its internal confidence rather than ground-truth accuracy, enhancing model transparency and reliability by ensuring closer alignment between the two confidence measures. We evaluate DCA across multiple open-weight LLMs on a wide range of datasets. To further assess this alignment, we also introduce three new calibration error-based metrics. Our results show that DCA improves alignment metrics on certain model architectures, reducing inconsistencies in a model's confidence expression. However, we also show that it can be ineffective on others, highlighting the need for more model-aware approaches in the pursuit of more interpretable and trustworthy LLMs.

</details>


### [5] [Hold Onto That Thought: Assessing KV Cache Compression On Reasoning](https://arxiv.org/abs/2512.12008)
*Minghui Liu,Aadi Palnitkar,Tahseen Rabbani,Hyunwoo Jae,Kyle Rui Sang,Dixi Yao,Shayan Shabihi,Fuheng Zhao,Tian Li,Ce Zhang,Furong Huang,Kunpeng Zhang*

Main category: cs.CL

TL;DR: Benchmarking KV cache compression strategies for LLMs on long-reasoning tasks reveals H2O and decoding-enabled SnapKV perform best, with tradeoffs between cache size and inference costs.


<details>
  <summary>Details</summary>
Motivation: LLMs face memory bottlenecks from KV cache growth during long-context tasks, especially for reasoning tasks requiring long decoding sequences. Existing compression strategies focus on prefill phase but lack evaluation on reasoning tasks with complex prompts and multi-step thinking sequences.

Method: Benchmark multiple popular KV cache compression strategies on long-reasoning tasks using reasoning models. Test strategies including H2O and a decoding-enabled variant of SnapKV on benchmarks like GSM8K and MATH500 with complex prompts generating thousands of tokens.

Result: For non-reasoning Llama-3.1-8B-Instruct, no single strategy fits all datasets. For reasoning models, H2O and decoding-enabled SnapKV emerge as dominant strategies, showing heavy-hitter tracking works well for reasoning traces. Low-budget eviction strategies can produce longer reasoning traces, revealing cache size vs inference cost tradeoffs.

Conclusion: KV cache compression strategies perform differently on reasoning vs non-reasoning tasks. Heavy-hitter tracking methods like H2O and modified SnapKV are effective for reasoning models, while cache size reduction comes with inference cost tradeoffs that must be balanced.

Abstract: Large language models (LLMs) have demonstrated remarkable performance on long-context tasks, but are often bottlenecked by memory constraints. Namely, the KV cache, which is used to significantly speed up attention computations, grows linearly with context length. A suite of compression algorithms has been introduced to alleviate cache growth by evicting unimportant tokens. However, several popular strategies are targeted towards the prefill phase, i.e., processing long prompt context, and their performance is rarely assessed on reasoning tasks requiring long decoding. In particular, short but complex prompts, such as those in benchmarks like GSM8K and MATH500, often benefit from multi-step reasoning and self-reflection, resulting in thinking sequences thousands of tokens long. In this work, we benchmark the performance of several popular compression strategies on long-reasoning tasks. For the non-reasoning Llama-3.1-8B-Instruct, we determine that no singular strategy fits all, and that performance is heavily influenced by dataset type. However, we discover that H2O and our decoding-enabled variant of SnapKV are dominant strategies for reasoning models, indicating the utility of heavy-hitter tracking for reasoning traces. We also find that eviction strategies at low budgets can produce longer reasoning traces, revealing a tradeoff between cache size and inference costs.

</details>


### [6] [Benchmarking Contextual Understanding for In-Car Conversational Systems](https://arxiv.org/abs/2512.12042)
*Philipp Habicht,Lev Sorokin,Abdullah Saydemir,Ken E. Friedl,Andrea Stocco*

Main category: cs.CL

TL;DR: LLM-based evaluation framework for in-car conversational QA systems using advanced prompting techniques to assess utterance-response coherence, with DeepSeek-R1 achieving near-perfect F1-score at low cost.


<details>
  <summary>Details</summary>
Motivation: Assessing accuracy and reliability of in-car conversational QA systems is challenging; need scalable alternative to human evaluation for benchmarking contextual understanding.

Method: Synthetic generation of user utterances with correct/modified failure responses; evaluation using 13 LLMs with input-output, chain-of-thought, self-consistency, and multi-agent prompting techniques.

Result: Best performance: DeepSeek-R1 with self-consistency prompting (F1=0.99, $0.002/request). Small non-reasoning models benefit most from advanced prompting. Best balance: DeepSeek-V3 for cost-time efficiency.

Conclusion: LLM-based evaluation offers scalable, accurate alternative to human evaluation for benchmarking contextual understanding in conversational QA systems.

Abstract: In-Car Conversational Question Answering (ConvQA) systems significantly enhance user experience by enabling seamless voice interactions. However, assessing their accuracy and reliability remains a challenge. This paper explores the use of Large Language Models (LLMs) alongside advanced prompting techniques and agent-based methods to evaluate the extent to which ConvQA system responses adhere to user utterances. The focus lies on contextual understanding and the ability to provide accurate venue recommendations considering user constraints and situational context. To evaluate utterance-response coherence using an LLM, we synthetically generate user utterances accompanied by correct and modified failure-containing system responses. We use input-output, chain-of-thought, self-consistency prompting, and multi-agent prompting techniques with 13 reasoning and non-reasoning LLMs of varying sizes and providers, including OpenAI, DeepSeek, Mistral AI, and Meta. We evaluate our approach on a case study involving restaurant recommendations. The most substantial improvements occur for small non-reasoning models when applying advanced prompting techniques, particularly multi-agent prompting. However, reasoning models consistently outperform non-reasoning models, with the best performance achieved using single-agent prompting with self-consistency. Notably, DeepSeek-R1 reaches an F1-score of 0.99 at a cost of 0.002 USD per request. Overall, the best balance between effectiveness and cost-time efficiency is reached with the non-reasoning model DeepSeek-V3. Our findings show that LLM-based evaluation offers a scalable and accurate alternative to traditional human evaluation for benchmarking contextual understanding in ConvQA systems.

</details>


### [7] [VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs](https://arxiv.org/abs/2512.12072)
*Avinash Amballa,Yashas Malur Saidutta,Chi-Heng Lin,Vivek Kulkarni,Srinivas Chappidi*

Main category: cs.CL

TL;DR: Voyager is a training-free, scalable method that uses determinantal point processes to generate diverse synthetic datasets from LLMs, achieving 1.5-3x diversity improvement over baselines.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used to generate synthetic datasets for evaluation and training, but prior work shows such generated data lacks diversity, limiting its usefulness for downstream applications.

Method: Voyager uses an iterative approach that directly optimizes dataset diversity using determinantal point processes. It's training-free, applicable to closed-source models, and scalable.

Result: Voyager significantly outperforms popular baseline approaches by providing 1.5-3x improvement in diversity, as demonstrated through comprehensive experiments.

Conclusion: Voyager offers a principled, theoretically justified approach to generating diverse synthetic datasets from LLMs, addressing a key limitation of current LLM-based data generation methods.

Abstract: Large language models (LLMs) are increasingly being used to generate synthetic datasets for the evaluation and training of downstream models. However, prior work has noted that such generated data lacks diversity. In this paper, we propose Voyager, a novel principled approach to generate diverse datasets. Our approach is iterative and directly optimizes a mathematical quantity that optimizes the diversity of the dataset using the machinery of determinantal point processes. Furthermore, our approach is training-free, applicable to closed-source models, and scalable. In addition to providing theoretical justification for the working of our method, we also demonstrate through comprehensive experiments that Voyager significantly outperforms popular baseline approaches by providing a 1.5-3x improvement in diversity.

</details>


### [8] [BLASST: Dynamic BLocked Attention Sparsity via Softmax Thresholding](https://arxiv.org/abs/2512.12087)
*Jiayi Yuan,Cameron Shinn,Kai Xu,Jingze Cui,George Klimiashvili,Guangxuan Xiao,Perkz Zheng,Bo Li,Yuxin Zhou,Zhouhai Ye,Weijie You,Tian Zheng,Dominic Brown,Pengbo Wang,Richard Cai,Julien Demouth,John D. Owens,Xia Hu,Song Han,Timmy Liu,Huizi Mao*

Main category: cs.CL

TL;DR: BLASST is a drop-in sparse attention method that dynamically prunes attention matrices using a fixed threshold and existing online softmax information, providing speedups for long-context LLM inference without pre-computation.


<details>
  <summary>Details</summary>
Motivation: Address computational and memory bottlenecks in standard attention mechanisms for long-context LLM inference, which is increasingly demanded but resource-intensive.

Method: Uses fixed threshold with existing online softmax information to identify negligible attention scores, skipping softmax computation, Value block loading, and matrix multiplication. Fits into FlashAttention kernels with minimal overhead. Works for all attention variants (MHA, GQA, MQA, MLA) and both prefill/decode stages.

Result: Achieves 1.62x prefill speedup at 74.7% sparsity and 1.48x decode speedup at 73.2% sparsity on modern GPUs while maintaining high accuracy. Automated calibration reveals inverse relationship between optimal threshold and context length.

Conclusion: BLASST provides a unified, efficient solution for accelerating long-context LLM inference through dynamic sparse attention, with potential for further improvements via sparsity-aware training to enhance robustness to sparse patterns.

Abstract: The growing demand for long-context inference capabilities in Large Language Models (LLMs) has intensified the computational and memory bottlenecks inherent to the standard attention mechanism. To address this challenge, we introduce BLASST, a drop-in sparse attention method that dynamically prunes the attention matrix without any pre-computation or proxy scores. Our method uses a fixed threshold and existing information from online softmax to identify negligible attention scores, skipping softmax computation, Value block loading, and the subsequent matrix multiplication. This fits seamlessly into existing FlashAttention kernel designs with negligible latency overhead. The approach is applicable to both prefill and decode stages across all attention variants (MHA, GQA, MQA, and MLA), providing a unified solution for accelerating long-context inference. We develop an automated calibration procedure that reveals a simple inverse relationship between optimal threshold and context length, enabling robust deployment across diverse scenarios. Maintaining high accuracy, we demonstrate a 1.62x speedup for prefill at 74.7% sparsity and a 1.48x speedup for decode at 73.2% sparsity on modern GPUs. Furthermore, we explore sparsity-aware training as a natural extension, showing that models can be trained to be inherently more robust to sparse attention patterns, pushing the accuracy-sparsity frontier even further.

</details>


### [9] [Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings](https://arxiv.org/abs/2512.12167)
*Yoav Gelberg,Koshi Eguchi,Takuya Akiba,Edoardo Cetin*

Main category: cs.CL

TL;DR: DroPE enables zero-shot context extension for language models by removing positional embeddings after pretraining, eliminating the need for expensive long-context finetuning.


<details>
  <summary>Details</summary>
Motivation: Current methods require expensive finetuning beyond pretraining sequence length for effective context extension. Positional embeddings create a bottleneck that prevents test-time generalization to unseen sequence lengths, even with popular scaling methods.

Method: Dropping Positional Embeddings (DroPE) - remove positional embeddings after pretraining following a short recalibration phase. This simple approach allows models to adapt quickly without compromising original capabilities.

Result: DroPE enables seamless zero-shot context extension without long-context finetuning, outperforming previous specialized architectures and established rotary positional embedding scaling methods across different models and dataset sizes.

Conclusion: Positional embeddings are not inherently required for effective language modeling and can be safely removed after pretraining, enabling efficient context extension without compromising model performance.

Abstract: So far, expensive finetuning beyond the pretraining sequence length has been a requirement for effectively extending the context of language models (LM). In this work, we break this key bottleneck by Dropping the Positional Embeddings of LMs after training (DroPE). Our simple method is motivated by three key theoretical and empirical observations. First, positional embeddings (PEs) serve a crucial role during pretraining, providing an important inductive bias that significantly facilitates convergence. Second, over-reliance on this explicit positional information is also precisely what prevents test-time generalization to sequences of unseen length, even when using popular PE-scaling methods. Third, positional embeddings are not an inherent requirement of effective language modeling and can be safely removed after pretraining, following a short recalibration phase. Empirically, DroPE yields seamless zero-shot context extension without any long-context finetuning, quickly adapting pretrained LMs without compromising their capabilities in the original training context. Our findings hold across different models and dataset sizes, far outperforming previous specialized architectures and established rotary positional embedding scaling methods.

</details>


### [10] [Diffusion Language Model Inference with Monte Carlo Tree Search](https://arxiv.org/abs/2512.12168)
*Zheng Huang,Kiran Ramnath,Yueyan Chen,Aosong Feng,Sangmin Woo,Balasubramaniam Srinivasan,Zhichao Xu,Kang Zhou,Shuai Wang,Haibo Ding,Lin Lee Cheong*

Main category: cs.CL

TL;DR: MEDAL integrates Monte Carlo Tree Search at initialization to improve diffusion language model inference by exploring promising unmasking trajectories, achieving up to 22% improvement over existing methods.


<details>
  <summary>Details</summary>
Motivation: Diffusion language models offer parallel generation and better coherence than autoregressive models, but their inference involves a combinatorial search problem for determining which positions to unmask and which tokens to commit. Existing methods use heuristics that yield suboptimal decoding paths or require additional training.

Method: MEDAL integrates Monte Carlo Tree Search at the initialization stage to explore promising unmasking trajectories, restricting the search space to high-confidence actions and prioritizing token choices that improve model confidence over remaining masked positions.

Result: Across multiple benchmarks, MEDAL achieves up to 22.0% improvement over existing inference strategies for diffusion language models.

Conclusion: MEDAL establishes a new paradigm for search-based inference in diffusion language models by introducing a principled search mechanism that improves decoding quality without requiring additional training.

Abstract: Diffusion language models (DLMs) have recently emerged as a compelling alternative to autoregressive generation, offering parallel generation and improved global coherence. During inference, DLMs generate text by iteratively denoising masked sequences in parallel; however, determining which positions to unmask and which tokens to commit forms a large combinatorial search problem. Existing inference methods approximate this search using heuristics, which often yield suboptimal decoding paths; other approaches instead rely on additional training to guide token selection. To introduce a principled search mechanism for DLMs inference, we introduce MEDAL, a framework that integrates Monte Carlo Tree SEarch initialization for Diffusion LAnguage Model inference. We employ Monte Carlo Tree Search at the initialization stage to explore promising unmasking trajectories, providing a robust starting point for subsequent refinement. This integration is enabled by restricting the search space to high-confidence actions and prioritizing token choices that improve model confidence over remaining masked positions. Across multiple benchmarks, MEDAL achieves up to 22.0% improvement over existing inference strategies, establishing a new paradigm for search-based inference in diffusion language models.

</details>


### [11] [Semantic Distance Measurement based on Multi-Kernel Gaussian Processes](https://arxiv.org/abs/2512.12238)
*Yinzhu Cheng,Haihua Xie,Yaqing Wang,Miao He,Mingming Sun*

Main category: cs.CL

TL;DR: Proposes a semantic distance measure using multi-kernel Gaussian processes that learns kernel parameters from data automatically, applied to fine-grained sentiment classification with LLMs under in-context learning.


<details>
  <summary>Details</summary>
Motivation: Most classical semantic distance methods are fixed and difficult to adapt to specific data distributions and task requirements, lacking flexibility for different applications.

Method: Models latent semantic function as Gaussian process with combined kernel (Matérn + polynomial), learns kernel parameters automatically from supervised data, and applies to fine-grained sentiment classification with LLMs using in-context learning.

Result: Experimental results demonstrate the effectiveness of the proposed semantic distance measure in the context of fine-grained sentiment classification.

Conclusion: The multi-kernel Gaussian process approach provides an adaptive semantic distance measure that learns from data rather than using fixed hand-crafted parameters, showing practical effectiveness in sentiment classification tasks.

Abstract: Semantic distance measurement is a fundamental problem in computational linguistics, providing a quantitative characterization of similarity or relatedness between text segments, and underpinning tasks such as text retrieval and text classification. From a mathematical perspective, a semantic distance can be viewed as a metric defined on a space of texts or on a representation space derived from them. However, most classical semantic distance methods are essentially fixed, making them difficult to adapt to specific data distributions and task requirements. In this paper, a semantic distance measure based on multi-kernel Gaussian processes (MK-GP) was proposed. The latent semantic function associated with texts was modeled as a Gaussian process, with its covariance function given by a combined kernel combining Matérn and polynomial components. The kernel parameters were learned automatically from data under supervision, rather than being hand-crafted. This semantic distance was instantiated and evaluated in the context of fine-grained sentiment classification with large language models under an in-context learning (ICL) setup. The experimental results demonstrated the effectiveness of the proposed measure.

</details>


### [12] [Adversarially Probing Cross-Family Sound Symbolism in 27 Languages](https://arxiv.org/abs/2512.12245)
*Anika Sharma,Tianyi Niu,Emma Wrenn,Shashank Srivastava*

Main category: cs.CL

TL;DR: First computational cross-linguistic analysis shows sound symbolism in size semantics exists across unrelated languages, with both vowels and consonants contributing to size prediction.


<details>
  <summary>Details</summary>
Motivation: Sound symbolism (non-arbitrary mapping between word sounds and meanings) has been demonstrated through anecdotal experiments like Bouba Kiki but rarely tested at scale across multiple languages.

Method: Compiled typologically broad dataset of 810 adjectives (27 languages, 30 words each), phonemically transcribed with native-speaker validation. Used interpretable classifiers over bag-of-segment features and trained adversarial scrubber to suppress language identity while preserving size signal.

Result: Phonological form predicts size semantics above chance across unrelated languages. Language prediction fell below chance while size prediction remained significantly above chance, indicating cross-family sound-symbolic bias.

Conclusion: Sound symbolism in size semantics exists universally beyond genealogy, with both vowels and consonants contributing to cross-linguistic sound-symbolic patterns.

Abstract: The phenomenon of sound symbolism, the non-arbitrary mapping between word sounds and meanings, has long been demonstrated through anecdotal experiments like Bouba Kiki, but rarely tested at scale. We present the first computational cross-linguistic analysis of sound symbolism in the semantic domain of size. We compile a typologically broad dataset of 810 adjectives (27 languages, 30 words each), each phonemically transcribed and validated with native-speaker audio. Using interpretable classifiers over bag-of-segment features, we find that phonological form predicts size semantics above chance even across unrelated languages, with both vowels and consonants contributing. To probe universality beyond genealogy, we train an adversarial scrubber that suppresses language identity while preserving size signal (also at family granularity). Language prediction averaged across languages and settings falls below chance while size prediction remains significantly above chance, indicating cross-family sound-symbolic bias. We release data, code, and diagnostic tools for future large-scale studies of iconicity.

</details>


### [13] [Market-Bench: Evaluating Large Language Models on Introductory Quantitative Trading and Market Dynamics](https://arxiv.org/abs/2512.12264)
*Abhay Srivastava,Sam Jung,Spencer Mateega*

Main category: cs.CL

TL;DR: MARKET-BENCH is a benchmark that tests LLMs on quantitative trading tasks by having them generate executable backtesters from natural language strategy descriptions, evaluating both code reliability and numerical accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to assess whether current large language models can reliably generate correct quantitative trading infrastructure from natural language descriptions, which is important for automating financial analysis and strategy implementation.

Method: The benchmark evaluates LLMs on three canonical trading strategies: scheduled trading on MSFT, pairs trading on KO/PEP, and delta hedging on MSFT. Models must produce executable code whose P&L, drawdown, and position paths match reference implementations. Evaluation uses a multi-round pass@k metric that separates structural reliability (whether code runs) from numerical accuracy (mean absolute error of metrics).

Result: Most models reliably execute the simplest strategy (average pass@3 of 0.80), but error rates vary significantly across models and tasks. Gemini 3 Pro and Claude 4.5 Sonnet combine strong reliability with low error on simpler strategies. GPT-5.1 Codex-Max achieves perfect pass@1 on the first two strategies and lowest best-run error on the easiest task. Qwen3 Max attains perfect pass@3 but sometimes produces inaccurate P&L paths.

Conclusion: Current LLMs can scaffold basic trading infrastructure but still struggle with robust reasoning about prices, inventory, and risk. The authors release MARKET-BENCH and a public leaderboard to facilitate further research.

Abstract: We introduce MARKET-BENCH, a benchmark that evaluates large language models (LLMs) on introductory quantitative trading tasks by asking them to construct executable backtesters from natural-language strategy descriptions and market assumptions. Each instance specifies one of three canonical strategies -- scheduled trading on Microsoft (NASDAQ: MSFT), pairs trading on Coca-Cola (NASDAQ: KO) and Pepsi (NASDAQ: PEP), or delta hedging on MSFT -- and models must produce code whose P\&L, drawdown, and position paths match a verifiable reference implementation. We assess twelve state-of-the-art models using a multi-round pass@k metric that separates structural reliability (whether the backtest runs) from numerical accuracy (mean absolute error of the backtest metrics). While most models reliably execute the simplest strategy (average pass@3 of 0.80), errors vary by orders of magnitude across models and tasks: Gemini 3 Pro and Claude 4.5 Sonnet combine strong reliability with low error on simpler strategies, GPT-5.1 Codex-Max achieves perfect pass@1 on the first two strategies and the lowest best-run error on the easiest task, and Qwen3 Max attains perfect pass@3 yet sometimes produces inaccurate P\&L paths. These results show that current LLMs can scaffold basic trading infrastructure but still struggle to reason robustly about prices, inventory, and risk; we release MARKET-BENCH and a public leaderboard at https://marketbench.ai.

</details>


### [14] [F5-TTS-RO: Extending F5-TTS to Romanian TTS via Lightweight Input Adaptation](https://arxiv.org/abs/2512.12297)
*Radu-Gabriel Chivereanu,Tiberiu Boros*

Main category: cs.CL

TL;DR: A lightweight adapter for F5-TTS enables Romanian language support while preserving existing voice cloning and multilingual capabilities through a frozen original model with added sub-network.


<details>
  <summary>Details</summary>
Motivation: To extend F5-TTS to support Romanian language while maintaining its existing capabilities (voice cloning, English and Chinese support) without retraining the entire model.

Method: Freeze original F5-TTS weights and append a sub-network to the textual embedding matrix of the text encoder. Use ConvNeXt module to model co-dependencies between new character-level embeddings, creating a "soft" letter-to-sound layer for Romanian text conversion.

Result: The approach maintains voice cloning capabilities and enables Romanian-English code-switching to some extent, though residual English accent characteristics remain. Human evaluation across three tasks shows successful Romanian speech generation.

Conclusion: The lightweight adapter successfully extends F5-TTS to Romanian while preserving original capabilities, demonstrating effective language extension with minimal model modification, though some accent issues persist.

Abstract: This work introduces a lightweight input-level adapter for the F5-TTS model that enables Romanian Language support. To preserve the existing capabilities of the model (voice cloning, English and Chinese support), we keep the original weights frozen, append a sub-network to the model and train it as an extension for the textual embedding matrix of the text encoder. For simplicity, we rely on ConvNeXt module implemented in F5-TTS to also model the co-dependencies between the new character-level embeddings. The module serves as a ``soft`` letter-to-sound layer, converting Romanian text into a continuous representation that the F5-TTS model uses to produce naturally sounding Romanian utterances. We evaluate the model with a pool of 20 human listeners across three tasks: (a) audio similarity between reference and generated speech, (b) pronunciation and naturalness and (c) Romanian-English code-switching. The results indicate that our approach maintains voice cloning capabilities and enables, to a certain extent, code-switching within the same utterance; however, residual English accent characteristics remain. We open-source our code and provide example audio samples at https://github.com/racai-ro/Ro-F5TTS.

</details>


### [15] [SCIR: A Self-Correcting Iterative Refinement Framework for Enhanced Information Extraction Based on Schema](https://arxiv.org/abs/2512.12337)
*Yushen Fang,Jianjun Li,Mingqian Ding,Chang Liu,Xinchi Zou,Wenqi Yang*

Main category: cs.CL

TL;DR: SCIR framework reduces LLM-powered IE training costs by 87% with plug-and-play self-correction, achieving 5.27% average F1 improvement across NER, RE, and EE tasks.


<details>
  <summary>Details</summary>
Motivation: Current LLM-powered IE systems face high training costs and difficulty aligning with LLM preferences during fine-tuning.

Method: Proposes Self-Correcting Iterative Refinement (SCIR) framework with Dual-Path Self-Correcting module and feedback-driven optimization, plus MBSC dataset with 100k+ entries for preference alignment.

Result: Outperforms SOTA methods with 5.27% average span-based Micro-F1 improvement across NER, relation extraction, and event extraction, while reducing training costs by 87%.

Conclusion: SCIR enhances IE flexibility and accuracy while enabling lightweight, efficient IE paradigms through plug-and-play compatibility and reduced training overhead.

Abstract: Although Large language Model (LLM)-powered information extraction (IE) systems have shown impressive capabilities, current fine-tuning paradigms face two major limitations: high training costs and difficulties in aligning with LLM preferences. To address these issues, we propose a novel universal IE paradigm, the Self-Correcting Iterative Refinement (SCIR) framework, along with a Multi-task Bilingual (Chinese-English) Self-Correcting (MBSC) dataset containing over 100,000 entries. The SCIR framework achieves plug-and-play compatibility with existing LLMs and IE systems through its Dual-Path Self-Correcting module and feedback-driven optimization, thereby significantly reducing training costs. Concurrently, the MBSC dataset tackles the challenge of preference alignment by indirectly distilling GPT-4's capabilities into IE result detection models. Experimental results demonstrate that SCIR outperforms state-of-the-art IE methods across three key tasks: named entity recognition, relation extraction, and event extraction, achieving a 5.27 percent average improvement in span-based Micro-F1 while reducing training costs by 87 percent compared to baseline approaches. These advancements not only enhance the flexibility and accuracy of IE systems but also pave the way for lightweight and efficient IE paradigms.

</details>


### [16] [Can GPT replace human raters? Validity and reliability of machine-generated norms for metaphors](https://arxiv.org/abs/2512.12444)
*Veronica Mangiaterra,Hamad Al-Azary,Chiara Barattieri di San Pietro,Paolo Canal,Valentina Bambini*

Main category: cs.CL

TL;DR: GPT models can validly and reliably generate metaphor ratings for familiarity, comprehensibility, and imageability, with larger models performing better and showing high stability across sessions.


<details>
  <summary>Details</summary>
Motivation: As LLMs are increasingly used in scientific research, their trustworthiness is crucial. While LLMs have shown promise in augmenting human-rated datasets for single words, their performance on complex items like metaphors remains unexplored.

Method: Assessed validity and reliability of metaphor ratings generated by three GPT models for 687 items from Italian Figurative Archive and three English studies. Validation included alignment with human data and ability to predict behavioral (response times) and electrophysiological (EEG amplitude) responses.

Result: Machine-generated ratings positively correlated with human-generated ones. Familiarity showed moderate-to-strong correlations (weaker for high sensorimotor load metaphors). Imageability showed moderate correlations in English, moderate-to-strong in Italian. Comprehensibility for English metaphors had strongest correlations. Larger models outperformed smaller ones. GPT ratings significantly predicted response times and EEG amplitude comparably to human ratings, and were highly stable across sessions.

Conclusion: GPT, especially larger models, can validly and reliably replace or augment human subjects in rating metaphor properties. However, LLMs align worse with humans when dealing with conventionality and multimodal aspects of metaphorical meaning, requiring careful stimulus consideration.

Abstract: As Large Language Models (LLMs) are increasingly being used in scientific research, the issue of their trustworthiness becomes crucial. In psycholinguistics, LLMs have been recently employed in automatically augmenting human-rated datasets, with promising results obtained by generating ratings for single words. Yet, performance for ratings of complex items, i.e., metaphors, is still unexplored. Here, we present the first assessment of the validity and reliability of ratings of metaphors on familiarity, comprehensibility, and imageability, generated by three GPT models for a total of 687 items gathered from the Italian Figurative Archive and three English studies. We performed a thorough validation in terms of both alignment with human data and ability to predict behavioral and electrophysiological responses. We found that machine-generated ratings positively correlated with human-generated ones. Familiarity ratings reached moderate-to-strong correlations for both English and Italian metaphors, although correlations weakened for metaphors with high sensorimotor load. Imageability showed moderate correlations in English and moderate-to-strong in Italian. Comprehensibility for English metaphors exhibited the strongest correlations. Overall, larger models outperformed smaller ones and greater human-model misalignment emerged with familiarity and imageability. Machine-generated ratings significantly predicted response times and the EEG amplitude, with a strength comparable to human ratings. Moreover, GPT ratings obtained across independent sessions were highly stable. We conclude that GPT, especially larger models, can validly and reliably replace - or augment - human subjects in rating metaphor properties. Yet, LLMs align worse with humans when dealing with conventionality and multimodal aspects of metaphorical meaning, calling for careful consideration of the nature of stimuli.

</details>


### [17] [Large language models have learned to use language](https://arxiv.org/abs/2512.12447)
*Gary Lupyan*

Main category: cs.CL

TL;DR: The paper argues that recognizing LLMs' language capabilities enables scientific breakthroughs, requiring abandonment of traditional evaluation methods and acceptance of a post-Turing test era.


<details>
  <summary>Details</summary>
Motivation: To achieve breakthrough language science by acknowledging that large language models have genuinely learned to use language, which requires moving beyond traditional evaluation paradigms.

Method: Conceptual/philosophical analysis proposing paradigm shift: abandoning long-held evaluation ideas and accepting the post-Turing test reality where machines demonstrate sophisticated language use.

Result: Establishes the need for new evaluation frameworks that recognize LLMs' genuine language capabilities rather than dismissing them as mere pattern matching.

Conclusion: We must embrace the post-Turing test era and develop new scientific approaches to study language through LLMs, which requires fundamental changes to how we conceptualize and evaluate language knowledge.

Abstract: Acknowledging that large language models have learned to use language can open doors to breakthrough language science. Achieving these breakthroughs may require abandoning some long-held ideas about how language knowledge is evaluated and reckoning with the difficult fact that we have entered a post-Turing test era.

</details>


### [18] [The American Ghost in the Machine: How language models align culturally and the effects of cultural prompting](https://arxiv.org/abs/2512.12488)
*James Luther,Donald Brown*

Main category: cs.CL

TL;DR: LLMs show cultural bias toward the US by default, but can be shifted toward other cultures through prompting, though alignment with Asian cultures remains challenging.


<details>
  <summary>Details</summary>
Motivation: As LLMs become more prevalent in human-computer interaction, understanding and ensuring their cultural alignment is crucial for effective and appropriate interactions across diverse global contexts.

Method: Used VSM13 International Survey and Hofstede's cultural dimensions to assess cultural alignment of 8 popular LLMs, then tested cultural prompting to shift models toward specific countries (China, France, India, Iran, Japan, US).

Result: Most LLMs default to US cultural alignment when unspecified; 7 of 8 models successfully shifted toward prompted cultures; models struggled to align with Japan and China despite some being developed by Chinese company DeepSeek.

Conclusion: Cultural prompting is effective for adapting LLMs to different cultures, but significant challenges remain in achieving accurate alignment with certain cultural contexts, particularly Asian cultures.

Abstract: Culture is the bedrock of human interaction; it dictates how we perceive and respond to everyday interactions. As the field of human-computer interaction grows via the rise of generative Large Language Models (LLMs), the cultural alignment of these models become an important field of study. This work, using the VSM13 International Survey and Hofstede's cultural dimensions, identifies the cultural alignment of popular LLMs (DeepSeek-V3, V3.1, GPT-5, GPT-4.1, GPT-4, Claude Opus 4, Llama 3.1, and Mistral Large). We then use cultural prompting, or using system prompts to shift the cultural alignment of a model to a desired country, to test the adaptability of these models to other cultures, namely China, France, India, Iran, Japan, and the United States. We find that the majority of the eight LLMs tested favor the United States when the culture is not specified, with varying results when prompted for other cultures. When using cultural prompting, seven of the eight models shifted closer to the expected culture. We find that models had trouble aligning with Japan and China, despite two of the models tested originating with the Chinese company DeepSeek.

</details>


### [19] [NagaNLP: Bootstrapping NLP for Low-Resource Nagamese Creole with Human-in-the-Loop Synthetic Data](https://arxiv.org/abs/2512.12537)
*Agniva Maiti,Manya Pandey,Murari Mandal*

Main category: cs.CL

TL;DR: NagaNLP introduces an open-source toolkit for Nagamese using LLM-driven synthetic data generation with human validation, achieving state-of-the-art performance on NLP tasks.


<details>
  <summary>Details</summary>
Motivation: Most world languages, especially creoles like Nagamese, are severely under-resourced in NLP, creating barriers to digital representation and technological inclusion.

Method: Multi-stage pipeline using expert-guided LLM (Gemini) to generate candidate corpus, refined and annotated by native speakers, creating synthetic-hybrid datasets for foundational NLP tasks.

Result: Fine-tuned XLM-RoBERTa-base achieved 93.81% accuracy on POS tagging and 0.75 F1-Macro on NER. NagaLLaMA (Llama-3.2-3B) achieved Perplexity of 3.85 on conversational tasks, massively outperforming baselines.

Conclusion: NagaNLP provides foundational resources for Nagamese and a reproducible framework for addressing data scarcity in other low-resource languages through synthetic-hybrid data generation.

Abstract: The vast majority of the world's languages, particularly creoles like Nagamese, remain severely under-resourced in Natural Language Processing (NLP), creating a significant barrier to their representation in digital technology. This paper introduces NagaNLP, a comprehensive open-source toolkit for Nagamese, bootstrapped through a novel methodology that relies on LLM-driven but human-validated synthetic data generation. We detail a multi-stage pipeline where an expert-guided LLM (Gemini) generates a candidate corpus, which is then refined and annotated by native speakers. This synthetic-hybrid approach yielded a 10K pair conversational dataset and a high-quality annotated corpus for foundational tasks. To assess the effectiveness of our methodology, we trained both discriminative and generative models. Our fine-tuned XLM-RoBERTa-base model establishes a new benchmark for Nagamese, achieving a 93.81\% accuracy (0.90 F1-Macro) on Part-of-Speech tagging and a 0.75 F1-Macro on Named Entity Recognition, massively outperforming strong zero-shot baselines. Furthermore, we fine-tuned a Llama-3.2-3B Instruct model, named NagaLLaMA, which demonstrates superior performance on conversational tasks, achieving a Perplexity of 3.85, an order of magnitude improvement over its few-shot counterpart (96.76). We release the NagaNLP toolkit, including all datasets, models, and code, providing a foundational resource for a previously underserved language and a reproducible framework for reducing data scarcity in other low-resource contexts.

</details>


### [20] [HyperEdit: Unlocking Instruction-based Text Editing in LLMs via Hypernetworks](https://arxiv.org/abs/2512.12544)
*Yiming Zeng,Jinghan Cao,Zexin Li,Wanhao Yu,Zhankai Ye,Dawei Xiang,Ting Hua,Xin Liu,Shangqian Gao,Tingting Yu*

Main category: cs.CL

TL;DR: HyperEdit improves instruction-based text editing by using hypernetworks for dynamic adaptation and difference-aware regularization to prevent over-editing, achieving 9-30% BLEU improvement on modified regions with only 3B parameters.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with instruction-based text editing tasks because they treat editing as generic text generation, leading to failures in faithfully aligning edits with user intents and over-editing unchanged regions, which can break functionality in applications like code editors.

Method: HyperEdit introduces two key innovations: 1) hypernetwork-based dynamic adaptation that generates request-specific parameters to tailor editing strategies to each instruction, and 2) difference-aware regularization that focuses supervision on modified spans to prevent over-editing while ensuring precise, minimal changes.

Result: HyperEdit achieves a 9%--30% relative improvement in BLEU on modified regions over state-of-the-art baselines, despite utilizing only 3B parameters, demonstrating superior performance in faithful instruction-based editing.

Conclusion: HyperEdit effectively addresses the core challenges of instruction-based text editing by combining dynamic adaptation to user intents with regularization that prevents over-editing, making it a promising solution for real-world applications like code editors where faithful editing is critical.

Abstract: Instruction-based text editing is increasingly critical for real-world applications such as code editors (e.g., Cursor), but Large Language Models (LLMs) continue to struggle with this task. Unlike free-form generation, editing requires faithfully implementing user instructions while preserving unchanged content, as even minor unintended modifications can break functionality. Existing approaches treat editing as generic text generation, leading to two key failures: they struggle to faithfully align edits with diverse user intents, and they often over-edit unchanged regions. We propose HyperEdit to address both issues. First, we introduce hypernetwork-based dynamic adaptation that generates request-specific parameters, enabling the model to tailor its editing strategy to each instruction. Second, we develop difference-aware regularization that focuses supervision on modified spans, preventing over-editing while ensuring precise, minimal changes. HyperEdit achieves a 9%--30% relative improvement in BLEU on modified regions over state-of-the-art baselines, despite utilizing only 3B parameters.

</details>


### [21] [Coupled Variational Reinforcement Learning for Language Model General Reasoning](https://arxiv.org/abs/2512.12576)
*Xueru Wen,Jie Lou,Yanjiang Liu,Hongyu Lin,Ben He,Xianpei Han,Le Sun,Yaojie Lu,Debing Zhang*

Main category: cs.CL

TL;DR: CoVRL is a verifier-free RL method that couples variational inference with RL to improve reasoning in language models by ensuring coherence between reasoning traces and final answers.


<details>
  <summary>Details</summary>
Motivation: Existing verifier-free RL methods for language model reasoning sample reasoning traces conditioned only on questions, which decouples reasoning from answer information, leading to inefficient exploration and incoherence between traces and final answers.

Method: CoVRL bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. It constructs and optimizes a composite distribution that integrates these two distributions.

Result: CoVRL improves performance by 12.4% over the base model and achieves an additional 2.3% improvement over strong state-of-the-art verifier-free RL baselines on mathematical and general reasoning benchmarks.

Conclusion: CoVRL provides a principled framework for enhancing the general reasoning capabilities of language models by enabling efficient exploration while preserving strong thought-answer coherence.

Abstract: While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \textit{\b{Co}upled \b{V}ariational \b{R}einforcement \b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\% over the base model and achieves an additional 2.3\% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.

</details>


### [22] [Human-Inspired Learning for Large Language Models via Obvious Record and Maximum-Entropy Method Discovery](https://arxiv.org/abs/2512.12608)
*Hong Su*

Main category: cs.CL

TL;DR: A human-inspired learning framework for LLMs that combines explicit symbolic memory storage with entropy-guided method discovery to handle rare, low-resource scenarios that standard LLMs struggle with.


<details>
  <summary>Details</summary>
Motivation: LLMs fail with rare, unseen scenarios (like niche hardware issues or irregular IoT behaviors) because such cases are sparse in training data. They rely on implicit parametric memory, acting as intuition-driven predictors rather than method-oriented learners, lacking explicit acquisition and refinement of methods.

Method: Proposes a two-component framework: 1) Obvious Record - explicitly stores cause-result/question-solution relationships as symbolic memory for persistent learning from single encounters; 2) Maximum-Entropy Method Discovery - prioritizes and preserves methods with high semantic dissimilarity to capture diverse, underrepresented strategies missed by next-token prediction.

Result: On a benchmark of 60 semantically diverse question-solution pairs, the entropy-guided approach achieves stronger coverage of unseen questions and significantly greater internal diversity than a random baseline, demonstrating effectiveness in discovering more generalizable, human-inspired methods.

Conclusion: The proposed human-inspired learning framework successfully addresses LLMs' limitations with rare scenarios by combining explicit symbolic memory with entropy-driven method discovery, enabling more deliberate, method-oriented learning rather than just intuition-driven prediction.

Abstract: Large Language Models (LLMs) excel at extracting common patterns from large-scale corpora, yet they struggle with rare, low-resource, or previously unseen scenarios-such as niche hardware deployment issues or irregular IoT device behaviors-because such cases are sparsely represented in training data. Moreover, LLMs rely primarily on implicit parametric memory, which limits their ability to explicitly acquire, recall, and refine methods, causing them to behave predominantly as intuition-driven predictors rather than deliberate, method-oriented learners. Inspired by how humans learn from rare experiences, this paper proposes a human-inspired learning framework that integrates two complementary mechanisms. The first, Obvious Record, explicitly stores cause--result (or question--solution) relationships as symbolic memory, enabling persistent learning even from single or infrequent encounters. The second, Maximum-Entropy Method Discovery, prioritizes and preserves methods with high semantic dissimilarity, allowing the system to capture diverse and underrepresented strategies that are typically overlooked by next-token prediction. Verification on a benchmark of 60 semantically diverse question--solution pairs demonstrates that the proposed entropy-guided approach achieves stronger coverage of unseen questions and significantly greater internal diversity than a random baseline, confirming its effectiveness in discovering more generalizable and human-inspired methods.

</details>


### [23] [StruProKGR: A Structural and Probabilistic Framework for Sparse Knowledge Graph Reasoning](https://arxiv.org/abs/2512.12613)
*Yucan Guo,Saiping Guan,Miao Su,Zeya Zhao,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CL

TL;DR: StruProKGR is a structural and probabilistic framework for sparse KG reasoning that uses distance-guided path collection and probabilistic aggregation to improve efficiency and interpretability.


<details>
  <summary>Details</summary>
Motivation: Sparse KGs are common in real-world applications but challenging for reasoning due to limited knowledge. Existing path-based methods are computationally intensive with random walks and fail to leverage graph structure by treating paths independently.

Method: Proposes StruProKGR with: 1) Distance-guided path collection to reduce computational costs while finding relevant paths, 2) Probabilistic path aggregation that incorporates structural information by prioritizing mutually reinforcing paths.

Result: Extensive experiments on five sparse KG reasoning benchmarks show StruProKGR surpasses existing path-based methods in both effectiveness and efficiency.

Conclusion: StruProKGR provides an effective, efficient, and interpretable solution for sparse KG reasoning by addressing computational and structural limitations of existing path-based methods.

Abstract: Sparse Knowledge Graphs (KGs) are commonly encountered in real-world applications, where knowledge is often incomplete or limited. Sparse KG reasoning, the task of inferring missing knowledge over sparse KGs, is inherently challenging due to the scarcity of knowledge and the difficulty of capturing relational patterns in sparse scenarios. Among all sparse KG reasoning methods, path-based ones have attracted plenty of attention due to their interpretability. Existing path-based methods typically rely on computationally intensive random walks to collect paths, producing paths of variable quality. Additionally, these methods fail to leverage the structured nature of graphs by treating paths independently. To address these shortcomings, we propose a Structural and Probabilistic framework named StruProKGR, tailored for efficient and interpretable reasoning on sparse KGs. StruProKGR utilizes a distance-guided path collection mechanism to significantly reduce computational costs while exploring more relevant paths. It further enhances the reasoning process by incorporating structural information through probabilistic path aggregation, which prioritizes paths that reinforce each other. Extensive experiments on five sparse KG reasoning benchmarks reveal that StruProKGR surpasses existing path-based methods in both effectiveness and efficiency, providing an effective, efficient, and interpretable solution for sparse KG reasoning.

</details>


### [24] [Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives](https://arxiv.org/abs/2512.12620)
*Aheli Poddar,Saptarshi Sahoo,Sujata Ghosh*

Main category: cs.CL

TL;DR: LLMs show varying syllogistic reasoning capabilities across 14 models, with some achieving perfect symbolic performance, raising questions about whether LLMs are developing formal reasoning mechanisms rather than capturing human reasoning nuances.


<details>
  <summary>Details</summary>
Motivation: To investigate fundamental reasoning capabilities of LLMs through syllogistic reasoning from both logical and natural language perspectives, and to understand the research direction in this area.

Method: Evaluated 14 large language models on syllogistic reasoning tasks, examining both symbolic inference capabilities and natural language understanding of syllogisms.

Result: Syllogistic reasoning is not uniformly emergent across LLMs, but certain models achieve perfect symbolic performance, suggesting some LLMs may be developing formal reasoning mechanisms.

Conclusion: The findings raise questions about whether LLMs are evolving toward formal reasoning systems rather than capturing the nuanced aspects of human reasoning, indicating an important research direction for understanding LLM reasoning capabilities.

Abstract: We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.

</details>


### [25] [Which Pieces Does Unigram Tokenization Really Need?](https://arxiv.org/abs/2512.12641)
*Sander Land,Yuval Pinter*

Main category: cs.CL

TL;DR: The paper provides a practical implementation guide for the Unigram tokenization algorithm and proposes a simpler variant that trades slightly higher training loss for better compression.


<details>
  <summary>Details</summary>
Motivation: The Unigram tokenization algorithm offers theoretical elegance but has complex practical implementation, limiting its adoption to SentencePiece and its adapters. There's a gap between theory and practice that needs to be bridged.

Method: The authors provide a clear guide to implementation and parameter choices for the Unigram algorithm, and identify a simpler algorithm variant that accepts slightly higher training loss in exchange for improved compression.

Result: The paper bridges the gap between theory and practice of Unigram tokenization, making it more accessible for implementation, and demonstrates that a simpler algorithm can achieve better compression at the cost of slightly higher training loss.

Conclusion: The Unigram tokenization algorithm can be made more practical through clear implementation guidance, and a simpler variant offers a useful trade-off between training loss and compression efficiency.

Abstract: The Unigram tokenization algorithm offers a probabilistic alternative to the greedy heuristics of Byte-Pair Encoding. Despite its theoretical elegance, its implementation in practice is complex, limiting its adoption to the SentencePiece package and adapters thereof. We bridge this gap between theory and practice by providing a clear guide to implementation and parameter choices. We also identify a simpler algorithm that accepts slightly higher training loss in exchange for improved compression.

</details>


### [26] [LexRel: Benchmarking Legal Relation Extraction for Chinese Civil Cases](https://arxiv.org/abs/2512.12643)
*Yida Cai,Ranjuexiao Hu,Huiyuan Xie,Chenyang Li,Yun Liu,Yuxiao Ye,Zhenghao Liu,Weixing Shen,Zhiyuan Liu*

Main category: cs.CL

TL;DR: This paper introduces LexRel, a comprehensive schema and expert-annotated benchmark for legal relation extraction in Chinese civil law, showing current LLMs struggle with this task but that legal relations improve downstream legal AI performance.


<details>
  <summary>Details</summary>
Motivation: Legal relations are crucial in civil law systems for dispute resolution and rule of law values, but remain underexplored in legal AI due to lack of comprehensive schemas for Chinese civil cases.

Method: 1) Created a comprehensive schema with hierarchical taxonomy and argument definitions for legal relations in Chinese civil cases; 2) Formulated legal relation extraction task; 3) Built LexRel, an expert-annotated benchmark; 4) Evaluated state-of-the-art LLMs on legal relation extraction; 5) Tested legal relations' impact on downstream legal AI tasks.

Result: Current LLMs show significant limitations in accurately identifying civil legal relations. Incorporating legal relations information leads to consistent performance gains on other downstream legal AI tasks.

Conclusion: The paper establishes a foundation for legal relation extraction in Chinese civil law through LexRel benchmark, revealing LLMs' current limitations and demonstrating the value of legal relations for improving legal AI systems.

Abstract: Legal relations form a highly consequential analytical framework of civil law system, serving as a crucial foundation for resolving disputes and realizing values of the rule of law in judicial practice. However, legal relations in Chinese civil cases remain underexplored in the field of legal artificial intelligence (legal AI), largely due to the absence of comprehensive schemas. In this work, we firstly introduce a comprehensive schema, which contains a hierarchical taxonomy and definitions of arguments, for AI systems to capture legal relations in Chinese civil cases. Based on this schema, we then formulate legal relation extraction task and present LexRel, an expert-annotated benchmark for legal relation extraction in Chinese civil law. We use LexRel to evaluate state-of-the-art large language models (LLMs) on legal relation extractions, showing that current LLMs exhibit significant limitations in accurately identifying civil legal relations. Furthermore, we demonstrate that incorporating legal relations information leads to consistent performance gains on other downstream legal AI tasks.

</details>


### [27] [Modeling Authorial Style in Urdu Novels Using Character Interaction Graphs and Graph Neural Networks](https://arxiv.org/abs/2512.12654)
*Hassan Mujtaba,Hamza Naveed,Hanzlah Munir*

Main category: cs.CL

TL;DR: Graph-based framework models Urdu novels as character interaction networks to analyze authorial style from narrative structure, achieving 0.857 accuracy with learned graph representations.


<details>
  <summary>Details</summary>
Motivation: Traditional authorship analysis focuses on lexical/stylistic cues, while higher-level narrative structure remains underexplored, especially for low-resource languages like Urdu.

Method: Proposes graph-based framework representing Urdu novels as character interaction networks (nodes=characters, edges=co-occurrence). Compares multiple graph representations: global structural features, node-level semantic summaries, unsupervised graph embeddings, and supervised graph neural networks.

Result: Experiments on 52 Urdu novels by 7 authors show learned graph representations substantially outperform hand-crafted and unsupervised baselines, achieving up to 0.857 accuracy under strict author-aware evaluation protocol.

Conclusion: Authorial style can be inferred from narrative structure alone in Urdu literature, with learned graph representations being particularly effective for this task.

Abstract: Authorship analysis has traditionally focused on lexical and stylistic cues within text, while higher-level narrative structure remains underexplored, particularly for low-resource languages such as Urdu. This work proposes a graph-based framework that models Urdu novels as character interaction networks to examine whether authorial style can be inferred from narrative structure alone. Each novel is represented as a graph where nodes correspond to characters and edges denote their co-occurrence within narrative proximity. We systematically compare multiple graph representations, including global structural features, node-level semantic summaries, unsupervised graph embeddings, and supervised graph neural networks. Experiments on a dataset of 52 Urdu novels written by seven authors show that learned graph representations substantially outperform hand-crafted and unsupervised baselines, achieving up to 0.857 accuracy under a strict author-aware evaluation protocol.

</details>


### [28] [Fine-Tuning Causal LLMs for Text Classification: Embedding-Based vs. Instruction-Based Approaches](https://arxiv.org/abs/2512.12677)
*Amirhossein Yousefiramandi,Ciaran Cooney*

Main category: cs.CL

TL;DR: Efficient fine-tuning of decoder-only LLMs for text classification using 4-bit quantization + LoRA, showing embedding-based approach outperforms instruction-tuning and rivals domain-specific models.


<details>
  <summary>Details</summary>
Motivation: To develop resource-efficient methods for fine-tuning large decoder-only LLMs for text classification tasks when computational resources are limited, comparing different adaptation strategies.

Method: Two approaches: (1) embedding-based - attach classification head to pre-trained causal LLM using final token embedding as sequence representation, (2) instruction-tuning in prompt→response format. Both use 4-bit quantization with LoRA for parameter-efficient training on single GPU for models up to 8B parameters.

Result: Embedding-based method significantly outperforms instruction-tuned method in F1-score on both proprietary single-label dataset and WIPO-Alpha patent dataset (extreme multi-label classification). Embedding approach is competitive with and even surpasses fine-tuned domain-specific models like BERT on same tasks.

Conclusion: Directly leveraging internal representations of causal LLMs with efficient fine-tuning techniques yields impressive classification performance under limited resources. Provides practical guidelines and future directions for optimizing LLM fine-tuning in classification scenarios.

Abstract: We explore efficient strategies to fine-tune decoder-only Large Language Models (LLMs) for downstream text classification under resource constraints. Two approaches are investigated: (1) attaching a classification head to a pre-trained causal LLM and fine-tuning on the task (using the LLM's final token embedding as a sequence representation), and (2) instruction-tuning the LLM in a prompt->response format for classification. To enable single-GPU fine-tuning of models up to 8B parameters, we combine 4-bit model quantization with Low-Rank Adaptation (LoRA) for parameter-efficient training. Experiments on two datasets - a proprietary single-label dataset and the public WIPO-Alpha patent dataset (extreme multi-label classification) - show that the embedding-based method significantly outperforms the instruction-tuned method in F1-score, and is very competitive with - even surpassing - fine-tuned domain-specific models (e.g. BERT) on the same tasks. These results demonstrate that directly leveraging the internal representations of causal LLMs, along with efficient fine-tuning techniques, yields impressive classification performance under limited computational resources. We discuss the advantages of each approach while outlining practical guidelines and future directions for optimizing LLM fine-tuning in classification scenarios.

</details>


### [29] [CoDA: A Context-Decoupled Hierarchical Agent with Reinforcement Learning](https://arxiv.org/abs/2512.12716)
*Xuanzhang Liu,Jianglun Feng,Zhuoran Zhuang,Junzhe Zhao,Maofei Que,Jieting Li,Dianlei Wang,Hao Tong,Ye Chen,Pan Li*

Main category: cs.CL

TL;DR: CoDA is a hierarchical LLM agent framework that decouples planning from execution to prevent context explosion, using a single LLM backbone in two isolated roles trained end-to-end with PECO reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: LLM agents trained with RL suffer from "Context Explosion" - accumulation of long text outputs overwhelms context windows and causes reasoning failures during complex multi-step tasks.

Method: CoDA (Context-Decoupled hierarchical Agent) uses a single shared LLM backbone operating in two contextually isolated roles: high-level Planner for task decomposition in concise strategic context, and low-level Executor for tool interactions in ephemeral isolated workspace. Trained end-to-end with PECO (Planner-Executor Co-Optimization) RL methodology using trajectory-level reward to jointly optimize both roles.

Result: Significant performance improvements over SOTA baselines on complex multi-hop QA benchmarks. Strong robustness in long-context scenarios - maintains stable performance while all other baselines suffer severe degradation.

Conclusion: Hierarchical design effectively mitigates context overload by decoupling planning from execution, preventing context explosion and enabling stable performance in long-context scenarios.

Abstract: Large Language Model (LLM) agents trained with reinforcement learning (RL) show great promise for solving complex, multi-step tasks. However, their performance is often crippled by "Context Explosion", where the accumulation of long text outputs overwhelms the model's context window and leads to reasoning failures. To address this, we introduce CoDA, a Context-Decoupled hierarchical Agent, a simple but effective reinforcement learning framework that decouples high-level planning from low-level execution. It employs a single, shared LLM backbone that learns to operate in two distinct, contextually isolated roles: a high-level Planner that decomposes tasks within a concise strategic context, and a low-level Executor that handles tool interactions in an ephemeral, isolated workspace. We train this unified agent end-to-end using PECO (Planner-Executor Co-Optimization), a reinforcement learning methodology that applies a trajectory-level reward to jointly optimize both roles, fostering seamless collaboration through context-dependent policy updates. Extensive experiments demonstrate that CoDA achieves significant performance improvements over state-of-the-art baselines on complex multi-hop question-answering benchmarks, and it exhibits strong robustness in long-context scenarios, maintaining stable performance while all other baselines suffer severe degradation, thus further validating the effectiveness of our hierarchical design in mitigating context overload.

</details>


### [30] [NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents](https://arxiv.org/abs/2512.12730)
*Jingzhe Ding,Shengda Long,Changxin Pu,Huan Zhou,Hongwan Gao,Xiang Gao,Chao He,Yue Hou,Fei Hu,Zhaojian Li,Weiran Shi,Zaiyuan Wang,Daoguang Zan,Chenchen Zhang,Xiaoxu Zhang,Qizhi Chen,Xianfu Cheng,Bo Deng,Qingshui Gu,Kai Hua,Juntao Lin,Pai Liu,Mingchen Li,Xuanguang Pan,Zifan Peng,Yujia Qin,Yong Shan,Zhewen Tan,Weihao Xie,Zihan Wang,Yishuo Yuan,Jiayu Zhang,Enduo Zhao,Yunfei Zhao,He Zhu,Chenyang Zou,Ming Ding,Jianpeng Jiao,Jiaheng Liu,Minghao Liu,Qian Liu,Chongyao Tao,Jian Yang,Tong Yang,Zhaoxiang Zhang,Xinjie Chen,Wenhao Huang,Ge Zhang*

Main category: cs.CL

TL;DR: NL2Repo Bench is a new benchmark for evaluating coding agents' long-horizon repository generation capabilities, revealing current models struggle with complete software system creation despite advances in localized code generation.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on localized code generation, scaffolded completion, or short-term repair tasks, but fail to evaluate the long-horizon capabilities needed for building complete software systems from natural language requirements.

Method: Created NL2Repo Bench where agents must autonomously design architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library from a single natural-language requirements document in an empty workspace.

Result: Even state-of-the-art models achieve below 40% average test pass rates and rarely complete entire repositories correctly, revealing fundamental long-horizon failure modes including premature termination, loss of global coherence, and fragile cross-file dependencies.

Conclusion: Long-horizon repository generation remains largely unsolved, establishing long-horizon reasoning as a central bottleneck for autonomous coding agents, with NL2Repo Bench providing a rigorous testbed for measuring sustained agentic competence.

Abstract: Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents.

</details>


### [31] [Curió-Edu 7B: Examining Data Selection Impacts in LLM Continued Pretraining](https://arxiv.org/abs/2512.12770)
*Thales Sales Almeida,Rodrigo Nogueira,Hélio Pedrini*

Main category: cs.CL

TL;DR: Curió 7B is a Portuguese-adapted LLM trained on 100B tokens, while Curió-Edu 7B uses only 10B educational/STEM tokens but outperforms the larger model, showing data quality matters more than quantity for language adaptation.


<details>
  <summary>Details</summary>
Motivation: To investigate whether continued pretraining for language adaptation benefits more from data quantity or quality, especially when adapting general-purpose models to specific linguistic contexts like Portuguese.

Method: Created two models: Curió 7B (from LLaMA-2, trained on 100B Portuguese tokens from ClassiCC-PT corpus) and Curió-Edu 7B (trained on only 10B educational/STEM-filtered tokens from same corpus). Compared their performance to assess data quality vs quantity impact.

Result: Curió-Edu 7B, despite using only 10% of the data and 20% of the computation, outperformed the full-corpus Curió 7B model in evaluations, demonstrating that data selection quality is more important than quantity for linguistic adaptation.

Conclusion: Data quality plays a decisive role in continued pretraining for language adaptation, with carefully selected educational/STEM content yielding better results than larger but less focused datasets, even with limited prior exposure to the target language.

Abstract: Continued pretraining extends a language model's capabilities by further exposing it to additional data, often tailored to a specific linguistic or domain context. This strategy has emerged as an efficient alternative to full retraining when adapting general-purpose models to new settings. In this work, we investigate this paradigm through Curió 7B, a 7-billion-parameter model derived from LLaMA-2 and trained on 100 billion Portuguese tokens from the ClassiCC-PT corpus - the most extensive Portuguese-specific continued-pretraining effort above the three-billion-parameter scale to date. Beyond scale, we investigate whether quantity alone suffices or whether data quality plays a decisive role in linguistic adaptation. To this end, we introduce Curió-Edu 7B, a variant trained exclusively on the educational and STEM-filtered subset of the same corpus, totaling just 10 billion tokens. Despite using only 10% of the data and 20% of the computation, Curió-Edu 7B surpasses the full-corpus model in our evaluations, demonstrating that data selection can be fundamental even when adapting models with limited prior exposure to the target language. The developed models are available at https://huggingface.co/collections/ClassiCC-Corpus/curio-edu

</details>


### [32] [Persistent Personas? Role-Playing, Instruction Following, and Safety in Extended Interactions](https://arxiv.org/abs/2512.12775)
*Pedro Henrique Luz de Araujo,Michael A. Hedderich,Ali Modarressi,Hinrich Schuetze,Benjamin Roth*

Main category: cs.CL

TL;DR: Persona-assigned LLMs degrade in persona fidelity during long dialogues (100+ rounds), especially in goal-oriented conversations, revealing a trade-off between persona fidelity and instruction following that worsens over time.


<details>
  <summary>Details</summary>
Motivation: Current evaluation of persona-assigned LLMs is limited to short, single-round settings that don't reflect real-world usage in domains like education, healthcare, and sociodemographic simulation, where extended interactions are common.

Method: Introduced an evaluation protocol combining long persona dialogues (over 100 rounds) with evaluation datasets to create dialogue-conditioned benchmarks for measuring long-context effects. Evaluated seven state-of-the-art open- and closed-weight LLMs on persona fidelity, instruction-following, and safety.

Result: Persona fidelity degrades over long dialogues, especially in goal-oriented conversations requiring both persona fidelity and instruction following. Found trade-off between fidelity and instruction following - non-persona baselines initially outperform persona-assigned models, but as fidelity fades, persona responses become similar to baseline responses.

Conclusion: Persona applications are fragile in extended interactions. The proposed protocol provides a systematic way to measure such failures, highlighting the need for better long-context persona maintenance in LLMs.

Abstract: Persona-assigned large language models (LLMs) are used in domains such as education, healthcare, and sociodemographic simulation. Yet, they are typically evaluated only in short, single-round settings that do not reflect real-world usage. We introduce an evaluation protocol that combines long persona dialogues (over 100 rounds) and evaluation datasets to create dialogue-conditioned benchmarks that can robustly measure long-context effects. We then investigate the effects of dialogue length on persona fidelity, instruction-following, and safety of seven state-of-the-art open- and closed-weight LLMs. We find that persona fidelity degrades over the course of dialogues, especially in goal-oriented conversations, where models must sustain both persona fidelity and instruction following. We identify a trade-off between fidelity and instruction following, with non-persona baselines initially outperforming persona-assigned models; as dialogues progress and fidelity fades, persona responses become increasingly similar to baseline responses. Our findings highlight the fragility of persona applications in extended interactions and our work provides a protocol to systematically measure such failures.

</details>


### [33] [State over Tokens: Characterizing the Role of Reasoning Tokens](https://arxiv.org/abs/2512.12777)
*Mosh Levy,Zohar Elyoseph,Shauli Ravfogel,Yoav Goldberg*

Main category: cs.CL

TL;DR: LLMs generate reasoning tokens that appear like human thought but aren't faithful explanations. The paper introduces State over Tokens (SoT) framework that reframes reasoning tokens as externalized computational state rather than linguistic narrative.


<details>
  <summary>Details</summary>
Motivation: There's a gap between the appearance of LLM reasoning tokens (which seem like human thought processes) and their actual function, as empirical evidence shows they're not faithful explanations of the model's reasoning process.

Method: Introduces the State over Tokens (SoT) conceptual framework that reframes reasoning tokens as externalized computational state - the sole persistent information carrier across the model's stateless generation cycles.

Result: SoT explains how reasoning tokens can drive correct reasoning without being faithful explanations when read as text, and surfaces previously overlooked research questions about these tokens.

Conclusion: To truly understand LLM reasoning processes, research must move beyond reading reasoning tokens as text and focus on decoding them as computational state.

Abstract: Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.

</details>


### [34] [Does Tone Change the Answer? Evaluating Prompt Politeness Effects on Modern LLMs: GPT, Gemini, LLaMA](https://arxiv.org/abs/2512.12812)
*Hanyu Cai,Binqi Shen,Lier Jin,Lan Hu,Xiaojing Fan*

Main category: cs.CL

TL;DR: Systematic evaluation shows LLM performance is affected by prompt tone (friendly vs. rude), but effects are model-dependent and domain-specific, with modern LLMs being broadly robust to tonal variation in mixed-domain use.


<details>
  <summary>Details</summary>
Motivation: To systematically examine how pragmatic elements like linguistic tone and politeness affect LLM performance across different model families, as this impact remains underexplored despite prompt engineering being critical for LLM performance.

Method: Proposed evaluation framework using MMMLU benchmark to test three LLMs (GPT-4o mini, Gemini 2.0 Flash, Llama 4 Scout) under Very Friendly, Neutral, and Very Rude prompt variants across six STEM and Humanities tasks, with statistical significance testing of pairwise accuracy differences.

Result: Tone sensitivity is model-dependent and domain-specific: Neutral/Friendly prompts generally yield higher accuracy than Rude prompts, but statistically significant effects appear only in Humanities tasks where rude tone reduces accuracy for GPT and Llama (Gemini remains tone-insensitive). Aggregated across domains, tone effects diminish and lose statistical significance.

Conclusion: While interaction tone matters in specific interpretive settings, modern LLMs are broadly robust to tonal variation in typical mixed-domain use, providing practical guidance for prompt design and model selection in real-world deployments.

Abstract: Prompt engineering has emerged as a critical factor influencing large language model (LLM) performance, yet the impact of pragmatic elements such as linguistic tone and politeness remains underexplored, particularly across different model families. In this work, we propose a systematic evaluation framework to examine how interaction tone affects model accuracy and apply it to three recently released and widely available LLMs: GPT-4o mini (OpenAI), Gemini 2.0 Flash (Google DeepMind), and Llama 4 Scout (Meta). Using the MMMLU benchmark, we evaluate model performance under Very Friendly, Neutral, and Very Rude prompt variants across six tasks spanning STEM and Humanities domains, and analyze pairwise accuracy differences with statistical significance testing.
  Our results show that tone sensitivity is both model-dependent and domain-specific. Neutral or Very Friendly prompts generally yield higher accuracy than Very Rude prompts, but statistically significant effects appear only in a subset of Humanities tasks, where rude tone reduces accuracy for GPT and Llama, while Gemini remains comparatively tone-insensitive. When performance is aggregated across tasks within each domain, tone effects diminish and largely lose statistical significance. Compared with earlier researches, these findings suggest that dataset scale and coverage materially influence the detection of tone effects. Overall, our study indicates that while interaction tone can matter in specific interpretive settings, modern LLMs are broadly robust to tonal variation in typical mixed-domain use, providing practical guidance for prompt design and model selection in real-world deployments.

</details>


### [35] [Hindsight is 20/20: Building Agent Memory that Retains, Recalls, and Reflects](https://arxiv.org/abs/2512.12818)
*Chris Latimer,Nicoló Boschi,Andrew Neeser,Chris Bartholomew,Gaurav Srivastava,Xuan Wang,Naren Ramakrishnan*

Main category: cs.CL

TL;DR: Hindsight is a structured memory architecture for LLM agents that organizes memory into four logical networks and supports retain/recall/reflect operations, achieving state-of-the-art performance on long-horizon conversational benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current agent memory systems treat memory as an external layer that extracts snippets and retrieves top-k items, which blurs evidence/inference lines, struggles with long-horizon organization, and offers limited support for reasoning explanation.

Method: Hindsight organizes memory into four structured networks: world facts, agent experiences, synthesized entity summaries, and evolving beliefs. It implements three core operations (retain, recall, reflect) with a temporal entity-aware memory layer that converts conversations into structured memory, and a reflection layer that reasons over this memory.

Result: On LongMemEval and LoCoMo benchmarks, Hindsight with a 20B model improved accuracy from 39% to 83.6% over full-context baseline, outperforming GPT-4o. Scaling further achieved 91.4% on LongMemEval and 89.61% on LoCoMo (vs. 75.78% for prior best open system).

Conclusion: Hindsight demonstrates that treating memory as a structured, first-class reasoning substrate with clear separation of evidence types and systematic operations enables superior long-horizon conversational performance and reasoning traceability compared to existing memory architectures.

Abstract: Agent memory has been touted as a dimension of growth for LLM-based applications, enabling agents that can accumulate experience, adapt across sessions, and move beyond single-shot question answering. The current generation of agent memory systems treats memory as an external layer that extracts salient snippets from conversations, stores them in vector or graph-based stores, and retrieves top-k items into the prompt of an otherwise stateless model. While these systems improve personalization and context carry-over, they still blur the line between evidence and inference, struggle to organize information over long horizons, and offer limited support for agents that must explain their reasoning. We present Hindsight, a memory architecture that treats agent memory as a structured, first-class substrate for reasoning by organizing it into four logical networks that distinguish world facts, agent experiences, synthesized entity summaries, and evolving beliefs. This framework supports three core operations -- retain, recall, and reflect -- that govern how information is added, accessed, and updated. Under this abstraction, a temporal, entity aware memory layer incrementally turns conversational streams into a structured, queryable memory bank, while a reflection layer reasons over this bank to produce answers and to update information in a traceable way. On key long-horizon conversational memory benchmarks like LongMemEval and LoCoMo, Hindsight with an open-source 20B model lifts overall accuracy from 39% to 83.6% over a full-context baseline with the same backbone and outperforms full context GPT-4o. Scaling the backbone further pushes Hindsight to 91.4% on LongMemEval and up to 89.61% on LoCoMo (vs. 75.78% for the strongest prior open system), consistently outperforming existing memory architectures on multi-session and open-domain questions.

</details>


### [36] [What Matters in Evaluating Book-Length Stories? A Systematic Study of Long Story Evaluation](https://arxiv.org/abs/2512.12839)
*Dingyi Yang,Qin Jin*

Main category: cs.CL

TL;DR: This paper introduces LongStoryEval, the first large-scale benchmark for evaluating book-length stories (>100K tokens), and proposes NovelCritique, an 8B model that outperforms GPT-4o in aligning with human evaluations using an efficient summary-based framework.


<details>
  <summary>Details</summary>
Motivation: There's a lack of systematic research and benchmarks for automatically evaluating book-length stories (>100K tokens), which is challenging due to their length. The paper aims to understand what evaluation aspects matter most to readers and explore effective methods for evaluating lengthy stories.

Method: 1) Created LongStoryEval benchmark with 600 newly published books (avg 121K tokens, max 397K) including ratings and reader reviews organized by evaluation aspects. 2) Analyzed user-mentioned aspects to propose evaluation criteria structure. 3) Compared three evaluation methods: aggregation-based, incremental-updated, and summary-based. 4) Developed NovelCritique, an 8B model using the efficient summary-based framework to review and score stories across specified aspects.

Result: 1) Identified 8 top-level evaluation criteria from reader feedback. 2) Found that aggregation- and summary-based evaluations perform best - aggregation excels in detail assessment while summary offers greater efficiency. 3) NovelCritique outperforms commercial models like GPT-4o in aligning with human evaluations.

Conclusion: The paper establishes the first comprehensive benchmark for long story evaluation, provides insights into effective evaluation methods, and demonstrates that a specialized 8B model (NovelCritique) can outperform larger commercial models in this challenging domain through efficient summary-based evaluation.

Abstract: In this work, we conduct systematic research in a challenging area: the automatic evaluation of book-length stories (>100K tokens). Our study focuses on two key questions: (1) understanding which evaluation aspects matter most to readers, and (2) exploring effective methods for evaluating lengthy stories. We introduce the first large-scale benchmark, LongStoryEval, comprising 600 newly published books with an average length of 121K tokens (maximum 397K). Each book includes its average rating and multiple reader reviews, presented as critiques organized by evaluation aspects. By analyzing all user-mentioned aspects, we propose an evaluation criteria structure and conduct experiments to identify the most significant aspects among the 8 top-level criteria. For evaluation methods, we compare the effectiveness of three types: aggregation-based, incremental-updated, and summary-based evaluations. Our findings reveal that aggregation- and summary-based evaluations perform better, with the former excelling in detail assessment and the latter offering greater efficiency. Building on these insights, we further propose NovelCritique, an 8B model that leverages the efficient summary-based framework to review and score stories across specified aspects. NovelCritique outperforms commercial models like GPT-4o in aligning with human evaluations. Our datasets and codes are available at https://github.com/DingyiYang/LongStoryEval.

</details>


### [37] [Counting Clues: A Lightweight Probabilistic Baseline Can Match an LLM](https://arxiv.org/abs/2512.12868)
*Furong Jia,Yuan Pu,Finn Guo,Monica Agrawal*

Main category: cs.CL

TL;DR: LLMs perform well on clinical diagnosis benchmarks, but FBPR (a simple frequency-based probabilistic method) achieves comparable performance, suggesting LLMs aren't just doing frequency aggregation and that traditional probabilistic methods still provide valuable complementary signals.


<details>
  <summary>Details</summary>
Motivation: To understand whether LLMs' strong performance on clinical diagnosis benchmarks reflects genuine probabilistic reasoning or just pattern matching of frequency statistics from their training data.

Method: Introduced FBPR (Frequency-Based Probabilistic Ranker) - a lightweight method using smoothed Naive Bayes over concept-diagnosis co-occurrence statistics from large corpora. Compared FBPR performance to LLMs (OLMo and Llama) on MedQA diagnosis questions.

Result: FBPR achieved comparable performance to LLMs pretrained on the same corpus. LLMs and FBPR largely got different questions correct, with overlap only slightly above random chance, showing complementary strengths.

Conclusion: Explicit probabilistic baselines remain valuable as performance reference points and complementary signals for potential hybridization. While LLMs use mechanisms beyond simple frequency aggregation, low-complexity expert systems still account for substantial benchmark performance.

Abstract: Large language models (LLMs) excel on multiple-choice clinical diagnosis benchmarks, yet it is unclear how much of this performance reflects underlying probabilistic reasoning. We study this through questions from MedQA, where the task is to select the most likely diagnosis. We introduce the Frequency-Based Probabilistic Ranker (FBPR), a lightweight method that scores options with a smoothed Naive Bayes over concept-diagnosis co-occurrence statistics from a large corpus. When co-occurrence statistics were sourced from the pretraining corpora for OLMo and Llama, FBPR achieves comparable performance to the corresponding LLMs pretrained on that same corpus. Direct LLM inference and FBPR largely get different questions correct, with an overlap only slightly above random chance, indicating complementary strengths of each method. These findings highlight the continued value of explicit probabilistic baselines: they provide a meaningful performance reference point and a complementary signal for potential hybridization. While the performance of LLMs seems to be driven by a mechanism other than simple frequency aggregation, we show that an approach similar to the historically grounded, low-complexity expert systems still accounts for a substantial portion of benchmark performance.

</details>


### [38] [Building from Scratch: A Multi-Agent Framework with Human-in-the-Loop for Multilingual Legal Terminology Mapping](https://arxiv.org/abs/2512.12950)
*Lingyi Meng,Maolin Liu,Hao Wang,Yilan Cheng,Qi Yang,Idlkaid Mohanmmed*

Main category: cs.CL

TL;DR: Human-AI collaborative multi-agent framework for building multilingual legal terminology database, tested on Chinese-English-Japanese statutes with improved precision and scalability.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of accurately mapping legal terminology across languages, especially for Chinese-Japanese homographs, where existing resources and tools are limited.

Method: Human-AI collaborative multi-agent framework integrating LLMs and legal experts throughout the workflow: AI handles repetitive tasks (OCR, segmentation, alignment, initial extraction) while humans provide oversight, review, and supervision with legal judgment.

Result: Tested on trilingual parallel corpus of 35 key Chinese statutes with English/Japanese translations. The framework improves precision and consistency of terminology mapping while offering greater scalability than traditional manual methods.

Conclusion: The human-in-the-loop multi-agent workflow effectively addresses multilingual legal terminology mapping challenges, balancing automation with expert oversight for better results.

Abstract: Accurately mapping legal terminology across languages remains a significant challenge, especially for language pairs like Chinese and Japanese, which share a large number of homographs with different meanings. Existing resources and standardized tools for these languages are limited. To address this, we propose a human-AI collaborative approach for building a multilingual legal terminology database, based on a multi-agent framework. This approach integrates advanced large language models and legal domain experts throughout the entire process-from raw document preprocessing, article-level alignment, to terminology extraction, mapping, and quality assurance. Unlike a single automated pipeline, our approach places greater emphasis on how human experts participate in this multi-agent system. Humans and AI agents take on different roles: AI agents handle specific, repetitive tasks, such as OCR, text segmentation, semantic alignment, and initial terminology extraction, while human experts provide crucial oversight, review, and supervise the outputs with contextual knowledge and legal judgment. We tested the effectiveness of this framework using a trilingual parallel corpus comprising 35 key Chinese statutes, along with their English and Japanese translations. The experimental results show that this human-in-the-loop, multi-agent workflow not only improves the precision and consistency of multilingual legal terminology mapping but also offers greater scalability compared to traditional manual methods.

</details>


### [39] [QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management](https://arxiv.org/abs/2512.12967)
*Weizhou Shen,Ziyi Yang,Chenliang Li,Zhiyuan Lu,Miao Peng,Huashan Sun,Yingcheng Shi,Shengyi Liao,Shaopeng Lai,Bo Zhang,Dayiheng Liu,Fei Huang,Jingren Zhou,Ming Yan*

Main category: cs.CL

TL;DR: QwenLong-L1.5 achieves state-of-the-art long-context reasoning through three innovations: synthetic data generation for complex reasoning tasks, stabilized RL training for long contexts, and memory-augmented architecture for ultra-long sequences up to 4M tokens.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in current long-context models that struggle with genuine multi-hop reasoning over globally distributed evidence and face instability in reinforcement learning for long sequences, while also addressing the practical constraint that even extended context windows cannot handle arbitrarily long documents.

Method: Three key innovations: (1) Long-context data synthesis pipeline that deconstructs documents into atomic facts and relationships to generate challenging reasoning tasks; (2) Stabilized RL with task-balanced sampling and Adaptive Entropy-Controlled Policy Optimization (AEPO); (3) Memory-augmented architecture with multi-stage fusion RL training for sequences exceeding 4M tokens.

Result: Achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context benchmarks, with 9.90-point average improvement over baseline. On ultra-long tasks (1M-4M tokens), memory-agent framework yields 9.48-point gain. Also shows enhanced performance in scientific reasoning, memory tool usage, and extended dialogue.

Conclusion: QwenLong-L1.5 demonstrates that systematic post-training innovations can enable superior long-context reasoning capabilities, moving beyond simple retrieval to genuine multi-hop reasoning, with the acquired abilities transferring to improved performance across general domains.

Abstract: We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.

</details>


### [40] [Authors Should Annotate](https://arxiv.org/abs/2512.12976)
*Marcus Ma,Cole Johnson,Nolan Bridges,Jackson Trager,Georgios Chochlakis,Shrikanth Narayanan*

Main category: cs.CL

TL;DR: Author labeling: having document writers annotate their own text at creation time yields higher quality, faster, cheaper annotations for subjective features like sentiment, with 534% CTR improvement in product recommendation.


<details>
  <summary>Details</summary>
Motivation: Third-party annotation is standard but inadequate for egocentric features like sentiment and belief where the document author's perspective is more accurate and valuable than third-person proxies.

Method: Collaborated with commercial chatbot (10k+ users) to deploy author labeling system that identifies task-relevant queries, generates on-the-fly labeling questions, and records authors' answers in real time. Used online-learning model architecture for product recommendation.

Result: Author labeling achieved 534% increase in click-through rate compared to industry advertising baseline. Compared to three traditional annotation approaches, author labeling was higher quality, faster to acquire, and cheaper.

Conclusion: Author labeling significantly outperforms third-party annotation for subjective/egocentric features, reinforcing literature that author-labeled annotations are higher quality. Released author labeling service for research community at academic.echollm.io.

Abstract: The status quo for labeling text is third-party annotation, but there are many cases where information directly from the document's source would be preferable over a third-person proxy, especially for egocentric features like sentiment and belief. We introduce author labeling, an annotation technique where the writer of the document itself annotates the data at the moment of creation. We collaborate with a commercial chatbot with over 10,000 users to deploy an author labeling annotation system for subjective features related to product recommendation. This system identifies task-relevant queries, generates on-the-fly labeling questions, and records authors' answers in real time. We train and deploy an online-learning model architecture for product recommendation that continuously improves from author labeling and find it achieved a 534% increase in click-through rate compared to an industry advertising baseline running concurrently. We then compare the quality and practicality of author labeling to three traditional annotation approaches for sentiment analysis and find author labeling to be higher quality, faster to acquire, and cheaper. These findings reinforce existing literature that annotations, especially for egocentric and subjective beliefs, are significantly higher quality when labeled by the author rather than a third party. To facilitate broader scientific adoption, we release an author labeling service for the research community at academic.echollm.io.

</details>


### [41] [An Open and Reproducible Deep Research Agent for Long-Form Question Answering](https://arxiv.org/abs/2512.13059)
*Ikuya Yamada,Wataru Ikeda,Ko Yoshida,Mengyu Ye,Hinata Sugimoto,Masatoshi Suzuki,Hisanori Ozaki,Jun Suzuki*

Main category: cs.CL

TL;DR: Open deep research system for long-form QA that combines open-source LLM with web search API for iterative retrieval, reasoning, and synthesis, enhanced by preference tuning using LLM-as-a-judge feedback.


<details>
  <summary>Details</summary>
Motivation: To create an effective system for long-form question answering in real-world open-domain settings, addressing the need for high-quality reasoning and factual accuracy in complex information retrieval tasks.

Method: Combines open-source LLM with open web search API for iterative retrieval, reasoning, and synthesis. Applies preference tuning based on LLM-as-a-judge feedback that evaluates multiple aspects including clarity, insightfulness, and factuality.

Result: The proposed method consistently improves answer quality across all three aspects (clarity, insightfulness, factuality). The system was selected as a winning system in the text-to-text track of the MMU-RAG competition at NeurIPS 2025.

Conclusion: The open deep research system demonstrates effective long-form QA capabilities through iterative retrieval and preference tuning, with publicly available source code for community use and further development.

Abstract: We present an open deep research system for long-form question answering, selected as a winning system in the text-to-text track of the MMU-RAG competition at NeurIPS 2025. The system combines an open-source large language model (LLM) with an open web search API to perform iterative retrieval, reasoning, and synthesis in real-world open-domain settings. To enhance reasoning quality, we apply preference tuning based on LLM-as-a-judge feedback that evaluates multiple aspects, including clarity, insightfulness, and factuality. Our experimental results show that the proposed method consistently improves answer quality across all three aspects. Our source code is publicly available at https://github.com/efficient-deep-research/efficient-deep-research.

</details>


### [42] [LLM Rationalis? Measuring Bargaining Capabilities of AI Negotiators](https://arxiv.org/abs/2512.13063)
*Cheril Shah,Akshit Agarwal,Kanak Garg,Mourad Heddaya*

Main category: cs.CL

TL;DR: LLMs fail to negotiate like humans - they anchor at extremes, lack context adaptation, and show no improvement with better models.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs compare to humans in bilateral negotiation, particularly in adapting to power asymmetries and context, and to identify fundamental limitations in current LLM negotiation capabilities.

Method: Developed mathematical framework with hyperbolic tangent curve for concession dynamics, introduced burstiness tau and Concession-Rigidity Index (CRI) metrics. Conducted large-scale empirical comparison between human negotiators and four state-of-the-art LLMs across multiple settings (natural-language, numeric-offers, with/without market context) and six power-asymmetry scenarios.

Result: LLMs systematically anchor at extremes of agreement zones, optimize for fixed points regardless of leverage/context, show limited strategy diversity, use occasional deceptive tactics, and their negotiation ability doesn't improve with better models. Humans smoothly adapt and infer opponent positions.

Conclusion: Current LLMs have fundamental limitations in negotiation capabilities and need better internalization of opponent reasoning and context-dependent strategy.

Abstract: Bilateral negotiation is a complex, context-sensitive task in which human negotiators dynamically adjust anchors, pacing, and flexibility to exploit power asymmetries and informal cues. We introduce a unified mathematical framework for modeling concession dynamics based on a hyperbolic tangent curve, and propose two metrics burstiness tau and the Concession-Rigidity Index (CRI) to quantify the timing and rigidity of offer trajectories. We conduct a large-scale empirical comparison between human negotiators and four state-of-the-art large language models (LLMs) across natural-language and numeric-offers settings, with and without rich market context, as well as six controlled power-asymmetry scenarios. Our results reveal that, unlike humans who smoothly adapt to situations and infer the opponents position and strategies, LLMs systematically anchor at extremes of the possible agreement zone for negotiations and optimize for fixed points irrespective of leverage or context. Qualitative analysis further shows limited strategy diversity and occasional deceptive tactics used by LLMs. Moreover the ability of LLMs to negotiate does not improve with better models. These findings highlight fundamental limitations in current LLM negotiation capabilities and point to the need for models that better internalize opponent reasoning and context-dependent strategy.

</details>


### [43] [Uncovering the Role of Initial Saliency in U-Shaped Attention Bias: Scaling Initial Token Weight for Enhanced Long-Text Processing](https://arxiv.org/abs/2512.13109)
*Zewen Qiang,Sendong Zhao,Haochun Wang,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: The paper addresses the "lost in the middle" problem in LLMs by identifying initial saliency as a key factor in attention bias, proposing a method to scale attention weights that improves long-context processing by up to 3.6%.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with long-text sequences due to the "lost in the middle" phenomenon caused by U-shaped attention bias, where attention focuses disproportionately on beginning and end tokens while neglecting the middle section.

Method: The research identifies initial saliency as a factor in attention bias and proposes scaling attention weights between the initial token and others to improve long-context processing. This approach is also combined with existing methods to reduce position encoding bias.

Result: The proposed method achieves maximum improvements of 3.6% on the MDQA dataset and 3.4% on KV-Retrieval tasks when combined with existing position encoding bias reduction techniques.

Conclusion: Initial saliency is an important factor in attention bias beyond position encoding, and scaling attention weights based on this property effectively mitigates the "lost in the middle" problem in LLMs, enhancing their long-context processing capabilities.

Abstract: Large language models (LLMs) have demonstrated strong performance on a variety of natural language processing (NLP) tasks. However, they often struggle with long-text sequences due to the ``lost in the middle'' phenomenon. This issue has been shown to arise from a U-shaped attention bias, where attention is disproportionately focused on the beginning and end of a text, leaving the middle section underrepresented. While previous studies have attributed this bias to position encoding, our research first identifies an additional factor: initial saliency. It means that in the attention computation for each token, tokens with higher attention weights relative to the initial token tend to receive more attention in the prediction of the next token. We further find that utilizing this property by scaling attention weight between the initial token and others improves the model's ability to process long contexts, achieving a maximum improvement of 3.6\% in MDQA dataset. Moreover, combining this approach with existing methods to reduce position encoding bias further enhances performance, achieving a maximum improvement of 3.4\% in KV-Retrieval tasks.

</details>


### [44] [Efficient Adaptive Rejection Sampling for Accelerating Speculative Decoding in Large Language Models](https://arxiv.org/abs/2512.13194)
*Chendong Sun*

Main category: cs.CL

TL;DR: EARS introduces adaptive rejection sampling for speculative decoding by dynamically adjusting acceptance thresholds based on target model uncertainty, reducing random rejections and improving throughput by up to 18.12% with minimal accuracy impact.


<details>
  <summary>Details</summary>
Motivation: Current speculative decoding suffers from "random rejection" problem where plausible candidate tokens are frequently rejected due to fixed, context-independent random thresholds, especially in high-uncertainty scenarios, undermining inference efficiency.

Method: EARS (Efficient Adaptive Rejection Sampling) dynamically adjusts acceptance thresholds by incorporating target model's predictive uncertainty (1 - max(P_target)), introducing tolerance proportional to uncertainty to relax acceptance criteria when model is uncertain while maintaining strict standards when confident.

Result: EARS significantly enhances speculative decoding efficiency, achieving up to 18.12% increase in throughput with only 0.84% accuracy drop on GSM8K benchmark, and shows effectiveness on creative writing and open-domain QA tasks.

Conclusion: EARS provides an effective solution to the random rejection problem in speculative decoding, improving inference efficiency without requiring model architecture modifications and being seamlessly integrable into existing frameworks.

Abstract: Speculative Decoding is a prominent technique for accelerating the autoregressive inference of large language models (LLMs) by employing a fast draft model to propose candidate token sequences and a large target model to verify them in parallel. However, its core component -- the rejection sampling mechanism -- relies on a fixed, context-independent random threshold. This leads to a significant "random rejection" problem in high-uncertainty generation scenarios, where plausible candidate tokens are frequently rejected due to random chance, undermining inference efficiency. This paper introduces Efficient Adaptive Rejection Sampling (EARS), a novel method that dynamically adjusts the acceptance threshold by incorporating the target model's own predictive uncertainty, measured as \(1 - \max(P_{\mathrm{target}})\). By introducing a tolerance term proportional to this uncertainty, EARS intelligently relaxes the acceptance criterion when the model is uncertain, effectively reducing random rejections while maintaining strict standards when the model is confident. Experiments on creative writing and open-domain QA tasks demonstrate that EARS significantly enhances the efficiency of speculative decoding, achieving up to an 18.12% increase in throughput with a negligible 0.84% accuracy drop on the GSM8K benchmark. The method requires no modifications to model architectures and can be seamlessly integrated into existing speculative decoding frameworks.

</details>


### [45] [AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning](https://arxiv.org/abs/2512.13278)
*Jiaru Zou,Ling Yang,Yunzhe Qi,Sirui Chen,Mengting Ai,Ke Shen,Jingrui He,Mengdi Wang*

Main category: cs.CL

TL;DR: AutoTool is a framework that enables LLM agents to dynamically select tools during reasoning, overcoming limitations of fixed tool inventories through a dual-phase optimization pipeline and large-scale training data.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agent approaches assume fixed tool inventories, limiting adaptability to new or evolving toolsets. There's a need for agents that can dynamically select appropriate tools throughout reasoning trajectories.

Method: 1) Constructed 200k dataset with tool-selection rationales across 1,000+ tools and 100+ tasks; 2) Dual-phase optimization: supervised/RL-based trajectory stabilization and KL-regularized Plackett-Luce ranking for consistent multi-step tool selection; 3) Trained on Qwen3-8B and Qwen2.5-VL-7B models.

Result: Outperforms advanced LLM agents and tool-integration methods across 10 benchmarks: 6.4% gain in math/science reasoning, 4.5% in search-based QA, 7.7% in code generation, 6.9% in multimodal understanding. Shows strong generalization to unseen tools.

Conclusion: AutoTool enables LLM agents with dynamic tool-selection capabilities, achieving state-of-the-art performance across diverse reasoning tasks while maintaining adaptability to evolving toolsets.

Abstract: Agentic reinforcement learning has advanced large language models (LLMs) to reason through long chain-of-thought trajectories while interleaving external tool use. Existing approaches assume a fixed inventory of tools, limiting LLM agents' adaptability to new or evolving toolsets. We present AutoTool, a framework that equips LLM agents with dynamic tool-selection capabilities throughout their reasoning trajectories. We first construct a 200k dataset with explicit tool-selection rationales across 1,000+ tools and 100+ tasks spanning mathematics, science, code generation, and multimodal reasoning. Building on this data foundation, AutoTool employs a dual-phase optimization pipeline: (i) supervised and RL-based trajectory stabilization for coherent reasoning, and (ii) KL-regularized Plackett-Luce ranking to refine consistent multi-step tool selection. Across ten diverse benchmarks, we train two base models, Qwen3-8B and Qwen2.5-VL-7B, with AutoTool. With fewer parameters, AutoTool consistently outperforms advanced LLM agents and tool-integration methods, yielding average gains of 6.4% in math & science reasoning, 4.5% in search-based QA, 7.7% in code generation, and 6.9% in multimodal understanding. In addition, AutoTool exhibits stronger generalization by dynamically leveraging unseen tools from evolving toolsets during inference.

</details>


### [46] [AIR: Post-training Data Selection for Reasoning via Attention Head Influence](https://arxiv.org/abs/2512.13279)
*Jinrui Liu,Jeff Wu,Xuanguang Pan,Gavin Cheung,Shuai Ma,Chongyang Tao*

Main category: cs.CL

TL;DR: AIR is a training-free framework that uses attention head influence to select high-value reasoning data for LLM distillation, outperforming heuristic baselines.


<details>
  <summary>Details</summary>
Motivation: Current data selection methods for LLM reasoning distillation (manual curation, length/entropy/loss heuristics) fail to capture causal importance of individual reasoning steps, limiting distillation efficiency.

Method: AIR identifies reasoning-critical attention heads, constructs weakened reference model by disabling head influence, quantifies loss divergence as Attention Influence Score for fine-grained step/sample assessment, enabling weighted fine-tuning and sample selection.

Result: Experiments across multiple reasoning benchmarks show AIR consistently improves reasoning accuracy, surpasses heuristic baselines, and effectively isolates critical steps and samples.

Conclusion: AIR establishes a mechanism-driven, data-efficient approach for reasoning distillation in LLMs by leveraging attention head influence for principled data selection.

Abstract: LLMs achieve remarkable multi-step reasoning capabilities, yet effectively transferring these skills via post-training distillation remains challenging. Existing data selection methods, ranging from manual curation to heuristics based on length, entropy, or overall loss, fail to capture the causal importance of individual reasoning steps, limiting distillation efficiency. To address this, we propose Attention Influence for Reasoning (AIR), a principled, unsupervised and training-free framework that leverages mechanistic insights of the retrieval head to select high-value post-training data. AIR first identifies reasoning-critical attention heads of an off-the-shelf model, then constructs a weakened reference model with disabled head influence, and finally quantifies the resulting loss divergence as the Attention Influence Score. This score enables fine-grained assessment at both the step and sample levels, supporting step-level weighted fine-tuning and global sample selection. Experiments across multiple reasoning benchmarks show that AIR consistently improves reasoning accuracy, surpassing heuristic baselines and effectively isolating the most critical steps and samples. Our work establishes a mechanism-driven, data-efficient approach for reasoning distillation in LLMs.

</details>


### [47] [Integrating Causal Reasoning into Automated Fact-Checking](https://arxiv.org/abs/2512.13286)
*Youssra Rebboud,Pasquale Lisena,Raphael Troncy*

Main category: cs.CL

TL;DR: Proposes a methodology combining event relation extraction, semantic similarity, and rule-based reasoning to detect causal inconsistencies in fact-checking, establishing first baseline for causal event relationships.


<details>
  <summary>Details</summary>
Motivation: Current automated fact-checking methods lack dedicated causal-based reasoning, missing opportunities for semantically rich explainability, especially for detecting erroneous cause-effect relationships between events.

Method: Combines event relation extraction, semantic similarity computation, and rule-based reasoning to detect logical inconsistencies between chains of events in claims and evidence.

Result: Evaluated on two fact-checking datasets, establishes the first baseline for integrating fine-grained causal event relationships into fact-checking and enhances explainability of verdict prediction.

Conclusion: The proposed methodology successfully addresses the gap in causal-based reasoning for fact-checking, providing a valuable approach for detecting erroneous cause-effect relationships and improving explainability.

Abstract: In fact-checking applications, a common reason to reject a claim is to detect the presence of erroneous cause-effect relationships between the events at play. However, current automated fact-checking methods lack dedicated causal-based reasoning, potentially missing a valuable opportunity for semantically rich explainability. To address this gap, we propose a methodology that combines event relation extraction, semantic similarity computation, and rule-based reasoning to detect logical inconsistencies between chains of events mentioned in a claim and in an evidence. Evaluated on two fact-checking datasets, this method establishes the first baseline for integrating fine-grained causal event relationships into fact-checking and enhance explainability of verdict prediction.

</details>


### [48] [MiniLingua: A Small Open-Source LLM for European Languages](https://arxiv.org/abs/2512.13298)
*Anna Aksenova,Boris Zverkov,Nicola Dainese,Alexander Nikitin,Pekka Marttinen*

Main category: cs.CL

TL;DR: MiniLingua is a 1B parameter multilingual LLM trained from scratch for 13 European languages that outperforms similar models with larger budgets while remaining competitive with SOTA models on generation tasks.


<details>
  <summary>Details</summary>
Motivation: Address limitations of large language models: high computational cost, privacy concerns, and English-centric training. Need for small, efficient models that can run on-device while maintaining multilingual capabilities.

Method: Train MiniLingua from scratch as a 1B parameter multilingual LLM for 13 European languages. Use instruction tuning to enhance capabilities. Release includes model weights, tokenizer, and source code for data processing and training.

Result: Instruction-tuned MiniLingua outperforms EuroLLM (similar approach but larger budget) on summarization, classification, and both open-/closed-book QA. Remains competitive with more advanced SOTA models on open-ended generation tasks.

Conclusion: MiniLingua demonstrates that small, efficient multilingual models can achieve strong performance while addressing computational, privacy, and language coverage concerns. The open-source release enables broader adoption and on-device applications.

Abstract: Large language models are powerful but often limited by high computational cost, privacy concerns, and English-centric training. Recent progress demonstrates that small, efficient models with around one billion parameters can deliver strong results and enable on-device use. This paper introduces MiniLingua, a multilingual open-source LLM of one billion parameters trained from scratch for 13 European languages, designed to balance coverage and instruction-following capabilities. Based on evaluation results, the instruction-tuned version of MiniLingua outperforms EuroLLM, a model with a similar training approach but a larger training budget, on summarization, classification and both open- and closed-book question answering. Moreover, it remains competitive with more advanced state-of-the-art models on open-ended generation tasks. We release model weights, tokenizer and source code used for data processing and model training.

</details>


### [49] [FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models](https://arxiv.org/abs/2512.13330)
*Joona Kytöniemi,Jousia Piha,Akseli Reunamo,Fedor Vitiugin,Farrokh Mehryary,Sampo Pyysalo*

Main category: cs.CL

TL;DR: FIN-bench-v2 is a unified benchmark suite for evaluating LLMs in Finnish, consolidating existing Finnish benchmarks with expanded coverage across multiple task types and domains.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive, standardized evaluation framework for large language models in Finnish, addressing the need for consistent benchmarking across diverse tasks and ensuring high-quality, validated resources.

Method: Consolidated Finnish versions of widely used benchmarks with original FIN-bench; converted all datasets to HuggingFace format with cloze and multiple-choice prompts; incorporated human annotation for machine-translated resources; used pretrained 2.15B-parameter models to select robust tasks based on learning curve metrics; evaluated larger instruction-tuned models across tasks.

Result: Created a unified benchmark suite covering reading comprehension, commonsense reasoning, sentiment analysis, world knowledge, and alignment tasks; developed systematic prompt formulations; established task selection criteria based on model learning curves; made all resources publicly available via GitHub repositories.

Conclusion: FIN-bench-v2 provides a comprehensive, standardized evaluation framework for Finnish LLMs with validated resources, systematic task selection, and public availability, enabling better benchmarking and development of language models for Finnish.

Abstract: We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolidates Finnish versions of widely used benchmarks together with an updated and expanded version of the original FIN-bench into a single, consistently formatted collection, covering multiple-choice and generative tasks across reading comprehension, commonsense reasoning, sentiment analysis, world knowledge, and alignment. All datasets are converted to HuggingFace Datasets, which include both cloze and multiple-choice prompt formulations with five variants per task, and we incorporate human annotation or review for machine-translated resources such as GoldenSwag and XED. To select robust tasks, we pretrain a set of 2.15B-parameter decoder-only models and use their learning curves to compute monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks that satisfy all criteria. We further evaluate a set of larger instruction-tuned models to characterize performance across tasks and prompt formulations. All datasets, prompts, and evaluation configurations are publicly available via our fork of the Language Model Evaluation Harness at https://github.com/LumiOpen/lm-evaluation-harness. Supplementary resources are released in a separate repository at https://github.com/TurkuNLP/FIN-bench-v2.

</details>


### [50] [Detecting Emotion Drift in Mental Health Text Using Pre-Trained Transformers](https://arxiv.org/abs/2512.13363)
*Shibani Sankpal*

Main category: cs.CL

TL;DR: Study investigates emotion drift in mental health messages using transformer models to track emotional changes across sentences, revealing patterns of escalation or relief.


<details>
  <summary>Details</summary>
Motivation: Traditional sentiment analysis treats entire messages as single emotional units, overlooking nuanced emotional shifts within messages, especially important in mental health contexts where emotional dynamics matter.

Method: Uses pre-trained transformer models (DistilBERT and RoBERTa) to detect sentence-level emotions and compute emotion drift scores to measure emotional changes across messages.

Result: Reveals patterns of emotional escalation or relief in mental health conversations, providing insights into emotional dynamics that traditional sentiment analysis misses.

Conclusion: The methodology enables better understanding of emotional dynamics in content, particularly valuable for mental health applications where tracking emotional shifts is crucial.

Abstract: This study investigates emotion drift: the change in emotional state across a single text, within mental health-related messages. While sentiment analysis typically classifies an entire message as positive, negative, or neutral, the nuanced shift of emotions over the course of a message is often overlooked. This study detects sentence-level emotions and measures emotion drift scores using pre-trained transformer models such as DistilBERT and RoBERTa. The results provide insights into patterns of emotional escalation or relief in mental health conversations. This methodology can be applied to better understand emotional dynamics in content.

</details>


### [51] [Large language models are not about language](https://arxiv.org/abs/2512.13441)
*Johan J. Bolhuis,Andrea Moro,Stephen Crain,Sandiway Fong*

Main category: cs.CL

TL;DR: The paper argues that LLMs are fundamentally inadequate for linguistics because they are probabilistic models of surface strings, while human language is based on an internal computational system that generates hierarchical thought structures with minimal input.


<details>
  <summary>Details</summary>
Motivation: The motivation is to challenge the use of Large Language Models in linguistic research, arguing that they fail to capture the true nature of human language which is based on an internal computational system rather than statistical patterns in external data.

Method: The method appears to be conceptual/argumentative rather than empirical - contrasting the nature of LLMs (probabilistic models requiring vast data) with the nature of human language (internal computational system with minimal input requirements). The argument relies on theoretical linguistics principles about language acquisition and competence.

Result: The paper concludes that LLMs are "useless for linguistics" because they cannot capture the essential properties of human language: its internal computational nature, ability to grow from minimal input, and capacity to distinguish possible from impossible languages.

Conclusion: LLMs are fundamentally mismatched for linguistic analysis because they model surface strings statistically, while human language is an internal computational system that generates hierarchical structures. Linguistics should focus on the mind-internal language faculty rather than statistical patterns in external data.

Abstract: Large Language Models are useless for linguistics, as they are probabilistic models that require a vast amount of data to analyse externalized strings of words. In contrast, human language is underpinned by a mind-internal computational system that recursively generates hierarchical thought structures. The language system grows with minimal external input and can readily distinguish between real language and impossible languages.

</details>


### [52] [Scaling Laws for Code: Every Programming Language Matters](https://arxiv.org/abs/2512.13472)
*Jian Yang,Shawn Guo,Lin Jing,Wei Zhang,Aishan Liu,Chuan Hao,Zhoujun Li,Wayne Xin Zhao,Xianglong Liu,Weifeng Lv,Bryan Dai*

Main category: cs.CL

TL;DR: First systematic study of scaling laws for multilingual code LLMs, showing interpreted languages benefit more from scaling than compiled languages, and multilingual training provides synergistic benefits between similar languages.


<details>
  <summary>Details</summary>
Motivation: Current scaling laws for Code LLMs don't account for programming language differences, and existing work ignores the multilingual nature of modern software development. Need to understand how different PLs scale and influence each other.

Method: Conducted 1000+ experiments (336k+ H800 hours) across multiple programming languages, model sizes (0.2B-14B parameters), and dataset sizes (1T tokens). Analyzed scaling properties, multilingual synergies, and developed optimal token allocation strategies.

Result: Interpreted languages (Python) benefit more from scaling than compiled languages (Rust). Multilingual pre-training provides synergistic benefits, especially between syntactically similar languages. Parallel pairing strategy enhances cross-lingual abilities with good scaling properties.

Conclusion: Proposed proportion-dependent multilingual scaling law that optimally allocates training tokens by prioritizing high-utility languages, balancing high-synergy pairs, and reducing allocation to fast-saturating languages, achieving superior performance across all PLs.

Abstract: Code large language models (Code LLMs) are powerful but costly to train, with scaling laws predicting performance from model size, data, and compute. However, different programming languages (PLs) have varying impacts during pre-training that significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate the scaling laws of different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration of scaling laws for multilingual code pre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensive scaling laws for code LLMs across multiple PLs, revealing that interpreted languages (e.g., Python) benefit more from increased model size and data than compiled languages (e.g., Rust). The study demonstrates that multilingual pre-training provides synergistic benefits, particularly between syntactically similar PLs. Further, the pre-training strategy of the parallel pairing (concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, a proportion-dependent multilingual scaling law is proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget.

</details>


### [53] [Non-Resolution Reasoning: A Framework for Preserving Semantic Ambiguity in Language Models](https://arxiv.org/abs/2512.13478)
*Kei Saito*

Main category: cs.CL

TL;DR: NRR framework prevents premature semantic collapse in language models by preserving ambiguity during inference and making resolution explicit and controllable.


<details>
  <summary>Details</summary>
Motivation: Current language models suffer from premature semantic collapse where they commit to single meanings too early, discarding valid interpretations before sufficient context is available, leading to brittle reasoning and context failures.

Method: Non-Resolution Reasoning (NRR) framework with three components: Multi-Vector Embeddings (maintain multiple interpretations per token), Non-Collapsing Attention (prevent winner-take-all dynamics), and Contextual Identity Tracking (assign context-specific identities to entities). Unified by external Resolution Operator ρ for explicit semantic commitment.

Result: NRR achieves 90.9% accuracy on out-of-distribution identity-shift tasks compared to 9.1% for transformer baselines, demonstrating effective ambiguity preservation and context tracking.

Conclusion: NRR provides principled alternative to premature collapse, reframing ambiguity as explicit representational state rather than failure mode. Enables models to shift between creative, factual, and ambiguity-preserving reasoning without retraining.

Abstract: Premature semantic collapse -- the forced early commitment to a single meaning -- remains a core architectural limitation of current language models. Softmax-driven competition and greedy decoding cause models to discard valid interpretations before sufficient context is available, resulting in brittle reasoning and context failures. We introduce Non-Resolution Reasoning (NRR), a general computational framework that preserves semantic ambiguity during inference and performs resolution only when explicitly required. NRR integrates three components: (1) Multi-Vector Embeddings that maintain multiple viable interpretations per token, (2) Non-Collapsing Attention that prevents winner-take-all dynamics across layers, and (3) Contextual Identity Tracking (CIT), which assigns context-specific identities to recurring entities (e.g., distinguishing "Dr. Smith the cardiologist" from "Dr. Smith the researcher"). These mechanisms are unified by an external Resolution Operator $ρ$ that makes semantic commitment explicit, controllable, and task-dependent. Unlike standard architectures, NRR separates representation from resolution, allowing a single model to shift between creative, factual, and ambiguity-preserving reasoning without retraining. A synthetic evaluation demonstrates NRR's ability to preserve ambiguity and track context: CIT-enhanced models achieve 90.9% accuracy on out-of-distribution identity-shift tasks, compared to 9.1% for transformer baselines. NRR provides a principled alternative to premature collapse, reframing ambiguity as an explicit representational state rather than a failure mode. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.

</details>


### [54] [Advancing Bangla Machine Translation Through Informal Datasets](https://arxiv.org/abs/2512.13487)
*Ayon Roy,Risat Rahaman,Sadat Shibly,Udoy Saha Joy,Abdulla Al Kafi,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: This paper addresses the gap in Bangla machine translation by focusing on informal language, developing datasets from social media and conversational texts to improve accessibility for Bangla speakers.


<details>
  <summary>Details</summary>
Motivation: Bangla is the 6th most spoken language with 234M native speakers, but open-source Bangla MT lags behind. Most online resources remain untranslated, excluding millions from essential information. Existing research focuses only on formal language, neglecting informal Bangla which is more commonly used, due to lack of pairwise data and advanced models.

Method: The researchers explore current state-of-the-art models and propose improvements by developing a dataset from informal sources like social media and conversational texts to handle natural, informal Bangla language.

Result: The paper aims to advance Bangla machine translation by focusing on informal language translation, though specific quantitative results are not provided in the abstract.

Conclusion: By enhancing datasets and models to handle informal Bangla, millions of people will benefit from improved online information access, advancing Bangla machine translation and improving digital accessibility for Bangla speakers.

Abstract: Bangla is the sixth most widely spoken language globally, with approximately 234 million native speakers. However, progress in open-source Bangla machine translation remains limited. Most online resources are in English and often remain untranslated into Bangla, excluding millions from accessing essential information. Existing research in Bangla translation primarily focuses on formal language, neglecting the more commonly used informal language. This is largely due to the lack of pairwise Bangla-English data and advanced translation models. If datasets and models can be enhanced to better handle natural, informal Bangla, millions of people will benefit from improved online information access. In this research, we explore current state-of-the-art models and propose improvements to Bangla translation by developing a dataset from informal sources like social media and conversational texts. This work aims to advance Bangla machine translation by focusing on informal language translation and improving accessibility for Bangla speakers in the digital world.

</details>


### [55] [SkipCat: Rank-Maximized Low-Rank Compression of Large Language Models via Shared Projection and Block Skipping](https://arxiv.org/abs/2512.13494)
*Yu-Chen Lu,Sheng-Feng Yu,Hui-Hsien Weng,Pei-Shuo Wang,Yu-Fang Hu,Liang Hung-Chun,Hung-Yueh Chiang,Kai-Chiang Wu*

Main category: cs.CL

TL;DR: SkipCat is a novel low-rank compression framework for LLMs that enables higher rank retention under same compression rates through intra-layer shared projections and block skipping, achieving 7% better accuracy than previous methods.


<details>
  <summary>Details</summary>
Motivation: LLMs have large parameter sizes that make deployment on edge devices challenging due to limited computational and memory resources. While low-rank compression can help, naive methods require aggressive rank reduction (more than half) to achieve efficiency gains, which causes substantial performance degradation.

Method: SkipCat uses two key techniques: 1) Intra-layer shared low-rank projection where multiple matrices sharing the same input use a common projection to reduce redundancy, and 2) Block skipping that omits computations and memory transfers for selected sub-blocks within low-rank decomposition. These enable retaining more effective ranks under the same compression budget.

Result: Experimental results show that without additional fine-tuning, SkipCat outperforms previous low-rank compression approaches by 7% accuracy improvement on zero-shot tasks under the same compression rate.

Conclusion: SkipCat's rank-maximized compression strategy effectively preserves model performance under tight resource constraints, making LLMs more suitable for deployment on edge devices while maintaining better accuracy than previous compression methods.

Abstract: Large language models (LLM) have achieved remarkable performance across a wide range of tasks. However, their substantial parameter sizes pose significant challenges for deployment on edge devices with limited computational and memory resources. Low-rank compression is a promising approach to address this issue, as it reduces both computational and memory costs, making LLM more suitable for resource-constrained environments. Nonetheless, naïve low-rank compression methods require a significant reduction in the retained rank to achieve meaningful memory and computation savings. For a low-rank model, the ranks need to be reduced by more than half to yield efficiency gains. Such aggressive truncation, however, typically results in substantial performance degradation. To address this trade-off, we propose SkipCat, a novel low-rank compression framework that enables the use of higher ranks while achieving the same compression rates. First, we introduce an intra-layer shared low-rank projection method, where multiple matrices that share the same input use a common projection. This reduces redundancy and improves compression efficiency. Second, we propose a block skipping technique that omits computations and memory transfers for selected sub-blocks within the low-rank decomposition. These two techniques jointly enable our compressed model to retain more effective ranks under the same compression budget. Experimental results show that, without any additional fine-tuning, our method outperforms previous low-rank compression approaches by 7% accuracy improvement on zero-shot tasks under the same compression rate. These results highlight the effectiveness of our rank-maximized compression strategy in preserving model performance under tight resource constraints.

</details>


### [56] [PrahokBART: A Pre-trained Sequence-to-Sequence Model for Khmer Natural Language Generation](https://arxiv.org/abs/2512.13552)
*Hour Kaing,Raj Dabre,Haiyue Song,Van-Hien Tran,Hideki Tanaka,Masao Utiyama*

Main category: cs.CL

TL;DR: PrahokBART is a compact pre-trained sequence-to-sequence model specifically designed for Khmer language, outperforming multilingual models like mBART50 on generative tasks through improved corpus quality and linguistic processing.


<details>
  <summary>Details</summary>
Motivation: Existing multilingual models ignore Khmer's linguistic issues and lack quality pre-training corpora. The authors aim to create a specialized model that addresses Khmer's unique challenges like word segmentation and normalization.

Method: Train PrahokBART from scratch using carefully curated Khmer and English corpora. Incorporate linguistic components including word segmentation and normalization to handle Khmer's specific characteristics. Evaluate on three generative tasks.

Result: PrahokBART outperforms mBART50 (a strong multilingual model) on machine translation, text summarization, and headline generation tasks. The analysis provides insights into linguistic module impact and space handling during text generation.

Conclusion: Specialized monolingual models with linguistic processing outperform multilingual models for low-resource languages like Khmer. Proper handling of linguistic features and space management is crucial for natural Khmer text generation.

Abstract: This work introduces {\it PrahokBART}, a compact pre-trained sequence-to-sequence model trained from scratch for Khmer using carefully curated Khmer and English corpora. We focus on improving the pre-training corpus quality and addressing the linguistic issues of Khmer, which are ignored in existing multilingual models, by incorporating linguistic components such as word segmentation and normalization. We evaluate PrahokBART on three generative tasks: machine translation, text summarization, and headline generation, where our results demonstrate that it outperforms mBART50, a strong multilingual pre-trained model. Additionally, our analysis provides insights into the impact of each linguistic module and evaluates how effectively our model handles space during text generation, which is crucial for the naturalness of texts in Khmer.

</details>


### [57] [Verifying Rumors via Stance-Aware Structural Modeling](https://arxiv.org/abs/2512.13559)
*Gibson Nkhata,Uttamasha Anjally Oyshi,Quan Mai,Susan Gauch*

Main category: cs.CL

TL;DR: A stance-aware structural modeling approach for rumor verification on social media that encodes posts with stance signals and aggregates replies by stance category, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle to jointly capture semantic content, stance information, and conversation structure for rumor verification, especially under transformer sequence length constraints.

Method: Proposes stance-aware structural modeling that encodes each post with its stance signal, aggregates reply embeddings by stance category, and introduces stance distribution and hierarchical depth as covariates to capture stance imbalance and reply depth influence.

Result: Extensive experiments on benchmark datasets show the approach significantly outperforms prior methods in predicting rumor truthfulness, and demonstrates versatility for early detection and cross-platform generalization.

Conclusion: The stance-aware structural modeling provides a scalable and semantically enriched representation of conversation threads, effectively addressing limitations of existing models for rumor verification on social media.

Abstract: Verifying rumors on social media is critical for mitigating the spread of false information. The stances of conversation replies often provide important cues to determine a rumor's veracity. However, existing models struggle to jointly capture semantic content, stance information, and conversation strructure, especially under the sequence length constraints of transformer-based encoders. In this work, we propose a stance-aware structural modeling that encodes each post in a discourse with its stance signal and aggregates reply embedddings by stance category enabling a scalable and semantically enriched representation of the entire thread. To enhance structural awareness, we introduce stance distribution and hierarchical depth as covariates, capturing stance imbalance and the influence of reply depth. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms prior methods in the ability to predict truthfulness of a rumor. We also demonstrate that our model is versatile for early detection and cross-platfrom generalization.

</details>


### [58] [Memory in the Age of AI Agents](https://arxiv.org/abs/2512.13564)
*Yuyang Hu,Shichun Liu,Yanwei Yue,Guibin Zhang,Boyang Liu,Fangyi Zhu,Jiahang Lin,Honglin Guo,Shihan Dou,Zhiheng Xi,Senjie Jin,Jiejun Tan,Yanbin Yin,Jiongnan Liu,Zeyu Zhang,Zhongxiang Sun,Yutao Zhu,Hao Sun,Boci Peng,Zhenrong Cheng,Xuanbo Fan,Jiaxin Guo,Xinlei Yu,Zhenhong Zhou,Zewen Hu,Jiahao Huo,Junhao Wang,Yuwei Niu,Yu Wang,Zhenfei Yin,Xiaobin Hu,Yue Liao,Qiankun Li,Kun Wang,Wangchunshu Zhou,Yixin Liu,Dawei Cheng,Qi Zhang,Tao Gui,Shirui Pan,Yan Zhang,Philip Torr,Zhicheng Dou,Ji-Rong Wen,Xuanjing Huang,Yu-Gang Jiang,Shuicheng Yan*

Main category: cs.CL

TL;DR: This paper provides a comprehensive survey of agent memory research, offering a unified framework to analyze memory through forms, functions, and dynamics, while distinguishing it from related concepts and identifying future research directions.


<details>
  <summary>Details</summary>
Motivation: The field of agent memory research has become increasingly fragmented with loosely defined terminologies and diverse implementations. Traditional taxonomies like long/short-term memory are insufficient for capturing the complexity of modern agent memory systems, creating a need for conceptual clarity and a unified framework.

Method: The authors analyze agent memory through three unified lenses: forms (token-level, parametric, and latent memory), functions (factual, experiential, and working memory), and dynamics (formation, evolution, and retrieval). They also compile benchmarks and frameworks, and distinguish agent memory from related concepts like LLM memory and RAG.

Result: The paper provides a comprehensive landscape of current agent memory research, establishing clear conceptual distinctions and proposing a refined taxonomy. It identifies three dominant memory forms, three functional categories, and analyzes memory dynamics, while also compiling practical resources for development.

Conclusion: This survey serves as both a reference for existing work and a conceptual foundation for treating memory as a first-class primitive in future agent design. It identifies emerging frontiers including memory automation, RL integration, multimodal memory, multi-agent memory, and trustworthiness issues.

Abstract: Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.

</details>


### [59] [ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding](https://arxiv.org/abs/2512.13586)
*Jia-Nan Li,Jian Guan,Wei Wu,Chongxuan Li*

Main category: cs.CL

TL;DR: ReFusion is a masked diffusion model that uses slot-level parallel decoding with plan-and-infill process to overcome limitations of autoregressive models (slow inference) and masked diffusion models (computational overhead, incoherent generation).


<details>
  <summary>Details</summary>
Motivation: Autoregressive models suffer from slow sequential inference, while masked diffusion models have high computational overhead (no KV caching) and generate incoherent outputs due to learning dependencies over intractable token combinations.

Method: ReFusion elevates parallel decoding from token level to slot level (contiguous sub-sequences). It uses iterative "plan-and-infill": 1) diffusion-based planning identifies weakly dependent slots, 2) autoregressive infilling decodes selected slots in parallel. Slot design enables KV cache reuse and reduces learning complexity.

Result: On seven benchmarks, ReFusion surpasses prior masked diffusion models with 34% performance gains and 18× speedup on average. It bridges performance gap to strong autoregressive models while maintaining 2.33× average speedup.

Conclusion: ReFusion successfully addresses key limitations of both autoregressive and masked diffusion models through slot-level parallel decoding, achieving superior performance and efficiency.

Abstract: Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\times$ average speedup.

</details>


### [60] [Textual Gradients are a Flawed Metaphor for Automatic Prompt Optimization](https://arxiv.org/abs/2512.13598)
*Daniel Melcer,Qi Chen,Wen-Hao Chiang,Shweta Garg,Pranav Garg,Christian Bock*

Main category: cs.CL

TL;DR: Textual gradient methods for prompt optimization often improve LLM performance, but the gradient analogy doesn't accurately explain their behavior.


<details>
  <summary>Details</summary>
Motivation: To investigate whether textual gradient methods for automatic prompt optimization actually work as described by the gradient analogy, and to understand their true behavior.

Method: Conducted a series of experiments and case studies to analyze the behavior of textual gradient prompt optimization techniques.

Result: While textual gradient methods often result in performance improvements, the experiments suggest the gradient analogy does not accurately explain their behavior.

Conclusion: These insights can inform better selection of prompt optimization strategies and development of new approaches that more accurately reflect how these methods work.

Abstract: A well-engineered prompt can increase the performance of large language models; automatic prompt optimization techniques aim to increase performance without requiring human effort to tune the prompts. One leading class of prompt optimization techniques introduces the analogy of textual gradients. We investigate the behavior of these textual gradient methods through a series of experiments and case studies. While such methods often result in a performance improvement, our experiments suggest that the gradient analogy does not accurately explain their behavior. Our insights may inform the selection of prompt optimization strategies, and development of new approaches.

</details>


### [61] [Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models](https://arxiv.org/abs/2512.13607)
*Boxin Wang,Chankyu Lee,Nayeon Lee,Sheng-Chieh Lin,Wenliang Dai,Yang Chen,Yangyi Chen,Zhuolin Yang,Zihan Liu,Mohammad Shoeybi,Bryan Catanzaro,Wei Ping*

Main category: cs.CL

TL;DR: Cascade RL trains general-purpose reasoning models by sequentially applying RL to different domains, reducing complexity and achieving SOTA performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Cross-domain heterogeneity in RL training for reasoning models causes infrastructure complexity, slow training, and curriculum/hyperparameter challenges. Conventional approaches blend heterogeneous prompts, which complicates engineering.

Method: Cascade RL (sequential domain-wise RL) instead of blending heterogeneous prompts. RLHF for alignment as pre-step, followed by domain-wise RLVR stages. Models operate in both instruct and deep thinking modes.

Result: 14B model outperforms SFT teacher DeepSeek-R1-0528 on LiveCodeBench v5/v6/Pro, achieves silver-medal in 2025 IOI. Domain-wise RL rarely degrades earlier domain performance and may improve it. State-of-the-art across wide benchmarks.

Conclusion: Cascade RL effectively addresses cross-domain heterogeneity, reduces engineering complexity, and delivers superior reasoning performance. RLHF boosts reasoning beyond preference optimization. Training recipes are transparently shared.

Abstract: Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.

</details>


### [62] [Temporal Tokenization Strategies for Event Sequence Modeling with Large Language Models](https://arxiv.org/abs/2512.13618)
*Zefang Liu,Nam Nguyen,Yinzhu Quan,Austin Zhang*

Main category: cs.CL

TL;DR: First empirical study comparing temporal tokenization strategies for event sequences in LLMs, finding no universal best approach - performance depends on matching tokenizer to data's statistical properties.


<details>
  <summary>Details</summary>
Motivation: Continuous time representation is a critical but under-explored challenge in modeling temporal event sequences with LLMs. Existing strategies like byte-level representations or calendar tokens exist, but optimal approach remains unclear given diverse statistical distributions of real-world event data.

Method: Empirical study comparing five distinct temporal encoding strategies: naive numeric strings, high-precision byte-level representations, human-semantic calendar tokens, classic uniform binning, and adaptive residual scalar quantization. Evaluated by fine-tuning LLMs on real-world datasets representing diverse statistical distributions.

Result: No single strategy is universally superior. Prediction performance depends heavily on aligning the tokenizer with the data's statistical properties. Log-based strategies excel on skewed distributions, while human-centric formats prove robust for mixed modalities.

Conclusion: Temporal tokenization strategy should be chosen based on the statistical properties of the data rather than using a one-size-fits-all approach. Different encoding methods perform best for different types of temporal distributions.

Abstract: Representing continuous time is a critical and under-explored challenge in modeling temporal event sequences with large language models (LLMs). Various strategies like byte-level representations or calendar tokens have been proposed. However, the optimal approach remains unclear, especially given the diverse statistical distributions of real-world event data, which range from smooth log-normal to discrete, spiky patterns. This paper presents the first empirical study of temporal tokenization for event sequences, comparing distinct encoding strategies: naive numeric strings, high-precision byte-level representations, human-semantic calendar tokens, classic uniform binning, and adaptive residual scalar quantization. We evaluate these strategies by fine-tuning LLMs on real-world datasets that exemplify these diverse distributions. Our analysis reveals that no single strategy is universally superior; instead, prediction performance depends heavily on aligning the tokenizer with the data's statistical properties, with log-based strategies excelling on skewed distributions and human-centric formats proving robust for mixed modalities.

</details>


### [63] [Large-Language Memorization During the Classification of United States Supreme Court Cases](https://arxiv.org/abs/2512.13654)
*John E. Ortega,Dhruv D. Joshi,Matt P. Borkowski*

Main category: cs.CL

TL;DR: LLMs with memory-enhanced prompting outperform BERT-based models on SCOTUS classification tasks by ~2 points, showing robustness despite complex legal text challenges.


<details>
  <summary>Details</summary>
Motivation: To understand LLM memory accuracy and response patterns (including "hallucinations") in challenging classification tasks, using SCOTUS decisions as an ideal testbed due to their complexity, length, legal terminology, and domain-specific vocabulary.

Method: Deep dive analysis of SCOTUS classification tasks using latest LLM fine-tuning and retrieval approaches including parameter-efficient fine-tuning, auto-modeling, and prompt-based models with memories (like DeepSeek) on two category-based tasks (15 and 279 labels).

Result: Prompt-based models with memories (e.g., DeepSeek) are more robust than previous BERT-based models, scoring about 2 points better on both SCOTUS classification tasks.

Conclusion: Memory-enhanced prompting strategies in LLMs show superior performance over traditional BERT-based approaches for complex legal text classification, demonstrating the value of studying LLM memory mechanisms in challenging domain-specific tasks.

Abstract: Large-language models (LLMs) have been shown to respond in a variety of ways for classification tasks outside of question-answering. LLM responses are sometimes called "hallucinations" since the output is not what is ex pected. Memorization strategies in LLMs are being studied in detail, with the goal of understanding how LLMs respond. We perform a deep dive into a classification task based on United States Supreme Court (SCOTUS) decisions. The SCOTUS corpus is an ideal classification task to study for LLM memory accuracy because it presents significant challenges due to extensive sentence length, complex legal terminology, non-standard structure, and domain-specific vocabulary. Experimentation is performed with the latest LLM fine tuning and retrieval-based approaches, such as parameter-efficient fine-tuning, auto-modeling, and others, on two traditional category-based SCOTUS classification tasks: one with 15 labeled topics and another with 279. We show that prompt-based models with memories, such as DeepSeek, can be more robust than previous BERT-based models on both tasks scoring about 2 points better than previous models not based on prompting.

</details>


### [64] [Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation](https://arxiv.org/abs/2512.13655)
*Richard J. Young*

Main category: cs.CL

TL;DR: This study evaluates four abliteration tools (Heretic, DECCP, ErisForge, FailSpy) for removing safety refusal mechanisms from LLMs, finding that single-pass methods best preserve capabilities while Bayesian-optimized approaches cause variable distribution shifts, with mathematical reasoning being most sensitive to these interventions.


<details>
  <summary>Details</summary>
Motivation: Safety alignment in LLMs prevents harmful responses but also blocks legitimate research applications like cognitive modeling and security analysis. Abliteration techniques can remove refusal mechanisms, but their relative effectiveness across different tools and models remains uncharacterized.

Method: Evaluated four abliteration tools across sixteen instruction-tuned models (7B-14B parameters). Reported tool compatibility on all models and quantitative metrics on subsets based on tool support. Compared single-pass methods vs Bayesian-optimized approaches using metrics like GSM8K performance change and KL divergence.

Result: Single-pass methods showed superior capability preservation (avg GSM8K change: ErisForge -0.28 pp; DECCP -0.13 pp). Bayesian-optimized abliteration caused variable distribution shift (KL divergence: 0.043-1.646). Mathematical reasoning capabilities were most sensitive to interventions, with GSM8K changes ranging from +1.51 pp to -18.81 pp (-26.5% relative).

Conclusion: The study provides evidence-based selection criteria for abliteration tools, showing that tool choice significantly impacts capability preservation, especially for mathematical reasoning. Single-pass methods generally preserve capabilities better than Bayesian-optimized approaches, though results vary by model architecture.

Abstract: Safety alignment mechanisms in large language models prevent responses to harmful queries through learned refusal behavior, yet these same mechanisms impede legitimate research applications including cognitive modeling, adversarial testing, and security analysis. While abliteration techniques enable surgical removal of refusal representations through directional orthogonalization, the relative effectiveness of available implementations remains uncharacterized. This study evaluates four abliteration tools (Heretic, DECCP, ErisForge, FailSpy) across sixteen instruction-tuned models (7B-14B parameters), reporting tool compatibility on all 16 models and quantitative metrics on subsets dictated by tool support. Single-pass methods demonstrated superior capability preservation on the benchmarked subset (avg GSM8K change across three models: ErisForge -0.28 pp; DECCP -0.13 pp), while Bayesian-optimized abliteration produced variable distribution shift (KL divergence: 0.043-1.646) with model-dependent capability impact. These findings provide researchers with evidence-based selection criteria for abliteration tool deployment across diverse model architectures. The principal finding indicates that mathematical reasoning capabilities exhibit the highest sensitivity to abliteration interventions, with GSM8K change ranging from +1.51 pp to -18.81 pp (-26.5% relative) depending on tool selection and model architecture.

</details>


### [65] [A stylometric analysis of speaker attribution from speech transcripts](https://arxiv.org/abs/2512.13667)
*Cristina Aggazzotti,Elizabeth Allyn Smith*

Main category: cs.CL

TL;DR: The paper introduces StyloSpeaker, a stylometric method for speaker attribution using transcribed speech, showing better performance on normalized transcripts and comparing it to neural approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional speaker recognition relies on acoustic properties, which fail when voices are disguised or synthesized. Authorship attribution methods work for written text but haven't been systematically applied to transcribed speech for speaker identification.

Method: StyloSpeaker method extracts character, word, token, sentence, and style features from stylometric literature. Evaluated on two transcript formats: prescriptive (with capitalization/punctuation) and normalized (without these conventions). Controlled for conversation topics and compared with neural approaches.

Result: Higher attribution performance on normalized transcripts except under strongest topic control, where overall performance was highest. Identified which stylistic features most effectively distinguish speakers and compared explainable stylometric model to black-box neural approaches.

Conclusion: Stylometric methods can effectively attribute speakers from transcribed speech, with normalization generally improving performance. The approach provides an explainable alternative to neural methods when acoustic features are unreliable due to voice disguise or synthesis.

Abstract: Forensic scientists often need to identify an unknown speaker or writer in cases such as ransom calls, covert recordings, alleged suicide notes, or anonymous online communications, among many others. Speaker recognition in the speech domain usually examines phonetic or acoustic properties of a voice, and these methods can be accurate and robust under certain conditions. However, if a speaker disguises their voice or employs text-to-speech software, vocal properties may no longer be reliable, leaving only their linguistic content available for analysis. Authorship attribution methods traditionally use syntactic, semantic, and related linguistic information to identify writers of written text (authorship attribution). In this paper, we apply a content-based authorship approach to speech that has been transcribed into text, using what a speaker says to attribute speech to individuals (speaker attribution). We introduce a stylometric method, StyloSpeaker, which incorporates character, word, token, sentence, and style features from the stylometric literature on authorship, to assess whether two transcripts were produced by the same speaker. We evaluate this method on two types of transcript formatting: one approximating prescriptive written text with capitalization and punctuation and another normalized style that removes these conventions. The transcripts' conversation topics are also controlled to varying degrees. We find generally higher attribution performance on normalized transcripts, except under the strongest topic control condition, in which overall performance is highest. Finally, we compare this more explainable stylometric model to black-box neural approaches on the same data and investigate which stylistic features most effectively distinguish speakers.

</details>


### [66] [Towards Effective Model Editing for LLM Personalization](https://arxiv.org/abs/2512.13676)
*Baixiang Huang,Limeng Cui,Jiapeng Liu,Haoran Wang,Jiawei Xu,Zhuiyue Tan,Yutong Chen,Chen Luo,Yi Liu,Kai Shu*

Main category: cs.CL

TL;DR: Personalization Editing: A model editing framework for LLM personalization using clustered preference representations, evaluated on new UPQA benchmark for user preference recall.


<details>
  <summary>Details</summary>
Motivation: Current LLM personalization approaches are computationally expensive, data-intensive, prone to catastrophic forgetting, and degrade in multi-turn interactions or with implicit queries. Existing benchmarks use synthetic persona dialogs rather than real user-LLM interactions and focus on style imitation over information-seeking tasks.

Method: Conceptualizes personalization as model editing task. Introduces Personalization Editing framework that applies localized edits guided by clustered preference representations to enable precise preference-aligned updates while preserving overall model capabilities.

Result: Personalization Editing achieves higher editing accuracy and greater computational efficiency than fine-tuning, outperforms prompting-based baselines in multi-turn conversations and implicit preference questions settings.

Conclusion: Personalization Editing provides an effective framework for LLM personalization that addresses limitations of current approaches, with strong performance demonstrated on the new UPQA benchmark for evaluating user preference recall.

Abstract: Personalization is becoming indispensable for LLMs to align with individual user preferences and needs. Yet current approaches are often computationally expensive, data-intensive, susceptible to catastrophic forgetting, and prone to performance degradation in multi-turn interactions or when handling implicit queries. To address these challenges, we conceptualize personalization as a model editing task and introduce Personalization Editing, a framework that applies localized edits guided by clustered preference representations. This design enables precise preference-aligned updates while preserving overall model capabilities. In addition, existing personalization benchmarks frequently rely on persona-based dialogs between LLMs rather than user-LLM interactions, or focus primarily on stylistic imitation while neglecting information-seeking tasks that require accurate recall of user-specific preferences. We introduce User Preference Question Answering (UPQA), a short-answer QA dataset constructed from in-situ user queries with varying levels of difficulty. Unlike prior benchmarks, UPQA directly evaluates a model's ability to recall and apply specific user preferences. Across experimental settings, Personalization Editing achieves higher editing accuracy and greater computational efficiency than fine-tuning, while outperforming prompting-based baselines in multi-turn conversations and implicit preference questions settings.

</details>


### [67] [Beyond surface form: A pipeline for semantic analysis in Alzheimer's Disease detection from spontaneous speech](https://arxiv.org/abs/2512.13685)
*Dylan Phelps,Rodrigo Wilkens,Edward Gow-Smith,Lilian Hubner,Bárbara Malcorra,César Rennó-Costa,Marco Idiart,Maria-Cruz Villa-Uriol,Aline Villavicencio*

Main category: cs.CL

TL;DR: Language models can detect Alzheimer's Disease from semantic content alone, even when surface text features are transformed, showing they capture true cognitive decline markers rather than just surface patterns.


<details>
  <summary>Details</summary>
Motivation: Current language models for AD detection lack interpretability - it's unclear if they identify true linguistic markers of cognitive decline or just surface-level textual patterns. Need to isolate semantic information from surface form variations.

Method: Transform text surface forms by altering syntax and vocabulary while preserving semantic content (low BLEU/chrF scores but high semantic similarity). Also test image-based transformations to see if picture descriptions retain enough detail for image reconstruction.

Result: Models perform similarly with transformed vs original texts (small macro-F1 deviations), showing they rely on semantic information. Image transformations add noise and reduce classification accuracy. AD can be detected from semantic content alone.

Conclusion: Language models capture true semantic impairment in AD, not just surface patterns. This addresses overlooked features of linguistic deterioration and opens new pathways for early detection systems.

Abstract: Alzheimer's Disease (AD) is a progressive neurodegenerative condition that adversely affects cognitive abilities. Language-related changes can be automatically identified through the analysis of outputs from linguistic assessment tasks, such as picture description. Language models show promise as a basis for screening tools for AD, but their limited interpretability poses a challenge in distinguishing true linguistic markers of cognitive decline from surface-level textual patterns. To address this issue, we examine how surface form variation affects classification performance, with the goal of assessing the ability of language models to represent underlying semantic indicators. We introduce a novel approach where texts surface forms are transformed by altering syntax and vocabulary while preserving semantic content. The transformations significantly modify the structure and lexical content, as indicated by low BLEU and chrF scores, yet retain the underlying semantics, as reflected in high semantic similarity scores, isolating the effect of semantic information, and finding models perform similarly to if they were using the original text, with only small deviations in macro-F1. We also investigate whether language from picture descriptions retains enough detail to reconstruct the original image using generative models. We found that image-based transformations add substantial noise reducing classification accuracy. Our methodology provides a novel way of looking at what features influence model predictions, and allows the removal of possible spurious correlations. We find that just using semantic information, language model based classifiers can still detect AD. This work shows that difficult to detect semantic impairment can be identified, addressing an overlooked feature of linguistic deterioration, and opening new pathways for early detection systems.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [68] [Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation](https://arxiv.org/abs/2512.11865)
*Ju-Young Kim,Ji-Hong Park,Myeongjun Kim,Gun-Woo Kim*

Main category: cs.CV

TL;DR: Proposed explainable adversarial-robust Vision-Language-Action model for smart farming that detects photometric perturbations and provides natural language explanations, improving action prediction accuracy under adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Smart farming systems using RGB cameras and robotic manipulators are vulnerable to photometric perturbations (hue, illumination, noise changes) that can cause malfunctions under adversarial attacks, creating a need for robust and explainable solutions.

Method: Developed an explainable adversarial-robust Vision-Language-Action model based on OpenVLA-OFT framework, integrating an Evidence-3 module that detects photometric perturbations and generates natural language explanations of their causes and effects.

Result: The model reduces Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to baseline, demonstrating improved action prediction accuracy and explainability under adversarial conditions.

Conclusion: The proposed model successfully addresses photometric perturbation vulnerabilities in smart farming systems by combining adversarial robustness with explainability, enhancing both performance and transparency in agricultural automation.

Abstract: Smart farming has emerged as a key technology for advancing modern agriculture through automation and intelligent control. However, systems relying on RGB cameras for perception and robotic manipulators for control, common in smart farming, are vulnerable to photometric perturbations such as hue, illumination, and noise changes, which can cause malfunction under adversarial attacks. To address this issue, we propose an explainable adversarial-robust Vision-Language-Action model based on the OpenVLA-OFT framework. The model integrates an Evidence-3 module that detects photometric perturbations and generates natural language explanations of their causes and effects. Experiments show that the proposed model reduces Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to the baseline, demonstrating improved action prediction accuracy and explainability under adversarial conditions.

</details>


### [69] [Temporal-Anchor3DLane: Enhanced 3D Lane Detection with Multi-Task Losses and LSTM Fusion](https://arxiv.org/abs/2512.11869)
*D. Shainu Suhas,G. Rahul,K. Muni*

Main category: cs.CV

TL;DR: Temporal-Anchor3DLane enhances 3D lane detection with improved loss functions, lightweight temporal fusion, and training refinements, achieving +6.2 F1 improvement on OpenLane dataset.


<details>
  <summary>Details</summary>
Motivation: Monocular 3D lane detection faces challenges including depth ambiguity, occlusion, temporal instability, and limitations in existing anchor-based approaches like Anchor3DLane which suffer from sensitivity to regression outliers, weak supervision of global curve geometry, difficulty balancing multiple loss terms, and limited exploitation of temporal continuity.

Method: Three key enhancements: (1) Multi-task loss improvements including Balanced L1 regression, Chamfer point-set distance, uncertainty-based loss weighting, and focal/Dice components for classification/visibility; (2) Lightweight Temporal LSTM Fusion module for cross-frame feature aggregation; (3) ESCOP-style training refinements coupling curve-level supervision with temporal consistency.

Result: On OpenLane dataset, Temporal-Anchor3DLane improves F1 score by +6.2 and produces smoother temporal trajectories, demonstrating significant robustness enhancement without requiring additional sensors or scaling.

Conclusion: Small architectural and loss refinements can significantly enhance 3D lane detection robustness, showing that careful improvements to existing anchor-based approaches yield substantial performance gains in monocular 3D lane detection tasks.

Abstract: Monocular 3D lane detection remains challenging due to depth ambiguity, occlusion, and temporal instability across frames. Anchor-based approaches such as Anchor3DLane have demonstrated strong performance by regressing continuous 3D lane curves from multi-camera surround views. However, the baseline model still exhibits (i) sensitivity to regression outliers, (ii) weak supervision of global curve geometry, (iii) difficulty in balancing multiple loss terms, and (iv) limited exploitation of temporal continuity. We propose Temporal-Anchor3DLane, an enhanced 3D lane detection framework that extends Anchor3DLane with three key contributions: (1) a set of multi-task loss improvements, including Balanced L1 regression, Chamfer point-set distance, and uncertainty-based loss weighting, together with focal and Dice components for classification and visibility; (2) a lightweight Temporal LSTM Fusion module that aggregates per-anchor features across frames, replacing a heavier Transformer-style temporal fusion; and (3) ESCOP-style training refinements that couple curve-level supervision with temporal consistency. On OpenLane, Temporal-Anchor3DLane improves F1 by +6.2 and yields smoother temporal trajectories, showing that small architectural and loss refinements significantly enhance 3D lane robustness without extra sensors or scaling.

</details>


### [70] [Automated Plant Disease and Pest Detection System Using Hybrid Lightweight CNN-MobileViT Models for Diagnosis of Indigenous Crops](https://arxiv.org/abs/2512.11871)
*Tekleab G. Gebremedhin,Hailom S. Asegede,Bruh W. Tesheme,Tadesse B. Gebremichael,Kalayu G. Redae*

Main category: cs.CV

TL;DR: Researchers developed an offline crop disease detection system for Ethiopia's Tigray region, focusing on cactus-fig with three mobile-efficient models, achieving up to 97.3% accuracy while optimizing for edge deployment constraints.


<details>
  <summary>Details</summary>
Motivation: To address limited expert access for crop disease diagnosis in Ethiopia's Tigray region (where 80% depend on agriculture) due to infrastructural disruptions, requiring offline-capable solutions for post-conflict edge environments.

Method: Created a new indigenous cactus-fig dataset (3,587 field images across 3 symptom classes), benchmarked three mobile-efficient architectures: custom lightweight CNN, EfficientNet-Lite1, and MobileViT-XS (CNN-Transformer hybrid), deployed in localized Flutter app for offline inference on ARM devices.

Result: EfficientNet-Lite1 achieved 90.7% test accuracy, lightweight CNN reached 89.5% with best deployment profile (42 ms latency, 4.8 MB size), MobileViT-XS delivered 97.3% mean cross-validation accuracy, showing transformer attention better disambiguates pest clusters from fungal lesions than CNN kernels.

Conclusion: MobileViT-XS demonstrates superior accuracy for indigenous crop disease detection through global reasoning, while lightweight CNN offers best deployment efficiency - providing practical offline diagnostic tools for food security in resource-constrained regions.

Abstract: Agriculture supports over 80% of the population in the Tigray region of Ethiopia, where infrastructural disruptions limit access to expert crop disease diagnosis. We present an offline-first detection system centered on a newly curated indigenous cactus-fig (Opuntia ficus-indica) dataset consisting of 3,587 field images across three core symptom classes. Given deployment constraints in post-conflict edge environments, we benchmark three mobile-efficient architectures: a custom lightweight CNN, EfficientNet-Lite1, and the CNN-Transformer hybrid MobileViT-XS. While the broader system contains independent modules for potato, apple, and corn, this study isolates cactus-fig model performance to evaluate attention sensitivity and inductive bias transfer on indigenous morphology alone. Results establish a clear Pareto trade-off: EfficientNet-Lite1 achieves 90.7% test accuracy, the lightweight CNN reaches 89.5% with the most favorable deployment profile (42 ms inference latency, 4.8 MB model size), and MobileViT-XS delivers 97.3% mean cross-validation accuracy, demonstrating that MHSA-based global reasoning disambiguates pest clusters from two dimensional fungal lesions more reliably than local texture CNN kernels. The ARM compatible models are deployed in a Tigrigna and Amharic localized Flutter application supporting fully offline inference on Cortex-A53 class devices, strengthening inclusivity for food security critical diagnostics.

</details>


### [71] [Pseudo-Label Refinement for Robust Wheat Head Segmentation via Two-Stage Hybrid Training](https://arxiv.org/abs/2512.11874)
*Jiahao Jiang,Zhangrui Yang,Xuanhan Wang,Jingkuan Song*

Main category: cs.CV

TL;DR: SegFormer-based self-training framework with teacher-student loop achieves competitive wheat segmentation performance


<details>
  <summary>Details</summary>
Motivation: To develop an effective solution for the Global Wheat Full Semantic Segmentation Competition, addressing the challenge of accurate wheat segmentation in agricultural imagery

Method: Systematic self-training framework combining two-stage hybrid training strategy with extensive data augmentation, using SegFormer with MiT-B4 backbone and iterative teacher-student loop for progressive refinement

Result: Achieved competitive performance on both Development and Testing Phase datasets of the competition

Conclusion: The proposed self-training framework with teacher-student loop effectively improves segmentation accuracy and maximizes data utilization for wheat segmentation tasks

Abstract: This extended abstract details our solution for the Global Wheat Full Semantic Segmentation Competition. We developed a systematic self-training framework. This framework combines a two-stage hybrid training strategy with extensive data augmentation. Our core model is SegFormer with a Mix Transformer (MiT-B4) backbone. We employ an iterative teacher-student loop. This loop progressively refines model accuracy. It also maximizes data utilization. Our method achieved competitive performance. This was evident on both the Development and Testing Phase datasets.

</details>


### [72] [Generalization vs. Specialization: Evaluating Segment Anything Model (SAM3) Zero-Shot Segmentation Against Fine-Tuned YOLO Detectors](https://arxiv.org/abs/2512.11884)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee,Nikolaos D. Tselikas*

Main category: cs.CV

TL;DR: SAM3 (Segment Anything Model) in zero-shot mode vs. fine-tuned YOLO11 variants for apple instance segmentation on MinneApple dataset, showing YOLO excels at detection completeness but SAM3 has superior boundary stability and mask precision.


<details>
  <summary>Details</summary>
Motivation: To comprehensively compare specialized fine-tuned models (YOLO11 variants) versus generalist foundation models (SAM3) for dense instance segmentation tasks, particularly understanding their relative strengths in detection completeness versus mask precision.

Method: Evaluated SAM3 in pure zero-shot mode against three fine-tuned YOLO11 variants (nano, medium, large) on MinneApple dataset (670 orchard images, 28,179 apple instances). Used rigorous validation under high object density and occlusion, with analysis of IoU threshold impacts on performance metrics.

Result: At IoU=0.15, YOLO models achieved 68.9%, 72.2%, and 71.9% F1 scores while SAM3 reached 59.8%. However, YOLO showed steep degradation (48-50 points) across IoU ranges while SAM3 dropped only 4 points, revealing SAM3 has 12 times superior boundary stability. IoU choices can inflate performance gaps by up to 30%.

Conclusion: SAM3 excels in mask precision and boundary stability while YOLO11 specializes in detection completeness. The study provides methodological recommendations for choosing between specialized fine-tuned models or generalist foundation models for dense instance segmentation tasks.

Abstract: Deep learning has advanced two fundamentally different paradigms for instance segmentation: specialized models optimized through task-specific fine-tuning and generalist foundation models capable of zero-shot segmentation. This work presents a comprehensive comparison between SAM3 (Segment Anything Model, also called SAMv3) operating in zero-shot mode and three variants of Ultralytics YOLO11 (nano, medium, and large) fine-tuned for instance segmentation. The evaluation is conducted on the MinneApple dataset, a dense benchmark comprising 670 orchard images with 28,179 annotated apple instances, enabling rigorous validation of model behavior under high object density and occlusion. Our analysis shows IoU choices can inflate performance gaps by up to 30%. At the appropriate IoU = 0.15 threshold, YOLO models achieve 68.9%, 72.2%, and 71.9% F1, while SAM3 reaches 59.8% in pure zero-shot mode. However, YOLO exhibits steep degradation 48-50 points across IoU ranges whereas SAM3 drops only 4 points, revealing 12 times superior boundary stability of SAM3. This highlights the strength of SAMv3 in mask precision versus specialization in detection completeness of YOLO11. We provide open-source code, evaluation pipelines, and methodological recommendations, contributing to a deeper understanding of when specialized fine-tuned models or generalist foundation models are preferable for dense instance segmentation tasks. This project repository is available on GitHub as https://github.com/Applied-AI-Research-Lab/Segment-Anything-Model-SAM3-Zero-Shot-Segmentation-Against-Fine-Tuned-YOLO-Detectors

</details>


### [73] [mmWEAVER: Environment-Specific mmWave Signal Synthesis from a Photo and Activity Description](https://arxiv.org/abs/2512.11894)
*Mahathir Monjur,Shahriar Nirjon*

Main category: cs.CV

TL;DR: mmWeaver is a framework that uses Implicit Neural Representations and hypernetworks to synthesize realistic mmWave radar signals from environmental context and human motion features, achieving high compression and faster generation than simulation.


<details>
  <summary>Details</summary>
Motivation: Realistic mmWave signal generation is crucial for radar applications like activity recognition and pose estimation, but physical simulation is computationally expensive due to the complex, sparse, and high-dimensional nature of mmWave signals.

Method: Uses Implicit Neural Representations (INRs) to model mmWave signals as continuous functions, achieving up to 49-fold compression. Incorporates hypernetworks that generate INR parameters based on environmental context (from RGB-D images) and human motion features (from MotionGPT text-to-pose generation).

Result: Achieves complex SSIM of 0.88 and PSNR of 35 dB, outperforming existing methods in signal realism. Improves activity recognition accuracy by up to 7% and reduces human pose estimation error by up to 15%. Operates 6-35 times faster than simulation-based approaches.

Conclusion: mmWeaver enables efficient, adaptive synthesis of realistic mmWave signals by leveraging semantic and geometric priors, providing a practical solution for dataset augmentation in radar applications while preserving critical phase information for downstream tasks.

Abstract: Realistic signal generation and dataset augmentation are essential for advancing mmWave radar applications such as activity recognition and pose estimation, which rely heavily on diverse, and environment-specific signal datasets. However, mmWave signals are inherently complex, sparse, and high-dimensional, making physical simulation computationally expensive. This paper presents mmWeaver, a novel framework that synthesizes realistic, environment-specific complex mmWave signals by modeling them as continuous functions using Implicit Neural Representations (INRs), achieving up to 49-fold compression. mmWeaver incorporates hypernetworks that dynamically generate INR parameters based on environmental context (extracted from RGB-D images) and human motion features (derived from text-to-pose generation via MotionGPT), enabling efficient and adaptive signal synthesis. By conditioning on these semantic and geometric priors, mmWeaver generates diverse I/Q signals at multiple resolutions, preserving phase information critical for downstream tasks such as point cloud estimation and activity classification. Extensive experiments show that mmWeaver achieves a complex SSIM of 0.88 and a PSNR of 35 dB, outperforming existing methods in signal realism while improving activity recognition accuracy by up to 7% and reducing human pose estimation error by up to 15%, all while operating 6-35 times faster than simulation-based approaches.

</details>


### [74] [Hot Hém: Sài Gòn Giũa Cái Nóng Hông Còng Bàng -- Saigon in Unequal Heat](https://arxiv.org/abs/2512.11896)
*Tessa Vu*

Main category: cs.CV

TL;DR: Hot Hém is a GeoAI workflow that estimates pedestrian heat exposure in Ho Chi Minh City using Google Street View imagery, semantic segmentation, and remote sensing to enable heat-aware routing.


<details>
  <summary>Details</summary>
Motivation: Standard routing algorithms ignore micro-scale thermal variation, which is a critical health risk for pedestrians in dense tropical cities like Ho Chi Minh City.

Method: Combines Google Street View imagery, semantic image segmentation, and remote sensing. Trains two XGBoost models to predict land surface temperature using GSV training data from selected wards, then deploys models across all OSMnx-derived pedestrian network nodes in a patchwork manner.

Result: Creates a spatial data science pipeline that can estimate pedestrian heat exposure across the city, enabling heat-aware routing and identifying disproportionately hot city corridors.

Conclusion: Hot Hém provides a foundation for pinpointing where and understanding why certain urban corridors experience higher temperatures at an infrastructural scale, addressing a critical health risk in tropical cities.

Abstract: Pedestrian heat exposure is a critical health risk in dense tropical cities, yet standard routing algorithms often ignore micro-scale thermal variation. Hot Hém is a GeoAI workflow that estimates and operationalizes pedestrian heat exposure in Hô Chí Minh City (HCMC), Vi\d{e}t Nam, colloquially known as Sài Gòn. This spatial data science pipeline combines Google Street View (GSV) imagery, semantic image segmentation, and remote sensing. Two XGBoost models are trained to predict land surface temperature (LST) using a GSV training dataset in selected administrative wards, known as phŏng, and are deployed in a patchwork manner across all OSMnx-derived pedestrian network nodes to enable heat-aware routing. This is a model that, when deployed, can provide a foundation for pinpointing where and further understanding why certain city corridors may experience disproportionately higher temperatures at an infrastructural scale.

</details>


### [75] [Microscopic Vehicle Trajectory Datasets from UAV-collected Video for Heterogeneous, Area-Based Urban Traffic](https://arxiv.org/abs/2512.11898)
*Yawar Ali,K. Ramachandra Rao,Ashish Bhaskar,Niladri Chatterjee*

Main category: cs.CV

TL;DR: This paper provides open-access microscopic vehicle trajectory datasets collected via UAVs in heterogeneous urban traffic, addressing limitations of traditional roadside video collection.


<details>
  <summary>Details</summary>
Motivation: Traditional roadside video collection struggles with dense mixed traffic due to occlusion, limited viewing angles, and irregular vehicle movements. There's a need for better data collection methods to capture complex urban traffic dynamics.

Method: Used unmanned aerial vehicles (UAVs) with top-down perspective to collect traffic data via the Data from Sky (DFS) platform. Collected at six mid-block locations in India's national capital region at 30 fps, capturing vehicle positions, speeds, accelerations, and classifications.

Result: Created validated datasets showing key behavioral patterns including lane-keeping preferences, speed distributions, and lateral maneuvers. Data validated against manual counts, space mean speeds, and probe trajectories from earlier work.

Conclusion: These openly available datasets provide a valuable resource for global researchers to develop, test, and validate traffic simulation models, safety assessments, and behavioral studies in complex urban environments.

Abstract: This paper offers openly available microscopic vehicle trajectory (MVT) datasets collected using unmanned aerial vehicles (UAVs) in heterogeneous, area-based urban traffic conditions. Traditional roadside video collection often fails in dense mixed traffic due to occlusion, limited viewing angles, and irregular vehicle movements. UAV-based recording provides a top-down perspective that reduces these issues and captures rich spatial and temporal dynamics. The datasets described here were extracted using the Data from Sky (DFS) platform and validated against manual counts, space mean speeds, and probe trajectories in earlier work. Each dataset contains time-stamped vehicle positions, speeds, longitudinal and lateral accelerations, and vehicle classifications at a resolution of 30 frames per second. Data were collected at six mid-block locations in the national capital region of India, covering diverse traffic compositions and density levels. Exploratory analyses highlight key behavioural patterns, including lane-keeping preferences, speed distributions, and lateral manoeuvres typical of heterogeneous and area-based traffic settings. These datasets are intended as a resource for the global research community to support simulation modelling, safety assessment, and behavioural studies under area-based traffic conditions. By making these empirical datasets openly available, this work offers researchers a unique opportunity to develop, test, and validate models that more accurately represent complex urban traffic environments.

</details>


### [76] [Read or Ignore? A Unified Benchmark for Typographic-Attack Robustness and Text Recognition in Vision-Language Models](https://arxiv.org/abs/2512.11899)
*Futa Waseda,Shojiro Yamabe,Daiki Shiono,Kento Sasaki,Tsubasa Takahashi*

Main category: cs.CV

TL;DR: LVLMs are vulnerable to typographic attacks, but existing defenses encourage ignoring all text. RIO-VQA introduces a task requiring selective text use, with RIO-Bench providing evaluation via same-scene counterfactuals. Current models fail at balancing robustness and text-reading, but RIO-Bench enables adaptive defenses.


<details>
  <summary>Details</summary>
Motivation: Current LVLM defenses against typographic attacks focus on object recognition and encourage ignoring all text, which is problematic for real-world scenarios requiring joint reasoning over both objects and text (e.g., reading traffic signs while recognizing pedestrians). There's a misalignment between existing evaluation scope and practical requirements.

Method: Introduces Read-or-Ignore VQA (RIO-VQA) task that formalizes selective text use in VQA. Creates Read-or-Ignore Benchmark (RIO-Bench) with standardized dataset and protocol providing same-scene counterfactuals (read/ignore scenarios) by varying only textual content and question type. Enables data-driven defense learning adaptive selective text use.

Result: Strong LVLMs and existing defenses fail to balance typographic robustness and text-reading capability. RIO-Bench reveals current approaches' limitations and enables novel adaptive defenses that move beyond prior non-adaptive, text-ignoring methods.

Conclusion: The work identifies fundamental misalignment between current evaluation scope and real-world LVLM requirements. RIO-VQA and RIO-Bench provide principled path toward reliable LVLMs that can selectively use text based on context, addressing both robustness and functional needs.

Abstract: Large vision-language models (LVLMs) are vulnerable to typographic attacks, where misleading text within an image overrides visual understanding. Existing evaluation protocols and defenses, largely focused on object recognition, implicitly encourage ignoring text to achieve robustness; however, real-world scenarios often require joint reasoning over both objects and text (e.g., recognizing pedestrians while reading traffic signs). To address this, we introduce a novel task, Read-or-Ignore VQA (RIO-VQA), which formalizes selective text use in visual question answering (VQA): models must decide, from context, when to read text and when to ignore it. For evaluation, we present the Read-or-Ignore Benchmark (RIO-Bench), a standardized dataset and protocol that, for each real image, provides same-scene counterfactuals (read / ignore) by varying only the textual content and question type. Using RIO-Bench, we show that strong LVLMs and existing defenses fail to balance typographic robustness and text-reading capability, highlighting the need for improved approaches. Finally, RIO-Bench enables a novel data-driven defense that learns adaptive selective text use, moving beyond prior non-adaptive, text-ignoring defenses. Overall, this work reveals a fundamental misalignment between the existing evaluation scope and real-world requirements, providing a principled path toward reliable LVLMs. Our Project Page is at https://turingmotors.github.io/rio-vqa/.

</details>


### [77] [CLARGA: Multimodal Graph Representation Learning over Arbitrary Sets of Modalities](https://arxiv.org/abs/2512.11901)
*Santosh Patapati*

Main category: cs.CV

TL;DR: CLARGA is a flexible multimodal fusion architecture that works with any number/type of modalities, using attention-weighted graphs and GATs for efficient sub-quadratic fusion, with robustness to missing inputs via learnable masks.


<details>
  <summary>Details</summary>
Motivation: Need for a general-purpose multimodal fusion framework that can handle any number and type of modalities without architectural changes, while being adaptive to different samples and robust to missing inputs.

Method: Builds attention-weighted graphs over modality features, uses multi-head Graph Attention Networks for message passing, includes learnable masks for missing modalities, and trains with hybrid supervised+contrastive InfoNCE loss.

Result: Outperforms baselines and SOTA models across 7 diverse datasets (finance, HCI, multimedia classification, affective computing), shows robustness to missing inputs, and excels on niche tasks.

Conclusion: CLARGA provides an effective, efficient, and easily pluggable solution for multimodal representation learning across diverse tasks with adaptive fusion and missing modality robustness.

Abstract: We introduce CLARGA, a general-purpose multimodal fusion architecture for multimodal representation learning that works with any number and type of modalities without changing the underlying framework. Given a supervised dataset, CLARGA can be applied to virtually any machine learning task to fuse different multimodal representations for processing by downstream layers. On a sample-by-sample basis, CLARGA learns how modalities should inform one another by building an attention weighted graph over their features and passing messages along this graph with a multi-head Graph Attention Network. Not only does this make CLARGA highly adaptive, as it constructs unique graphs for different samples, it makes for efficient fusion with sub-quadratic complexity as the number of modalities grows. Through a learnable mask, it can also adapt to missing modality inputs. The model is trained with a hybrid objective that combines a supervised task loss with contrastive InfoNCE loss, improving cross-modal consistency and robustness to noisy inputs. We demonstrate CLARGA's effectiveness in diverse multimodal representation learning tasks across 7 datasets spanning finance, human-computer interaction, general multimedia classification, and affective computing. It consistently outperforms baselines, state-of-the-art models, and ablations. Additional experiments also demonstrate its robustness to missing inputs and ability to excel on niche tasks. Overall, CLARGA can be easily plugged into machine learning models for effective and efficient learning of representations across a wide variety of tasks.

</details>


### [78] [Smartphone monitoring of smiling as a behavioral proxy of well-being in everyday life](https://arxiv.org/abs/2512.11905)
*Ming-Zher Poh,Shun Liao,Marco Andreetto,Daniel McDuff,Jonathan Wang,Paolo Di Achille,Jiang Wu,Yun Liu,Lawrence Cai,Eric Teasley,Mark Malhotra,Anupam Pathak,Shwetak Patel*

Main category: cs.CV

TL;DR: Passive smartphone video analysis of candid smiles correlates strongly with traditional happiness measures, offering scalable objective assessment of well-being.


<details>
  <summary>Details</summary>
Motivation: Traditional self-report methods for measuring subjective well-being suffer from recall bias and high participant burden, creating a gap in understanding everyday well-being dynamics.

Method: Analyzed 405,448 video clips from 233 participants over one week using deep learning to quantify smile intensity, examining diurnal/daily patterns and correlations with physical activity, light exposure, and smartphone use.

Result: Smile intensity patterns strongly correlated with national happiness surveys (r=0.92) and day reconstruction method results (r=0.80). Higher smile intensity associated with more physical activity and greater light exposure, but not smartphone use.

Conclusion: Passive smartphone sensing of candid smiles provides ecologically valid, scalable methodology for studying affective behavior dynamics at population scale.

Abstract: Subjective well-being is a cornerstone of individual and societal health, yet its scientific measurement has traditionally relied on self-report methods prone to recall bias and high participant burden. This has left a gap in our understanding of well-being as it is expressed in everyday life. We hypothesized that candid smiles captured during natural smartphone interactions could serve as a scalable, objective behavioral correlate of positive affect. To test this, we analyzed 405,448 video clips passively recorded from 233 consented participants over one week. Using a deep learning model to quantify smile intensity, we identified distinct diurnal and daily patterns. Daily patterns of smile intensity across the week showed strong correlation with national survey data on happiness (r=0.92), and diurnal rhythms documented close correspondence with established results from the day reconstruction method (r=0.80). Higher daily mean smile intensity was significantly associated with more physical activity (Beta coefficient = 0.043, 95% CI [0.001, 0.085]) and greater light exposure (Beta coefficient = 0.038, [0.013, 0.063]), whereas no significant effects were found for smartphone use. These findings suggest that passive smartphone sensing could serve as a powerful, ecologically valid methodology for studying the dynamics of affective behavior and open the door to understanding this behavior at a population scale.

</details>


### [79] [MPath: Multimodal Pathology Report Generation from Whole Slide Images](https://arxiv.org/abs/2512.11906)
*Noorul Wahab,Nasir Rajpoot*

Main category: cs.CV

TL;DR: MPath is a lightweight multimodal framework that generates diagnostic pathology reports from whole slide images by conditioning a biomedical language model on visual embeddings through visual-prefix prompting, achieving 4th place in the RED 2025 Grand Challenge.


<details>
  <summary>Details</summary>
Motivation: Automated generation of diagnostic pathology reports from whole slide images is challenging due to large morphological variability and complex narrative structures in pathology reports. Current methods struggle to translate high-resolution tissue patterns into clinically coherent text.

Method: MPath uses a pretrained biomedical language model (BioBART) conditioned on WSI-derived visual embeddings through learned visual-prefix prompting. Instead of end-to-end vision-language pretraining, it leverages foundation-model WSI features (CONCH + Titan) and injects them into BioBART via a compact projection module while keeping the language backbone frozen for stability and data efficiency.

Result: MPath was developed and evaluated on the RED 2025 Grand Challenge dataset and ranked 4th in Test Phase 2, despite limited submission opportunities. The framework demonstrates effective pathology report generation capabilities.

Conclusion: The results highlight the potential of prompt-based multimodal conditioning as a scalable and interpretable strategy for pathology report generation, offering a lightweight alternative to end-to-end vision-language pretraining approaches.

Abstract: Automated generation of diagnostic pathology reports directly from whole slide images (WSIs) is an emerging direction in computational pathology. Translating high-resolution tissue patterns into clinically coherent text remains difficult due to large morphological variability and the complex structure of pathology narratives. We introduce MPath, a lightweight multimodal framework that conditions a pretrained biomedical language model (BioBART) on WSI-derived visual embeddings through a learned visual-prefix prompting mechanism. Instead of end-to-end vision-language pretraining, MPath leverages foundation-model WSI features (CONCH + Titan) and injects them into BioBART via a compact projection module, keeping the language backbone frozen for stability and data efficiency. MPath was developed and evaluated on the RED 2025 Grand Challenge dataset and ranked 4th in Test Phase 2, despite limited submission opportunities. The results highlight the potential of prompt-based multimodal conditioning as a scalable and interpretable strategy for pathology report generation.

</details>


### [80] [FloraForge: LLM-Assisted Procedural Generation of Editable and Analysis-Ready 3D Plant Geometric Models For Agricultural Applications](https://arxiv.org/abs/2512.11925)
*Mozhgan Hadadi,Talukder Z. Jubery,Patrick S. Schnable,Arti Singh,Bedrich Benes,Adarsh Krishnamurthy,Baskar Ganapathysubramanian*

Main category: cs.CV

TL;DR: FloraForge is an LLM-assisted framework that enables domain experts to generate biologically accurate, parametric 3D plant models through natural language interactions, eliminating the need for extensive programming expertise.


<details>
  <summary>Details</summary>
Motivation: Current 3D plant modeling approaches have significant limitations: learning-based methods require extensive training data and lack editability, while procedural modeling demands specialized geometric modeling expertise and understanding of complex rules, making it inaccessible to domain scientists.

Method: FloraForge uses LLM-enabled co-design to refine Python scripts that generate parameterized plant geometries as hierarchical B-spline surface representations with botanical constraints. It employs natural language Plant Refinements (PR) and Plant Descriptor (PD) files for human-readable control, producing both visualization meshes and parametric metadata for analysis.

Result: The framework successfully generates accurate 3D plant models for maize, soybean, and mung bean by fitting procedural models to empirical point cloud data through manual refinement of Plant Descriptor files, producing dual outputs for visualization and quantitative analysis.

Conclusion: FloraForge democratizes sophisticated geometric modeling for plant science by combining LLM-assisted template creation, mathematically continuous representations for both phenotyping and rendering, and direct parametric control, while maintaining mathematical rigor and compatibility with functional structural plant analysis workflows.

Abstract: Accurate 3D plant models are crucial for computational phenotyping and physics-based simulation; however, current approaches face significant limitations. Learning-based reconstruction methods require extensive species-specific training data and lack editability. Procedural modeling offers parametric control but demands specialized expertise in geometric modeling and an in-depth understanding of complex procedural rules, making it inaccessible to domain scientists. We present FloraForge, an LLM-assisted framework that enables domain experts to generate biologically accurate, fully parametric 3D plant models through iterative natural language Plant Refinements (PR), minimizing programming expertise. Our framework leverages LLM-enabled co-design to refine Python scripts that generate parameterized plant geometries as hierarchical B-spline surface representations with botanical constraints with explicit control points and parametric deformation functions. This representation can be easily tessellated into polygonal meshes with arbitrary precision, ensuring compatibility with functional structural plant analysis workflows such as light simulation, computational fluid dynamics, and finite element analysis. We demonstrate the framework on maize, soybean, and mung bean, fitting procedural models to empirical point cloud data through manual refinement of the Plant Descriptor (PD), human-readable files. The pipeline generates dual outputs: triangular meshes for visualization and triangular meshes with additional parametric metadata for quantitative analysis. This approach uniquely combines LLM-assisted template creation, mathematically continuous representations enabling both phenotyping and rendering, and direct parametric control through PD. The framework democratizes sophisticated geometric modeling for plant science while maintaining mathematical rigor.

</details>


### [81] [TransBridge: Boost 3D Object Detection by Scene-Level Completion with Transformer Decoder](https://arxiv.org/abs/2512.11926)
*Qinghao Meng,Chenming Wu,Liangjun Zhang,Jianbing Shen*

Main category: cs.CV

TL;DR: TransBridge: A joint completion and detection framework for 3D object detection that addresses sparse LiDAR points in distant regions using transformer-based up-sampling and dynamic-static reconstruction.


<details>
  <summary>Details</summary>
Motivation: 3D object detection in autonomous driving faces challenges with sparse LiDAR points in distant regions, where existing methods struggle to detect objects effectively due to point cloud sparsity.

Method: Proposes TransBridge, a transformer-based up-sampling block that fuses detection and completion network features. Includes DSRecon module for dense point cloud generation, and uses transformer mechanisms for channel-spatial feature connections to create high-resolution feature maps.

Result: Improves end-to-end 3D object detection with mAP gains of 0.7-1.5 across multiple methods on nuScenes and Waymo datasets. For two-stage detection frameworks, boosts mAP up to 5.78 points.

Conclusion: The joint completion-detection framework effectively addresses sparse point cloud challenges in 3D object detection, demonstrating strong generalization ability and significant performance improvements across different detection methods and datasets.

Abstract: 3D object detection is essential in autonomous driving, providing vital information about moving objects and obstacles. Detecting objects in distant regions with only a few LiDAR points is still a challenge, and numerous strategies have been developed to address point cloud sparsity through densification.This paper presents a joint completion and detection framework that improves the detection feature in sparse areas while maintaining costs unchanged. Specifically, we propose TransBridge, a novel transformer-based up-sampling block that fuses the features from the detection and completion networks.The detection network can benefit from acquiring implicit completion features derived from the completion network. Additionally, we design the Dynamic-Static Reconstruction (DSRecon) module to produce dense LiDAR data for the completion network, meeting the requirement for dense point cloud ground truth.Furthermore, we employ the transformer mechanism to establish connections between channels and spatial relations, resulting in a high-resolution feature map used for completion purposes.Extensive experiments on the nuScenes and Waymo datasets demonstrate the effectiveness of the proposed framework.The results show that our framework consistently improves end-to-end 3D object detection, with the mean average precision (mAP) ranging from 0.7 to 1.5 across multiple methods, indicating its generalization ability. For the two-stage detection framework, it also boosts the mAP up to 5.78 points.

</details>


### [82] [MONET -- Virtual Cell Painting of Brightfield Images and Time Lapses Using Reference Consistent Diffusion](https://arxiv.org/abs/2512.11928)
*Alexander Peysakhovich,William Berman,Joseph Rufo,Felix Wong,Maxwell Z. Wilson*

Main category: cs.CV

TL;DR: MONET is a diffusion model that predicts cell paint images from brightfield microscopy, enabling virtual cell painting without chemical fixation and allowing time-lapse studies.


<details>
  <summary>Details</summary>
Motivation: Traditional cell painting is labor-intensive and requires chemical fixation, which prevents studying cell dynamics over time. There's a need for a non-invasive, scalable alternative.

Method: Train a diffusion model (MONET) on a large dataset to predict cell paint channels from brightfield images. Use consistency architecture for time-lapse video generation and enable in-context learning for out-of-distribution transfer.

Result: Model quality improves with scale. The architecture successfully generates time-lapse videos despite lacking cell paint video training data, and enables partial transfer to different cell lines and imaging protocols.

Conclusion: Virtual cell painting serves as a complementary tool to physical cell painting, enabling novel biological research workflows by overcoming limitations of traditional methods.

Abstract: Cell painting is a popular technique for creating human-interpretable, high-contrast images of cell morphology. There are two major issues with cell paint: (1) it is labor-intensive and (2) it requires chemical fixation, making the study of cell dynamics impossible. We train a diffusion model (Morphological Observation Neural Enhancement Tool, or MONET) on a large dataset to predict cell paint channels from brightfield images. We show that model quality improves with scale. The model uses a consistency architecture to generate time-lapse videos, despite the impossibility of obtaining cell paint video training data. In addition, we show that this architecture enables a form of in-context learning, allowing the model to partially transfer to out-of-distribution cell lines and imaging protocols. Virtual cell painting is not intended to replace physical cell painting completely, but to act as a complementary tool enabling novel workflows in biological research.

</details>


### [83] [Contextual Peano Scan and Fast Image Segmentation Using Hidden and Evidential Markov Chains](https://arxiv.org/abs/2512.11939)
*Clément Fernandes,Wojciech Pieczynski*

Main category: cs.CV

TL;DR: This paper introduces HEMC-CPS, a new model combining contextual Peano scans with hidden evidential Markov chains for unsupervised Bayesian image segmentation, showing improved performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve image segmentation by combining two recent advances: contextual Peano scans (which transform 2D images to 1D sequences while preserving spatial context) and hidden evidential Markov chains (which enhance HMC-based segmentation). The goal is to create a more powerful model for complex image segmentation tasks.

Method: The method introduces HEMC-CPS (Hidden Evidential Markov Chain with Contextual Peano Scan), which transforms 2D images into 1D sequences using contextual PS, then applies evidential HMC for segmentation. Parameters are estimated unsupervised using the stochastic EM (SEM) method, and segmentation is performed via Bayesian maximum posterior mode (MPM) estimation.

Result: The model demonstrates effectiveness for Bayesian MPM segmentation on both synthetic and real images. It shows potential for modeling more complex images like 3D or multi-sensor multi-resolution images, and is not limited to image segmentation but applicable to any spatially correlated data.

Conclusion: HEMC-CPS successfully combines contextual PS with evidential HMC to create a powerful unsupervised segmentation model that outperforms previous HMC-based methods and competes with HMF-based approaches while being computationally faster. The model has broad applicability beyond traditional image segmentation.

Abstract: Transforming bi-dimensional sets of image pixels into mono-dimensional sequences with a Peano scan (PS) is an established technique enabling the use of hidden Markov chains (HMCs) for unsupervised image segmentation. Related Bayesian segmentation methods can compete with hidden Markov fields (HMFs)-based ones and are much faster. PS has recently been extended to the contextual PS, and some initial experiments have shown the value of the associated HMC model, denoted as HMC-CPS, in image segmentation. Moreover, HMCs have been extended to hidden evidential Markov chains (HEMCs), which are capable of improving HMC-based Bayesian segmentation. In this study, we introduce a new HEMC-CPS model by simultaneously considering contextual PS and evidential HMC. We show its effectiveness for Bayesian maximum posterior mode (MPM) segmentation using synthetic and real images. Segmentation is performed in an unsupervised manner, with parameters being estimated using the stochastic expectation--maximization (SEM) method. The new HEMC-CPS model presents potential for the modeling and segmentation of more complex images, such as three-dimensional or multi-sensor multi-resolution images. Finally, the HMC-CPS and HEMC-CPS models are not limited to image segmentation and could be used for any kind of spatially correlated data.

</details>


### [84] [DynaPURLS: Dynamic Refinement of Part-aware Representations for Skeleton-based Zero-Shot Action Recognition](https://arxiv.org/abs/2512.11941)
*Jingmin Zhu,Anqi Zhu,James Bailey,Jun Liu,Hossein Rahmani,Mohammed Bennamoun,Farid Boussaid,Qiuhong Ke*

Main category: cs.CV

TL;DR: DynaPURLS is a novel framework for zero-shot skeleton-based action recognition that establishes multi-scale visual-semantic correspondences and dynamically refines them at inference time to bridge the domain gap between seen and unseen classes.


<details>
  <summary>Details</summary>
Motivation: Current zero-shot skeleton-based action recognition methods rely on coarse-grained alignment between skeleton features and static class-level semantics, which fails to bridge the domain shift between seen and unseen classes and impedes transfer of fine-grained visual knowledge.

Method: The framework uses a large language model to generate hierarchical textual descriptions (global movements + local body-part dynamics), an adaptive partitioning module for fine-grained visual representations by grouping skeleton joints, and a dynamic refinement module that adapts textual features to incoming visual streams during inference using a lightweight learnable projection stabilized by a confidence-aware, class-balanced memory bank.

Result: Extensive experiments on NTU RGB+D 60/120 and PKU-MMD datasets show DynaPURLS significantly outperforms prior methods and sets new state-of-the-art records.

Conclusion: DynaPURLS successfully addresses limitations of coarse-grained alignment in zero-shot skeleton-based action recognition by establishing robust multi-scale visual-semantic correspondences with dynamic refinement, enabling effective transfer of fine-grained knowledge across domains.

Abstract: Zero-shot skeleton-based action recognition (ZS-SAR) is fundamentally constrained by prevailing approaches that rely on aligning skeleton features with static, class-level semantics. This coarse-grained alignment fails to bridge the domain shift between seen and unseen classes, thereby impeding the effective transfer of fine-grained visual knowledge. To address these limitations, we introduce \textbf{DynaPURLS}, a unified framework that establishes robust, multi-scale visual-semantic correspondences and dynamically refines them at inference time to enhance generalization. Our framework leverages a large language model to generate hierarchical textual descriptions that encompass both global movements and local body-part dynamics. Concurrently, an adaptive partitioning module produces fine-grained visual representations by semantically grouping skeleton joints. To fortify this fine-grained alignment against the train-test domain shift, DynaPURLS incorporates a dynamic refinement module. During inference, this module adapts textual features to the incoming visual stream via a lightweight learnable projection. This refinement process is stabilized by a confidence-aware, class-balanced memory bank, which mitigates error propagation from noisy pseudo-labels. Extensive experiments on three large-scale benchmark datasets, including NTU RGB+D 60/120 and PKU-MMD, demonstrate that DynaPURLS significantly outperforms prior art, setting new state-of-the-art records. The source code is made publicly available at https://github.com/Alchemist0754/DynaPURLS

</details>


### [85] [A Comparative Analysis of Semiconductor Wafer Map Defect Detection with Image Transformer](https://arxiv.org/abs/2512.11977)
*Sushmita Nath*

Main category: cs.CV

TL;DR: DeiT transformer model outperforms CNNs for wafer defect classification under limited/imbalanced data, achieving 90.83% accuracy and better F1-scores.


<details>
  <summary>Details</summary>
Motivation: Semiconductor wafer defect detection requires predictive maintenance, but CNN models (VGG-19, Xception, Squeeze-Net) perform poorly with limited and imbalanced data.

Method: Investigates Data-Efficient Image Transformer (DeiT) for wafer map defect classification under data-constrained conditions, comparing against CNN models.

Result: DeiT achieves highest classification accuracy (90.83%), superior F1-score (90.78%), faster training convergence, and better robustness for minority defect classes compared to CNNs.

Conclusion: Transformer-based models like DeiT show strong potential for semiconductor wafer defect detection and can support predictive maintenance strategies in fabrication processes.

Abstract: Predictive maintenance is an important sector in modern industries which improves fault detection and cost reduction processes. By using machine learning algorithms in the whole process, the defects detection process can be implemented smoothly. Semiconductor is a sensitive maintenance field that requires predictability in work. While convolutional neural networks (CNNs) such as VGG-19, Xception and Squeeze-Net have demonstrated solid performance in image classification for semiconductor wafer industry, their effectiveness often declines in scenarios with limited and imbalanced data. This study investigates the use of the Data-Efficient Image Transformer (DeiT) for classifying wafer map defects under data-constrained conditions. Experimental results reveal that the DeiT model achieves highest classification accuracy of 90.83%, outperforming CNN models such as VGG-19(65%), SqueezeNet(82%), Xception(66%) and Hybrid(67%). DeiT also demonstrated superior F1-score (90.78%) and faster training convergence, with enhanced robustness in detecting minority defect classes. These findings highlight the potential of transformer-based models like DeiT in semiconductor wafer defect detection and support predictive maintenance strategies within semiconductor fabrication processes.

</details>


### [86] [CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction](https://arxiv.org/abs/2512.11988)
*Xianghui Xie,Bowen Wen,Yan Chang,Hesam Rabeti,Jiefeng Li,Ye Yuan,Gerard Pons-Moll,Stan Birchfield*

Main category: cs.CV

TL;DR: CARI4D: First category-agnostic method for reconstructing spatially and temporally consistent 4D human-object interactions at metric scale from monocular RGB videos, outperforming prior methods by 36-38%.


<details>
  <summary>Details</summary>
Motivation: Accurate capture of human-object interaction from RGB cameras is important for human understanding, gaming, and robot learning, but current methods are limited by requiring ground truth object templates or being constrained to specific object categories.

Method: Proposes a pose hypothesis selection algorithm that integrates predictions from foundation models, jointly refines them through a learned render-and-compare paradigm for spatial, temporal, and pixel alignment, and reasons about intricate contacts with physical constraints.

Result: Outperforms prior art by 38% on in-distribution dataset and 36% on unseen dataset in reconstruction error. Generalizes beyond training categories and works zero-shot on in-the-wild internet videos.

Conclusion: CARI4D enables category-agnostic 4D human-object interaction reconstruction from monocular RGB videos with strong generalization capabilities, making it applicable to real-world scenarios beyond constrained settings.

Abstract: Accurate capture of human-object interaction from ubiquitous sensors like RGB cameras is important for applications in human understanding, gaming, and robot learning. However, inferring 4D interactions from a single RGB view is highly challenging due to the unknown object and human information, depth ambiguity, occlusion, and complex motion, which hinder consistent 3D and temporal reconstruction. Previous methods simplify the setup by assuming ground truth object template or constraining to a limited set of object categories. We present CARI4D, the first category-agnostic method that reconstructs spatially and temporarily consistent 4D human-object interaction at metric scale from monocular RGB videos. To this end, we propose a pose hypothesis selection algorithm that robustly integrates the individual predictions from foundation models, jointly refine them through a learned render-and-compare paradigm to ensure spatial, temporal and pixel alignment, and finally reasoning about intricate contacts for further refinement satisfying physical constraints. Experiments show that our method outperforms prior art by 38% on in-distribution dataset and 36% on unseen dataset in terms of reconstruction error. Our model generalizes beyond the training categories and thus can be applied zero-shot to in-the-wild internet videos. Our code and pretrained models will be publicly released.

</details>


### [87] [V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions](https://arxiv.org/abs/2512.11995)
*Chenrui Fan,Yijun Liang,Shweta Bhardwaj,Kwesi Cobbina,Ming Li,Tianyi Zhou*

Main category: cs.CV

TL;DR: V-REX is an evaluation suite for assessing VLMs' multi-step exploratory reasoning capabilities through Chain-of-Questions approach, revealing planning vs. following performance gaps.


<details>
  <summary>Details</summary>
Motivation: Current VLMs struggle with complex open-ended visual tasks requiring multi-step exploration and reasoning, but existing benchmarks focus on straightforward questions with specific targets, lacking proper evaluation for visual thinking paths.

Method: Developed V-REX evaluation suite with challenging visual reasoning tasks requiring native multi-step exploration. Uses Chain-of-Questions (CoQ) approach to disentangle VLMs' planning (breaking down tasks into exploratory questions) and following (answering curated CoQ sequentially) capabilities. Curates finite options of questions and answers per step for reliable quantitative analysis.

Result: Evaluation of SOTA proprietary and open-sourced VLMs revealed consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.

Conclusion: V-REX provides a comprehensive framework for evaluating VLMs' multi-step exploratory reasoning capabilities, highlighting current limitations and offering a path for future improvement in complex visual reasoning tasks.

Abstract: While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.

</details>


### [88] [Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus](https://arxiv.org/abs/2512.12012)
*Antonio Guillen-Perez*

Main category: cs.CV

TL;DR: Semantic-Drive is a local-first neuro-symbolic framework that uses symbolic grounding and cognitive analysis to efficiently mine rare safety-critical events from AV video logs, achieving high recall while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: AV development is bottlenecked by scarcity of "long-tail" training data for rare safety-critical events. Existing solutions either lack precision (coarse metadata search) or are privacy-invasive/expensive (cloud-based VLMs).

Method: Two-stage neuro-symbolic approach: (1) Symbolic Grounding via real-time open-vocabulary detector (YOLOE) to anchor attention, (2) Cognitive Analysis via Reasoning VLM for forensic scene analysis. Uses "System 2" inference-time alignment with multi-model "Judge-Scout" consensus to mitigate hallucination.

Result: Achieves Recall of 0.966 (vs. 0.475 for CLIP) on nuScenes dataset against Waymo Open Dataset taxonomy, reduces Risk Assessment Error by 40% compared to single models. Runs entirely on consumer hardware (NVIDIA RTX 3090).

Conclusion: Semantic-Drive provides a privacy-preserving, efficient alternative to cloud-based solutions for mining rare safety-critical events from AV video logs, enabling better training data collection for robust autonomous vehicles.

Abstract: The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of "Long-Tail" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a "System 2" inference-time alignment strategy, utilizing a multi-model "Judge-Scout" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40\% compared to single models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.

</details>


### [89] [Exploring Spatial-Temporal Representation via Star Graph for mmWave Radar-based Human Activity Recognition](https://arxiv.org/abs/2512.12013)
*Senhao Gao,Junqing Zhang,Luoyu Mei,Shuai Wang,Xuyu Wang*

Main category: cs.CV

TL;DR: A graph neural network approach using star graphs and discrete dynamic GNNs achieves 94.27% accuracy for mmWave radar-based human activity recognition, addressing sparsity and variable-size point cloud problems.


<details>
  <summary>Details</summary>
Motivation: mmWave radar point clouds for HAR suffer from sparsity and variable-size issues, and existing preprocessing methods from vision-based systems are not optimal for radar data.

Method: Proposed a graph representation with discrete dynamic graph neural network (DDGNN) using star graphs that connect a static center point to dynamic radar points across consecutive frames to capture spatial-temporal features.

Result: Achieved 94.27% overall classification accuracy on real-world HAR datasets, near-optimal compared to vision-based skeleton data (97.25%). Outperformed baseline methods and recent radar-specific approaches without requiring resampling or frame aggregators.

Conclusion: The DDGNN with star graph representation effectively handles sparse, variable-size mmWave radar point clouds for HAR, achieving high accuracy and demonstrating effectiveness on resource-constrained platforms like Raspberry Pi 4.

Abstract: Human activity recognition (HAR) requires extracting accurate spatial-temporal features with human movements. A mmWave radar point cloud-based HAR system suffers from sparsity and variable-size problems due to the physical features of the mmWave signal. Existing works usually borrow the preprocessing algorithms for the vision-based systems with dense point clouds, which may not be optimal for mmWave radar systems. In this work, we proposed a graph representation with a discrete dynamic graph neural network (DDGNN) to explore the spatial-temporal representation of human movement-related features. Specifically, we designed a star graph to describe the high-dimensional relative relationship between a manually added static center point and the dynamic mmWave radar points in the same and consecutive frames. We then adopted DDGNN to learn the features residing in the star graph with variable sizes. Experimental results demonstrated that our approach outperformed other baseline methods using real-world HAR datasets. Our system achieved an overall classification accuracy of 94.27\%, which gets the near-optimal performance with a vision-based skeleton data accuracy of 97.25\%. We also conducted an inference test on Raspberry Pi~4 to demonstrate its effectiveness on resource-constraint platforms. \sh{ We provided a comprehensive ablation study for variable DDGNN structures to validate our model design. Our system also outperformed three recent radar-specific methods without requiring resampling or frame aggregators.

</details>


### [90] [Adaptive federated learning for ship detection across diverse satellite imagery sources](https://arxiv.org/abs/2512.12053)
*Tran-Vu La,Minh-Tan Pham,Yu Li,Patrick Matgen,Marco Chini*

Main category: cs.CV

TL;DR: Federated Learning improves ship detection across satellite datasets without data sharing, achieving near-global performance while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: Need for privacy-preserving ship detection across diverse satellite datasets, especially for commercial/sensitive imagery where data sharing is restricted.

Method: Evaluated four FL models (FedAvg, FedProx, FedOpt, FedMedian) using YOLOv8 for ship detection, compared to local training baseline and global training reference.

Result: FL models significantly outperform local training on smaller datasets and approach performance of global training using all data, with FL configuration choices being critical.

Conclusion: Federated Learning offers effective privacy-preserving solution for satellite ship detection, with proper configuration enabling near-optimal performance without data centralization.

Abstract: We investigate the application of Federated Learning (FL) for ship detection across diverse satellite datasets, offering a privacy-preserving solution that eliminates the need for data sharing or centralized collection. This approach is particularly advantageous for handling commercial satellite imagery or sensitive ship annotations. Four FL models including FedAvg, FedProx, FedOpt, and FedMedian, are evaluated and compared to a local training baseline, where the YOLOv8 ship detection model is independently trained on each dataset without sharing learned parameters. The results reveal that FL models substantially improve detection accuracy over training on smaller local datasets and achieve performance levels close to global training that uses all datasets during the training. Furthermore, the study underscores the importance of selecting appropriate FL configurations, such as the number of communication rounds and local training epochs, to optimize detection precision while maintaining computational efficiency.

</details>


### [91] [Enhancing deep learning performance on burned area delineation from SPOT-6/7 imagery for emergency management](https://arxiv.org/abs/2512.12056)
*Maria Rodriguez,Minh-Tan Pham,Martin Sudmanns,Quentin Poterek,Oscar Narvaez*

Main category: cs.CV

TL;DR: This paper introduces a semantic segmentation workflow for burned area delineation using SPOT-6/7 imagery, comparing U-Net and SegFormer models for emergency management scenarios.


<details>
  <summary>Details</summary>
Motivation: Current burned area mapping approaches overlook applicability to time-constrained emergency management scenarios, despite the crucial need for rapid damage assessment and ecosystem recovery support after wildfires.

Method: Supervised semantic segmentation workflow using SPOT-6/7 imagery, comparing U-Net and SegFormer models, incorporating land cover data as auxiliary task, and testing Test-Time Augmentation with optimization methods like Mixed Precision.

Result: U-Net and SegFormer perform similarly with limited training data, but SegFormer requires more resources. Land cover data enhances robustness without increasing inference time. Test-Time Augmentation improves performance but raises inference time, which can be mitigated with Mixed Precision optimization.

Conclusion: The proposed workflow balances performance and efficiency for emergency burned area delineation, with U-Net being more practical than SegFormer for resource-constrained scenarios, and optimization techniques enabling performance improvements without excessive computational costs.

Abstract: After a wildfire, delineating burned areas (BAs) is crucial for quantifying damages and supporting ecosystem recovery. Current BA mapping approaches rely on computer vision models trained on post-event remote sensing imagery, but often overlook their applicability to time-constrained emergency management scenarios. This study introduces a supervised semantic segmentation workflow aimed at boosting both the performance and efficiency of BA delineation. It targets SPOT-6/7 imagery due to its very high resolution and on-demand availability. Experiments are evaluated based on Dice score, Intersection over Union, and inference time. The results show that U-Net and SegFormer models perform similarly with limited training data. However, SegFormer requires more resources, challenging its practical use in emergencies. Incorporating land cover data as an auxiliary task enhances model robustness without increasing inference time. Lastly, Test-Time Augmentation improves BA delineation performance but raises inference time, which can be mitigated with optimization methods like Mixed Precision.

</details>


### [92] [CreativeVR: Diffusion-Prior-Guided Approach for Structure and Motion Restoration in Generative and Real Videos](https://arxiv.org/abs/2512.12060)
*Tejas Panambur,Ishan Rajendrakumar Dave,Chongjian Ge,Ersin Yumer,Xue Bai*

Main category: cs.CV

TL;DR: CreativeVR is a diffusion-prior-guided video restoration framework that fixes severe structural and temporal artifacts in AI-generated and real videos through a precision-controlled approach.


<details>
  <summary>Details</summary>
Motivation: Current text-to-video diffusion models produce distorted faces/hands, warped backgrounds, and inconsistent motion, while traditional video restoration methods fail on these severe structural artifacts and offer poor control over quality-fidelity trade-offs.

Method: Uses deep-adapter-based approach with a single precision knob controlling input fidelity, and a novel temporally coherent degradation module that applies realistic structural failure transformations during training.

Result: Achieves SOTA on videos with severe artifacts, competitive on standard benchmarks, runs at ~13 FPS at 720p on A100, and introduces AIGC54 benchmark for evaluation.

Conclusion: CreativeVR effectively addresses structural artifacts in AI-generated videos with precise control over restoration strength, bridging the gap between traditional video restoration and AI-generated content repair.

Abstract: Modern text-to-video (T2V) diffusion models can synthesize visually compelling clips, yet they remain brittle at fine-scale structure: even state-of-the-art generators often produce distorted faces and hands, warped backgrounds, and temporally inconsistent motion. Such severe structural artifacts also appear in very low-quality real-world videos. Classical video restoration and super-resolution (VR/VSR) methods, in contrast, are tuned for synthetic degradations such as blur and downsampling and tend to stabilize these artifacts rather than repair them, while diffusion-prior restorers are usually trained on photometric noise and offer little control over the trade-off between perceptual quality and fidelity.
  We introduce CreativeVR, a diffusion-prior-guided video restoration framework for AI-generated (AIGC) and real videos with severe structural and temporal artifacts. Our deep-adapter-based method exposes a single precision knob that controls how strongly the model follows the input, smoothly trading off between precise restoration on standard degradations and stronger structure- and motion-corrective behavior on challenging content. Our key novelty is a temporally coherent degradation module used during training, which applies carefully designed transformations that produce realistic structural failures.
  To evaluate AIGC-artifact restoration, we propose the AIGC54 benchmark with FIQA, semantic and perceptual metrics, and multi-aspect scoring. CreativeVR achieves state-of-the-art results on videos with severe artifacts and performs competitively on standard video restoration benchmarks, while running at practical throughput (about 13 FPS at 720p on a single 80-GB A100). Project page: https://daveishan.github.io/creativevr-webpage/.

</details>


### [93] [BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models](https://arxiv.org/abs/2512.12080)
*Ryan Po,Eric Ryan Chan,Changan Chen,Gordon Wetzstein*

Main category: cs.CV

TL;DR: BAgger introduces a self-supervised training scheme that uses the model's own rollouts to create corrective trajectories, teaching autoregressive video models to recover from their mistakes and reducing exposure bias in long-horizon video generation.


<details>
  <summary>Details</summary>
Motivation: Autoregressive video models suffer from exposure bias - a mismatch between training on clean contexts and inference on self-generated frames. This causes errors to compound and quality to drift over time during long-horizon video generation.

Method: Backwards Aggregation (BAgger) constructs corrective trajectories from the model's own rollouts in a self-supervised manner. Unlike prior approaches that use few-step distillation and distribution-matching losses, BAgger trains with standard score or flow matching objectives, avoiding large teachers and long-chain backpropagation through time.

Result: BAgger was instantiated on causal diffusion transformers and evaluated on text-to-video, video extension, and multi-prompt generation. The method showed more stable long-horizon motion, better visual consistency, and reduced drift compared to previous approaches.

Conclusion: BAgger effectively addresses exposure bias in autoregressive video models by teaching them to recover from their own mistakes through self-supervised corrective trajectories, leading to improved long-horizon video generation without the drawbacks of prior methods.

Abstract: Autoregressive video models are promising for world modeling via next-frame prediction, but they suffer from exposure bias: a mismatch between training on clean contexts and inference on self-generated frames, causing errors to compound and quality to drift over time. We introduce Backwards Aggregation (BAgger), a self-supervised scheme that constructs corrective trajectories from the model's own rollouts, teaching it to recover from its mistakes. Unlike prior approaches that rely on few-step distillation and distribution-matching losses, which can hurt quality and diversity, BAgger trains with standard score or flow matching objectives, avoiding large teachers and long-chain backpropagation through time. We instantiate BAgger on causal diffusion transformers and evaluate on text-to-video, video extension, and multi-prompt generation, observing more stable long-horizon motion and better visual consistency with reduced drift.

</details>


### [94] [RePack: Representation Packing of Vision Foundation Model Features Enhances Diffusion Transformer](https://arxiv.org/abs/2512.12083)
*Guanfang Dong,Luke Schultz,Negar Hassanpour,Chao Gao*

Main category: cs.CV

TL;DR: RePack is a framework that compresses high-dimensional VFM features for DiTs, accelerating convergence and improving image generation by filtering noise while preserving semantic information.


<details>
  <summary>Details</summary>
Motivation: While pre-trained vision foundation models (VFMs) enhance latent diffusion models, their high-dimensional representations can cause Information Overload, especially when features exceed original image size, hindering efficient learning and generation.

Method: RePack transforms VFM representations into compact, decoder-friendly forms by projecting onto low-dimensional manifolds, filtering non-semantic noise while preserving core structural information for reconstruction.

Result: RePack significantly accelerates DiT convergence, achieving FID of 3.66 in only 64 epochs on DiT-XL/2, which is 35% faster than state-of-the-art methods that directly inject raw VFM features.

Conclusion: RePack successfully extracts core semantics from VFM representations while avoiding high-dimensionality side effects, demonstrating that compact, filtered representations are more effective than raw high-dimensional features for diffusion transformers.

Abstract: The superior representation capability of pre-trained vision foundation models (VFMs) has been harnessed for enhancing latent diffusion models (LDMs). These approaches inject the rich semantics from high-dimensional VFM representations (e.g., DINOv3) into LDMs at different phases, resulting in accelerated learning and better generation performance. However, the high-dimensionality of VFM representations may also lead to Information Overload, particularly when the VFM features exceed the size of the original image for decoding. To address this issue while preserving the utility of VFM features, we propose RePack (Representation Packing), a simple yet effective framework for improving Diffusion Transformers (DiTs). RePack transforms the VFM representation into a more compact, decoder-friendly representation by projecting onto low-dimensional manifolds. We find that RePack can effectively filter out non-semantic noise while preserving the core structural information needed for high-fidelity reconstruction. Experimental results show that RePack significantly accelerates DiT convergence and outperforms recent methods that directly inject raw VFM features into the decoder for image reconstruction. On DiT-XL/2, RePack achieves an FID of 3.66 in only 64 epochs, which is 35% faster than the state-of-the-art method. This demonstrates that RePack successfully extracts the core semantics of VFM representations while bypassing their high-dimensionality side effects.

</details>


### [95] [VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering](https://arxiv.org/abs/2512.12089)
*Zihu Wang,Boxun Xu,Yuxuan Xia,Peng Li*

Main category: cs.CV

TL;DR: VEGAS: Vision Encoder Guided Attention Steering - a simple inference-time method that reduces hallucinations in LVLMs by injecting vision encoder attention maps into language model mid-layers.


<details>
  <summary>Details</summary>
Motivation: Large vision-language models (LVLMs) often produce linguistically fluent but factually inconsistent outputs (hallucinations) that don't align with visual evidence. Current methods struggle to effectively suppress these hallucinations during decoding.

Method: VEGAS integrates the vision encoder's attention maps into the language model's middle layers during inference. It adaptively steers tokens that fail to concentrate on key image objects, using the vision encoder's more concentrated attention to guide the language model's attention.

Result: Extensive experiments across multiple benchmarks show VEGAS consistently achieves state-of-the-art performance in reducing hallucinations in LVLMs.

Conclusion: The vision encoder's own attention maps are effective for suppressing hallucinations during decoding, especially when injected into the language model's middle layers where vision-text conflicts peak. VEGAS provides a simple yet powerful inference-time solution.

Abstract: Large vision-language models (LVLMs) exhibit impressive ability to jointly reason over visual and textual inputs. However, they often produce outputs that are linguistically fluent but factually inconsistent with the visual evidence, i.e., they hallucinate. Despite growing efforts to mitigate such hallucinations, a key question remains: what form of visual attention can effectively suppress hallucinations during decoding? In this work, we provide a simple answer: the vision encoder's own attention map. We show that LVLMs tend to hallucinate when their final visual-attention maps fail to concentrate on key image objects, whereas the vision encoder's more concentrated attention maps substantially reduce hallucinations. To further investigate the cause, we analyze vision-text conflicts during decoding and find that these conflicts peak in the language model's middle layers. Injecting the vision encoder's attention maps into these layers effectively suppresses hallucinations. Building on these insights, we introduce VEGAS, a simple yet effective inference-time method that integrates the vision encoder's attention maps into the language model's mid-layers and adaptively steers tokens which fail to concentrate on key image objects. Extensive experiments across multiple benchmarks demonstrate that VEGAS consistently achieves state-of-the-art performance in reducing hallucinations.

</details>


### [96] [SPDMark: Selective Parameter Displacement for Robust Video Watermarking](https://arxiv.org/abs/2512.12090)
*Samar Fares,Nurbek Tastan,Karthik Nandakumar*

Main category: cs.CV

TL;DR: SPDMark is a novel in-generation video watermarking framework that embeds watermarks by selectively displacing diffusion model parameters using LoRA-based basis shifts, enabling imperceptible, robust, and computationally efficient watermarking with temporal tamper detection.


<details>
  <summary>Details</summary>
Motivation: The rise of high-quality video generation models creates a critical need for reliable watermarking to detect and track generated video provenance. Existing methods fail to simultaneously achieve imperceptibility, robustness, and computational efficiency.

Method: SPDMark uses selective parameter displacement of video diffusion models, modeling displacement as additive composition of layer-wise basis shifts indexed by watermarking keys. It leverages LoRA for parameter efficiency, jointly trains basis shifts and watermark extractor using message recovery, perceptual similarity, and temporal consistency losses. Uses cryptographic hashing for frame-specific messages and maximum bipartite matching for temporal tamper detection.

Result: Evaluations on text-to-video and image-to-video generation models demonstrate SPDMark's ability to generate imperceptible watermarks with high recovery accuracy and robustness against various common video modifications.

Conclusion: SPDMark provides an effective framework for in-generation video watermarking that addresses the limitations of existing methods by achieving imperceptibility, robustness, and computational efficiency simultaneously through selective parameter displacement and LoRA-based implementation.

Abstract: The advent of high-quality video generation models has amplified the need for robust watermarking schemes that can be used to reliably detect and track the provenance of generated videos. Existing video watermarking methods based on both post-hoc and in-generation approaches fail to simultaneously achieve imperceptibility, robustness, and computational efficiency. This work introduces a novel framework for in-generation video watermarking called SPDMark (pronounced `SpeedMark') based on selective parameter displacement of a video diffusion model. Watermarks are embedded into the generated videos by modifying a subset of parameters in the generative model. To make the problem tractable, the displacement is modeled as an additive composition of layer-wise basis shifts, where the final composition is indexed by the watermarking key. For parameter efficiency, this work specifically leverages low-rank adaptation (LoRA) to implement the basis shifts. During the training phase, the basis shifts and the watermark extractor are jointly learned by minimizing a combination of message recovery, perceptual similarity, and temporal consistency losses. To detect and localize temporal modifications in the watermarked videos, we use a cryptographic hashing function to derive frame-specific watermark messages from the given base watermarking key. During watermark extraction, maximum bipartite matching is applied to recover the correct frame order, even from temporally tampered videos. Evaluations on both text-to-video and image-to-video generation models demonstrate the ability of SPDMark to generate imperceptible watermarks that can be recovered with high accuracy and also establish its robustness against a variety of common video modifications.

</details>


### [97] [AI-Augmented Pollen Recognition in Optical and Holographic Microscopy for Veterinary Imaging](https://arxiv.org/abs/2512.12101)
*Swarn S. Warshaneyan,Maksims Ivanovs,Blaž Cugmas,Inese Bērziņa,Laura Goldberga,Mindaugas Tamosiunas,Roberts Kadiķis*

Main category: cs.CV

TL;DR: Automated pollen recognition study comparing optical microscopy vs digital holographic microscopy (DIHM), showing significant performance gap. GAN-based data augmentation improves DIHM detection modestly.


<details>
  <summary>Details</summary>
Motivation: To enable fully automated pollen recognition in digital in-line holographic microscopy (DIHM) images, which is challenging due to speckle noise, twin-image artifacts, and appearance differences from conventional optical microscopy.

Method: Used YOLOv8s for object detection and MobileNetV3L for classification on dual-modality dataset. Applied Wasserstein GAN with spectral normalization (WGAN-SN) to create synthetic DIHM images for data augmentation.

Result: Optical microscopy achieved 91.3% mAP50 for detection and 97% classification accuracy, while DIHM only reached 8.15% mAP50 and 50% accuracy. GAN augmentation improved DIHM detection to 15.4% mAP50 with FID score of 58.246.

Conclusion: GAN-based data augmentation can modestly reduce the performance gap between optical and holographic microscopy, bringing automated DIHM workflows closer to practical veterinary imaging applications.

Abstract: We present a comprehensive study on fully automated pollen recognition across both conventional optical and digital in-line holographic microscopy (DIHM) images of sample slides. Visually recognizing pollen in unreconstructed holographic images remains challenging due to speckle noise, twin-image artifacts and substantial divergence from bright-field appearances. We establish the performance baseline by training YOLOv8s for object detection and MobileNetV3L for classification on a dual-modality dataset of automatically annotated optical and affinely aligned DIHM images. On optical data, detection mAP50 reaches 91.3% and classification accuracy reaches 97%, whereas on DIHM data, we achieve only 8.15% for detection mAP50 and 50% for classification accuracy. Expanding the bounding boxes of pollens in DIHM images over those acquired in aligned optical images achieves 13.3% for detection mAP50 and 54% for classification accuracy. To improve object detection in DIHM images, we employ a Wasserstein GAN with spectral normalization (WGAN-SN) to create synthetic DIHM images, yielding an FID score of 58.246. Mixing real-world and synthetic data at the 1.0 : 1.5 ratio for DIHM images improves object detection up to 15.4%. These results demonstrate that GAN-based augmentation can reduce the performance divide, bringing fully automated DIHM workflows for veterinary imaging a small but important step closer to practice.

</details>


### [98] [EchoVLM: Measurement-Grounded Multimodal Learning for Echocardiography](https://arxiv.org/abs/2512.12107)
*Yuheng Li,Yue Zhang,Abdoul Aziz Amadou,Yuxiang Lai,Jike Zhong,Tiziano Passerini,Dorin Comaniciu,Puneet Sharma*

Main category: cs.CV

TL;DR: EchoVLM: A vision-language model for echocardiography interpretation using novel pretraining objectives and a new measurement-grounded dataset (EchoGround-MIMIC), achieving state-of-the-art performance across multiple clinical tasks.


<details>
  <summary>Details</summary>
Motivation: Echocardiography interpretation is labor-intensive and multimodal, requiring view recognition, measurements, assessments, and guideline reasoning. Current VLMs are limited in echocardiography due to lack of large-scale clinically grounded datasets and measurement-based reasoning capabilities.

Method: Created EchoGround-MIMIC dataset (19,065 image-text pairs from 1,572 patients) with standardized views, structured measurements, measurement-grounded captions, and disease labels. Developed EchoVLM with two novel pretraining objectives: view-informed contrastive loss and negation-aware contrastive loss.

Result: Achieved state-of-the-art performance across 36 tasks in 5 clinical applications: 86.5% AUC in zero-shot disease classification and 95.1% accuracy in view classification. Demonstrated transferable visual representations and foundation model capabilities.

Conclusion: EchoVLM establishes a foundation model for end-to-end echocardiography interpretation. The release of EchoGround-MIMIC dataset and curation code enables reproducibility and advances multimodal echocardiography research.

Abstract: Echocardiography is the most widely used imaging modality in cardiology, yet its interpretation remains labor-intensive and inherently multimodal, requiring view recognition, quantitative measurements, qualitative assessments, and guideline-based reasoning. While recent vision-language models (VLMs) have achieved broad success in natural images and certain medical domains, their potential in echocardiography has been limited by the lack of large-scale, clinically grounded image-text datasets and the absence of measurement-based reasoning central to echo interpretation. We introduce EchoGround-MIMIC, the first measurement-grounded multimodal echocardiography dataset, comprising 19,065 image-text pairs from 1,572 patients with standardized views, structured measurements, measurement-grounded captions, and guideline-derived disease labels. Building on this resource, we propose EchoVLM, a vision-language model that incorporates two novel pretraining objectives: (i) a view-informed contrastive loss that encodes the view-dependent structure of echocardiographic imaging, and (ii) a negation-aware contrastive loss that distinguishes clinically critical negative from positive findings. Across five types of clinical applications with 36 tasks spanning multimodal disease classification, image-text retrieval, view classification, chamber segmentation, and landmark detection, EchoVLM achieves state-of-the-art performance (86.5% AUC in zero-shot disease classification and 95.1% accuracy in view classification). We demonstrate that clinically grounded multimodal pretraining yields transferable visual representations and establish EchoVLM as a foundation model for end-to-end echocardiography interpretation. We will release EchoGround-MIMIC and the data curation code, enabling reproducibility and further research in multimodal echocardiography interpretation.

</details>


### [99] [A Novel Patch-Based TDA Approach for Computed Tomography](https://arxiv.org/abs/2512.12108)
*Dashti A. Ali,Aras T. Asaad,Jacob J. Peoples,Mohammad Hamghalam,Alex Robins,Mane Piliposyan,Richard K. G. Do,Natalie Gangai,Yun S. Chun,Ahmad Bashir Barekzai,Jayasree Chakraborty,Hala Khasawneh,Camila Vilela,Natally Horvat,João Miranda,Alice C. Wei,Amber L. Simpson*

Main category: cs.CV

TL;DR: A novel patch-based persistent homology approach for 3D CT images outperforms traditional cubical complex methods in both classification performance and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional 3D cubical complex filtration for persistent homology in CT imaging has limitations in performance and computational efficiency, especially with higher resolution images. There's a need for more effective topological feature extraction methods for volumetric medical imaging data.

Method: Proposes a patch-based persistent homology construction approach specifically designed for volumetric CT imaging data. The method extracts topological features from image patches rather than using the full 3D cubical complex filtration.

Result: The patch-based TDA approach significantly outperforms the cubical complex method, achieving average improvements of 10.38% in accuracy, 6.94% in AUC, 2.06% in sensitivity, 11.58% in specificity, and 8.51% in F1 score across multiple datasets. It also demonstrates superior time-efficiency.

Conclusion: The patch-based persistent homology approach is a superior alternative to traditional cubical complex methods for topological feature extraction from 3D CT images, offering better classification performance and computational efficiency. The authors provide an open-source Python package (Patch-TDA) to facilitate adoption.

Abstract: The development of machine learning (ML) models based on computed tomography (CT) imaging modality has been a major focus of recent research in the medical imaging domain. Incorporating robust feature engineering approach can highly improve the performance of these models. Topological data analysis (TDA), a recent development based on the mathematical field of algebraic topology, mainly focuses on the data from a topological perspective, extracting deeper insight and higher dimensional structures from the data. Persistent homology (PH), a fundamental tool in the area of TDA, can extract topological features such as connected components, cycles and voids from the data. A popular approach to construct PH from 3D CT images is to utilize the 3D cubical complex filtration, a method adapted for grid-structured data. However, this approach may not always yield the best performance and can suffer from computational complexity with higher resolution CT images. This study introduces a novel patch-based PH construction approach tailored for volumetric medical imaging data, in particular CT modality. A wide range of experiments has been conducted on several datasets of 3D CT images to comprehensively analyze the performance of the proposed method with various parameters and benchmark it against the 3D cubical complex algorithm. Our results highlight the dominance of the patch-based TDA approach in terms of both classification performance and time-efficiency. The proposed approach outperformed the cubical complex method, achieving average improvement of 10.38%, 6.94%, 2.06%, 11.58%, and 8.51% in accuracy, AUC, sensitivity, specificity, and F1 score, respectively, across all datasets. Finally, we provide a convenient python package, Patch-TDA, to facilitate the utilization of the proposed approach.

</details>


### [100] [A Benchmark Dataset for Spatially Aligned Road Damage Assessment in Small Uncrewed Aerial Systems Disaster Imagery](https://arxiv.org/abs/2512.12128)
*Thomas Manzini,Priyankari Perali,Raisa Karnik,Robin R. Murphy*

Main category: cs.CV

TL;DR: Largest benchmark dataset for road damage assessment with 657.25km of labeled roads from 10 disasters, addressing limitations of prior datasets and providing 18 baseline models operationally validated during 2024 hurricane responses.


<details>
  <summary>Details</summary>
Motivation: Prior road damage assessment datasets are either too small or use low-resolution imagery insufficient for emergency management needs, and existing ML systems lack operational validation. The paper addresses three key challenges: dataset scale, resolution, and operational validation.

Method: Created CRASAR-U-DRIODs dataset with post-disaster sUAS imagery from 10 disasters, labeled 657.25km of roads using 10-class schema, trained 18 baseline ML models, and operationally deployed them during Hurricanes Debby and Helene in 2024. Also provided 9,184 road line adjustments to address spatial misalignment issues.

Result: When deployed against misaligned road lines, model performance degraded by 5.596% Macro IoU on average. Without spatial alignment, approximately 8% (11km) of adverse conditions would be labeled incorrectly, with 9% (59km) of road lines misaligned off actual roads.

Conclusion: The paper provides the largest benchmark dataset for road damage assessment with operationally validated models, highlighting critical spatial alignment challenges that degrade performance. These gaps need addressing by ML, CV, and robotics communities to enable effective disaster decision-making.

Abstract: This paper presents the largest known benchmark dataset for road damage assessment and road alignment, and provides 18 baseline models trained on the CRASAR-U-DRIODs dataset's post-disaster small uncrewed aerial systems (sUAS) imagery from 10 federally declared disasters, addressing three challenges within prior post-disaster road damage assessment datasets. While prior disaster road damage assessment datasets exist, there is no current state of practice, as prior public datasets have either been small-scale or reliant on low-resolution imagery insufficient for detecting phenomena of interest to emergency managers. Further, while machine learning (ML) systems have been developed for this task previously, none are known to have been operationally validated. These limitations are overcome in this work through the labeling of 657.25km of roads according to a 10-class labeling schema, followed by training and deploying ML models during the operational response to Hurricanes Debby and Helene in 2024. Motivated by observed road line misalignment in practice, 9,184 road line adjustments were provided for spatial alignment of a priori road lines, as it was found that when the 18 baseline models are deployed against real-world misaligned road lines, model performance degraded on average by 5.596\% Macro IoU. If spatial alignment is not considered, approximately 8\% (11km) of adverse conditions on road lines will be labeled incorrectly, with approximately 9\% (59km) of road lines misaligned off the actual road. These dynamics are gaps that should be addressed by the ML, CV, and robotics communities to enable more effective and informed decision-making during disasters.

</details>


### [101] [MeltwaterBench: Deep learning for spatiotemporal downscaling of surface meltwater](https://arxiv.org/abs/2512.12142)
*Björn Lütjens,Patrick Alexander,Raf Antwerpen,Til Widmann,Guido Cervone,Marco Tedesco*

Main category: cs.CV

TL;DR: Deep learning model fuses SAR, PMW, DEM, and RCM data to create high-resolution (100m daily) Greenland meltwater maps, achieving 95% accuracy vs. 83% for RCM-only and 72% for PMW-only approaches.


<details>
  <summary>Details</summary>
Motivation: Current Greenland meltwater maps face resolution trade-offs (high temporal OR spatial, not both), limiting understanding of accelerated ice sheet melting processes.

Method: Deep learning model spatiotemporally downscales RCM outputs using SAR, PMW, and DEM data over Helheim Glacier (2017-2023), with SAR-derived meltwater as ground truth. Evaluates UNet and DeepLabv3+ architectures.

Result: Fusion model achieves 95% accuracy vs. 83% for RCM-only and 72% for PMW-only methods. SAR running window approach achieves 90% accuracy but underestimates extremes. Dataset published as MeltwaterBench benchmark.

Conclusion: Deep learning data fusion enables high-resolution (100m daily) meltwater mapping, significantly outperforming existing approaches. Published benchmark dataset supports further development of data-driven downscaling methods.

Abstract: The Greenland ice sheet is melting at an accelerated rate due to processes that are not fully understood and hard to measure. The distribution of surface meltwater can help understand these processes and is observable through remote sensing, but current maps of meltwater face a trade-off: They are either high-resolution in time or space, but not both. We develop a deep learning model that creates gridded surface meltwater maps at daily 100m resolution by fusing data streams from remote sensing observations and physics-based models. In particular, we spatiotemporally downscale regional climate model (RCM) outputs using synthetic aperture radar (SAR), passive microwave (PMW), and a digital elevation model (DEM) over the Helheim Glacier in Eastern Greenland from 2017-2023. Using SAR-derived meltwater as "ground truth", we show that a deep learning-based method that fuses all data streams is over 10 percentage points more accurate over our study area than existing non deep learning-based approaches that only rely on a regional climate model (83% vs. 95% Acc.) or passive microwave observations (72% vs. 95% Acc.). Alternatively, creating a gridded product through a running window calculation with SAR data underestimates extreme melt events, but also achieves notable accuracy (90%) and does not rely on deep learning. We evaluate standard deep learning methods (UNet and DeepLabv3+), and publish our spatiotemporally aligned dataset as a benchmark, MeltwaterBench, for intercomparisons with more complex data-driven downscaling methods. The code and data are available at $\href{https://github.com/blutjens/hrmelt}{github.com/blutjens/hrmelt}$.

</details>


### [102] [Open Horizons: Evaluating Deep Models in the Wild](https://arxiv.org/abs/2512.12146)
*Ayush Vaibhav Bhatti,Deniz Karakay,Debottama Das,Nilotpal Rajbongshi,Yuito Sugimoto*

Main category: cs.CV

TL;DR: Unified experimental study comparing open-set recognition (OSR) and few-shot class-incremental learning (FSCIL) on CIFAR-10, evaluating different backbones, scoring functions, and incremental learning methods.


<details>
  <summary>Details</summary>
Motivation: Open-world deployment requires models to recognize known categories while remaining reliable when novel classes appear, addressing the challenges of both open-set recognition and few-shot class-incremental learning.

Method: For OSR: Compare three pretrained frozen visual encoders (ResNet-50, ConvNeXt-Tiny, CLIP ViT-B/16) using linear probe and four post-hoc scoring functions (MSP, Energy, Mahalanobis, kNN). For FSCIL: Compare modified SPPR, OrCo, and ConCM methods using partially frozen ResNet-50 across 1-, 5-, and 10-shot scenarios.

Result: CLIP consistently yields strongest separability between known/unknown samples in OSR, with Energy providing most stable performance. For FSCIL, ConCM achieves 84.7% accuracy in 10-shot setting with cleanest confusion matrix, while all methods show saturation beyond 5 shots.

Conclusion: Backbone architecture and scoring mechanisms significantly affect unknown detection, while prototype-based methods effectively mitigate catastrophic forgetting during incremental adaptation in open-world scenarios.

Abstract: Open-world deployment requires models to recognize both known categories and remain reliable when novel classes appear. We present a unified experimental study spanning open-set recognition (OSR) and few-shot class-incremental learning (FSCIL) on CIFAR-10. For OSR, we compare three pretrained frozen visual encoders: ResNet-50, ConvNeXt-Tiny and CLIP ViT-B/16,using a linear probe and four post-hoc scoring functions, namely MSP, Energy, Mahalanobis and kNN. Across metrics,such as, AUROC, AUPR, FPR@95, and OSCR, CLIP consistently yields the strongest separability between known and unknown samples, with Energy providing the most stable performance across backbones. For FSCIL, we compare modified SPPR, OrCo, and ConCM using partially frozen ResNet-50 across 1-, 5-, and 10-shot scenarios. ConCM achieves 84.7% accuracy in the 10-shot setting with the cleanest confusion matrix, while all methods show saturation beyond 5 shots. Our controlled evaluation reveals how the backbone architecture and scoring mechanisms affect unknown detection and how prototype-based methods mitigate catastrophic forgetting during incremental adaptation.

</details>


### [103] [Audio-Visual Camera Pose Estimationn with Passive Scene Sounds and In-the-Wild Video](https://arxiv.org/abs/2512.12165)
*Daniel Adebi,Sagnik Majumder,Kristen Grauman*

Main category: cs.CV

TL;DR: Audio-visual camera pose estimation using passive scene sounds to complement vision, especially in degraded visual conditions.


<details>
  <summary>Details</summary>
Motivation: Visual methods struggle under degraded conditions like motion blur or occlusions. Passive scene sounds provide complementary cues for relative camera pose estimation in real-world videos.

Method: Simple audio-visual framework integrating direction-of-arrival (DOA) spectra and binauralized embeddings into a state-of-the-art vision-only pose estimation model.

Result: Consistent gains over strong visual baselines on two large datasets, plus robustness when visual information is corrupted. First work to successfully leverage audio for relative camera pose estimation in real-world videos.

Conclusion: Incidental, everyday audio serves as an unexpected but promising signal for classic spatial challenges in camera pose estimation.

Abstract: Understanding camera motion is a fundamental problem in embodied perception and 3D scene understanding. While visual methods have advanced rapidly, they often struggle under visually degraded conditions such as motion blur or occlusions. In this work, we show that passive scene sounds provide complementary cues for relative camera pose estimation for in-the-wild videos. We introduce a simple but effective audio-visual framework that integrates direction-ofarrival (DOA) spectra and binauralized embeddings into a state-of-the-art vision-only pose estimation model. Our results on two large datasets show consistent gains over strong visual baselines, plus robustness when the visual information is corrupted. To our knowledge, this represents the first work to successfully leverage audio for relative camera pose estimation in real-world videos, and it establishes incidental, everyday audio as an unexpected but promising signal for a classic spatial challenge. Project: http://vision.cs.utexas.edu/projects/av_camera_pose.

</details>


### [104] [SMRABooth: Subject and Motion Representation Alignment for Customized Video Generation](https://arxiv.org/abs/2512.12193)
*Xuancheng Xu,Yaning Li,Sisi You,Bing-Kun Bao*

Main category: cs.CV

TL;DR: SMRABooth: A method for customized video generation that preserves subject appearance from reference images and motion patterns from reference videos using object-level guidance and sparse LoRA injection.


<details>
  <summary>Details</summary>
Motivation: Existing video generation methods struggle to simultaneously maintain subject appearance similarity and motion pattern consistency due to lack of object-level guidance for both subject and motion.

Method: Three-stage approach: (1) Use self-supervised encoder for subject representations to guide subject alignment, (2) Use optical flow encoder for motion representations to capture object-level motion trajectories, (3) Apply subject-motion association decoupling strategy with sparse LoRAs injection across locations and timing.

Result: Extensive experiments show SMRABooth excels in subject and motion customization, maintaining consistent subject appearance and motion patterns in controllable text-to-video generation.

Conclusion: SMRABooth effectively addresses the challenge of preserving both subject appearance and motion patterns in customized video generation through object-level representations and decoupled LoRA injection.

Abstract: Customized video generation aims to produce videos that faithfully preserve the subject's appearance from reference images while maintaining temporally consistent motion from reference videos. Existing methods struggle to ensure both subject appearance similarity and motion pattern consistency due to the lack of object-level guidance for subject and motion. To address this, we propose SMRABooth, which leverages the self-supervised encoder and optical flow encoder to provide object-level subject and motion representations. These representations are aligned with the model during the LoRA fine-tuning process. Our approach is structured in three core stages: (1) We exploit subject representations via a self-supervised encoder to guide subject alignment, enabling the model to capture overall structure of subject and enhance high-level semantic consistency. (2) We utilize motion representations from an optical flow encoder to capture structurally coherent and object-level motion trajectories independent of appearance. (3) We propose a subject-motion association decoupling strategy that applies sparse LoRAs injection across both locations and timing, effectively reducing interference between subject and motion LoRAs. Extensive experiments show that SMRABooth excels in subject and motion customization, maintaining consistent subject appearance and motion patterns, proving its effectiveness in controllable text-to-video generation.

</details>


### [105] [Thermal RGB Fusion for Micro-UAV Wildfire Perimeter Tracking with Minimal Comms](https://arxiv.org/abs/2512.12199)
*Ercan Erkalkan,Vedat Topuz,Ayça Ak*

Main category: cs.CV

TL;DR: Lightweight perimeter tracking method for micro UAV teams in wildfire environments using thermal/RGB fusion with minimal bandwidth requirements.


<details>
  <summary>Details</summary>
Motivation: Need for emergency wildfire reconnaissance with micro UAV teams operating under limited bandwidth conditions, requiring robust sensing with minimal communications.

Method: Thermal images generate hot region masks via adaptive thresholding and morphological refinement. RGB frames provide edge cues and suppress false detections with gradient filtering. Rule-level merging selects boundary candidates simplified by Ramer-Douglas-Peucker algorithm. System includes periodic beacons and inertial feedback for GPS degradation resilience.

Result: Small-scale simulations show reduced average path length and boundary jitter compared to pure edge tracking baseline, while maintaining environmental coverage. Achieves sub-50ms latency on embedded SoC platforms with 10-15 m/s forward motion feasibility on standard micro platforms.

Conclusion: The lightweight approach enables rapid field deployment for emergency wildfire reconnaissance with robust sensing and minimal communication requirements.

Abstract: This study introduces a lightweight perimeter tracking method designed for micro UAV teams operating over wildfire environments under limited bandwidth conditions. Thermal image frames generate coarse hot region masks through adaptive thresholding and morphological refinement, while RGB frames contribute edge cues and suppress texture related false detections using gradient based filtering. A rule level merging strategy selects boundary candidates and simplifies them via the Ramer Douglas Peucker algorithm. The system incorporates periodic beacons and an inertial feedback loop that maintains trajectory stability in the presence of GPS degradation. The guidance loop targets sub 50 ms latency on embedded System on Chip (SoC) platforms by constraining per frame pixel operations and precomputing gradient tables. Small scale simulations demonstrate reductions in average path length and boundary jitter compared to a pure edge tracking baseline, while maintaining environmental coverage measured through intersection merge analysis. Battery consumption and computational utilization confirm the feasibility of achieving 10, 15 m/s forward motion on standard micro platforms. This approach enables rapid deployment in the field, requiring robust sensing and minimal communications for emergency reconnaissance applications.

</details>


### [106] [A Multi-Year Urban Streetlight Imagery Dataset for Visual Monitoring and Spatio-Temporal Drift Detection](https://arxiv.org/abs/2512.12205)
*Peizheng Li,Ioannis Mavromatis,Ajith Sahadevan,Tim Farnham,Adnan Aijaz,Aftab Khan*

Main category: cs.CV

TL;DR: Large-scale visual dataset of urban streetlights from 22 cameras in Bristol (2021-2025) with 526K+ hourly images, metadata, and self-supervised CNN-VAE framework for studying visual drift and model stability in smart city deployments.


<details>
  <summary>Details</summary>
Motivation: To address the lack of realistic, long-term visual datasets for studying visual drift, anomaly detection, and MLOps strategies in real-world smart city deployments, particularly for evaluating model stability over time under diverse environmental conditions.

Method: Deployed 22 fixed-angle cameras across Bristol to collect hourly images over 4 years. Created a self-supervised framework using convolutional variational autoencoders (CNN-VAEs) trained per camera node and day/night sets. Defined two drift metrics: relative centroid drift (latent space deviation) and relative reconstruction error (image-domain degradation).

Result: Produced a comprehensive dataset of 526,000+ images with rich metadata (timestamps, GPS, device IDs) covering diverse lighting, weather, and seasonal conditions. The dataset enables detailed investigation of visual drift patterns and provides a realistic benchmark for evaluating long-term model stability in urban environments.

Conclusion: This dataset and framework provide a valuable resource for studying visual drift, anomaly detection, and deployment-ready vision systems in smart cities, supporting applications like streetlight monitoring, weather inference, and urban scene understanding while promoting reproducibility through publicly available data and code.

Abstract: We present a large-scale, longitudinal visual dataset of urban streetlights captured by 22 fixed-angle cameras deployed across Bristol, U.K., from 2021 to 2025. The dataset contains over 526,000 images, collected hourly under diverse lighting, weather, and seasonal conditions. Each image is accompanied by rich metadata, including timestamps, GPS coordinates, and device identifiers. This unique real-world dataset enables detailed investigation of visual drift, anomaly detection, and MLOps strategies in smart city deployments. To promtoe seconardary analysis, we additionally provide a self-supervised framework based on convolutional variational autoencoders (CNN-VAEs). Models are trained separately for each camera node and for day/night image sets. We define two per-sample drift metrics: relative centroid drift, capturing latent space deviation from a baseline quarter, and relative reconstruction error, measuring normalized image-domain degradation. This dataset provides a realistic, fine-grained benchmark for evaluating long-term model stability, drift-aware learning, and deployment-ready vision systems. The images and structured metadata are publicly released in JPEG and CSV formats, supporting reproducibility and downstream applications such as streetlight monitoring, weather inference, and urban scene understanding. The dataset can be found at https://doi.org/10.5281/zenodo.17781192 and https://doi.org/10.5281/zenodo.17859120.

</details>


### [107] [ALERT Open Dataset and Input-Size-Agnostic Vision Transformer for Driver Activity Recognition using IR-UWB](https://arxiv.org/abs/2512.12206)
*Jeongjun Park,Sunwook Hwang,Hyeonho Noh,Jin Mo Yang,Hyun Jong Yang,Saewoong Bahk*

Main category: cs.CV

TL;DR: This paper introduces ALERT dataset and ISA-ViT framework for distracted driving detection using UWB radar, achieving 22.68% accuracy improvement over existing methods.


<details>
  <summary>Details</summary>
Motivation: Distracted driving causes fatal crashes worldwide. Current driver activity recognition using IR-UWB radar faces two main challenges: lack of large-scale real-world UWB datasets covering diverse distracted behaviors, and difficulty adapting fixed-input Vision Transformers to UWB radar data with non-standard dimensions.

Method: Proposes ISA-ViT (input-size-agnostic Vision Transformer) framework for radar-based DAR. The method resizes UWB data while preserving radar-specific information like Doppler shifts and phase characteristics. Uses adjustable patch configurations and pre-trained positional embedding vectors to overcome naive resizing limitations. Also employs domain fusion strategy combining range- and frequency-domain features.

Result: ISA-ViT achieves 22.68% accuracy improvement over existing ViT-based approach for UWB-based DAR. The ALERT dataset contains 10,220 radar samples of seven distracted driving activities collected in real driving conditions.

Conclusion: This work addresses key challenges in distracted driving detection by providing the ALERT dataset and ISA-ViT framework, facilitating development of more robust and scalable systems for real-world deployment. The dataset and input-size-agnostic strategy are publicly released to advance the field.

Abstract: Distracted driving contributes to fatal crashes worldwide. To address this, researchers are using driver activity recognition (DAR) with impulse radio ultra-wideband (IR-UWB) radar, which offers advantages such as interference resistance, low power consumption, and privacy preservation. However, two challenges limit its adoption: the lack of large-scale real-world UWB datasets covering diverse distracted driving behaviors, and the difficulty of adapting fixed-input Vision Transformers (ViTs) to UWB radar data with non-standard dimensions.
  This work addresses both challenges. We present the ALERT dataset, which contains 10,220 radar samples of seven distracted driving activities collected in real driving conditions. We also propose the input-size-agnostic Vision Transformer (ISA-ViT), a framework designed for radar-based DAR. The proposed method resizes UWB data to meet ViT input requirements while preserving radar-specific information such as Doppler shifts and phase characteristics. By adjusting patch configurations and leveraging pre-trained positional embedding vectors (PEVs), ISA-ViT overcomes the limitations of naive resizing approaches. In addition, a domain fusion strategy combines range- and frequency-domain features to further improve classification performance.
  Comprehensive experiments demonstrate that ISA-ViT achieves a 22.68% accuracy improvement over an existing ViT-based approach for UWB-based DAR. By publicly releasing the ALERT dataset and detailing our input-size-agnostic strategy, this work facilitates the development of more robust and scalable distracted driving detection systems for real-world deployment.

</details>


### [108] [A Hybrid Deep Learning Framework for Emotion Recognition in Children with Autism During NAO Robot-Mediated Interaction](https://arxiv.org/abs/2512.12208)
*Indranil Bhattacharjee,Vartika Narayani Srinet,Anirudha Bhattacharjee,Braj Bhushan,Bishakh Bhattacharya*

Main category: cs.CV

TL;DR: Novel deep learning pipeline for emotion recognition in autistic children during human-robot interaction, using hybrid CNN-GCN model on facial frames with probabilistic labeling.


<details>
  <summary>Details</summary>
Motivation: Understanding emotional responses in children with Autism Spectrum Disorder (ASD) during social interaction is a critical challenge in developmental psychology and human-robot interaction, addressing a major gap in autism-specific HRI research.

Method: Hybrid model combining fine-tuned ResNet-50 CNN and three-layer Graph Convolutional Network (GCN) trained on visual and geometric features from MediaPipe FaceMesh landmarks. Emotions were probabilistically labeled using weighted ensemble of DeepFace and FER models across seven emotion classes, with final classification using fused embedding optimized via Kullback-Leibler divergence.

Result: The proposed method demonstrates robust performance in modeling subtle affective responses, effectively capturing micro emotional cues in neurodivergent children, and offers significant promise for affective profiling of ASD children in clinical and therapeutic human-robot interaction contexts.

Conclusion: This work represents the first large-scale, real-world dataset and pipeline from India on autism-focused emotion analysis using social robotics, contributing an essential foundation for future personalized assistive technologies.

Abstract: Understanding emotional responses in children with Autism Spectrum Disorder (ASD) during social interaction remains a critical challenge in both developmental psychology and human-robot interaction. This study presents a novel deep learning pipeline for emotion recognition in autistic children in response to a name-calling event by a humanoid robot (NAO), under controlled experimental settings. The dataset comprises of around 50,000 facial frames extracted from video recordings of 15 children with ASD. A hybrid model combining a fine-tuned ResNet-50-based Convolutional Neural Network (CNN) and a three-layer Graph Convolutional Network (GCN) trained on both visual and geometric features extracted from MediaPipe FaceMesh landmarks. Emotions were probabilistically labeled using a weighted ensemble of two models: DeepFace's and FER, each contributing to soft-label generation across seven emotion classes. Final classification leveraged a fused embedding optimized via Kullback-Leibler divergence. The proposed method demonstrates robust performance in modeling subtle affective responses and offers significant promise for affective profiling of ASD children in clinical and therapeutic human-robot interaction contexts, as the pipeline effectively captures micro emotional cues in neurodivergent children, addressing a major gap in autism-specific HRI research. This work represents the first such large-scale, real-world dataset and pipeline from India on autism-focused emotion analysis using social robotics, contributing an essential foundation for future personalized assistive technologies.

</details>


### [109] [CineLOG: A Training Free Approach for Cinematic Long Video Generation](https://arxiv.org/abs/2512.12209)
*Zahra Dehghanian,Morteza Abolghasemi,Hamid Beigy,Hamid R. Rabiee*

Main category: cs.CV

TL;DR: CineLOG introduces a new dataset of 5,000 high-quality video clips with detailed cinematic annotations and a novel pipeline for controllable video synthesis that outperforms SOTA models.


<details>
  <summary>Details</summary>
Motivation: Current video synthesis models struggle with fine-grained control beyond textual prompts, especially for cinematic attributes like camera trajectory and genre. Existing datasets have issues with data imbalance, noisy labels, and simulation-to-real gaps.

Method: 1) Created CineLOG dataset with 5,000 balanced, uncut video clips annotated with scene descriptions, explicit camera instructions (17 movements), and genre labels (15 genres). 2) Developed a novel pipeline that decouples text-to-video generation into four easier stages. 3) Introduced Trajectory Guided Transition Module for smooth spatio-temporal interpolation in multi-shot sequences.

Result: Extensive human evaluations show the pipeline significantly outperforms state-of-the-art end-to-end T2V models in adhering to specific camera and screenplay instructions while maintaining professional visual quality.

Conclusion: CineLOG addresses key limitations in controllable video synthesis through a balanced dataset and modular pipeline approach, enabling better fine-grained control over cinematic attributes like camera movement and genre.

Abstract: Controllable video synthesis is a central challenge in computer vision, yet current models struggle with fine grained control beyond textual prompts, particularly for cinematic attributes like camera trajectory and genre. Existing datasets often suffer from severe data imbalance, noisy labels, or a significant simulation to real gap. To address this, we introduce CineLOG, a new dataset of 5,000 high quality, balanced, and uncut video clips. Each entry is annotated with a detailed scene description, explicit camera instructions based on a standard cinematic taxonomy, and genre label, ensuring balanced coverage across 17 diverse camera movements and 15 film genres. We also present our novel pipeline designed to create this dataset, which decouples the complex text to video (T2V) generation task into four easier stages with more mature technology. To enable coherent, multi shot sequences, we introduce a novel Trajectory Guided Transition Module that generates smooth spatio-temporal interpolation. Extensive human evaluations show that our pipeline significantly outperforms SOTA end to end T2V models in adhering to specific camera and screenplay instructions, while maintaining professional visual quality. All codes and data are available at https://cine-log.pages.dev.

</details>


### [110] [Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking](https://arxiv.org/abs/2512.12218)
*Rheeya Uppaal,Phu Mon Htut,Min Bai,Nikolaos Pappas,Zheng Qi*

Main category: cs.CV

TL;DR: A framework to evaluate and improve visual faithfulness in reasoning chains of vision-language models, using step-level decomposition and self-reflection without training.


<details>
  <summary>Details</summary>
Motivation: Current VLMs can generate correct answers through visually unfaithful reasoning steps or fail despite faithful reasoning, but standard accuracy metrics cannot distinguish these failure modes, creating a need for evaluating visual faithfulness of reasoning chains.

Method: Proposes a training-free framework that decomposes reasoning chains into perception vs reasoning steps, uses off-the-shelf VLM judges for step-level faithfulness evaluation, and implements a lightweight self-reflection procedure to detect and regenerate unfaithful perception steps.

Result: The method reduces Unfaithful Perception Rate while preserving final-answer accuracy across multiple reasoning-trained VLMs and perception-heavy benchmarks, improving multimodal reasoning reliability.

Conclusion: Visual faithfulness is a crucial evaluation dimension for VLMs, and the proposed framework enables both assessment and improvement of reasoning chain reliability without requiring additional training data.

Abstract: Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.

</details>


### [111] [Fine-Grained Zero-Shot Learning with Attribute-Centric Representations](https://arxiv.org/abs/2512.12219)
*Zhi Chen,Jingcai Guo,Taotao Cai,Yuxiang Cai*

Main category: cs.CV

TL;DR: ACR framework uses attribute disentanglement with mixture-of-experts to solve attribute entanglement in zero-shot fine-grained recognition, achieving SOTA on CUB, AwA2, and SUN datasets.


<details>
  <summary>Details</summary>
Motivation: Fine-grained recognition requires distinguishing subtle visual differences, but conventional models suffer from attribute entanglement where distinct attributes (color, shape, texture) collapse into single embeddings, masking critical distinctions. Post-hoc solutions fail because they operate on already-mixed representations.

Method: AttributeCentric Representations (ACR) framework with two mixture-of-experts components: 1) Mixture of Patch Experts (MoPE) with dual-level routing to conditionally dispatch image patches to specialized experts, ensuring coherent attribute families are processed by dedicated experts; 2) Mixture of Attribute Experts (MoAE) head projects expert-refined features into sparse, part-aware attribute maps for robust zero-shot classification.

Result: Achieves consistent state-of-the-art results on zero-shot learning benchmark datasets CUB, AwA2, and SUN.

Conclusion: ACR effectively tackles attribute entanglement by imposing attribute disentanglement during representation learning through specialized mixture-of-experts architecture, enabling better transfer of visual-attribute relationships from seen to unseen classes.

Abstract: Recognizing unseen fine-grained categories demands a model that can distinguish subtle visual differences. This is typically achieved by transferring visual-attribute relationships from seen classes to unseen classes. The core challenge is attribute entanglement, where conventional models collapse distinct attributes like color, shape, and texture into a single visual embedding. This causes interference that masks these critical distinctions. The post-hoc solutions of previous work are insufficient, as they operate on representations that are already mixed. We propose a zero-shot learning framework that learns AttributeCentric Representations (ACR) to tackle this problem by imposing attribute disentanglement during representation learning. ACR is achieved with two mixture-of-experts components, including Mixture of Patch Experts (MoPE) and Mixture of Attribute Experts (MoAE). First, MoPE is inserted into the transformer using a dual-level routing mechanism to conditionally dispatch image patches to specialized experts. This ensures coherent attribute families are processed by dedicated experts. Finally, the MoAE head projects these expert-refined features into sparse, partaware attribute maps for robust zero-shot classification. On zero-shot learning benchmark datasets CUB, AwA2, and SUN, our ACR achieves consistent state-of-the-art results.

</details>


### [112] [ProImage-Bench: Rubric-Based Evaluation for Professional Image Generation](https://arxiv.org/abs/2512.12220)
*Minheng Ni,Zhengyuan Yang,Yaowen Zhang,Linjie Li,Chung-Ching Lin,Kevin Lin,Zhendong Wang,Xiaofei Wang,Shujie Liu,Lei Zhang,Wangmeng Zuo,Lijuan Wang*

Main category: cs.CV

TL;DR: ProImage-Bench: A rubric-based benchmark for evaluating professional image generation models on scientific illustrations, with automated evaluation and iterative refinement capabilities.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models struggle with generating scientifically precise, information-dense illustrations from technical descriptions, focusing more on visual plausibility than factual correctness. There's a need for rigorous evaluation of professional image generation capabilities.

Method: Created ProImage-Bench with 654 real textbook/technical report figures, detailed instructions, and hierarchical rubrics (6,076 criteria, 44,131 binary checks). Used LMMs to derive rubrics from surrounding text and reference figures, with automated LMM-based evaluation using principled penalty scheme. Tested iterative refinement by feeding failed checks back into editing models.

Result: Best base model achieved only 0.791 rubric accuracy and 0.553 criterion score, showing significant gaps in scientific fidelity. Iterative refinement boosted a strong generator from 0.653 to 0.865 rubric accuracy and 0.388 to 0.697 criterion score.

Conclusion: ProImage-Bench provides both a rigorous diagnostic tool for professional image generation and a scalable supervision signal for improving specification-faithful scientific illustrations, addressing the gap between visual plausibility and scientific precision.

Abstract: We study professional image generation, where a model must synthesize information-dense, scientifically precise illustrations from technical descriptions rather than merely produce visually plausible pictures. To quantify the progress, we introduce ProImage-Bench, a rubric-based benchmark that targets biology schematics, engineering/patent drawings, and general scientific diagrams. For 654 figures collected from real textbooks and technical reports, we construct detailed image instructions and a hierarchy of rubrics that decompose correctness into 6,076 criteria and 44,131 binary checks. Rubrics are derived from surrounding text and reference figures using large multimodal models, and are evaluated by an automated LMM-based judge with a principled penalty scheme that aggregates sub-question outcomes into interpretable criterion scores. We benchmark several representative text-to-image models on ProImage-Bench and find that, despite strong open-domain performance, the best base model reaches only 0.791 rubric accuracy and 0.553 criterion score overall, revealing substantial gaps in fine-grained scientific fidelity. Finally, we show that the same rubrics provide actionable supervision: feeding failed checks back into an editing model for iterative refinement boosts a strong generator from 0.653 to 0.865 in rubric accuracy and from 0.388 to 0.697 in criterion score. ProImage-Bench thus offers both a rigorous diagnostic for professional image generation and a scalable signal for improving specification-faithful scientific illustrations.

</details>


### [113] [Comparison of different segmentation algorithms on brain volume and fractal dimension in infant brain MRIs](https://arxiv.org/abs/2512.12222)
*Nathalie Alexander,Arnaud Gucciardi,Umberto Michelucci*

Main category: cs.CV

TL;DR: SynthSeg outperforms SamSeg for infant brain MRI segmentation, providing more accurate volumetric and fractal dimension estimates, though segmentation-related uncertainty remains for small morphological differences.


<details>
  <summary>Details</summary>
Motivation: Infant brain MRI segmentation is challenging due to ongoing myelination and reduced tissue contrast, making accurate automated segmentation essential for quantifying developmental changes in structure and complexity.

Method: Systematic comparison of SynthSeg and SamSeg segmentation methods on the Baby Open Brains dataset (71 scans, 1-9 months) using Dice, IoU, Hausdorff distance, and Normalised Mutual Information metrics against expert annotations, with evaluation of volumetric and fractal dimension estimates.

Result: SynthSeg outperformed SamSeg across all quality metrics (mean Dice > 0.8 for major regions), provided volumetric estimates closely matching manual reference (mean +4%), while SamSeg systematically overestimated ventricular and whole-brain volumes (mean +76%). Segmentation accuracy improved with age, and fractal dimension analyses showed significant regional differences between SynthSeg and expert segmentations.

Conclusion: SynthSeg provides the most reliable volumetric and fractal dimension results for pediatric MRI, but small morphological differences in volume and FD should be interpreted cautiously due to segmentation-related uncertainty, as segmentation bias directly affects FD estimation.

Abstract: Accurate segmentation of infant brain MRI is essential for quantifying developmental changes in structure and complexity. However, ongoing myelination and reduced tissue contrast make automated segmentation particularly challenging. This study systematically compared segmentation accuracy and its impact on volumetric and fractal dimension (FD) estimates in infant brain MRI using the Baby Open Brains (BOB) dataset (71 scans, 1-9 months). Two methods, SynthSeg and SamSeg, were evaluated against expert annotations using Dice, Intersection over Union, 95th-percentile Hausdorff distance, and Normalised Mutual Information. SynthSeg outperformed SamSeg across all quality metrics (mean Dice > 0.8 for major regions) and provided volumetric estimates closely matching the manual reference (mean +4% [-28% - 71%]). SamSeg systematically overestimated ventricular and whole-brain volumes (mean +76% [-12% - 190%]). Segmentation accuracy improved with age, consistent with increasing tissue contrast during myelination. Fractal dimension a(FD) nalyses revealed significant regional differences between SynthSeg and expert segmentations, and Bland-Altman limits of agreement indicated that segmentation-related FD variability exceeded most group differences reported in developmental cohorts. Volume and FD deviations were positively correlated across structures, indicating that segmentation bias directly affects FD estimation. Overall, SynthSeg provided the most reliable volumetric and FD results for paediatric MRI, yet small morphological differences in volume and FD should be interpreted with caution due to segmentation-related uncertainty.

</details>


### [114] [Ultra-Low Bitrate Perceptual Image Compression with Shallow Encoder](https://arxiv.org/abs/2512.12229)
*Tianyu Zhang,Dong Liu,Chang Wen Chen*

Main category: cs.CV

TL;DR: AEIC is a novel ultra-low bitrate image compression framework using shallow encoders and diffusion decoders to achieve both encoding simplicity and decoding quality at below 0.05 bpp.


<details>
  <summary>Details</summary>
Motivation: Existing ultra-low bitrate compression methods rely on heavy pretrained encoders (VAEs, tokenizers) that are unsuitable for weak edge devices with limited computation and bandwidth. There's a need for compression frameworks that work on resource-constrained sender devices while maintaining high reconstruction quality.

Method: Proposes Asymmetric Extreme Image Compression (AEIC) with shallow encoder networks and a one-step diffusion decoder. Uses dual-side feature distillation to transfer knowledge from moderate encoders to shallow variants, enhancing efficiency while maintaining reconstruction quality at extreme bitrates.

Result: AEIC outperforms existing methods on rate-distortion-perception performance at ultra-low bitrates (<0.05 bpp). Achieves exceptional encoding efficiency of 35.8 FPS on 1080P images while maintaining competitive decoding speed compared to existing methods.

Conclusion: AEIC successfully demonstrates that shallow encoders can be effectively used for ultra-low bitrate compression, providing a practical solution for edge devices by balancing encoding simplicity with high-fidelity decoding quality.

Abstract: Ultra-low bitrate image compression (below 0.05 bits per pixel) is increasingly critical for bandwidth-constrained and computation-limited encoding scenarios such as edge devices. Existing frameworks typically rely on large pretrained encoders (e.g., VAEs or tokenizer-based models) and perform transform coding within their generative latent space. While these approaches achieve impressive perceptual fidelity, their reliance on heavy encoder networks makes them unsuitable for deployment on weak sender devices. In this work, we explore the feasibility of applying shallow encoders for ultra-low bitrate compression and propose a novel Asymmetric Extreme Image Compression (AEIC) framework that pursues simultaneously encoding simplicity and decoding quality. Specifically, AEIC employs moderate or even shallow encoder networks, while leveraging an one-step diffusion decoder to maintain high-fidelity and high-realism reconstructions under extreme bitrates. To further enhance the efficiency of shallow encoders, we design a dual-side feature distillation scheme that transfers knowledge from AEIC with moderate encoders to its shallow encoder variants. Experiments demonstrate that AEIC not only outperforms existing methods on rate-distortion-perception performance at ultra-low bitrates, but also delivers exceptional encoding efficiency for 35.8 FPS on 1080P input images, while maintaining competitive decoding speed compared to existing methods.

</details>


### [115] [Moment and Highlight Detection via MLLM Frame Segmentation](https://arxiv.org/abs/2512.12246)
*I Putu Andika Bagas Jiwanta,Ayu Purwarianti*

Main category: cs.CV

TL;DR: A novel method that applies segmentation objectives directly on LLM output tokens for video moment retrieval and highlight detection, using binary character sequences as frame-level probabilities.


<details>
  <summary>Details</summary>
Motivation: Existing transformer-based methods unify video moment retrieval and highlight detection, while MLLM approaches use text generation but lack direct gradients for frame-level predictions. RL methods address this but have limitations. The paper proposes a more direct approach using segmentation losses on LLM tokens.

Method: Feed LLM with fixed number of frames and prompt to output continuous "0"/"1" characters (one per frame). "0" acts as background probability, "1" as foreground probability. Train with segmentation losses on probabilities plus causal LM loss. At inference, use beam search to generate sequences (moments) and logits (saliency scores).

Result: Achieved strong highlight detection (56.74 HIT@1) on QVHighlights with only 25 frames (less than half of comparable methods). Also scored above baseline (35.28 MAP) for moment retrieval. Segmentation losses provide stable complementary learning signal even when causal LM loss plateaus.

Conclusion: The proposed method effectively bridges the gap between text-based generation and frame-level predictions by leveraging LLM's language capabilities while applying segmentation objectives directly on output tokens, achieving competitive results with greater efficiency.

Abstract: Detecting video moments and highlights from natural-language queries have been unified by transformer-based methods. Other works use generative Multimodal LLM (MLLM) to predict moments and/or highlights as text timestamps, utilizing its reasoning capability. While effective, text-based generation cannot provide direct gradients for frame-level predictions because the model only emits language tokens. Although recent Reinforcement Learning (RL) methods attempt to address the issue, we propose a novel approach by applying segmentation objectives directly on the LLM's output tokens. The LLM is fed with a fixed number of frames alongside a prompt that enforces it to output a sequence of continuous "0" and/or "1" characters, with one character per frame. The "0"/"1" characters benefit from the LLM's inherent language capability while also acting as background and foreground probabilities, respectively. Training employs segmentation losses on the probabilities alongside a normal causal LM loss. At inference, beam search generates sequence and logits, acting as moments and saliency scores, respectively. Despite sampling only 25 frames -- less than half of comparable methods -- our method achieved strong highlight detection (56.74 HIT@1) on QVHighlights. Additionally, our efficient method scores above the baseline (35.28 MAP) for moment retrieval. Empirically, segmentation losses provide a stable complementary learning signal even when the causal LM loss plateaus.

</details>


### [116] [MetaTPT: Meta Test-time Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2512.12268)
*Yuqing Lei,Yingjun Du,Yawen Huang,Xiantong Zhen,Ling Shao*

Main category: cs.CV

TL;DR: MetaTPT is a meta-learning framework that learns self-supervised auxiliary tasks to guide test-time prompt tuning for vision-language models, improving domain adaptation by dynamically generating informative augmentations.


<details>
  <summary>Details</summary>
Motivation: Vision-language models like CLIP have strong zero-shot capabilities but remain sensitive to domain shifts at test time. Existing test-time prompt tuning methods use fixed augmentations that may fail in challenging domain adaptation scenarios.

Method: MetaTPT uses a meta-learning framework with dual-loop optimization: inner loop learns a self-supervised auxiliary task that generates parameterized augmentations for each sample, outer loop performs prompt tuning by enforcing consistency across these learned augmentations.

Result: Extensive experiments show MetaTPT achieves state-of-the-art performance on domain generalization and cross-dataset benchmarks, demonstrating improved test-time adaptation under domain shifts.

Conclusion: By coupling augmentation learning with prompt tuning through meta-learning, MetaTPT enables more expressive transformations that capture essential features in target domains, significantly enhancing test-time adaptation for vision-language models.

Abstract: Vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization but remain sensitive to domain shifts at test time. Test-time prompt tuning (TPT) mitigates this issue by adapting prompts with fixed augmentations, which may falter in more challenging settings. In this work, we propose Meta Test-Time Prompt Tuning (MetaTPT), a meta-learning framework that learns a self-supervised auxiliary task to guide test-time prompt tuning. The auxiliary task dynamically learns parameterized augmentations for each sample, enabling more expressive transformations that capture essential features in target domains. MetaTPT adopts a dual-loop optimization paradigm: an inner loop learns a self-supervised task that generates informative views, while the outer loop performs prompt tuning by enforcing consistency across these views. By coupling augmentation learning with prompt tuning, MetaTPT improves test-time adaptation under domain shifts. Extensive experiments demonstrate that MetaTPT achieves state-of-the-art performance on domain generalization and cross-dataset benchmarks.

</details>


### [117] [Feature Aggregation for Efficient Continual Learning of Complex Facial Expressions](https://arxiv.org/abs/2512.12277)
*Thibault Geoffroy,Myriam Maumy,Lionel Prevost*

Main category: cs.CV

TL;DR: Hybrid framework for continual facial expression recognition combining deep features and Action Units with Bayesian Gaussian Mixture Models to prevent catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: AI systems need emotion recognition for effective human-computer interaction, but facial expression recognition requires continuous learning without forgetting prior knowledge due to dynamic and culturally nuanced nature of emotions.

Method: Proposes hybrid framework integrating deep convolutional features and facial Action Units (AUs) from FACS, modeled through Bayesian Gaussian Mixture Models (BGMMs) for lightweight probabilistic continual learning.

Result: Using CFEE dataset, model learns basic expressions first then progressively recognizes compound expressions with improved accuracy, stronger knowledge retention, and reduced forgetting.

Conclusion: Framework contributes to emotionally intelligent AI systems with applications in education, healthcare, and adaptive user interfaces by enabling continual learning without catastrophic forgetting.

Abstract: As artificial intelligence (AI) systems become increasingly embedded in our daily life, the ability to recognize and adapt to human emotions is essential for effective human-computer interaction. Facial expression recognition (FER) provides a primary channel for inferring affective states, but the dynamic and culturally nuanced nature of emotions requires models that can learn continuously without forgetting prior knowledge. In this work, we propose a hybrid framework for FER in a continual learning setting that mitigates catastrophic forgetting. Our approach integrates two complementary modalities: deep convolutional features and facial Action Units (AUs) derived from the Facial Action Coding System (FACS). The combined representation is modelled through Bayesian Gaussian Mixture Models (BGMMs), which provide a lightweight, probabilistic solution that avoids retraining while offering strong discriminative power. Using the Compound Facial Expression of Emotion (CFEE) dataset, we show that our model can first learn basic expressions and then progressively recognize compound expressions. Experiments demonstrate improved accuracy, stronger knowledge retention, and reduced forgetting. This framework contributes to the development of emotionally intelligent AI systems with applications in education, healthcare, and adaptive user interfaces.

</details>


### [118] [Cognitive-YOLO: LLM-Driven Architecture Synthesis from First Principles of Data for Object Detection](https://arxiv.org/abs/2512.12281)
*Jiahao Zhao*

Main category: cs.CV

TL;DR: Cognitive-YOLO is an LLM-driven framework that generates object detection architectures directly from dataset characteristics, outperforming traditional NAS and LLM-based iterative approaches through data-driven reasoning.


<details>
  <summary>Details</summary>
Motivation: Traditional manual architecture design is labor-intensive, while Neural Architecture Search (NAS) is computationally expensive. Existing LLM-based approaches function as iterative optimizers rather than generating architectures from holistic data understanding.

Method: Three-stage framework: 1) Analysis module extracts dataset meta-features (object scale distribution, scene density), 2) LLM reasons on these features with RAG-retrieved SOTA components to synthesize architecture in Neural Architecture Description Language (NADL), 3) Compiler instantiates deployable model.

Result: Extensive experiments on five diverse object detection datasets show Cognitive-YOLO generates superior architectures with highly competitive performance and superior performance-per-parameter trade-off compared to strong baselines.

Conclusion: LLM's data-driven reasoning is the primary performance driver, proving that deep understanding of data "first principles" is more critical for superior architecture design than simply retrieving SOTA components.

Abstract: Designing high-performance object detection architectures is a complex task, where traditional manual design is time-consuming and labor-intensive, and Neural Architecture Search (NAS) is computationally prohibitive. While recent approaches using Large Language Models (LLMs) show promise, they often function as iterative optimizers within a search loop, rather than generating architectures directly from a holistic understanding of the data. To address this gap, we propose Cognitive-YOLO, a novel framework for LLM-driven architecture synthesis that generates network configurations directly from the intrinsic characteristics of the dataset. Our method consists of three stages: first, an analysis module extracts key meta-features (e.g., object scale distribution and scene density) from the target dataset; second, the LLM reasons upon these features, augmented with state-of-the-art components retrieved via Retrieval-Augmented Generation (RAG), to synthesize the architecture into a structured Neural Architecture Description Language (NADL); finally, a compiler instantiates this description into a deployable model. Extensive experiments on five diverse object detection datasets demonstrate that our proposed Cognitive-YOLO consistently generates superior architectures, achieving highly competitive performance and demonstrating a superior performance-per-parameter trade-off compared to strong baseline models across multiple benchmarks. Crucially, our ablation studies prove that the LLM's data-driven reasoning is the primary driver of performance, demonstrating that a deep understanding of data "first principles" is more critical for achieving a superior architecture than simply retrieving SOTA components.

</details>


### [119] [RealDrag: The First Dragging Benchmark with Real Target Image](https://arxiv.org/abs/2512.12287)
*Ahmad Zafarani,Zahra Dehghanian,Mohammadreza Davoodi,Mohsen Shadroo,MohammadAmin Fazli,Hamid R. Rabiee*

Main category: cs.CV

TL;DR: RealDrag introduces the first comprehensive benchmark for point-based image editing with ground truth target images, plus four novel metrics for objective evaluation of drag-based editing models.


<details>
  <summary>Details</summary>
Motivation: Current evaluation of drag-based image editing models is unreliable due to lack of standardized benchmarks, inconsistent evaluation protocols, and absence of datasets with ground truth target images, making objective comparisons difficult.

Method: Created RealDrag benchmark with 400+ human-annotated samples from diverse video sources, providing source/target images, handle/target points, editable region masks, and descriptive captions. Proposed four novel metrics: Semantical Distance (SeD), Outer Mask Preserving Score (OMPS), Inner Patch Preserving Score (IPPS), and Directional Similarity (DiS).

Result: Conducted first large-scale systematic analysis of the field by evaluating 17 state-of-the-art models, revealing clear trade-offs among current approaches and establishing a robust, reproducible baseline for future research.

Conclusion: RealDrag provides the first comprehensive benchmark with ground truth data and objective metrics for drag-based image editing, enabling reliable evaluation and comparison of methods, with dataset and toolkit to be made publicly available.

Abstract: The evaluation of drag based image editing models is unreliable due to a lack of standardized benchmarks and metrics. This ambiguity stems from inconsistent evaluation protocols and, critically, the absence of datasets containing ground truth target images, making objective comparisons between competing methods difficult. To address this, we introduce \textbf{RealDrag}, the first comprehensive benchmark for point based image editing that includes paired ground truth target images. Our dataset contains over 400 human annotated samples from diverse video sources, providing source/target images, handle/target points, editable region masks, and descriptive captions for both the image and the editing action.
  We also propose four novel, task specific metrics: Semantical Distance (SeD), Outer Mask Preserving Score (OMPS), Inner Patch Preserving Score (IPPS), and Directional Similarity (DiS). These metrics are designed to quantify pixel level matching fidelity, check preservation of non edited (out of mask) regions, and measure semantic alignment with the desired task. Using this benchmark, we conduct the first large scale systematic analysis of the field, evaluating 17 SOTA models. Our results reveal clear trade offs among current approaches and establish a robust, reproducible baseline to guide future research. Our dataset and evaluation toolkit will be made publicly available.

</details>


### [120] [GrowTAS: Progressive Expansion from Small to Large Subnets for Efficient ViT Architecture Search](https://arxiv.org/abs/2512.12296)
*Hyunju Lee,Youngmin Oh,Jeimin Jeon,Donghyeon Baek,Bumsub Ham*

Main category: cs.CV

TL;DR: GrowTAS is a progressive training framework for Transformer Architecture Search that starts with small subnets and gradually incorporates larger ones to reduce interference and stabilize training, outperforming existing TAS methods.


<details>
  <summary>Details</summary>
Motivation: Existing TAS methods use weight-sharing supernets where all subnets share weights, causing interference that severely degrades smaller subnets. The authors found that well-trained small subnets can serve as a good foundation for training larger ones.

Method: Progressive training framework (GrowTAS) that begins with training small subnets and gradually incorporates larger ones to reduce interference and stabilize training. Also introduces GrowTAS+ that fine-tunes only a subset of weights to further enhance large subnet performance.

Result: Extensive experiments on ImageNet and transfer learning benchmarks (CIFAR-10/100, Flowers, CARS, INAT-19) demonstrate effectiveness over current TAS methods.

Conclusion: The progressive training approach reduces interference in weight-sharing supernets and stabilizes training, enabling better performance across various vision transformer architectures and transfer learning tasks.

Abstract: Transformer architecture search (TAS) aims to automatically discover efficient vision transformers (ViTs), reducing the need for manual design. Existing TAS methods typically train an over-parameterized network (i.e., a supernet) that encompasses all candidate architectures (i.e., subnets). However, all subnets share the same set of weights, which leads to interference that degrades the smaller subnets severely. We have found that well-trained small subnets can serve as a good foundation for training larger ones. Motivated by this, we propose a progressive training framework, dubbed GrowTAS, that begins with training small subnets and incorporate larger ones gradually. This enables reducing the interference and stabilizing a training process. We also introduce GrowTAS+ that fine-tunes a subset of weights only to further enhance the performance of large subnets. Extensive experiments on ImageNet and several transfer learning benchmarks, including CIFAR-10/100, Flowers, CARS, and INAT-19, demonstrate the effectiveness of our approach over current TAS methods

</details>


### [121] [From Human Intention to Action Prediction: A Comprehensive Benchmark for Intention-driven End-to-End Autonomous Driving](https://arxiv.org/abs/2512.12302)
*Huan Zheng,Yucheng Zhou,Tianyi Yan,Jiayi Su,Hongjun Chen,Dubing Chen,Wencheng Han,Runzhou Tao,Zhongying Qiu,Jianfei Yang,Jianbing Shen*

Main category: cs.CV

TL;DR: Intention-Drive is a new benchmark for evaluating autonomous driving systems' ability to understand and fulfill high-level human intentions rather than just following low-level steering commands.


<details>
  <summary>Details</summary>
Motivation: Current autonomous driving systems only follow simple steering commands, but truly intelligent autonomy requires understanding high-level human intentions. There's no standardized benchmark to measure progress on this complex task of translating abstract intentions into driving actions.

Method: The authors introduce Intention-Drive benchmark with two core components: (1) a dataset of complex driving scenarios paired with natural language intentions, and (2) a novel evaluation protocol using Intent Success Rate (ISR) that measures semantic fulfillment of human goals beyond geometric accuracy.

Result: Extensive evaluation of baseline models on Intention-Drive reveals significant performance deficits, showing that current models struggle to achieve the comprehensive scene and intention understanding needed for this advanced task.

Conclusion: The paper introduces a critical benchmark to drive progress toward intention-fulfilling autonomous driving systems, highlighting the gap between current command-following approaches and the need for true intention understanding.

Abstract: Current end-to-end autonomous driving systems operate at a level of intelligence akin to following simple steering commands. However, achieving genuinely intelligent autonomy requires a paradigm shift: moving from merely executing low-level instructions to understanding and fulfilling high-level, abstract human intentions. This leap from a command-follower to an intention-fulfiller, as illustrated in our conceptual framework, is hindered by a fundamental challenge: the absence of a standardized benchmark to measure and drive progress on this complex task. To address this critical gap, we introduce Intention-Drive, the first comprehensive benchmark designed to evaluate the ability to translate high-level human intent into safe and precise driving actions. Intention-Drive features two core contributions: (1) a new dataset of complex scenarios paired with corresponding natural language intentions, and (2) a novel evaluation protocol centered on the Intent Success Rate (ISR), which assesses the semantic fulfillment of the human's goal beyond simple geometric accuracy. Through an extensive evaluation of a spectrum of baseline models on Intention-Drive, we reveal a significant performance deficit, showing that the baseline model struggle to achieve the comprehensive scene and intention understanding required for this advanced task.

</details>


### [122] [OMUDA: Omni-level Masking for Unsupervised Domain Adaptation in Semantic Segmentation](https://arxiv.org/abs/2512.12303)
*Yang Ou,Xiongwei Zhao,Xinye Yang,Yihan Wang,Yicheng Di,Rong Yuan,Xieyuanli Chen,Xu Zhu*

Main category: cs.CV

TL;DR: OMUDA is a hierarchical masking framework for unsupervised domain adaptation in semantic segmentation that addresses cross-domain contextual ambiguity, inconsistent features, and noisy pseudo-labels through three masking strategies at different representation levels.


<details>
  <summary>Details</summary>
Motivation: Existing UDA methods struggle with domain gaps due to cross-domain contextual ambiguity, inconsistent feature representations, and class-wise pseudo-label noise, limiting their effectiveness in semantic segmentation tasks.

Method: Proposes OMUDA with three hierarchical masking strategies: 1) Context-Aware Masking (CAM) for foreground-background distinction, 2) Feature Distillation Masking (FDM) for robust feature learning via pre-trained model knowledge transfer, and 3) Class Decoupling Masking (CDM) for mitigating noisy pseudo-labels through class-wise uncertainty modeling.

Result: Extensive experiments on SYNTHIA->Cityscapes and GTA5->Cityscapes benchmarks show OMUDA achieves state-of-the-art results with average 7% improvement, and can be seamlessly integrated into existing UDA methods.

Conclusion: OMUDA's hierarchical masking paradigm effectively reduces domain shift at contextual, representational, and categorical levels, providing a unified solution that outperforms existing approaches for cross-domain semantic segmentation.

Abstract: Unsupervised domain adaptation (UDA) enables semantic segmentation models to generalize from a labeled source domain to an unlabeled target domain. However, existing UDA methods still struggle to bridge the domain gap due to cross-domain contextual ambiguity, inconsistent feature representations, and class-wise pseudo-label noise. To address these challenges, we propose Omni-level Masking for Unsupervised Domain Adaptation (OMUDA), a unified framework that introduces hierarchical masking strategies across distinct representation levels. Specifically, OMUDA comprises: 1) a Context-Aware Masking (CAM) strategy that adaptively distinguishes foreground from background to balance global context and local details; 2) a Feature Distillation Masking (FDM) strategy that enhances robust and consistent feature learning through knowledge transfer from pre-trained models; and 3) a Class Decoupling Masking (CDM) strategy that mitigates the impact of noisy pseudo-labels by explicitly modeling class-wise uncertainty. This hierarchical masking paradigm effectively reduces the domain shift at the contextual, representational, and categorical levels, providing a unified solution beyond existing approaches. Extensive experiments on multiple challenging cross-domain semantic segmentation benchmarks validate the effectiveness of OMUDA. Notably, on the SYNTHIA->Cityscapes and GTA5->Cityscapes tasks, OMUDA can be seamlessly integrated into existing UDA methods and consistently achieving state-of-the-art results with an average improvement of 7%.

</details>


### [123] [MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding](https://arxiv.org/abs/2512.12307)
*Benjamin Beilharz,Thomas S. A. Wallis*

Main category: cs.CV

TL;DR: MRD uses differentiable rendering to find 3D scenes that produce identical neural activations in vision models, probing their implicit 3D understanding through physically-grounded metamers.


<details>
  <summary>Details</summary>
Motivation: Deep vision models are difficult to interpret, but are assumed to develop implicit 3D scene understanding. Current evaluation methods lack physical grounding, making it hard to isolate which specific 3D scene properties (shape, material, lighting) models are sensitive to.

Method: MRD (metamers rendered differentiably) uses physically based differentiable rendering to find 3D scene parameters that are physically different but produce identical model activations. It optimizes 3D scene parameters (geometry/shape and BRDF/material) while holding other factors constant to probe model sensitivity to specific physical properties.

Result: The method shows high similarity in model activation between target and optimized scenes, with varying visual reconstruction results. It successfully isolates model sensitivity to specific physical attributes like object shape and material properties.

Conclusion: MRD provides a physically-grounded framework for analyzing vision models' implicit 3D understanding, enabling systematic investigation of which physical scene parameters drive model responses. This approach holds promise for advancing interpretability in both computer and human vision research.

Abstract: While deep learning methods have achieved impressive success in many vision benchmarks, it remains difficult to understand and explain the representations and decisions of these models. Though vision models are typically trained on 2D inputs, they are often assumed to develop an implicit representation of the underlying 3D scene (for example, showing tolerance to partial occlusion, or the ability to reason about relative depth). Here, we introduce MRD (metamers rendered differentiably), an approach that uses physically based differentiable rendering to probe vision models' implicit understanding of generative 3D scene properties, by finding 3D scene parameters that are physically different but produce the same model activation (i.e. are model metamers). Unlike previous pixel-based methods for evaluating model representations, these reconstruction results are always grounded in physical scene descriptions. This means we can, for example, probe a model's sensitivity to object shape while holding material and lighting constant. As a proof-of-principle, we assess multiple models in their ability to recover scene parameters of geometry (shape) and bidirectional reflectance distribution function (material). The results show high similarity in model activation between target and optimized scenes, with varying visual results. Qualitatively, these reconstructions help investigate the physical scene attributes to which models are sensitive or invariant. MRD holds promise for advancing our understanding of both computer and human vision by enabling analysis of how physical scene parameters drive changes in model responses.

</details>


### [124] [WeDetect: Fast Open-Vocabulary Object Detection as Retrieval](https://arxiv.org/abs/2512.12309)
*Shenghao Fu,Yukun Su,Fengyun Rao,Jing Lyu,Xiaohua Xie,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: WeDetect is a family of open-vocabulary object detection models using a retrieval-based approach that achieves state-of-the-art performance while enabling fast object retrieval and integration with large multimodal models.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient open-vocabulary object detection system that leverages retrieval philosophy for faster inference, versatility, and integration with other applications like object retrieval and referring expression comprehension.

Method: Uses a dual-tower architecture without cross-modal fusion layers, treating recognition as a retrieval problem. Introduces three variants: WeDetect (main detector), WeDetect-Uni (universal proposal generator with frozen detector), and WeDetect-Ref (LMM-based classifier for complex referring expressions).

Result: Achieves state-of-the-art performance across 15 benchmarks with real-time inference. Enables fast backtracking of historical data through object retrieval and handles complex referring expressions efficiently.

Conclusion: The WeDetect family successfully unifies detection, proposal generation, object retrieval, and referring expression comprehension under a coherent retrieval framework, demonstrating the advantages of non-fusion approaches in open-vocabulary object detection.

Abstract: Open-vocabulary object detection aims to detect arbitrary classes via text prompts. Methods without cross-modal fusion layers (non-fusion) offer faster inference by treating recognition as a retrieval problem, \ie, matching regions to text queries in a shared embedding space. In this work, we fully explore this retrieval philosophy and demonstrate its unique advantages in efficiency and versatility through a model family named WeDetect: (1) State-of-the-art performance. WeDetect is a real-time detector with a dual-tower architecture. We show that, with well-curated data and full training, the non-fusion WeDetect surpasses other fusion models and establishes a strong open-vocabulary foundation. (2) Fast backtrack of historical data. WeDetect-Uni is a universal proposal generator based on WeDetect. We freeze the entire detector and only finetune an objectness prompt to retrieve generic object proposals across categories. Importantly, the proposal embeddings are class-specific and enable a new application, object retrieval, supporting retrieval objects in historical data. (3) Integration with LMMs for referring expression comprehension (REC). We further propose WeDetect-Ref, an LMM-based object classifier to handle complex referring expressions, which retrieves target objects from the proposal list extracted by WeDetect-Uni. It discards next-token prediction and classifies objects in a single forward pass. Together, the WeDetect family unifies detection, proposal generation, object retrieval, and REC under a coherent retrieval framework, achieving state-of-the-art performance across 15 benchmarks with high inference efficiency.

</details>


### [125] [Unified Control for Inference-Time Guidance of Denoising Diffusion Models](https://arxiv.org/abs/2512.12339)
*Maurya Goyal,Anuj Singh,Hadi Jamali-Rad*

Main category: cs.CV

TL;DR: UniCoDe is a universal algorithm that unifies sampling-based and gradient-guided methods for aligning diffusion models with downstream objectives, improving efficiency and reward-prior trade-offs.


<details>
  <summary>Details</summary>
Motivation: Current inference-time training-free alignment methods for diffusion models are either sampling-based (inefficient for complex rewards) or gradient-guided (limited by differentiable reward approximations). There's a need to combine both approaches for better efficiency and performance.

Method: UniCoDe integrates local gradient signals during sampling, creating a unified framework that brings together the strengths of both sampling-based and gradient-guided methods. It addresses sampling inefficiency while maintaining reward alignment.

Result: Empirical results show UniCoDe remains competitive with state-of-the-art baselines across various tasks, offering better trade-offs between reward alignment and divergence from the diffusion unconditional prior.

Conclusion: UniCoDe successfully unifies sampling and gradient-based guidance for diffusion model alignment, enabling more efficient sampling while maintaining competitive performance across different tasks.

Abstract: Aligning diffusion model outputs with downstream objectives is essential for improving task-specific performance. Broadly, inference-time training-free approaches for aligning diffusion models can be categorized into two main strategies: sampling-based methods, which explore multiple candidate outputs and select those with higher reward signals, and gradient-guided methods, which use differentiable reward approximations to directly steer the generation process. In this work, we propose a universal algorithm, UniCoDe, which brings together the strengths of sampling and gradient-based guidance into a unified framework. UniCoDe integrates local gradient signals during sampling, thereby addressing the sampling inefficiency inherent in complex reward-based sampling approaches. By cohesively combining these two paradigms, UniCoDe enables more efficient sampling while offering better trade-offs between reward alignment and divergence from the diffusion unconditional prior. Empirical results demonstrate that UniCoDe remains competitive with state-of-the-art baselines across a range of tasks. The code is available at https://github.com/maurya-goyal10/UniCoDe

</details>


### [126] [TCLeaf-Net: a transformer-convolution framework with global-local attention for robust in-field lesion-level plant leaf disease detection](https://arxiv.org/abs/2512.12357)
*Zishen Song,Yongjian Zhu,Dong Wang,Hongzhan Liu,Lingyu Jiang,Yongxing Duan,Zehua Zhang,Sihan Li,Jiarui Li*

Main category: cs.CV

TL;DR: TCLeaf-Net: A transformer-convolution hybrid detector for real-field foliar disease detection, achieving 78.2% mAP@50 with reduced computation and memory usage.


<details>
  <summary>Details</summary>
Motivation: Timely and accurate detection of foliar diseases is crucial for crop protection, but real-field conditions present challenges including cluttered backgrounds, domain shifts, and limited lesion-level datasets.

Method: Proposes TCLeaf-Net with three key components: 1) Transformer-convolution module (TCM) for suppressing non-leaf regions, 2) Raw-scale feature recalling and sampling (RSFRS) block to preserve spatial detail, and 3) Deformable alignment block with FPN (DFPN) for multi-scale fusion.

Result: On the Daylily-Leaf dataset's in-field split, TCLeaf-Net improves mAP@50 by 5.4 percentage points to 78.2%, reduces computation by 7.5 GFLOPs and GPU memory by 8.7%, and outperforms recent YOLO and RT-DETR models. Also shows strong performance on other plant disease datasets.

Conclusion: TCLeaf-Net effectively addresses real-field foliar disease detection challenges and demonstrates robust performance and generalizability across multiple plant disease datasets.

Abstract: Timely and accurate detection of foliar diseases is vital for safeguarding crop growth and reducing yield losses. Yet, in real-field conditions, cluttered backgrounds, domain shifts, and limited lesion-level datasets hinder robust modeling. To address these challenges, we release Daylily-Leaf, a paired lesion-level dataset comprising 1,746 RGB images and 7,839 lesions captured under both ideal and in-field conditions, and propose TCLeaf-Net, a transformer-convolution hybrid detector optimized for real-field use. TCLeaf-Net is designed to tackle three major challenges. To mitigate interference from complex backgrounds, the transformer-convolution module (TCM) couples global context with locality-preserving convolution to suppress non-leaf regions. To reduce information loss during downsampling, the raw-scale feature recalling and sampling (RSFRS) block combines bilinear resampling and convolution to preserve fine spatial detail. To handle variations in lesion scale and feature shifts, the deformable alignment block with FPN (DFPN) employs offset-based alignment and multi-receptive-field perception to strengthen multi-scale fusion. Experimental results show that on the in-field split of the Daylily-Leaf dataset, TCLeaf-Net improves mAP@50 by 5.4 percentage points over the baseline model, reaching 78.2\%, while reducing computation by 7.5 GFLOPs and GPU memory usage by 8.7\%. Moreover, the model outperforms recent YOLO and RT-DETR series in both precision and recall, and demonstrates strong performance on the PlantDoc, Tomato-Leaf, and Rice-Leaf datasets, validating its robustness and generalizability to other plant disease detection scenarios.

</details>


### [127] [VideoARM: Agentic Reasoning over Hierarchical Memory for Long-Form Video Understanding](https://arxiv.org/abs/2512.12360)
*Yufei Yin,Qianke Meng,Minghao Chen,Jiajun Ding,Zhenwei Shao,Zhou Yu*

Main category: cs.CV

TL;DR: VideoARM introduces an agentic reasoning-over-hierarchical-memory paradigm for long-form video understanding that adaptively processes videos on-the-fly rather than using exhaustive preprocessing, reducing token consumption while improving performance.


<details>
  <summary>Details</summary>
Motivation: Current approaches for long-form video understanding rely on hand-crafted reasoning pipelines or token-consuming video preprocessing, which are inefficient for handling extended temporal structure and dense multimodal cues in long videos.

Method: VideoARM uses an adaptive, continuous loop of observing, thinking, acting, and memorizing with a controller that autonomously invokes tools for coarse-to-fine video interpretation. It employs hierarchical multimodal memory that continuously captures and updates multi-level clues to support decision-making.

Result: VideoARM outperforms the state-of-the-art method DVD on prevalent benchmarks while significantly reducing token consumption for long-form videos.

Conclusion: The agentic reasoning-over-hierarchical-memory paradigm provides an effective solution for long-form video understanding by enabling adaptive, on-the-fly processing that reduces computational overhead while maintaining or improving performance.

Abstract: Long-form video understanding remains challenging due to the extended temporal structure and dense multimodal cues. Despite recent progress, many existing approaches still rely on hand-crafted reasoning pipelines or employ token-consuming video preprocessing to guide MLLMs in autonomous reasoning. To overcome these limitations, we introduce VideoARM, an Agentic Reasoning-over-hierarchical-Memory paradigm for long-form video understanding. Instead of static, exhaustive preprocessing, VideoARM performs adaptive, on-the-fly agentic reasoning and memory construction. Specifically, VideoARM performs an adaptive and continuous loop of observing, thinking, acting, and memorizing, where a controller autonomously invokes tools to interpret the video in a coarse-to-fine manner, thereby substantially reducing token consumption. In parallel, a hierarchical multimodal memory continuously captures and updates multi-level clues throughout the operation of the agent, providing precise contextual information to support the controller in decision-making. Experiments on prevalent benchmarks demonstrate that VideoARM outperforms the state-of-the-art method, DVD, while significantly reducing token consumption for long-form videos.

</details>


### [128] [STAGE: Storyboard-Anchored Generation for Cinematic Multi-shot Narrative](https://arxiv.org/abs/2512.12372)
*Peixuan Zhang,Zijian Jia,Kaiqi Liu,Shuchen Weng,Si Li,Boxin Shi*

Main category: cs.CV

TL;DR: STAGE introduces a storyboard-anchored workflow for multi-shot video generation using structural storyboards instead of sparse keyframes, with novel techniques for cross-shot consistency and cinematic transitions.


<details>
  <summary>Details</summary>
Motivation: Current keyframe-based video generation methods lack cross-shot consistency and fail to capture cinematic language, making coherent multi-shot narrative creation challenging despite advances in visual fidelity.

Method: STAGE uses STEP2 to predict structural storyboards (start-end frame pairs per shot), multi-shot memory packs for entity consistency, dual-encoding for intra-shot coherence, and two-stage training for cinematic transitions. Includes ConStoryBoard dataset with movie clips and annotations.

Result: Extensive experiments show STAGE achieves superior performance in structured narrative control and cross-shot coherence compared to existing methods.

Conclusion: STAGE provides an effective storyboard-anchored workflow that addresses key limitations in multi-shot video generation, enabling better narrative control and cinematic quality.

Abstract: While recent advancements in generative models have achieved remarkable visual fidelity in video synthesis, creating coherent multi-shot narratives remains a significant challenge. To address this, keyframe-based approaches have emerged as a promising alternative to computationally intensive end-to-end methods, offering the advantages of fine-grained control and greater efficiency. However, these methods often fail to maintain cross-shot consistency and capture cinematic language. In this paper, we introduce STAGE, a SToryboard-Anchored GEneration workflow to reformulate the keyframe-based multi-shot video generation task. Instead of using sparse keyframes, we propose STEP2 to predict a structural storyboard composed of start-end frame pairs for each shot. We introduce the multi-shot memory pack to ensure long-range entity consistency, the dual-encoding strategy for intra-shot coherence, and the two-stage training scheme to learn cinematic inter-shot transition. We also contribute the large-scale ConStoryBoard dataset, including high-quality movie clips with fine-grained annotations for story progression, cinematic attributes, and human preferences. Extensive experiments demonstrate that STAGE achieves superior performance in structured narrative control and cross-shot coherence.

</details>


### [129] [V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping](https://arxiv.org/abs/2512.12375)
*Hyunkoo Lee,Wooseok Jang,Jini Yang,Taehwan Kim,Sangoh Kim,Sangwon Jung,Seungryong Kim*

Main category: cs.CV

TL;DR: V-Warper is a training-free framework for video personalization that improves subject appearance consistency without video finetuning, using coarse appearance adaptation and fine appearance injection.


<details>
  <summary>Details</summary>
Motivation: Existing video personalization methods require heavy video-based finetuning or large video datasets, which are computationally expensive and difficult to scale. They also struggle with maintaining fine-grained appearance consistency across frames.

Method: Two-stage framework: (1) Coarse appearance adaptation using image-only LoRA and subject-embedding adaptation with reference images; (2) Inference-time fine appearance injection using semantic correspondences from RoPE-free mid-layer query-key features to warp appearance-rich value representations into semantically aligned regions.

Result: V-Warper significantly improves appearance fidelity while preserving prompt alignment and motion dynamics, achieving these gains efficiently without large-scale video finetuning.

Conclusion: V-Warper provides an effective training-free solution for video personalization that enhances fine-grained identity fidelity without the computational burden of video-based training.

Abstract: Video personalization aims to generate videos that faithfully reflect a user-provided subject while following a text prompt. However, existing approaches often rely on heavy video-based finetuning or large-scale video datasets, which impose substantial computational cost and are difficult to scale. Furthermore, they still struggle to maintain fine-grained appearance consistency across frames. To address these limitations, we introduce V-Warper, a training-free coarse-to-fine personalization framework for transformer-based video diffusion models. The framework enhances fine-grained identity fidelity without requiring any additional video training. (1) A lightweight coarse appearance adaptation stage leverages only a small set of reference images, which are already required for the task. This step encodes global subject identity through image-only LoRA and subject-embedding adaptation. (2) A inference-time fine appearance injection stage refines visual fidelity by computing semantic correspondences from RoPE-free mid-layer query--key features. These correspondences guide the warping of appearance-rich value representations into semantically aligned regions of the generation process, with masking ensuring spatial reliability. V-Warper significantly improves appearance fidelity while preserving prompt alignment and motion dynamics, and it achieves these gains efficiently without large-scale video finetuning.

</details>


### [130] [M4Human: A Large-Scale Multimodal mmWave Radar Benchmark for Human Mesh Reconstruction](https://arxiv.org/abs/2512.12378)
*Junqiao Fan,Yunjiao Zhou,Yizhuo Yang,Xinyuan Cui,Jiarui Zhang,Lihua Xie,Jianfei Yang,Chris Xiaoxuan Lu,Fangqiang Ding*

Main category: cs.CV

TL;DR: M4Human is the largest-scale multimodal benchmark (661K frames) for human mesh reconstruction, featuring high-resolution mmWave radar, RGB, and depth data with high-quality motion capture annotations, addressing limitations of vision-based sensing.


<details>
  <summary>Details</summary>
Motivation: Vision-based human mesh reconstruction faces limitations including occlusion, lighting variation, and privacy concerns. Existing radar datasets have sparse skeleton labels, limited scale, and simple actions, creating a need for a comprehensive multimodal benchmark.

Method: Created M4Human dataset with 661K frames (9× larger than prior datasets) featuring mmWave radar (both raw radar tensors and processed point clouds), RGB, and depth data. Includes 20 subjects performing 50 diverse actions with high-quality MoCap annotations (3D meshes and global trajectories).

Result: Established benchmarks on radar modalities (RT and RPC) and multimodal fusion with RGB-D. Results demonstrate the dataset's significance for radar-based human modeling while revealing persistent challenges with fast, unconstrained motion.

Conclusion: M4Human advances HMR research by providing the largest-scale multimodal benchmark that enables privacy-preserving human sensing and addresses limitations of vision-based approaches, with dataset and code to be released after publication.

Abstract: Human mesh reconstruction (HMR) provides direct insights into body-environment interaction, which enables various immersive applications. While existing large-scale HMR datasets rely heavily on line-of-sight RGB input, vision-based sensing is limited by occlusion, lighting variation, and privacy concerns. To overcome these limitations, recent efforts have explored radio-frequency (RF) mmWave radar for privacy-preserving indoor human sensing. However, current radar datasets are constrained by sparse skeleton labels, limited scale, and simple in-place actions. To advance the HMR research community, we introduce M4Human, the current largest-scale (661K-frame) ($9\times$ prior largest) multimodal benchmark, featuring high-resolution mmWave radar, RGB, and depth data. M4Human provides both raw radar tensors (RT) and processed radar point clouds (RPC) to enable research across different levels of RF signal granularity. M4Human includes high-quality motion capture (MoCap) annotations with 3D meshes and global trajectories, and spans 20 subjects and 50 diverse actions, including in-place, sit-in-place, and free-space sports or rehabilitation movements. We establish benchmarks on both RT and RPC modalities, as well as multimodal fusion with RGB-D modalities. Extensive results highlight the significance of M4Human for radar-based human modeling while revealing persistent challenges under fast, unconstrained motion. The dataset and code will be released after the paper publication.

</details>


### [131] [Speedrunning ImageNet Diffusion](https://arxiv.org/abs/2512.12386)
*Swayam Bhanded*

Main category: cs.CV

TL;DR: SR-DiT combines multiple efficiency techniques for diffusion transformers, achieving state-of-the-art results with a small 140M parameter model comparable to much larger models.


<details>
  <summary>Details</summary>
Motivation: Recent diffusion transformer efficiency improvements have been studied in isolation, leaving potential synergies from combining multiple approaches unexplored.

Method: Systematically integrates token routing, architectural improvements, and training modifications on top of representation alignment in a framework called SR-DiT.

Result: Achieves FID 3.49 and KDD 0.319 on ImageNet-256 with only 140M parameters at 400K iterations without classifier-free guidance, comparable to 685M parameter models trained longer.

Conclusion: Identifies effective technique combinations and releases framework as computationally accessible baseline for future research.

Abstract: Recent advances have significantly improved the training efficiency of diffusion transformers. However, these techniques have largely been studied in isolation, leaving unexplored the potential synergies from combining multiple approaches. We present SR-DiT (Speedrun Diffusion Transformer), a framework that systematically integrates token routing, architectural improvements, and training modifications on top of representation alignment. Our approach achieves FID 3.49 and KDD 0.319 on ImageNet-256 using only a 140M parameter model at 400K iterations without classifier-free guidance - comparable to results from 685M parameter models trained significantly longer. To our knowledge, this is a state-of the-art result at this model size. Through extensive ablation studies, we identify which technique combinations are most effective and document both synergies and incompatibilities. We release our framework as a computationally accessible baseline for future research.

</details>


### [132] [ArtGen: Conditional Generative Modeling of Articulated Objects in Arbitrary Part-Level States](https://arxiv.org/abs/2512.12395)
*Haowen Wang,Xiaoping Yuan,Fugang Zhang,Rui Jian,Yuanwei Zhu,Xiuquan Qiao,Yakun Huang*

Main category: cs.CV

TL;DR: ArtGen: A diffusion-based framework for generating articulated 3D objects with accurate geometry and coherent kinematics from single-view images or text descriptions.


<details>
  <summary>Details</summary>
Motivation: Existing generative models for articulated assets often produce ambiguous or unrealistic kinematic structures due to entanglement between geometric shape and joint dynamics, especially when using single-view inputs of closed states.

Method: 1) Cross-state Monte Carlo sampling to enforce global kinematic consistency; 2) Chain-of-Thought reasoning module to infer structural priors; 3) Sparse-expert Diffusion Transformer for kinematic interactions; 4) Compositional 3D-VAE latent prior with local-global attention.

Result: ArtGen significantly outperforms state-of-the-art methods on the PartNet-Mobility benchmark for generating articulated 3D objects.

Conclusion: ArtGen effectively addresses structural-motion entanglement in articulated object generation, producing accurate geometry and coherent kinematics from single-view inputs or text descriptions.

Abstract: Generating articulated assets is crucial for robotics, digital twins, and embodied intelligence. Existing generative models often rely on single-view inputs representing closed states, resulting in ambiguous or unrealistic kinematic structures due to the entanglement between geometric shape and joint dynamics. To address these challenges, we introduce ArtGen, a conditional diffusion-based framework capable of generating articulated 3D objects with accurate geometry and coherent kinematics from single-view images or text descriptions at arbitrary part-level states. Specifically, ArtGen employs cross-state Monte Carlo sampling to explicitly enforce global kinematic consistency, reducing structural-motion entanglement. Additionally, we integrate a Chain-of-Thought reasoning module to infer robust structural priors, such as part semantics, joint types, and connectivity, guiding a sparse-expert Diffusion Transformer to specialize in diverse kinematic interactions. Furthermore, a compositional 3D-VAE latent prior enhanced with local-global attention effectively captures fine-grained geometry and global part-level relationships. Extensive experiments on the PartNet-Mobility benchmark demonstrate that ArtGen significantly outperforms state-of-the-art methods.

</details>


### [133] [A Graph Attention Network-Based Framework for Reconstructing Missing LiDAR Beams](https://arxiv.org/abs/2512.12410)
*Khalfalla Awedat,Mohamed Abidalrekab,Mohammad El-Yabroudi*

Main category: cs.CV

TL;DR: GAT-based framework reconstructs missing vertical LiDAR beams using only single-frame point cloud geometry, achieving 11.67 cm height RMSE without camera or temporal data.


<details>
  <summary>Details</summary>
Motivation: Vertical beam dropout in spinning LiDAR sensors (caused by hardware aging, dust, snow, fog, or bright reflections) removes entire vertical slices from point clouds, severely degrading 3D perception for autonomous vehicles.

Method: Represent LiDAR sweeps as unstructured spatial graphs where points are nodes and edges connect nearby points while preserving beam-index ordering. Use multi-layer Graph Attention Network (GAT) to learn adaptive attention weights over local geometric neighborhoods and directly regress missing elevation (z) values at dropout locations.

Result: On 1,065 raw KITTI sequences with simulated channel dropout: average height RMSE of 11.67 cm, 87.98% of reconstructed points within 10 cm error threshold. Inference takes 14.65 seconds per frame on single GPU. Reconstruction quality remains stable across different neighborhood sizes k.

Conclusion: Pure graph attention model operating solely on raw point-cloud geometry can effectively recover dropped vertical beams under realistic sensor degradation, without requiring camera images or temporal information.

Abstract: Vertical beam dropout in spinning LiDAR sensors triggered by hardware aging, dust, snow, fog, or bright reflections removes entire vertical slices from the point cloud and severely degrades 3D perception in autonomous vehicles. This paper proposes a Graph Attention Network (GAT)-based framework that reconstructs these missing vertical channels using only the current LiDAR frame, with no camera images or temporal information required. Each LiDAR sweep is represented as an unstructured spatial graph: points are nodes and edges connect nearby points while preserving the original beam-index ordering. A multi-layer GAT learns adaptive attention weights over local geometric neighborhoods and directly regresses the missing elevation (z) values at dropout locations. Trained and evaluated on 1,065 raw KITTI sequences with simulated channel dropout, the method achieves an average height RMSE of 11.67 cm, with 87.98% of reconstructed points falling within a 10 cm error threshold. Inference takes 14.65 seconds per frame on a single GPU, and reconstruction quality remains stable for different neighborhood sizes k. These results show that a pure graph attention model operating solely on raw point-cloud geometry can effectively recover dropped vertical beams under realistic sensor degradation.

</details>


### [134] [ViInfographicVQA: A Benchmark for Single and Multi-image Visual Question Answering on Vietnamese Infographics](https://arxiv.org/abs/2512.12424)
*Tue-Thu Van-Dinh,Hoang-Duy Tran,Truong-Binh Duong,Mai-Hanh Pham,Binh-Nam Le-Nguyen,Quoc-Thai Nguyen*

Main category: cs.CV

TL;DR: ViInfographicVQA is the first Vietnamese benchmark for Infographic Visual Question Answering, featuring 6747 infographics and 20409 QA pairs across various domains, with both single-image and novel multi-image reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: There's a need to evaluate models' ability to understand data-rich, layout-heavy infographics in Vietnamese, which requires stronger integration of OCR, layout understanding, and numerical/semantic reasoning than traditional VQA tasks. Current benchmarks lack Vietnamese infographic evaluation, especially for cross-image reasoning.

Method: Created ViInfographicVQA benchmark with 6747 real-world infographics and 20409 human-verified question-answer pairs across economics, healthcare, education, etc. Includes two evaluation settings: Single-image (traditional VQA) and Multi-image (novel cross-image reasoning requiring synthesis across multiple related infographics).

Result: Evaluation of recent vision-language models revealed substantial performance disparities, with most significant errors occurring on Multi-image questions involving cross-image integration and non-span reasoning. The benchmark provides baseline results for Vietnamese InfographicVQA.

Conclusion: ViInfographicVQA contributes the first Vietnamese benchmark for infographic understanding and highlights limitations of current multimodal models in low-resource contexts, particularly in layout-aware and cross-image reasoning, encouraging future research in these areas.

Abstract: Infographic Visual Question Answering (InfographicVQA) evaluates a model's ability to read and reason over data-rich, layout-heavy visuals that combine text, charts, icons, and design elements. Compared with scene-text or natural-image VQA, infographics require stronger integration of OCR, layout understanding, and numerical and semantic reasoning. We introduce ViInfographicVQA, the first benchmark for Vietnamese InfographicVQA, comprising over 6747 real-world infographics and 20409 human-verified question-answer pairs across economics, healthcare, education, and more. The benchmark includes two evaluation settings. The Single-image task follows the traditional setup in which each question is answered using a single infographic. The Multi-image task requires synthesizing evidence across multiple semantically related infographics and is, to our knowledge, the first Vietnamese evaluation of cross-image reasoning in VQA. We evaluate a range of recent vision-language models on this benchmark, revealing substantial performance disparities, with the most significant errors occurring on Multi-image questions that involve cross-image integration and non-span reasoning. ViInfographicVQA contributes benchmark results for Vietnamese InfographicVQA and sheds light on the limitations of current multimodal models in low-resource contexts, encouraging future exploration of layout-aware and cross-image reasoning methods.

</details>


### [135] [BokehDepth: Enhancing Monocular Depth Estimation through Bokeh Generation](https://arxiv.org/abs/2512.12425)
*Hangwei Zhang,Armando Teles Fortes,Tianyi Wei,Xingang Pan*

Main category: cs.CV

TL;DR: BokehDepth: A two-stage framework that uses defocus as geometric cue to improve both bokeh rendering and monocular depth estimation by decoupling bokeh synthesis from depth prediction.


<details>
  <summary>Details</summary>
Motivation: Current methods inadequately exploit the connection between bokeh rendering and monocular depth estimation. Bokeh pipelines rely on noisy depth maps causing visible artifacts, while depth models struggle with weakly textured, distant, and ambiguous regions where defocus cues are most informative.

Method: Two-stage framework: 1) Physically guided controllable bokeh generator produces depth-free bokeh stacks with calibrated bokeh strength from single sharp input; 2) Lightweight defocus-aware aggregation module fuses features along defocus dimension and exposes depth-sensitive variations while keeping decoder unchanged.

Result: BokehDepth improves visual fidelity over depth-map-based bokeh baselines and consistently boosts metric accuracy and robustness of strong monocular depth foundation models across challenging benchmarks.

Conclusion: The framework successfully treats defocus as auxiliary supervision-free geometric cue, decoupling bokeh synthesis from depth prediction to improve both tasks by leveraging their inherent connection through lens imaging geometry.

Abstract: Bokeh and monocular depth estimation are tightly coupled through the same lens imaging geometry, yet current methods exploit this connection in incomplete ways. High-quality bokeh rendering pipelines typically depend on noisy depth maps, which amplify estimation errors into visible artifacts, while modern monocular metric depth models still struggle on weakly textured, distant and geometrically ambiguous regions where defocus cues are most informative. We introduce BokehDepth, a two-stage framework that decouples bokeh synthesis from depth prediction and treats defocus as an auxiliary supervision-free geometric cue. In Stage-1, a physically guided controllable bokeh generator, built on a powerful pretrained image editing backbone, produces depth-free bokeh stacks with calibrated bokeh strength from a single sharp input. In Stage-2, a lightweight defocus-aware aggregation module plugs into existing monocular depth encoders, fuses features along the defocus dimension, and exposes stable depth-sensitive variations while leaving downstream decoder unchanged. Across challenging benchmarks, BokehDepth improves visual fidelity over depth-map-based bokeh baselines and consistently boosts the metric accuracy and robustness of strong monocular depth foundation models.

</details>


### [136] [Endless World: Real-Time 3D-Aware Long Video Generation](https://arxiv.org/abs/2512.12430)
*Ke Zhang,Yiqun Mei,Jiacong Xu,Vishal M. Patel*

Main category: cs.CV

TL;DR: Endless World is a real-time framework for infinite, 3D-consistent video generation that uses conditional autoregressive training and global 3D-aware attention to produce long, stable videos on a single GPU.


<details>
  <summary>Details</summary>
Motivation: Producing long, coherent video sequences with stable 3D structure remains challenging, especially in streaming scenarios where infinite video generation is needed.

Method: Uses conditional autoregressive training to align new content with existing frames, integrates global 3D-aware attention for continuous geometric guidance, and employs a 3D injection mechanism for physical plausibility.

Result: Produces long, stable, and visually coherent videos with competitive or superior performance to existing methods in both visual fidelity and spatial consistency, enabling real-time inference on a single GPU.

Conclusion: Endless World successfully addresses key challenges in long-horizon and dynamic scene synthesis, providing an effective framework for infinite, 3D-consistent video generation in real-time applications.

Abstract: Producing long, coherent video sequences with stable 3D structure remains a major challenge, particularly in streaming scenarios. Motivated by this, we introduce Endless World, a real-time framework for infinite, 3D-consistent video generation.To support infinite video generation, we introduce a conditional autoregressive training strategy that aligns newly generated content with existing video frames. This design preserves long-range dependencies while remaining computationally efficient, enabling real-time inference on a single GPU without additional training overhead.Moreover, our Endless World integrates global 3D-aware attention to provide continuous geometric guidance across time. Our 3D injection mechanism enforces physical plausibility and geometric consistency throughout extended sequences, addressing key challenges in long-horizon and dynamic scene synthesis.Extensive experiments demonstrate that Endless World produces long, stable, and visually coherent videos, achieving competitive or superior performance to existing methods in both visual fidelity and spatial consistency. Our project has been available on https://bwgzk-keke.github.io/EndlessWorld/.

</details>


### [137] [From Particles to Fields: Reframing Photon Mapping with Continuous Gaussian Photon Fields](https://arxiv.org/abs/2512.12459)
*Jiachen Tao,Benjamin Planche,Van Nguyen Nguyen,Junyi Wu,Yuchun Liu,Haoxuan Wang,Zhongpai Gao,Gengyu Zhang,Meng Zheng,Feiran Wang,Anwesa Choudhuri,Zhenghao Zhao,Weitai Kang,Terrence Chen,Yan Yan,Ziyan Wu*

Main category: cs.CV

TL;DR: GPF (Gaussian Photon Field) learns a continuous radiance function from photon distributions to accelerate multi-view rendering while maintaining photon-level accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional photon mapping is computationally inefficient for multi-view rendering due to redundant photon tracing and kernel estimation at each viewpoint. There's a need to accelerate rendering while preserving physical accuracy.

Method: Introduces Gaussian Photon Field (GPF) - a learnable representation encoding photon distributions as anisotropic 3D Gaussian primitives parameterized by position, rotation, scale, and spectrum. Initialized from physically traced photons and optimized using multi-view radiance supervision.

Result: GPF achieves photon-level accuracy on complex light transport effects (caustics, specular-diffuse interactions) while reducing computation by orders of magnitude compared to traditional photon mapping.

Conclusion: GPF successfully unifies the physical rigor of photon-based rendering with the efficiency of neural scene representations, enabling differentiable radiance evaluation without repeated photon tracing.

Abstract: Accurately modeling light transport is essential for realistic image synthesis. Photon mapping provides physically grounded estimates of complex global illumination effects such as caustics and specular-diffuse interactions, yet its per-view radiance estimation remains computationally inefficient when rendering multiple views of the same scene. The inefficiency arises from independent photon tracing and stochastic kernel estimation at each viewpoint, leading to inevitable redundant computation. To accelerate multi-view rendering, we reformulate photon mapping as a continuous and reusable radiance function. Specifically, we introduce the Gaussian Photon Field (GPF), a learnable representation that encodes photon distributions as anisotropic 3D Gaussian primitives parameterized by position, rotation, scale, and spectrum. GPF is initialized from physically traced photons in the first SPPM iteration and optimized using multi-view supervision of final radiance, distilling photon-based light transport into a continuous field. Once trained, the field enables differentiable radiance evaluation along camera rays without repeated photon tracing or iterative refinement. Extensive experiments on scenes with complex light transport, such as caustics and specular-diffuse interactions, demonstrate that GPF attains photon-level accuracy while reducing computation by orders of magnitude, unifying the physical rigor of photon-based rendering with the efficiency of neural scene representations.

</details>


### [138] [More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models](https://arxiv.org/abs/2512.12487)
*Hoang Anh Just,Yifei Fan,Handong Zhao,Jiuxiang Gu,Ruiyi Zhang,Simon Jenni,Kushal Kafle,Ruoxi Jia,Jing Shi*

Main category: cs.CV

TL;DR: PeRL-VL improves RLVR-trained VLMs by decoupling visual perception and textual reasoning, adding description rewards for faithful image understanding and reasoning SFT for logical consistency.


<details>
  <summary>Details</summary>
Motivation: RLVR-trained VLMs still suffer from inaccurate visual extraction (missing/hallucinating details) and logically inconsistent chains-of-thought, as verifiable signals only supervise final answers, not intermediate reasoning.

Method: PeRL-VL decouples perception and reasoning: (1) VLM-based description reward scores self-generated image descriptions for faithfulness/sufficiency, (2) text-only Reasoning SFT stage on logic-rich chain-of-thought data enhances coherence independently of vision.

Result: Improves average Pass@1 accuracy from 63.3% (base Qwen2.5-VL-7B) to 68.8% across diverse multimodal benchmarks, outperforming standard RLVR, text-only reasoning SFT, and naive multimodal distillation from GPT-4o.

Conclusion: Decoupling perception and reasoning learning addresses persistent failure modes in RLVR-trained VLMs, with separate improvements for visual extraction and logical consistency leading to better multimodal reasoning performance.

Abstract: Reinforcement learning from verifiable rewards (RLVR) has recently been extended from text-only LLMs to vision-language models (VLMs) to elicit long-chain multimodal reasoning. However, RLVR-trained VLMs still exhibit two persistent failure modes: inaccurate visual extraction (missing or hallucinating details) and logically inconsistent chains-of-thought, largely because verifiable signals supervise only the final answer. We propose PeRL-VL (Perception and Reasoning Learning for Vision-Language Models), a decoupled framework that separately improves visual perception and textual reasoning on top of RLVR. For perception, PeRL-VL introduces a VLM-based description reward that scores the model's self-generated image descriptions for faithfulness and sufficiency. For reasoning, PeRL-VL adds a text-only Reasoning SFT stage on logic-rich chain-of-thought data, enhancing coherence and logical consistency independently of vision. Across diverse multimodal benchmarks, PeRL-VL improves average Pass@1 accuracy from 63.3% (base Qwen2.5-VL-7B) to 68.8%, outperforming standard RLVR, text-only reasoning SFT, and naive multimodal distillation from GPT-4o.

</details>


### [139] [Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings](https://arxiv.org/abs/2512.12492)
*Shengkai Xu,Hsiang Lun Kao,Tianxiang Xu,Honghui Zhang,Junqiao Wang,Runmeng Ding,Guanyu Liu,Tianyu Shi,Zhenyu Yu,Guofeng Pan,Ziqian Bi,Yuqi Ouyang*

Main category: cs.CV

TL;DR: AdaptiveDetector: A two-stage detector-verifier framework using YOLOv11 with VLM-guided adaptive confidence thresholds and GRPO fine-tuning for robust polyp detection under adverse clinical imaging conditions.


<details>
  <summary>Details</summary>
Motivation: Polyp detectors trained on clean datasets underperform in real-world endoscopy due to illumination changes, motion blur, and occlusions. Existing approaches struggle with the domain gap between controlled lab conditions and clinical practice with adverse imaging conditions.

Method: Two-stage detector-verifier framework: 1) YOLOv11 detector adaptively adjusts per-frame confidence thresholds under VLM guidance, 2) VLM verifier fine-tuned with Group Relative Policy Optimization (GRPO) using asymmetric, cost-sensitive reward function to discourage missed detections. Synthetic testbed created by degrading clean datasets with adverse clinical conditions for evaluation.

Result: Zero-shot evaluation on synthetically degraded CVC-ClinicDB and Kvasir-SEG images shows 14-22 percentage point recall improvement over YOLO alone, while precision remains within 0.7 points below to 1.7 points above baseline.

Conclusion: The combination of adaptive thresholding and cost-sensitive reinforcement learning achieves clinically aligned, open-world polyp detection with substantially fewer false negatives, reducing risk of missed precancerous polyps and improving patient outcomes.

Abstract: Polyp detectors trained on clean datasets often underperform in real-world endoscopy, where illumination changes, motion blur, and occlusions degrade image quality. Existing approaches struggle with the domain gap between controlled laboratory conditions and clinical practice, where adverse imaging conditions are prevalent. In this work, we propose AdaptiveDetector, a novel two-stage detector-verifier framework comprising a YOLOv11 detector with a vision-language model (VLM) verifier. The detector adaptively adjusts per-frame confidence thresholds under VLM guidance, while the verifier is fine-tuned with Group Relative Policy Optimization (GRPO) using an asymmetric, cost-sensitive reward function specifically designed to discourage missed detections -- a critical clinical requirement. To enable realistic assessment under challenging conditions, we construct a comprehensive synthetic testbed by systematically degrading clean datasets with adverse conditions commonly encountered in clinical practice, providing a rigorous benchmark for zero-shot evaluation. Extensive zero-shot evaluation on synthetically degraded CVC-ClinicDB and Kvasir-SEG images demonstrates that our approach improves recall by 14 to 22 percentage points over YOLO alone, while precision remains within 0.7 points below to 1.7 points above the baseline. This combination of adaptive thresholding and cost-sensitive reinforcement learning achieves clinically aligned, open-world polyp detection with substantially fewer false negatives, thereby reducing the risk of missed precancerous polyps and improving patient outcomes.

</details>


### [140] [Advancing Cache-Based Few-Shot Classification via Patch-Driven Relational Gated Graph Attention](https://arxiv.org/abs/2512.12498)
*Tasweer Ahmad,Arindam Sikdar,Sandip Pradhan,Ardhendu Behera*

Main category: cs.CV

TL;DR: Patch-driven relational refinement for few-shot image classification that learns cache adapter weights from intra-image patch dependencies via graph attention, improving discriminative power without inference overhead.


<details>
  <summary>Details</summary>
Motivation: Existing cache-based adaptation approaches (like Tip-Adapter) still inherit CLIP's tendency for global, general-purpose representations that aren't optimally discriminative for few-shot adaptation under domain shift. There's a need for more specialized representations in low-data regimes.

Method: Proposes a patch-driven relational refinement using a relational gated graph attention network that constructs patch graphs and performs edge-aware attention to capture inter-patch interactions. Uses learnable multi-aggregation pooling to create compact, task-discriminative representations. Graph refinement is only used during training to distill relational structure into the cache, with no additional inference cost.

Result: Extensive evaluations on 11 benchmarks show consistent gains over state-of-the-art CLIP adapter and cache-based baselines while preserving zero-shot efficiency. Also introduces and validates on a new Injured vs. Uninjured Soldier dataset for casualty recognition.

Conclusion: The proposed patch-driven relational refinement effectively improves few-shot classification by learning more discriminative representations from patch dependencies, achieving better adaptation to specialized domains while maintaining efficient inference. The method addresses practical needs in time-critical applications like combat casualty care.

Abstract: Few-shot image classification remains difficult under limited supervision and visual domain shift. Recent cache-based adaptation approaches (e.g., Tip-Adapter) address this challenge to some extent by learning lightweight residual adapters over frozen features, yet they still inherit CLIP's tendency to encode global, general-purpose representations that are not optimally discriminative to adapt the generalist to the specialist's domain in low-data regimes. We address this limitation with a novel patch-driven relational refinement that learns cache adapter weights from intra-image patch dependencies rather than treating an image embedding as a monolithic vector. Specifically, we introduce a relational gated graph attention network that constructs a patch graph and performs edge-aware attention to emphasize informative inter-patch interactions, producing context-enriched patch embeddings. A learnable multi-aggregation pooling then composes these into compact, task-discriminative representations that better align cache keys with the target few-shot classes. Crucially, the proposed graph refinement is used only during training to distil relational structure into the cache, incurring no additional inference cost beyond standard cache lookup. Final predictions are obtained by a residual fusion of cache similarity scores with CLIP zero-shot logits. Extensive evaluations on 11 benchmarks show consistent gains over state-of-the-art CLIP adapter and cache-based baselines while preserving zero-shot efficiency. We further validate battlefield relevance by introducing an Injured vs. Uninjured Soldier dataset for casualty recognition. It is motivated by the operational need to support triage decisions within the "platinum minutes" and the broader "golden hour" window in time-critical UAV-driven search-and-rescue and combat casualty care.

</details>


### [141] [Generative Spatiotemporal Data Augmentation](https://arxiv.org/abs/2512.12508)
*Jinfan Zhou,Lixin Luo,Sungmin Eum,Heesung Kwon,Jeong Joon Park*

Main category: cs.CV

TL;DR: Spatiotemporal data augmentation using video diffusion models to generate realistic 3D viewpoint and scene dynamics variations from images, improving performance in low-data regimes like UAV imagery.


<details>
  <summary>Details</summary>
Motivation: Existing data augmentation methods rely on simple geometric transforms or appearance perturbations, which don't capture realistic 3D spatial and temporal variations needed for robust computer vision models, especially in data-scarce scenarios like UAV-captured imagery.

Method: Leverages off-the-shelf video diffusion models to generate realistic 3D spatial and temporal variations from static image datasets. Provides guidelines for: (1) choosing appropriate spatiotemporal generative setup, (2) transferring annotations to synthetic frames, and (3) addressing disocclusion (newly revealed unlabeled regions).

Result: Consistent performance gains in low-data settings, particularly for UAV-captured imagery where annotations are scarce. Experiments on COCO subsets and UAV datasets show the method broadens data distribution along axes underrepresented by traditional and prior generative methods.

Conclusion: Spatiotemporal augmentation using video foundation models offers an effective approach for improving model performance in data-scarce regimes by generating realistic 3D viewpoint and scene dynamics variations that traditional methods cannot capture.

Abstract: We explore spatiotemporal data augmentation using video foundation models to diversify both camera viewpoints and scene dynamics. Unlike existing approaches based on simple geometric transforms or appearance perturbations, our method leverages off-the-shelf video diffusion models to generate realistic 3D spatial and temporal variations from a given image dataset. Incorporating these synthesized video clips as supplemental training data yields consistent performance gains in low-data settings, such as UAV-captured imagery where annotations are scarce. Beyond empirical improvements, we provide practical guidelines for (i) choosing an appropriate spatiotemporal generative setup, (ii) transferring annotations to synthetic frames, and (iii) addressing disocclusion - regions newly revealed and unlabeled in generated views. Experiments on COCO subsets and UAV-captured datasets show that, when applied judiciously, spatiotemporal augmentation broadens the data distribution along axes underrepresented by traditional and prior generative methods, offering an effective lever for improving model performance in data-scarce regimes.

</details>


### [142] [Animus3D: Text-driven 3D Animation via Motion Score Distillation](https://arxiv.org/abs/2512.12534)
*Qi Sun,Can Wang,Jiaxiang Shang,Wensen Feng,Jing Liao*

Main category: cs.CV

TL;DR: Animus3D is a text-driven 3D animation framework that generates motion fields for static 3D assets using text prompts, overcoming limitations of previous SDS-based methods through novel Motion Score Distillation and regularization techniques.


<details>
  <summary>Details</summary>
Motivation: Previous methods using vanilla Score Distillation Sampling (SDS) from text-to-video diffusion models produce animations with minimal movement or noticeable jitter, failing to generate substantial and detailed motion while preserving visual quality.

Method: Introduces Motion Score Distillation (MSD) as an SDS alternative, using a LoRA-enhanced video diffusion model with static source distribution and inversion-based noise estimation for appearance preservation. Includes temporal/spatial regularization to mitigate geometric distortions and a motion refinement module for temporal upscaling and detail enhancement.

Result: Animus3D successfully animates static 3D assets from diverse text prompts, generating significantly more substantial and detailed motion than state-of-the-art baselines while maintaining high visual integrity.

Conclusion: The proposed framework effectively addresses limitations of previous SDS-based animation methods through novel distillation techniques, regularization, and refinement modules, enabling high-quality text-driven 3D animation with substantial motion and preserved appearance.

Abstract: We present Animus3D, a text-driven 3D animation framework that generates motion field given a static 3D asset and text prompt. Previous methods mostly leverage the vanilla Score Distillation Sampling (SDS) objective to distill motion from pretrained text-to-video diffusion, leading to animations with minimal movement or noticeable jitter. To address this, our approach introduces a novel SDS alternative, Motion Score Distillation (MSD). Specifically, we introduce a LoRA-enhanced video diffusion model that defines a static source distribution rather than pure noise as in SDS, while another inversion-based noise estimation technique ensures appearance preservation when guiding motion. To further improve motion fidelity, we incorporate explicit temporal and spatial regularization terms that mitigate geometric distortions across time and space. Additionally, we propose a motion refinement module to upscale the temporal resolution and enhance fine-grained details, overcoming the fixed-resolution constraints of the underlying video model. Extensive experiments demonstrate that Animus3D successfully animates static 3D assets from diverse text prompts, generating significantly more substantial and detailed motion than state-of-the-art baselines while maintaining high visual integrity. Code will be released at https://qiisun.github.io/animus3d_page.

</details>


### [143] [Anatomy Guided Coronary Artery Segmentation from CCTA Using Spatial Frequency Joint Modeling](https://arxiv.org/abs/2512.12539)
*Huan Huang,Michele Esposito,Chen Zhao*

Main category: cs.CV

TL;DR: A novel coronary artery segmentation framework integrating myocardial priors, structure-aware encoding, and 3D wavelet transformations achieves state-of-the-art performance on the ImageCAS dataset.


<details>
  <summary>Details</summary>
Motivation: Accurate coronary artery segmentation from coronary CT angiography is crucial for quantitative analysis but remains challenging due to small vessel sizes, complex branching, blurred boundaries, and myocardial interference.

Method: Proposes a framework with three key components: 1) myocardial anatomical priors and residual attention-based feature enhancement during encoding, 2) wavelet-inverse wavelet based downsampling/upsampling for joint spatial-frequency modeling, and 3) multi-scale feature fusion module for semantic-geometric integration. Uses 3D overlapping patch strategy on ImageCAS dataset with 7:1:2 train/val/test split.

Result: Achieves Dice coefficient of 0.8082, Sensitivity of 0.7946, Precision of 0.8471, and HD95 of 9.77 mm, outperforming mainstream segmentation models. Ablation studies confirm complementary contributions of individual components.

Conclusion: The method enables more stable and consistent coronary artery segmentation under complex geometric conditions, providing reliable results for subsequent coronary structure analysis tasks.

Abstract: Accurate coronary artery segmentation from coronary computed tomography angiography is essential for quantitative coronary analysis and clinical decision support. Nevertheless, reliable segmentation remains challenging because of small vessel calibers, complex branching, blurred boundaries, and myocardial interference. We propose a coronary artery segmentation framework that integrates myocardial anatomical priors, structure aware feature encoding, and three dimensional wavelet inverse wavelet transformations. Myocardial priors and residual attention based feature enhancement are incorporated during encoding to strengthen coronary structure representation. Wavelet inverse wavelet based downsampling and upsampling enable joint spatial frequency modeling and preserve multi scale structural consistency, while a multi scale feature fusion module integrates semantic and geometric information in the decoding stage. The model is trained and evaluated on the public ImageCAS dataset using a 3D overlapping patch based strategy with a 7:1:2 split for training, validation, and testing. Experimental results demonstrate that the proposed method achieves a Dice coefficient of 0.8082, Sensitivity of 0.7946, Precision of 0.8471, and an HD95 of 9.77 mm, outperforming several mainstream segmentation models. Ablation studies further confirm the complementary contributions of individual components. The proposed method enables more stable and consistent coronary artery segmentation under complex geometric conditions, providing reliable segmentation results for subsequent coronary structure analysis tasks.

</details>


### [144] [Supervised Contrastive Frame Aggregation for Video Representation Learning](https://arxiv.org/abs/2512.12549)
*Shaif Chowdhury,Mushfika Rahman,Greg Hamerly*

Main category: cs.CV

TL;DR: Supervised contrastive learning for video representation using frame aggregation into single images, leveraging pre-trained CNNs and temporal sampling for diverse positive samples, achieving better accuracy than ViViT with less computation.


<details>
  <summary>Details</summary>
Motivation: To develop efficient video representation learning that avoids computationally expensive video transformer models while leveraging temporal context and reducing overfitting through natural frame variations rather than data augmentation.

Method: Aggregates multiple video frames into single input images to use pre-trained CNN backbones (ResNet50), creates positive pairs from videos with same labels, generates multiple natural views via different temporal frame samplings, and uses supervised contrastive learning objective.

Result: Outperforms ViViT with 76% vs 43% accuracy on Penn Action and 48% vs 37% on HMDB51, while requiring fewer computational resources. Effective for both supervised and self-supervised settings.

Conclusion: The Supervised Contrastive Frame Aggregation method provides an efficient and effective approach to video representation learning that leverages temporal context through frame aggregation and contrastive learning, achieving superior performance with reduced computational overhead.

Abstract: We propose a supervised contrastive learning framework for video representation learning that leverages temporally global context. We introduce a video to image aggregation strategy that spatially arranges multiple frames from each video into a single input image. This design enables the use of pre trained convolutional neural network backbones such as ResNet50 and avoids the computational overhead of complex video transformer models. We then design a contrastive learning objective that directly compares pairwise projections generated by the model. Positive pairs are defined as projections from videos sharing the same label while all other projections are treated as negatives. Multiple natural views of the same video are created using different temporal frame samplings from the same underlying video. Rather than relying on data augmentation these frame level variations produce diverse positive samples with global context and reduce overfitting. Experiments on the Penn Action and HMDB51 datasets demonstrate that the proposed method outperforms existing approaches in classification accuracy while requiring fewer computational resources. The proposed Supervised Contrastive Frame Aggregation method learns effective video representations in both supervised and self supervised settings and supports video based tasks such as classification and captioning. The method achieves seventy six percent classification accuracy on Penn Action compared to forty three percent achieved by ViVIT and forty eight percent accuracy on HMDB51 compared to thirty seven percent achieved by ViVIT.

</details>


### [145] [StreamingAssistant: Efficient Visual Token Pruning for Accelerating Online Video Understanding](https://arxiv.org/abs/2512.12560)
*Xinqi Jin,Hanxun Yu,Bohan Yu,Kebin Liu,Jian Liu,Keda Tao,Yixuan Pei,Huan Wang,Fan Dang,Jiangchuan Liu,Weiqiang Wang*

Main category: cs.CV

TL;DR: Token pruning method for MLLMs in video understanding reduces computational cost while improving accuracy by pruning redundant tokens using spatial and temporal redundancy metrics.


<details>
  <summary>Details</summary>
Motivation: Online video understanding applications (surveillance, AI glasses) face challenges with MLLMs due to high GPU memory usage and computational latency from processing many video frames.

Method: Proposes token pruning with MSSAVT (Maximum Similarity to Spatially Adjacent Video Tokens) redundancy metric, masked pruning strategy to handle bidirectional dependency, and integrates temporal redundancy pruning.

Result: Method improves accuracy by up to 4% on multiple video understanding benchmarks with negligible pruning latency (<1ms).

Conclusion: Token pruning effectively reduces context length while retaining critical information, making MLLMs more practical for online video understanding applications.

Abstract: Online video understanding is essential for applications like public surveillance and AI glasses. However, applying Multimodal Large Language Models (MLLMs) to this domain is challenging due to the large number of video frames, resulting in high GPU memory usage and computational latency. To address these challenges, we propose token pruning as a means to reduce context length while retaining critical information. Specifically, we introduce a novel redundancy metric, Maximum Similarity to Spatially Adjacent Video Tokens (MSSAVT), which accounts for both token similarity and spatial position. To mitigate the bidirectional dependency between pruning and redundancy, we further design a masked pruning strategy that ensures only mutually unadjacent tokens are pruned. We also integrate an existing temporal redundancy-based pruning method to eliminate temporal redundancy of the video modality. Experimental results on multiple online and offline video understanding benchmarks demonstrate that our method significantly improves the accuracy (i.e., by 4\% at most) while incurring a negligible pruning latency (i.e., less than 1ms). Our full implementation will be made publicly available.

</details>


### [146] [From Tokens to Photons: Test-Time Physical Prompting for Vison-Language Models](https://arxiv.org/abs/2512.12571)
*Boyeong Im,Wooseok Lee,Yoojin Kwon,Hyung-Sin Kim*

Main category: cs.CV

TL;DR: MVP is a test-time adaptation framework that uses camera exposure settings as physical prompts to adapt vision-language models to sensor-mediated environments, improving robustness without model modifications.


<details>
  <summary>Details</summary>
Motivation: To extend vision-language models from web images to physical environments by leveraging sensor control as physical prompts, moving beyond digital-only adaptation approaches.

Method: At inference, MVP captures multiple physical views per scene using different camera exposure settings (ISO, shutter speed, aperture), selects top-k settings via source-affinity scoring, applies lightweight digital augmentations, filters low-entropy views, and aggregates predictions using hard voting.

Result: Outperforms digital-only TTA by up to 25.6 percentage points on ImageNet-ES datasets, delivers additional 3.4 pp gains over sensor control + TTA pipelines, and remains effective with reduced parameter sets for practical latency.

Conclusion: Measurement-time control through physical view selection and combination substantially improves VLM robustness, demonstrating that physical prompting complements post-capture digital adaptation.

Abstract: To extend the application of vision-language models (VLMs) from web images to sensor-mediated physical environments, we propose Multi-View Physical-prompt for Test-Time Adaptation (MVP), a forward-only framework that moves test-time adaptation (TTA) from tokens to photons by treating the camera exposure triangle--ISO, shutter speed, and aperture--as physical prompts. At inference, MVP acquires a library of physical views per scene, selects the top-k sensor settings using a source-affinity score, evaluates each retained view under lightweight digital augmentations, filters the lowest-entropy subset of augmented views, and aggregates predictions with Zero-temperature softmax (i.e., hard voting). This selection-then-vote design is simple, calibration-friendly, and requires no gradients or model modifications. On ImageNet-ES and ImageNet-ES-Diverse, MVP consistently outperforms digital-only TTA on single Auto-Exposure captures, by up to 25.6 percentage points (pp), and delivers up to 3.4 pp additional gains over pipelines that combine conventional sensor control with TTA. MVP remains effective under reduced parameter candidate sets that lower capture latency, demonstrating practicality. These results support the main claim that, beyond post-capture prompting, measurement-time control--selecting and combining real physical views--substantially improves robustness for VLMs.

</details>


### [147] [Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space](https://arxiv.org/abs/2512.12623)
*Chengzhi Liu,Yuzhe Yang,Yue Fan,Qingyue Wei,Sheng Liu,Xin Eric Wang*

Main category: cs.CV

TL;DR: DMLR is a dynamic multimodal latent reasoning framework that enables interleaved visual-textual reasoning through confidence-guided latent policy optimization and dynamic visual injection, improving reasoning performance while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: Current multimodal reasoning methods rely on explicit step-by-step reasoning, suffer from unstable perception-reasoning interaction, and have high computational overhead. Inspired by human cognition where thinking involves dynamic interleaving of reasoning and perception, the authors aim to create a more efficient and effective reasoning framework.

Method: DMLR uses confidence-guided latent policy gradient optimization to refine latent think tokens for in-depth reasoning. It introduces a Dynamic Visual Injection Strategy that retrieves the most relevant visual features at each latent think token, updates the best visual patches, and injects them into latent think tokens to achieve dynamic visual-textual interleaving.

Result: Experiments across seven multimodal reasoning benchmarks and various model architectures demonstrate that DMLR significantly improves reasoning and perception performance while maintaining high inference efficiency.

Conclusion: DMLR successfully addresses limitations of existing multimodal reasoning methods by enabling dynamic interleaving of reasoning and perception, leading to improved performance and efficiency across diverse multimodal reasoning tasks.

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced cross-modal understanding and reasoning by incorporating Chain-of-Thought (CoT) reasoning in the semantic space. Building upon this, recent studies extend the CoT mechanism to the visual modality, enabling models to integrate visual information during reasoning through external tools or explicit image generation. However, these methods remain dependent on explicit step-by-step reasoning, unstable perception-reasoning interaction and notable computational overhead. Inspired by human cognition, we posit that thinking unfolds not linearly but through the dynamic interleaving of reasoning and perception within the mind. Motivated by this perspective, we propose DMLR, a test-time Dynamic Multimodal Latent Reasoning framework that employs confidence-guided latent policy gradient optimization to refine latent think tokens for in-depth reasoning. Furthermore, a Dynamic Visual Injection Strategy is introduced, which retrieves the most relevant visual features at each latent think token and updates the set of best visual patches. The updated patches are then injected into latent think token to achieve dynamic visual-textual interleaving. Experiments across seven multimodal reasoning benchmarks and various model architectures demonstrate that DMLR significantly improves reasoning and perception performance while maintaining high inference efficiency.

</details>


### [148] [StegaVAR: Privacy-Preserving Video Action Recognition via Steganographic Domain Analysis](https://arxiv.org/abs/2512.12586)
*Lixin Chen,Chaomeng Chen,Jiale Zhou,Zhijian Wu,Xun Lin*

Main category: cs.CV

TL;DR: StegaVAR is a novel privacy-preserving framework that embeds action videos into ordinary cover videos and performs video action recognition directly in the steganographic domain, addressing both concealment and spatiotemporal feature preservation issues.


<details>
  <summary>Details</summary>
Motivation: Current privacy-preserving methods for video action recognition suffer from two main issues: (1) low concealment where visually distorted videos attract attackers' attention during transmission, and (2) spatiotemporal disruption that degrades essential features needed for accurate action recognition.

Method: StegaVAR embeds action videos into ordinary cover videos and performs VAR directly in the steganographic domain. It introduces two key components: Secret Spatio-Temporal Promotion (STeP) which uses the secret video to guide spatiotemporal feature extraction during training, and Cross-Band Difference Attention (CroDA) which suppresses cover interference by capturing cross-band semantic differences.

Result: Experiments demonstrate that StegaVAR achieves superior VAR and privacy-preserving performance on widely used datasets. The framework is also effective for multiple steganographic models.

Conclusion: StegaVAR successfully addresses both concealment and spatiotemporal disruption issues in privacy-preserving video action recognition by performing analysis directly in the steganographic domain while maintaining complete spatiotemporal information and natural appearance of cover videos.

Abstract: Despite the rapid progress of deep learning in video action recognition (VAR) in recent years, privacy leakage in videos remains a critical concern. Current state-of-the-art privacy-preserving methods often rely on anonymization. These methods suffer from (1) low concealment, where producing visually distorted videos that attract attackers' attention during transmission, and (2) spatiotemporal disruption, where degrading essential spatiotemporal features for accurate VAR. To address these issues, we propose StegaVAR, a novel framework that embeds action videos into ordinary cover videos and directly performs VAR in the steganographic domain for the first time. Throughout both data transmission and action analysis, the spatiotemporal information of hidden secret video remains complete, while the natural appearance of cover videos ensures the concealment of transmission. Considering the difficulty of steganographic domain analysis, we propose Secret Spatio-Temporal Promotion (STeP) and Cross-Band Difference Attention (CroDA) for analysis within the steganographic domain. STeP uses the secret video to guide spatiotemporal feature extraction in the steganographic domain during training. CroDA suppresses cover interference by capturing cross-band semantic differences. Experiments demonstrate that StegaVAR achieves superior VAR and privacy-preserving performance on widely used datasets. Moreover, our framework is effective for multiple steganographic models.

</details>


### [149] [Automatic Wire-Harness Color Sequence Detector](https://arxiv.org/abs/2512.12590)
*Indiwara Nanayakkara,Dehan Jayawickrama,Mervyn Parakrama B. Ekanayake*

Main category: cs.CV

TL;DR: A semi-automated machine vision system for wire harness inspection achieves 100% accuracy and 44% time reduction using five CMOS cameras and color sequence classification.


<details>
  <summary>Details</summary>
Motivation: Wire harness inspection in EMS industry remains labor-intensive and error-prone, requiring automation to improve accuracy and efficiency.

Method: Five industrial CMOS cameras in modular framework with HSV/RGB color domain comparison classifier; system trained with 5+ reference samples per harness type.

Result: 100% detection accuracy and 44% reduction in inspection time compared to manual methods; deployed at GPV Lanka Pvt. Ltd.

Conclusion: The semi-automated machine vision system provides reliable and efficient wire harness inspection capabilities for industrial applications.

Abstract: Wire harness inspection process remains a labor-intensive process prone to errors in the modern Electronics Manufacturing Services (EMS) industry. This paper introduces a semiautomated machine vision system capable of verifying correct wire positioning, correctness of the connector polarity and correctness of color sequences for both linear and circular wire harness configurations. Five industrial standard CMOS cameras are integrated into a modularized mechanical framework in the physical structure of the solution and a HSV and RGB color domain value comparison based color sequence classifier is used in the operation. For each harness batch, a user can train the system using at least five reference samples; the trained file is stored and reused for similar harness types. The Solution is deployed at GPV Lanka Pvt. Ltd. (Fig. 2) and the system achieved 100% detection accuracy and reduced inspection time by 44% compared to manual methods. Additional features include user management, adjustable lighting, session data storage, and secure login. Results of this product usage in the real world situation demonstrate that this approach delivers reliable and efficient inspection capabilities.

</details>


### [150] [Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation](https://arxiv.org/abs/2512.12595)
*Karthikeya KV*

Main category: cs.CV

TL;DR: A vision-enhanced LLM framework using rectified flow and bidirectional tokenization for efficient high-resolution image synthesis and multimodal understanding, achieving 25% better resolution clarity with 20% lower computational cost than diffusion methods.


<details>
  <summary>Details</summary>
Motivation: To address challenges in high-resolution image synthesis and multimodal data interpretation by integrating vision-enhanced LLMs with advanced transformer architectures, overcoming limitations of existing methods in handling diverse data types and noisy distributions.

Method: Proposes a framework with rectified flow mechanism for linear noise-data paths, bidirectional tokenization for text-image-video fusion, spatial-temporal feature embedding, hybrid text-image sequence modeling, and noise-aware learning algorithm for distribution discrepancies.

Result: Achieves 25% increase in image resolution clarity and 20% reduction in computational requirements compared to diffusion-based methods, with robust scalability and adaptability for applications in autonomous systems, creative content generation, and video analysis.

Conclusion: The work demonstrates vision-enhanced LLMs' transformative potential in redefining computer vision and multimodal AI capabilities through efficient, high-fidelity synthesis and coherent multimodal understanding across diverse data types.

Abstract: This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.

</details>


### [151] [Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models](https://arxiv.org/abs/2512.12596)
*Kei Yoshitake,Kento Hosono,Ken Kobayashi,Kazuhide Nakata*

Main category: cs.CV

TL;DR: A VLM-based method for generating image-based advertisement layouts that analyzes image content and spatial relationships to create placement plans, then renders them as HTML layouts.


<details>
  <summary>Details</summary>
Motivation: Conventional ad layout methods rely on saliency mapping which often fails to account for detailed image composition and semantic content, limiting layout quality.

Method: Two-step pipeline: 1) VLM analyzes image to identify object types and spatial relationships, generating text-based placement plans; 2) Plans are rendered into final HTML-format layouts.

Result: Quantitative and qualitative evaluations show the method produces noticeably higher-quality advertisement layouts compared to existing methods by explicitly considering background image content.

Conclusion: Leveraging VLM for semantic understanding of image content enables more effective advertisement layout generation than traditional saliency-based approaches.

Abstract: In this paper, we propose a method for generating layouts for image-based advertisements by leveraging a Vision-Language Model (VLM). Conventional advertisement layout techniques have predominantly relied on saliency mapping to detect salient regions within a background image, but such approaches often fail to fully account for the image's detailed composition and semantic content. To overcome this limitation, our method harnesses a VLM to recognize the products and other elements depicted in the background and to inform the placement of text and logos. The proposed layout-generation pipeline consists of two steps. In the first step, the VLM analyzes the image to identify object types and their spatial relationships, then produces a text-based "placement plan" based on this analysis. In the second step, that plan is rendered into the final layout by generating HTML-format code. We validated the effectiveness of our approach through evaluation experiments, conducting both quantitative and qualitative comparisons against existing methods. The results demonstrate that by explicitly considering the background image's content, our method produces noticeably higher-quality advertisement layouts.

</details>


### [152] [Geometry-Aware Scene-Consistent Image Generation](https://arxiv.org/abs/2512.12598)
*Cong Xie,Che Wang,Yan Zhang,Zheng Pan,Han Zou,Zhenpeng Zhan*

Main category: cs.CV

TL;DR: A method for geometry-aware scene-consistent image generation that preserves reference scenes while generating entities according to spatial relations in text prompts.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to balance scene preservation with prompt adherence - they either replicate scenes well but respond poorly to prompts, or prioritize prompts at the expense of scene consistency.

Method: Two key contributions: (1) scene-consistent data construction pipeline generating diverse, geometrically-grounded training pairs, and (2) geometry-guided attention loss leveraging cross-view cues to regularize spatial reasoning.

Result: Achieves better scene alignment and text-image consistency than state-of-the-art baselines on scene-consistent benchmark, with superior performance on both automatic metrics and human preference studies.

Conclusion: The method produces geometrically coherent images with diverse compositions that remain faithful to textual instructions and underlying scene structure, resolving the trade-off between scene preservation and prompt adherence.

Abstract: We study geometry-aware scene-consistent image generation: given a reference scene image and a text condition specifying an entity to be generated in the scene and its spatial relation to the scene, the goal is to synthesize an output image that preserves the same physical environment as the reference scene while correctly generating the entity according to the spatial relation described in the text. Existing methods struggle to balance scene preservation with prompt adherence: they either replicate the scene with high fidelity but poor responsiveness to the prompt, or prioritize prompt compliance at the expense of scene consistency. To resolve this trade-off, we introduce two key contributions: (i) a scene-consistent data construction pipeline that generates diverse, geometrically-grounded training pairs, and (ii) a novel geometry-guided attention loss that leverages cross-view cues to regularize the model's spatial reasoning. Experiments on our scene-consistent benchmark show that our approach achieves better scene alignment and text-image consistency than state-of-the-art baselines, according to both automatic metrics and human preference studies. Our method produces geometrically coherent images with diverse compositions that remain faithful to the textual instructions and the underlying scene structure.

</details>


### [153] [Efficient Vision-Language Reasoning via Adaptive Token Pruning](https://arxiv.org/abs/2512.12701)
*Xue Li,Xiaonan Song,Henry Hu*

Main category: cs.CV

TL;DR: ATP is a dynamic token pruning method for VLMs that reduces computational costs by 40% with minimal accuracy loss, using hybrid importance scores to retain only the most informative tokens.


<details>
  <summary>Details</summary>
Motivation: Real-world deployment of Vision-Language Models is hindered by high computational demands due to inefficient uniform token processing across all tokens.

Method: ATP uses a lightweight gating module that assigns hybrid importance scores combining ViT CLS attention (intra-modal saliency) and CLIP text-image similarity (inter-modal relevance) to keep only top-K tokens for the LLM, adapting dynamically to each input without modifying backbone architectures.

Result: ATP reduces inference FLOPs by around 40%, achieves roughly 1.5x speedups in end-to-end latency with negligible accuracy loss (<1%) on VQAv2, GQA, and COCO benchmarks, and improves robustness under corruptions by suppressing spurious correlations.

Conclusion: Resource-constrained inference and model reliability are not competing objectives; ATP enables efficient multimodal edge computing while preserving visual grounding and enhancing interpretability.

Abstract: Real-world deployment of Vision-Language Models (VLMs) is hindered by high computational demands, as existing architectures inefficiently process all tokens uniformly. We introduce Adaptive Token Pruning (ATP), a dynamic inference mechanism that retains only the most informative tokens based on contextual relevance. ATP operates at the vision-language interface, assigning a hybrid importance score combining ViT CLS attention (intra-modal saliency) and CLIP text-image similarity (inter-modal relevance) to keep top-K tokens for the LLM. Unlike static compression, ATP adapts to each input without modifying the backbone. Proposed as a lightweight gating module, ATP is compatible with popular backbones like BLIP-2, LLaVA, and Flamingo. Preliminary evaluations across VQAv2, GQA, and COCO indicate that ATP reduces inference FLOPs by around 40% and achieves roughly 1.5x speedups in end-to-end latency with negligible accuracy loss (less than 1%). Qualitative analyses suggest ATP preserves visual grounding and enhances interpretability. Beyond efficiency, we investigate robustness under corruptions; observations suggest adaptive pruning suppresses spurious correlations, improving stability. These findings imply that resource-constrained inference and model reliability are not competing objectives. Finally, we discuss ATP's role in efficient multimodal edge computing pipelines.

</details>


### [154] [No Cache Left Idle: Accelerating diffusion model via Extreme-slimming Caching](https://arxiv.org/abs/2512.12604)
*Tingyan Wen,Haoyu Li,Yihuang Chen,Xing Zhou,Lifei Zhu,Xueqian Wang*

Main category: cs.CV

TL;DR: X-Slim is a training-free caching framework that accelerates diffusion models by exploiting redundancy across timesteps, blocks, and tokens through a dual-threshold controller that pushes reuse then polishes with lightweight refresh.


<details>
  <summary>Details</summary>
Motivation: Diffusion models have high computational overhead that scales with step count, model depth, and sequence length. Existing caching methods face a trade-off: aggressive timestep reuse speeds up but hurts fidelity, while block/token-level reuse is safer but offers limited savings.

Method: X-Slim introduces a unified framework with dual-threshold controller that turns caching into push-then-polish: first pushes timestep-level reuse up to early-warning line, then switches to block/token-level refresh to polish remaining redundancy, triggering full inference when critical line is crossed to reset error.

Result: Achieves up to 4.97x latency reduction on FLUX.1-dev and 3.52x on HunyuanVideo with minimal perceptual loss. On DiT-XL/2, reaches 3.13x acceleration and improves FID by 2.42 over prior methods.

Conclusion: X-Slim advances the speed-quality frontier for diffusion models by intelligently exploiting cacheable redundancy across multiple dimensions through a novel push-then-polish approach with context-aware caching decisions.

Abstract: Diffusion models achieve remarkable generative quality, but computational overhead scales with step count, model depth, and sequence length. Feature caching is effective since adjacent timesteps yield highly similar features. However, an inherent trade-off remains: aggressive timestep reuse offers large speedups but can easily cross the critical line, hurting fidelity, while block- or token-level reuse is safer but yields limited computational savings. We present X-Slim (eXtreme-Slimming Caching), a training-free, cache-based accelerator that, to our knowledge, is the first unified framework to exploit cacheable redundancy across timesteps, structure (blocks), and space (tokens). Rather than simply mixing levels, X-Slim introduces a dual-threshold controller that turns caching into a push-then-polish process: it first pushes reuse at the timestep level up to an early-warning line, then switches to lightweight block- and token-level refresh to polish the remaining redundancy, and triggers full inference once the critical line is crossed to reset accumulated error. At each level, context-aware indicators decide when and where to cache. Across diverse tasks, X-Slim advances the speed-quality frontier. On FLUX.1-dev and HunyuanVideo, it reduces latency by up to 4.97x and 3.52x with minimal perceptual loss. On DiT-XL/2, it reaches 3.13x acceleration and improves FID by 2.42 over prior methods.

</details>


### [155] [Patch-wise Retrieval: A Bag of Practical Techniques for Instance-level Matching](https://arxiv.org/abs/2512.12610)
*Wonseok Choi,Sohwi Lim,Nam Hyeon-Woo,Moon Ye-Bin,Dong-Ju Jeong,Jinyoung Hwang,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: Patchify is a patch-wise image retrieval framework that divides database images into structured patches for accurate, scalable, and interpretable instance-level retrieval without fine-tuning, with a new localization-aware metric called LocScore.


<details>
  <summary>Details</summary>
Motivation: Instance-level image retrieval needs to handle variations in object size, position, and appearance. Existing methods may lack spatial grounding and interpretability, and there's a need for better evaluation metrics that assess not just retrieval accuracy but also spatial correctness of matches.

Method: Patchify divides database images into a small number of structured patches and performs retrieval by comparing local patch features with global query descriptors. The paper also introduces LocScore, a localization-aware metric to evaluate spatial alignment of retrieved regions with target objects.

Result: Patchify outperforms global retrieval methods and complements state-of-the-art reranking pipelines across multiple benchmarks, backbones, and region selection strategies. Product Quantization integration enables efficient large-scale retrieval, with informative feature compression significantly boosting performance.

Conclusion: Patchify provides an effective, scalable, and interpretable solution for instance-level image retrieval with spatial grounding, while LocScore offers valuable diagnostic capabilities for understanding and improving retrieval behavior.

Abstract: Instance-level image retrieval aims to find images containing the same object as a given query, despite variations in size, position, or appearance. To address this challenging task, we propose Patchify, a simple yet effective patch-wise retrieval framework that offers high performance, scalability, and interpretability without requiring fine-tuning. Patchify divides each database image into a small number of structured patches and performs retrieval by comparing these local features with a global query descriptor, enabling accurate and spatially grounded matching. To assess not just retrieval accuracy but also spatial correctness, we introduce LocScore, a localization-aware metric that quantifies whether the retrieved region aligns with the target object. This makes LocScore a valuable diagnostic tool for understanding and improving retrieval behavior. We conduct extensive experiments across multiple benchmarks, backbones, and region selection strategies, showing that Patchify outperforms global methods and complements state-of-the-art reranking pipelines. Furthermore, we apply Product Quantization for efficient large-scale retrieval and highlight the importance of using informative features during compression, which significantly boosts performance. Project website: https://wons20k.github.io/PatchwiseRetrieval/

</details>


### [156] [D3D-VLP: Dynamic 3D Vision-Language-Planning Model for Embodied Grounding and Navigation](https://arxiv.org/abs/2512.12622)
*Zihan Wang,Seungjun Lee,Guangzhao Dai,Gim Hee Lee*

Main category: cs.CV

TL;DR: D3D-VLP is a unified 3D vision-language-planning model that combines interpretable 3D reasoning with end-to-end learning through dynamic 3D chain-of-thought and synergistic learning from fragmented supervision.


<details>
  <summary>Details</summary>
Motivation: Current embodied AI approaches have a critical dilemma: end-to-end models lack interpretability and explicit 3D reasoning, while modular systems ignore cross-component interdependencies and synergies. There's a need to bridge this gap.

Method: 1) Dynamic 3D Chain-of-Thought (3D CoT) that unifies planning, grounding, navigation, and question answering within a single 3D-VLM and CoT pipeline. 2) Synergistic Learning from Fragmented Supervision (SLFS) strategy using masked autoregressive loss to learn from massive partially-annotated hybrid data, allowing different CoT components to mutually reinforce each other.

Result: Achieves state-of-the-art results on multiple benchmarks: Vision-and-Language Navigation (R2R-CE, REVERIE-CE, NavRAG-CE), Object-goal Navigation (HM3D-OVON), and Task-oriented Sequential Grounding and Navigation (SG3D). Real-world mobile manipulation experiments further validate effectiveness.

Conclusion: D3D-VLP successfully bridges the gap between interpretable 3D reasoning and end-to-end learning, demonstrating superior performance across diverse embodied AI tasks through unified 3D chain-of-thought reasoning and synergistic learning from fragmented supervision.

Abstract: Embodied agents face a critical dilemma that end-to-end models lack interpretability and explicit 3D reasoning, while modular systems ignore cross-component interdependencies and synergies. To bridge this gap, we propose the Dynamic 3D Vision-Language-Planning Model (D3D-VLP). Our model introduces two key innovations: 1) A Dynamic 3D Chain-of-Thought (3D CoT) that unifies planning, grounding, navigation, and question answering within a single 3D-VLM and CoT pipeline; 2) A Synergistic Learning from Fragmented Supervision (SLFS) strategy, which uses a masked autoregressive loss to learn from massive and partially-annotated hybrid data. This allows different CoT components to mutually reinforce and implicitly supervise each other. To this end, we construct a large-scale dataset with 10M hybrid samples from 5K real scans and 20K synthetic scenes that are compatible with online learning methods such as RL and DAgger. Our D3D-VLP achieves state-of-the-art results on multiple benchmarks, including Vision-and-Language Navigation (R2R-CE, REVERIE-CE, NavRAG-CE), Object-goal Navigation (HM3D-OVON), and Task-oriented Sequential Grounding and Navigation (SG3D). Real-world mobile manipulation experiments further validate the effectiveness.

</details>


### [157] [SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition](https://arxiv.org/abs/2512.12885)
*Minghao Zhu,Zhihao Zhang,Anmol Sidhu,Keith Redmill*

Main category: cs.CV

TL;DR: Zero-shot road sign recognition using RAG paradigm: VLM generates textual descriptions, retrieves relevant candidates from vector database, and LLM makes final fine-grained classification, achieving high accuracy without task-specific training.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning methods struggle with the large number of road sign classes and the impracticality of creating exhaustive labeled datasets for automated road sign recognition in intelligent transportation systems.

Method: A zero-shot recognition framework adapting the Retrieval-Augmented Generation (RAG) paradigm: 1) Vision Language Model (VLM) generates textual description from input image, 2) Retrieves relevant sign candidates from vector database of reference designs, 3) Large Language Model (LLM) reasons over retrieved candidates for final fine-grained recognition.

Result: Achieved 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data when tested on 303 regulatory signs from the Ohio MUTCD.

Conclusion: Demonstrates viability of RAG-based architectures for creating scalable and accurate road sign recognition systems without task-specific training, offering a practical solution to the limitations of traditional deep learning approaches.

Abstract: Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.

</details>


### [158] [DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal Large Language Model](https://arxiv.org/abs/2512.12633)
*Zhou Tao,Shida Wang,Yongxiang Hua,Haoyu Cao,Linli Xu*

Main category: cs.CV

TL;DR: DiG introduces a differential grounding proxy task where MLLMs learn fine-grained perception by identifying all differences between similar image pairs without knowing how many differences exist, using automated 3D rendering and curriculum learning.


<details>
  <summary>Details</summary>
Motivation: Multimodal Large Language Models have impressive performance but limited fine-grained visual perception and precise spatial reasoning capabilities.

Method: Develops DiG framework with automated 3D rendering pipeline to generate paired images with controllable discrepancies, and employs curriculum learning that progresses from single to multiple differences for stable optimization.

Result: DiG significantly improves model performance across various visual perception benchmarks and transfers effectively to downstream tasks including RefCOCO, RefCOCO+, RefCOCOg, and general multimodal perception benchmarks.

Conclusion: Differential grounding is a scalable and robust approach for advancing fine-grained visual reasoning in MLLMs.

Abstract: Multimodal Large Language Models have achieved impressive performance on a variety of vision-language tasks, yet their fine-grained visual perception and precise spatial reasoning remain limited. In this work, we introduce DiG (Differential Grounding), a novel proxy task framework where MLLMs learn fine-grained perception by identifying and localizing all differences between similar image pairs without prior knowledge of their number. To support scalable training, we develop an automated 3D rendering-based data generation pipeline that produces high-quality paired images with fully controllable discrepancies. To address the sparsity of difference signals, we further employ curriculum learning that progressively increases complexity from single to multiple differences, enabling stable optimization. Extensive experiments demonstrate that DiG significantly improves model performance across a variety of visual perception benchmarks and that the learned fine-grained perception skills transfer effectively to standard downstream tasks, including RefCOCO, RefCOCO+, RefCOCOg, and general multimodal perception benchmarks. Our results highlight differential grounding as a scalable and robust approach for advancing fine-grained visual reasoning in MLLMs.

</details>


### [159] [Cross-modal Fundus Image Registration under Large FoV Disparity](https://arxiv.org/abs/2512.12657)
*Hongyang Li,Junyi Tao,Qijie Wei,Ningzhi Yang,Meng Wang,Weihong Yu,Xirong Li*

Main category: cs.CV

TL;DR: CARe is a simple yet effective method for cross-modal fundus image registration with large Field-of-View disparity, using crop and alignment operations to enable registration between OCTA and wide-field color fundus photographs.


<details>
  <summary>Details</summary>
Motivation: Previous cross-modal fundus image registration methods assume small Field-of-View disparity, but fail in challenging scenarios with large FoV disparity between modalities like OCTA and wide-field color fundus photographs.

Method: CARe uses two main operations: 1) Crop operation that exploits retinal physiological structure to crop a sub-image from the target wide-field image with roughly aligned FoV to the source OCTA image, and 2) Alignment module with double-fitting using RANSAC algorithm and polynomial-based coordinate fitting for improved spatial transformation.

Result: Extensive experiments on a newly developed test set of 60 OCTA-wfCFP pairs verify the viability of CARe for cross-modal fundus image registration with large FoV disparity.

Conclusion: CARe successfully addresses the challenging problem of cross-modal fundus image registration with large Field-of-View disparity by enabling the re-purposing of previous small-FoV-disparity methods through a simple crop and alignment approach.

Abstract: Previous work on cross-modal fundus image registration (CMFIR) assumes small cross-modal Field-of-View (FoV) disparity. By contrast, this paper is targeted at a more challenging scenario with large FoV disparity, to which directly applying current methods fails. We propose Crop and Alignment for cross-modal fundus image Registration(CARe), a very simple yet effective method. Specifically, given an OCTA with smaller FoV as a source image and a wide-field color fundus photograph (wfCFP) as a target image, our Crop operation exploits the physiological structure of the retina to crop from the target image a sub-image with its FoV roughly aligned with that of the source. This operation allows us to re-purpose the previous small-FoV-disparity oriented methods for subsequent image registration. Moreover, we improve spatial transformation by a double-fitting based Alignment module that utilizes the classical RANSAC algorithm and polynomial-based coordinate fitting in a sequential manner. Extensive experiments on a newly developed test set of 60 OCTA-wfCFP pairs verify the viability of CARe for CMFIR.

</details>


### [160] [CogDoc: Towards Unified thinking in Documents](https://arxiv.org/abs/2512.12658)
*Qixin Xu,Haozhe Wang,Che Liu,Fangzhen Lin,Wenhu Chen*

Main category: cs.CV

TL;DR: CogDoc is a unified coarse-to-fine thinking framework for document reasoning that balances scalability and fidelity by mimicking human cognitive processes, achieving SOTA performance with a 7B model that beats larger proprietary models.


<details>
  <summary>Details</summary>
Motivation: Current document reasoning systems face a fundamental trade-off between scalability (processing long documents) and fidelity (capturing fine-grained multimodal details), creating a gap that needs to be bridged.

Method: Proposes CogDoc, a unified coarse-to-fine thinking framework with two phases: 1) "Fast Reading" for scalable information localization at low resolution, and 2) "Focused Thinking" for deep reasoning at high resolution. Uses Direct Reinforcement Learning approach that outperforms RL with SFT initialization by avoiding "policy conflict."

Result: The 7B model achieves state-of-the-art performance within its parameter class and notably surpasses significantly larger proprietary models (e.g., GPT-4o) on challenging, visually rich document benchmarks.

Conclusion: CogDoc successfully bridges the scalability-fidelity trade-off in document reasoning by mimicking human cognitive processes, with direct RL proving superior to SFT-initialized RL, enabling a relatively small 7B model to outperform much larger proprietary models.

Abstract: Current document reasoning paradigms are constrained by a fundamental trade-off between scalability (processing long-context documents) and fidelity (capturing fine-grained, multimodal details). To bridge this gap, we propose CogDoc, a unified coarse-to-fine thinking framework that mimics human cognitive processes: a low-resolution "Fast Reading" phase for scalable information localization,followed by a high-resolution "Focused Thinking" phase for deep reasoning. We conduct a rigorous investigation into post-training strategies for the unified thinking framework, demonstrating that a Direct Reinforcement Learning (RL) approach outperforms RL with Supervised Fine-Tuning (SFT) initialization. Specifically, we find that direct RL avoids the "policy conflict" observed in SFT. Empirically, our 7B model achieves state-of-the-art performance within its parameter class, notably surpassing significantly larger proprietary models (e.g., GPT-4o) on challenging, visually rich document benchmarks.

</details>


### [161] [Anatomy-Guided Representation Learning Using a Transformer-Based Network for Thyroid Nodule Segmentation in Ultrasound Images](https://arxiv.org/abs/2512.12662)
*Muhammad Umar Farooq,Abd Ur Rehman,Azka Rehman,Muhammad Usman,Dong-Kyu Chae,Junaid Qadir*

Main category: cs.CV

TL;DR: SSMT-Net is a semi-supervised multi-task transformer network for thyroid nodule segmentation in ultrasound images that outperforms state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Thyroid nodule segmentation in ultrasound images faces challenges including ambiguous boundaries, size variations, and scarcity of annotated data. Existing deep learning models struggle to incorporate contextual information from thyroid glands and generalize across diverse cases.

Method: Proposes SSMT-Net with two phases: 1) Unsupervised phase leveraging unlabeled data to enhance transformer-centric encoder feature extraction, and 2) Supervised phase jointly optimizing nodule segmentation, gland segmentation, and nodule size estimation to integrate local and global contextual features.

Result: Extensive evaluations on TN3K and DDTI datasets show SSMT-Net outperforms state-of-the-art methods with higher accuracy and robustness.

Conclusion: SSMT-Net demonstrates potential for real-world clinical applications in thyroid nodule diagnosis and treatment planning.

Abstract: Accurate thyroid nodule segmentation in ultrasound images is critical for diagnosis and treatment planning. However, ambiguous boundaries between nodules and surrounding tissues, size variations, and the scarcity of annotated ultrasound data pose significant challenges for automated segmentation. Existing deep learning models struggle to incorporate contextual information from the thyroid gland and generalize effectively across diverse cases. To address these challenges, we propose SSMT-Net, a Semi-Supervised Multi-Task Transformer-based Network that leverages unlabeled data to enhance Transformer-centric encoder feature extraction capability in an initial unsupervised phase. In the supervised phase, the model jointly optimizes nodule segmentation, gland segmentation, and nodule size estimation, integrating both local and global contextual features. Extensive evaluations on the TN3K and DDTI datasets demonstrate that SSMT-Net outperforms state-of-the-art methods, with higher accuracy and robustness, indicating its potential for real-world clinical applications.

</details>


### [162] [Heart Disease Prediction using Case Based Reasoning (CBR)](https://arxiv.org/abs/2512.13078)
*Mohaiminul Islam Bhuiyan,Chan Hue Wah,Nur Shazwani Kamarudin,Nur Hafieza Ismail,Ahmad Fakhri Ab Nasir*

Main category: cs.CV

TL;DR: Heart disease prediction using intelligent systems, with CBR achieving 97.95% accuracy and showing higher risk for males (57.76%) than females (42.24%).


<details>
  <summary>Details</summary>
Motivation: Traditional heart disease prediction methods relying on doctor's experience lack precision, so intelligent systems are needed as more accurate alternatives.

Method: Compared three intelligent systems (Fuzzy Logic, Neural Networks, CBR), selected CBR, performed data pre-processing and splitting, then applied CBR for prediction on heart disease dataset.

Result: CBR achieved 97.95% accuracy in heart disease prediction, with males having 57.76% probability and females 42.24% probability of heart disease.

Conclusion: CBR is effective for heart disease prediction, with gender differences in risk and lifestyle factors like smoking/alcohol consumption being significant contributors, especially for males.

Abstract: This study provides an overview of heart disease prediction using an intelligent system. Predicting disease accurately is crucial in the medical field, but traditional methods relying solely on a doctor's experience often lack precision. To address this limitation, intelligent systems are applied as an alternative to traditional approaches. While various intelligent system methods exist, this study focuses on three: Fuzzy Logic, Neural Networks, and Case-Based Reasoning (CBR). A comparison of these techniques in terms of accuracy was conducted, and ultimately, Case-Based Reasoning (CBR) was selected for heart disease prediction. In the prediction phase, the heart disease dataset underwent data pre-processing to clean the data and data splitting to separate it into training and testing sets. The chosen intelligent system was then employed to predict heart disease outcomes based on the processed data. The experiment concluded with Case-Based Reasoning (CBR) achieving a notable accuracy rate of 97.95% in predicting heart disease. The findings also revealed that the probability of heart disease was 57.76% for males and 42.24% for females. Further analysis from related studies suggests that factors such as smoking and alcohol consumption are significant contributors to heart disease, particularly among males.

</details>


### [163] [InteracTalker: Prompt-Based Human-Object Interaction with Co-Speech Gesture Generation](https://arxiv.org/abs/2512.12664)
*Sreehari Rajan,Kunal Bhosikar,Charu Sharma*

Main category: cs.CV

TL;DR: InteracTalker: A novel framework that integrates speech-driven gestures with object interactions using multi-stage training and adaptive fusion for realistic human motion generation.


<details>
  <summary>Details</summary>
Motivation: Current methods address speech-driven gestures and object interactions independently, lacking integrated datasets and limiting real-world applicability for interactive digital experiences.

Method: Multi-stage training to learn unified motion-speech-prompt embedding space, Generalized Motion Adaptation Module for independent training, adaptive fusion strategy to balance heterogeneous conditioning signals during diffusion sampling.

Result: Outperforms prior methods in both co-speech gesture generation and object-interaction synthesis, yielding highly realistic, object-aware full-body motions with enhanced realism, flexibility, and control.

Conclusion: InteracTalker successfully unifies previously separate tasks of speech-driven gestures and object interactions, creating a comprehensive framework for realistic human motion generation in interactive applications.

Abstract: Generating realistic human motions that naturally respond to both spoken language and physical objects is crucial for interactive digital experiences. Current methods, however, address speech-driven gestures or object interactions independently, limiting real-world applicability due to a lack of integrated, comprehensive datasets. To overcome this, we introduce InteracTalker, a novel framework that seamlessly integrates prompt-based object-aware interactions with co-speech gesture generation. We achieve this by employing a multi-stage training process to learn a unified motion, speech, and prompt embedding space. To support this, we curate a rich human-object interaction dataset, formed by augmenting an existing text-to-motion dataset with detailed object interaction annotations. Our framework utilizes a Generalized Motion Adaptation Module that enables independent training, adapting to the corresponding motion condition, which is then dynamically combined during inference. To address the imbalance between heterogeneous conditioning signals, we propose an adaptive fusion strategy, which dynamically reweights the conditioning signals during diffusion sampling. InteracTalker successfully unifies these previously separate tasks, outperforming prior methods in both co-speech gesture generation and object-interaction synthesis, outperforming gesture-focused diffusion methods, yielding highly realistic, object-aware full-body motions with enhanced realism, flexibility, and control.

</details>


### [164] [Open-World Deepfake Attribution via Confidence-Aware Asymmetric Learning](https://arxiv.org/abs/2512.12667)
*Haiyang Zheng,Nan Pu,Wenjing Li,Teng Long,Nicu Sebe,Zhun Zhong*

Main category: cs.CV

TL;DR: CAL framework addresses Open-World DeepFake Attribution limitations with confidence-aware asymmetric learning and dynamic prototype pruning, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Existing Open-World DeepFake Attribution methods suffer from confidence skew causing unreliable pseudo-labels for novel forgeries, and unrealistic assumption that number of unknown forgery types is known beforehand.

Method: Proposes Confidence-Aware Asymmetric Learning (CAL) framework with two components: Confidence-Aware Consistency Regularization (CCR) to mitigate pseudo-label bias by dynamically scaling losses based on normalized confidence, and Asymmetric Confidence Reinforcement (ACR) to separately calibrate confidence for known/novel classes. Also introduces Dynamic Prototype Pruning (DPP) to automatically estimate number of novel forgery types.

Result: Extensive experiments on standard OW-DFA benchmark and extended benchmark with advanced manipulations show CAL consistently outperforms previous methods, achieving new state-of-the-art performance on both known and novel forgery attribution.

Conclusion: CAL framework effectively addresses key limitations in Open-World DeepFake Attribution through confidence-aware learning and automatic novel type estimation, enhancing real-world applicability and scalability.

Abstract: The proliferation of synthetic facial imagery has intensified the need for robust Open-World DeepFake Attribution (OW-DFA), which aims to attribute both known and unknown forgeries using labeled data for known types and unlabeled data containing a mixture of known and novel types. However, existing OW-DFA methods face two critical limitations: 1) A confidence skew that leads to unreliable pseudo-labels for novel forgeries, resulting in biased training. 2) An unrealistic assumption that the number of unknown forgery types is known *a priori*. To address these challenges, we propose a Confidence-Aware Asymmetric Learning (CAL) framework, which adaptively balances model confidence across known and novel forgery types. CAL mainly consists of two components: Confidence-Aware Consistency Regularization (CCR) and Asymmetric Confidence Reinforcement (ACR). CCR mitigates pseudo-label bias by dynamically scaling sample losses based on normalized confidence, gradually shifting the training focus from high- to low-confidence samples. ACR complements this by separately calibrating confidence for known and novel classes through selective learning on high-confidence samples, guided by their confidence gap. Together, CCR and ACR form a mutually reinforcing loop that significantly improves the model's OW-DFA performance. Moreover, we introduce a Dynamic Prototype Pruning (DPP) strategy that automatically estimates the number of novel forgery types in a coarse-to-fine manner, removing the need for unrealistic prior assumptions and enhancing the scalability of our methods to real-world OW-DFA scenarios. Extensive experiments on the standard OW-DFA benchmark and a newly extended benchmark incorporating advanced manipulations demonstrate that CAL consistently outperforms previous methods, achieving new state-of-the-art performance on both known and novel forgery attribution.

</details>


### [165] [Progressive Conditioned Scale-Shift Recalibration of Self-Attention for Online Test-time Adaptation](https://arxiv.org/abs/2512.12673)
*Yushun Tang,Ziqiong Liu,Jiyuan Jia,Yi Zhang,Zhihai He*

Main category: cs.CV

TL;DR: PCSR method improves online test-time adaptation for transformers by progressively recalibrating self-attention with conditioned scale-shift factors to handle domain shifts.


<details>
  <summary>Details</summary>
Motivation: Transformer models suffer significant performance degradation when applied to new target domains due to substantial changes in Query, Key, and Value features in self-attention modules during domain shifts.

Method: Progressive Conditioned Scale-Shift Recalibration (PCSR) uses local linear transforms parameterized by conditioned scale and shift factors to recalibrate self-attention at each layer. It employs a Domain Separation Network to extract domain shift features and a Factor Generator Network to predict scale-shift parameters, both adapted online during inference.

Result: The method achieves significant improvement in online test-time domain adaptation performance, with up to 3.9% accuracy gain on ImageNet-C dataset compared to baseline approaches.

Conclusion: PCSR effectively addresses transformer performance degradation in domain shifts through progressive self-attention recalibration with lightweight online adaptation networks, demonstrating substantial improvements in test-time adaptation.

Abstract: Online test-time adaptation aims to dynamically adjust a network model in real-time based on sequential input samples during the inference stage. In this work, we find that, when applying a transformer network model to a new target domain, the Query, Key, and Value features of its self-attention module often change significantly from those in the source domain, leading to substantial performance degradation of the transformer model. To address this important issue, we propose to develop a new approach to progressively recalibrate the self-attention at each layer using a local linear transform parameterized by conditioned scale and shift factors. We consider the online model adaptation from the source domain to the target domain as a progressive domain shift separation process. At each transformer network layer, we learn a Domain Separation Network to extract the domain shift feature, which is used to predict the scale and shift parameters for self-attention recalibration using a Factor Generator Network. These two lightweight networks are adapted online during inference. Experimental results on benchmark datasets demonstrate that the proposed progressive conditioned scale-shift recalibration (PCSR) method is able to significantly improve the online test-time domain adaptation performance by a large margin of up to 3.9\% in classification accuracy on the ImageNet-C dataset.

</details>


### [166] [Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling](https://arxiv.org/abs/2512.12675)
*Yuran Wang,Bohan Zeng,Chengzhuo Tong,Wenxuan Liu,Yang Shi,Xiaochen Ma,Hao Liang,Yuanxing Zhang,Wentao Zhang*

Main category: cs.CV

TL;DR: Scone is a unified understanding-generation method for multi-subject image generation that addresses both composition (arranging subjects) and distinction (identifying correct subjects), outperforming existing models through semantic bridging and two-stage training.


<details>
  <summary>Details</summary>
Motivation: Current subject-driven image generation focuses on multi-subject composition but neglects distinction - the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings.

Method: Scone integrates composition and distinction through a unified understanding-generation approach. It uses an understanding expert as a semantic bridge to convey semantic information and guide the generation expert. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking.

Result: Scone outperforms existing open-source models in both composition and distinction tasks on two benchmarks. The authors also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios.

Conclusion: Scone successfully addresses the distinction problem in multi-subject image generation through unified understanding-generation, semantic bridging, and two-stage training, enabling more effective performance in complex visual settings.

Abstract: Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.

</details>


### [167] [$β$-CLIP: Text-Conditioned Contrastive Learning for Multi-Granular Vision-Language Alignment](https://arxiv.org/abs/2512.12678)
*Fatimah Zohra,Chen Zhao,Hani Itani,Bernard Ghanem*

Main category: cs.CV

TL;DR: β-CLIP is a multi-granular contrastive learning framework that achieves hierarchical alignment between textual granularities (captions, sentences, phrases) and corresponding visual regions, outperforming CLIP on fine-grained tasks.


<details>
  <summary>Details</summary>
Motivation: CLIP performs well on zero-shot image-text retrieval but struggles with fine-grained tasks even when fine-tuned on detailed captions, due to its focus on global alignment rather than dense, multi-granular correspondence.

Method: β-CLIP uses cross-attention to dynamically pool image patches for each textual granularity level, producing contextualized visual embeddings. It introduces β-Contextualized Contrastive Alignment Loss (β-CAL) to handle semantic overlap in the hierarchy, balancing strict query-specific matching with relaxed intra-image contextualization.

Result: Achieves state-of-the-art performance: 91.8% T2I and 92.3% I2T at R@1 on Urban1K, and 30.9% on FG-OVD (Hard), establishing best results among methods trained without hard negatives.

Conclusion: β-CLIP provides a robust, adaptive baseline for dense vision-language correspondence, significantly improving fine-grained alignment through hierarchical multi-granular learning.

Abstract: CLIP achieves strong zero-shot image-text retrieval by aligning global vision and text representations, yet it falls behind on fine-grained tasks even when fine-tuned on long, detailed captions. In this work, we propose $β$-CLIP, a multi-granular text-conditioned contrastive learning framework designed to achieve hierarchical alignment between multiple textual granularities-from full captions to sentences and phrases-and their corresponding visual regions. For each level of granularity, $β$-CLIP utilizes cross-attention to dynamically pool image patches, producing contextualized visual embeddings. To address the semantic overlap inherent in this hierarchy, we introduce the $β$-Contextualized Contrastive Alignment Loss ($β$-CAL). This objective parameterizes the trade-off between strict query-specific matching and relaxed intra-image contextualization, supporting both soft Cross-Entropy and hard Binary Cross-Entropy formulations. Through extensive experiments, we demonstrate that $β$-CLIP significantly improves dense alignment: achieving 91.8% T2I 92.3% I2T at R@1 on Urban1K and 30.9% on FG-OVD (Hard), setting state-of-the-art among methods trained without hard negatives. $β$-CLIP establishes a robust, adaptive baseline for dense vision-language correspondence. The code and models are released at https://github.com/fzohra/B-CLIP.

</details>


### [168] [Robust Motion Generation using Part-level Reliable Data from Videos](https://arxiv.org/abs/2512.12703)
*Boyuan Li,Sipeng Zheng,Bin Cao,Ruihua Song,Zongqing Lu*

Main category: cs.CV

TL;DR: A method for extracting human motion from web videos using part-aware masked autoregression to handle missing/occluded body parts, with a new benchmark K700-M for evaluation.


<details>
  <summary>Details</summary>
Motivation: Web videos offer scalable motion data but often have missing/occluded body parts, creating a trade-off between data scale/diversity and quality. Need to handle noisy/incomplete data while maintaining motion generation quality.

Method: 1) Decompose human body into 5 parts, detect "credible" visible parts per frame; 2) Encode credible parts into latent tokens using part-aware VAE; 3) Use robust part-level masked generation model to predict masked credible parts while ignoring noisy parts.

Result: Method outperforms baselines on both clean and noisy datasets in motion quality, semantic consistency, and diversity. Introduces K700-M benchmark with ~200k real-world motion sequences.

Conclusion: The proposed part-aware masked autoregression approach effectively handles incomplete/occluded motion data from web videos, enabling scalable motion extraction without compromising quality.

Abstract: Extracting human motion from large-scale web videos offers a scalable solution to the data scarcity issue in character animation. However, some human parts in many video frames cannot be seen due to off-screen captures or occlusions. It brings a dilemma: discarding the data missing any part limits scale and diversity, while retaining it compromises data quality and model performance.
  To address this problem, we propose leveraging credible part-level data extracted from videos to enhance motion generation via a robust part-aware masked autoregression model. First, we decompose a human body into five parts and detect the parts clearly seen in a video frame as "credible". Second, the credible parts are encoded into latent tokens by our proposed part-aware variational autoencoder. Third, we propose a robust part-level masked generation model to predict masked credible parts, while ignoring those noisy parts.
  In addition, we contribute K700-M, a challenging new benchmark comprising approximately 200k real-world motion sequences, for evaluation. Experimental results indicate that our method successfully outperforms baselines on both clean and noisy datasets in terms of motion quality, semantic consistency and diversity. Project page: https://boyuaner.github.io/ropar-main/

</details>


### [169] [Spinal Line Detection for Posture Evaluation through Train-ing-free 3D Human Body Reconstruction with 2D Depth Images](https://arxiv.org/abs/2512.12718)
*Sehyun Kim,Hye Jun Lee,Jiwoo Lee,Changgyun Kim,Taemin Lee*

Main category: cs.CV

TL;DR: Proposes a 3D body posture analysis system using four-direction depth images to restore 3D human models and estimate spine center lines without complex neural networks.


<details>
  <summary>Details</summary>
Motivation: Existing multi-image body restoration methods require expensive equipment and complex procedures, while single-image methods struggle with occlusion and viewpoint limitations for accurately estimating internal structures like spine center lines.

Method: Integrates depth images from four directions, uses hierarchical matching with global and fine registration, applies Adaptive Vertex Reduction to maintain mesh resolution and shape reliability, and employs Level of Detail ensemble for spinal angle estimation.

Result: Achieves high-precision 3D spine registration estimation without training data or complex neural network models, with verification confirming improved matching quality.

Conclusion: The proposed system effectively compensates for multi-image method shortcomings while solving single-image limitations, providing accurate and stable spinal angle estimation for body posture analysis.

Abstract: The spinal angle is an important indicator of body balance. It is important to restore the 3D shape of the human body and estimate the spine center line. Existing mul-ti-image-based body restoration methods require expensive equipment and complex pro-cedures, and single image-based body restoration methods have limitations in that it is difficult to accurately estimate the internal structure such as the spine center line due to occlusion and viewpoint limitation. This study proposes a method to compensate for the shortcomings of the multi-image-based method and to solve the limitations of the sin-gle-image method. We propose a 3D body posture analysis system that integrates depth images from four directions to restore a 3D human model and automatically estimate the spine center line. Through hierarchical matching of global and fine registration, restora-tion to noise and occlusion is performed. Also, the Adaptive Vertex Reduction is applied to maintain the resolution and shape reliability of the mesh, and the accuracy and stabil-ity of spinal angle estimation are simultaneously secured by using the Level of Detail en-semble. The proposed method achieves high-precision 3D spine registration estimation without relying on training data or complex neural network models, and the verification confirms the improvement of matching quality.

</details>


### [170] [Towards Interactive Intelligence for Digital Humans](https://arxiv.org/abs/2512.13674)
*Yiyi Cai,Xuangeng Chu,Xiwei Gao,Sitong Gong,Yifei Huang,Caixin Kang,Kunhang Li,Haiyang Liu,Ruicong Liu,Yun Liu,Dianwen Ng,Zixiong Su,Erwin Wu,Yuhan Wu,Dingkun Yan,Tianyu Yan,Chang Zeng,Bo Zheng,You Zhou*

Main category: cs.CV

TL;DR: Mio is an end-to-end framework for creating Interactive Intelligence - digital humans with personality-aligned expression, adaptive interaction, and self-evolution capabilities through five specialized modules.


<details>
  <summary>Details</summary>
Motivation: Current digital humans are limited to superficial imitation rather than intelligent interaction. The paper aims to move beyond simple animation to create digital humans capable of personality-aligned expression, adaptive interaction, and self-evolution.

Method: Proposes Mio (Multimodal Interactive Omni-Avatar) framework with five specialized modules: Thinker (cognitive reasoning), Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment.

Result: Extensive experiments show superior performance compared to state-of-the-art methods across all evaluated dimensions. The paper also establishes a new benchmark for evaluating interactive intelligence capabilities.

Conclusion: The Mio framework moves digital humans beyond superficial imitation toward intelligent interaction, representing a significant advancement in creating Interactive Intelligence with personality-aligned expression and adaptive capabilities.

Abstract: We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.

</details>


### [171] [GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation](https://arxiv.org/abs/2512.12751)
*Zhenya Yang,Zhe Liu,Yuxiang Lu,Liping Hou,Chenxuan Miao,Siyi Peng,Bailan Feng,Xiang Bai,Hengshuang Zhao*

Main category: cs.CV

TL;DR: GenieDrive is a physics-aware driving video generation framework that uses 4D occupancy as a physics-informed foundation, achieving better forecasting accuracy and video quality with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Existing driving world models using single diffusion models directly mapping actions to videos are difficult to learn and produce physically inconsistent outputs, necessitating a physics-aware approach.

Method: Generates 4D occupancy first as physics foundation, uses VAE with latent tri-plane representation to compress occupancy, introduces Mutual Control Attention for control modeling, and Normalized Multi-View Attention for video generation guided by occupancy.

Result: 7.2% improvement in forecasting mIoU at 41 FPS with only 3.47M parameters, 20.7% reduction in FVD for video quality, enabling controllable, multi-view consistent physics-aware driving videos.

Conclusion: GenieDrive provides an effective framework for physics-aware driving video generation that outperforms existing methods in both forecasting accuracy and video quality while being more parameter-efficient.

Abstract: Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.

</details>


### [172] [FysicsWorld: A Unified Full-Modality Benchmark for Any-to-Any Understanding, Generation, and Reasoning](https://arxiv.org/abs/2512.12756)
*Yue Jiang,Dingkang Yang,Minghao Han,Jinghang Han,Zizhi Chen,Yizhou Liu,Mingcheng Li,Peng Zhai,Lihua Zhang*

Main category: cs.CV

TL;DR: FysicsWorld is the first unified full-modality benchmark supporting bidirectional input-output across image, video, audio, and text for comprehensive any-to-any evaluation of multimodal models.


<details>
  <summary>Details</summary>
Motivation: Current multimodal benchmarks are limited in scope and integration, suffering from incomplete modality coverage, text-centric outputs, and weak interdependence among modalities. There's a need for comprehensive evaluation across all modalities with bidirectional capabilities.

Method: Created FysicsWorld benchmark with 16 primary tasks and 3,268 curated samples from over 40 sources. Introduced Cross-Modal Complementarity Screening (CMCS) strategy within a systematic data construction framework to produce omni-modal data for spoken interaction and fusion-dependent cross-modal reasoning.

Result: Evaluation of over 30 state-of-the-art baselines (MLLMs, modality-specific models, unified understanding-generation models, omni-modal language models) exposed performance disparities and limitations across models in understanding, generation, and reasoning tasks.

Conclusion: FysicsWorld establishes a unified foundation and strong baselines for evaluating and advancing next-generation full-modality architectures, addressing current limitations in multimodal benchmarking.

Abstract: Despite rapid progress in multimodal large language models (MLLMs) and emerging omni-modal architectures, current benchmarks remain limited in scope and integration, suffering from incomplete modality coverage, restricted interaction to text-centric outputs, and weak interdependence and complementarity among modalities. To bridge these gaps, we introduce FysicsWorld, the first unified full-modality benchmark that supports bidirectional input-output across image, video, audio, and text, enabling comprehensive any-to-any evaluation across understanding, generation, and reasoning. FysicsWorld encompasses 16 primary tasks and 3,268 curated samples, aggregated from over 40 high-quality sources and covering a rich set of open-domain categories with diverse question types. We also propose the Cross-Modal Complementarity Screening (CMCS) strategy integrated in a systematic data construction framework that produces omni-modal data for spoken interaction and fusion-dependent cross-modal reasoning. Through a comprehensive evaluation of over 30 state-of-the-art baselines, spanning MLLMs, modality-specific models, unified understanding-generation models, and omni-modal language models, FysicsWorld exposes the performance disparities and limitations across models in understanding, generation, and reasoning. Our benchmark establishes a unified foundation and strong baselines for evaluating and advancing next-generation full-modality architectures.

</details>


### [173] [CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence](https://arxiv.org/abs/2512.12768)
*Tianjiao Yu,Xinzhuo Li,Yifan Shen,Yuanzhe Liu,Ismini Lourentzou*

Main category: cs.CV

TL;DR: CoRe3D introduces a unified reasoning framework for 3D understanding and generation that combines semantic chain-of-thought inference with structured spatial reasoning to improve 3D content creation from language descriptions.


<details>
  <summary>Details</summary>
Motivation: While reasoning mechanisms have proven effective in language and vision tasks, their extension to 3D remains underdeveloped. The paper aims to improve 3D model reliability, interpretability, and cross-modal alignment by introducing explicit reasoning mechanisms for 3D tasks.

Method: CoRe3D uses a unified 3D understanding and generation reasoning framework that operates over semantic and spatial abstractions. It employs a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, enabling compositional and procedural reasoning over geometry. The approach tightly couples semantic chain-of-thought inference with structured spatial reasoning.

Result: The framework produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions, enabling high-level intent from language to directly guide low-level 3D content formation.

Conclusion: CoRe3D successfully extends reasoning-centric approaches to 3D tasks, demonstrating that explicit reasoning mechanisms can significantly improve 3D model performance by enabling better cross-modal alignment and more interpretable 3D content generation.

Abstract: Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.

</details>


### [174] [Fast 2DGS: Efficient Image Representation with Deep Gaussian Prior](https://arxiv.org/abs/2512.12774)
*Hao Wang,Ashish Bastola,Chaoyi Zhou,Wenhui Zhu,Xiwen Chen,Xuanzhao Dong,Siyu Huang,Abolfazl Razi*

Main category: cs.CV

TL;DR: Fast-2DGS is a lightweight framework that uses a conditional network (Deep Gaussian Prior) and attribute regression network to efficiently initialize 2D Gaussian Splatting for high-quality image representation in a single forward pass with minimal fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current 2D Gaussian Splatting methods require slow post-optimization (over 10 seconds) and use inefficient initialization strategies (random or heuristic-based). Recent learnable network approaches add computational complexity. There's a need for efficient, high-quality Gaussian image representations suitable for industry deployment.

Method: Proposes Fast-2DGS with two key components: 1) Deep Gaussian Prior - a conditional network that captures spatial distribution of Gaussian primitives based on image complexity, and 2) Attribute regression network to predict dense Gaussian properties. Uses disentangled architecture for single-pass reconstruction with minimal fine-tuning.

Result: Achieves high-quality reconstruction in a single forward pass with minimal fine-tuning. Significantly reduces computational cost while maintaining visual quality. Enables faster convergence compared to existing methods (>10s optimization).

Conclusion: Fast-2DGS bridges the gap between quality and efficiency in 2D Gaussian Splatting, making it more suitable for industry-ready deployment by reducing computational overhead without compromising reconstruction quality.

Abstract: As generative models become increasingly capable of producing high-fidelity visual content, the demand for efficient, interpretable, and editable image representations has grown substantially. Recent advances in 2D Gaussian Splatting (2DGS) have emerged as a promising solution, offering explicit control, high interpretability, and real-time rendering capabilities (>1000 FPS). However, high-quality 2DGS typically requires post-optimization. Existing methods adopt random or heuristics (e.g., gradient maps), which are often insensitive to image complexity and lead to slow convergence (>10s). More recent approaches introduce learnable networks to predict initial Gaussian configurations, but at the cost of increased computational and architectural complexity. To bridge this gap, we present Fast-2DGS, a lightweight framework for efficient Gaussian image representation. Specifically, we introduce Deep Gaussian Prior, implemented as a conditional network to capture the spatial distribution of Gaussian primitives under different complexities. In addition, we propose an attribute regression network to predict dense Gaussian properties. Experiments demonstrate that this disentangled architecture achieves high-quality reconstruction in a single forward pass, followed by minimal fine-tuning. More importantly, our approach significantly reduces computational cost without compromising visual quality, bringing 2DGS closer to industry-ready deployment.

</details>


### [175] [L-STEC: Learned Video Compression with Long-term Spatio-Temporal Enhanced Context](https://arxiv.org/abs/2512.12790)
*Tiange Zhang,Zhimeng Huang,Xiandong Meng,Kai Zhang,Zhipin Deng,Siwei Ma*

Main category: cs.CV

TL;DR: L-STEC method improves neural video compression by capturing long-term dependencies and preserving spatial details, achieving 37% bitrate savings over DCVC-TCM.


<details>
  <summary>Details</summary>
Motivation: Existing neural video compression methods rely only on previous frame features, missing long-term dependencies and fine texture details, while accumulating errors over frames.

Method: Proposes Long-term Spatio-Temporal Enhanced Context (L-STEC) with LSTM for long-term dependencies and warped spatial context from pixel domain, fused through multi-receptive field network.

Result: Achieves 37.01% bitrate savings in PSNR and 31.65% in MS-SSIM compared to DCVC-TCM, outperforming VTM-17.0 and DCVC-FM for state-of-the-art performance.

Conclusion: L-STEC significantly improves video compression by enriching contextual information through long-term spatio-temporal enhancement, establishing new state-of-the-art performance.

Abstract: Neural Video Compression has emerged in recent years, with condition-based frameworks outperforming traditional codecs. However, most existing methods rely solely on the previous frame's features to predict temporal context, leading to two critical issues. First, the short reference window misses long-term dependencies and fine texture details. Second, propagating only feature-level information accumulates errors over frames, causing prediction inaccuracies and loss of subtle textures. To address these, we propose the Long-term Spatio-Temporal Enhanced Context (L-STEC) method. We first extend the reference chain with LSTM to capture long-term dependencies. We then incorporate warped spatial context from the pixel domain, fusing spatio-temporal information through a multi-receptive field network to better preserve reference details. Experimental results show that L-STEC significantly improves compression by enriching contextual information, achieving 37.01% bitrate savings in PSNR and 31.65% in MS-SSIM compared to DCVC-TCM, outperforming both VTM-17.0 and DCVC-FM and establishing new state-of-the-art performance.

</details>


### [176] [DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning](https://arxiv.org/abs/2512.12799)
*Zhe Liu,Runhui Huang,Rui Yang,Siming Yan,Zining Wang,Lu Hou,Di Lin,Xiang Bai,Hengshuang Zhao*

Main category: cs.CV

TL;DR: DrivePI is a spatial-aware 4D MLLM that unifies vision-language-action capabilities for autonomous driving, performing 3D perception, prediction, and planning in parallel with impressive performance using only a 0.5B backbone.


<details>
  <summary>Details</summary>
Motivation: Multi-modal LLMs have strong capabilities but their application in generating fine-grained 3D perception and prediction outputs for autonomous driving remains underexplored. There's a need for unified frameworks that can handle spatial understanding, 3D perception, prediction, and planning together.

Method: Proposes DrivePI, a spatial-aware 4D MLLM that integrates point clouds, multi-view images, and language instructions in a unified architecture. Uses a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Performs spatial understanding, 3D occupancy, occupancy flow, and action planning through end-to-end optimization.

Result: With only a 0.5B Qwen2.5 backbone, DrivePI matches or exceeds both existing VLA models and specialized VA models. Outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA, reduces collision rate by 70% over ORION, surpasses FB-OCC by 10.3 RayIoU for 3D occupancy, reduces mAVE from 0.591 to 0.509 for occupancy flow, and achieves 32% lower L2 error than VAD for planning.

Conclusion: DrivePI demonstrates that a unified spatial-aware 4D MLLM can effectively handle multiple autonomous driving tasks simultaneously, achieving state-of-the-art performance with a relatively small model size, showing promise for comprehensive autonomous driving systems.

Abstract: Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI

</details>


### [177] [Learning Common and Salient Generative Factors Between Two Image Datasets](https://arxiv.org/abs/2512.12800)
*Yunlong He,Gwilherm Lesné,Ziqian Liu,Michaël Soumm,Pietro Gori*

Main category: cs.CV

TL;DR: A novel framework for Contrastive Analysis (CA) that separates common generative factors (shared across two datasets) from salient ones (specific to one dataset) using only dataset signals, adaptable to both GAN and Diffusion models.


<details>
  <summary>Details</summary>
Motivation: Most existing work focuses on conditional manipulation or disentangled representation learning using attribute supervision. This paper addresses the less studied problem of Contrastive Analysis - automatically separating shared vs. dataset-specific generative factors using only dataset-level signals without attribute supervision.

Method: Proposes a novel framework with new learning strategies and losses specifically designed for Contrastive Analysis. The method is adaptable to both GAN and Diffusion models, ensuring relevant separation between common and salient factors while maintaining high-quality generation.

Result: The framework demonstrates superior separation ability and image quality synthesis compared to prior methods across diverse datasets including human faces, animal images, and medical scans.

Conclusion: The proposed Contrastive Analysis framework successfully addresses the understudied problem of separating common vs. salient generative factors using only dataset signals, outperforming existing methods in both separation quality and image synthesis across multiple domains.

Abstract: Recent advancements in image synthesis have enabled high-quality image generation and manipulation. Most works focus on: 1) conditional manipulation, where an image is modified conditioned on a given attribute, or 2) disentangled representation learning, where each latent direction should represent a distinct semantic attribute. In this paper, we focus on a different and less studied research problem, called Contrastive Analysis (CA). Given two image datasets, we want to separate the common generative factors, shared across the two datasets, from the salient ones, specific to only one dataset. Compared to existing methods, which use attributes as supervised signals for editing (e.g., glasses, gender), the proposed method is weaker, since it only uses the dataset signal. We propose a novel framework for CA, that can be adapted to both GAN and Diffusion models, to learn both common and salient factors. By defining new and well-adapted learning strategies and losses, we ensure a relevant separation between common and salient factors, preserving a high-quality generation. We evaluate our approach on diverse datasets, covering human faces, animal images and medical scans. Our framework demonstrates superior separation ability and image quality synthesis compared to prior methods.

</details>


### [178] [Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding](https://arxiv.org/abs/2512.12822)
*Yongyuan Liang,Xiyao Wang,Yuanchen Ju,Jianwei Yang,Furong Huang*

Main category: cs.CV

TL;DR: Lemon is a unified transformer architecture for 3D multimodal understanding that jointly processes 3D point cloud patches and language tokens as a single sequence, enabling early spatial-linguistic fusion and eliminating fragmented modality-specific encoders.


<details>
  <summary>Details</summary>
Motivation: Current large multimodal models face challenges in scaling to 3D understanding: point cloud data is sparse/irregular, existing architectures use fragmented modality-specific encoders, and training pipelines suffer from instability and poor scalability.

Method: Unified transformer architecture processing 3D point cloud patches and language tokens as single sequence; structured patchification/tokenization scheme preserving spatial context; three-stage training curriculum from object-level recognition to scene-level spatial reasoning.

Result: Establishes new state-of-the-art performance across comprehensive 3D understanding tasks (object recognition, captioning, spatial reasoning) while demonstrating robust scaling properties as model size and training data increase.

Conclusion: Provides a unified foundation for advancing 3D spatial intelligence in real-world applications through early spatial-linguistic fusion, parameter efficiency, and effective model scaling.

Abstract: Scaling large multimodal models (LMMs) to 3D understanding poses unique challenges: point cloud data is sparse and irregular, existing models rely on fragmented architectures with modality-specific encoders, and training pipelines often suffer from instability and poor scalability. We introduce Lemon, a unified transformer architecture that addresses these challenges by jointly processing 3D point cloud patches and language tokens as a single sequence. Unlike prior work that relies on modality-specific encoders and cross-modal alignment modules, this design enables early spatial-linguistic fusion, eliminates redundant encoders, improves parameter efficiency, and supports more effective model scaling. To handle the complexity of 3D data, we develop a structured patchification and tokenization scheme that preserves spatial context, and a three-stage training curriculum that progressively builds capabilities from object-level recognition to scene-level spatial reasoning. Lemon establishes new state-of-the-art performance across comprehensive 3D understanding and reasoning tasks, from object recognition and captioning to spatial reasoning in 3D scenes, while demonstrating robust scaling properties as model size and training data increase. Our work provides a unified foundation for advancing 3D spatial intelligence in real-world applications.

</details>


### [179] [Adapting Multimodal Foundation Models for Few-Shot Learning: A Comprehensive Study on Contrastive Captioners](https://arxiv.org/abs/2512.12824)
*N. K. B. M. P. K. B. Narasinghe,Uthayasanker Thayasivam*

Main category: cs.CV

TL;DR: Comprehensive study on adapting CoCa foundation models for few-shot image classification, evaluating strategies from training-free prototyping to LoRA fine-tuning, with findings on augmentation divergence and hybrid objectives.


<details>
  <summary>Details</summary>
Motivation: While CoCa models show strong zero-shot capabilities, their adaptation to few-shot learning with extreme data scarcity remains under-explored, especially compared to CLIP-style models. There's a gap in understanding how CoCa's unique latent space responds to parameter-efficient fine-tuning methods.

Method: Systematic evaluation of adaptation strategies for CoCa visual backbone: from training-free hybrid prototyping to deep parameter adaptation via Low-Rank Adaptation (LoRA). Investigated augmentation effects, hybrid objectives (Supervised Contrastive loss + Cross-Entropy), and sensitivity analysis of training configurations to data scarcity.

Result: Identified "augmentation divergence": strong augmentation harms linear probing but stabilizes LoRA fine-tuning. Hybrid SupCon+CE objectives consistently outperform standard CE across shot counts. Characterized sensitivity of training configurations to data scarcity, providing empirical reference settings for regularization, rank, and sampling strategies.

Conclusion: The study provides practical guidance for efficiently adapting generative-contrastive foundation models like CoCa to few-shot tasks, addressing the gap in understanding their adaptation behavior compared to dual-encoder architectures like CLIP.

Abstract: Large-scale multimodal foundation models, particularly Contrastive Captioners (CoCa), have achieved state-of-the-art results by unifying contrastive alignment with generative captioning. While zero-shot transfer capabilities are well-documented, the adaptation of these generative-contrastive hybrids to downstream tasks with extreme data scarcity (few-shot learning) remains under-explored. Existing literature predominantly focuses on dual-encoder architectures like CLIP, leaving a gap in understanding how CoCa's distinct latent space responds to parameter-efficient fine-tuning (PEFT). This paper presents a comprehensive empirical study on adapting the CoCa visual backbone for few-shot image classification. We systematically evaluate a hierarchy of strategies, ranging from training-free hybrid prototyping to deep parameter adaptation via Low-Rank Adaptation (LoRA). First, we identify an "augmentation divergence": while strong data augmentation degrades the performance of linear probing in low-shot settings, it is essential for stabilizing LoRA fine-tuning. We also demonstrate that hybrid objectives incorporating Supervised Contrastive (SupCon) loss yield consistent performance improvements over standard Cross-Entropy across varying shot counts. Crucially, we characterize the sensitivity of training configurations to data scarcity, providing empirical reference settings for scaling regularization, rank, and sampling strategies to facilitate the efficient adaptation of generative-contrastive foundation models.

</details>


### [180] [Schrodinger Audio-Visual Editor: Object-Level Audiovisual Removal](https://arxiv.org/abs/2512.12875)
*Weihan Xu,Kan Jen Cheng,Koichi Saito,Muhammad Jehanzeb Mirza,Tingle Li,Yisi Liu,Alexander H. Liu,Liming Wang,Masato Ishii,Takashi Shibuya,Yuki Mitsufuji,Gopala Anumanchipalli,Paul Pu Liang*

Main category: cs.CV

TL;DR: SAVE is a joint audio-visual editing model that uses flow-matching and Schrodinger Bridge to edit both modalities in parallel while maintaining synchronization, trained on the SAVEBench dataset.


<details>
  <summary>Details</summary>
Motivation: Joint editing of audio and visual content is important for precise content creation, but faces challenges due to limited paired audio-visual data and modality heterogeneity.

Method: Introduced SAVEBench dataset with text and mask conditions for object-grounded learning. Developed SAVE model using end-to-end flow-matching with Schrodinger Bridge to directly transport source to target audiovisual mixtures while keeping modalities aligned.

Result: SAVE successfully removes target objects in both audio and visual content while preserving remaining content, achieving better temporal synchronization and audiovisual semantic correspondence compared to separate audio/video editor combinations.

Conclusion: The proposed SAVE model effectively addresses joint audio-visual editing challenges through parallel processing with maintained alignment, demonstrating superior performance over pairwise editing approaches.

Abstract: Joint editing of audio and visual content is crucial for precise and controllable content creation. This new task poses challenges due to the limitations of paired audio-visual data before and after targeted edits, and the heterogeneity across modalities. To address the data and modeling challenges in joint audio-visual editing, we introduce SAVEBench, a paired audiovisual dataset with text and mask conditions to enable object-grounded source-to-target learning. With SAVEBench, we train the Schrodinger Audio-Visual Editor (SAVE), an end-to-end flow-matching model that edits audio and video in parallel while keeping them aligned throughout processing. SAVE incorporates a Schrodinger Bridge that learns a direct transport from source to target audiovisual mixtures. Our evaluation demonstrates that the proposed SAVE model is able to remove the target objects in audio and visual content while preserving the remaining content, with stronger temporal synchronization and audiovisual semantic correspondence compared with pairwise combinations of an audio editor and a video editor.

</details>


### [181] [Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection](https://arxiv.org/abs/2512.12884)
*Xiangzhong Liu,Jiajie Zhang,Hao Shen*

Main category: cs.CV

TL;DR: Proposes an end-to-end cross-level fusion Transformer that integrates object lists with raw camera images for 3D object detection, using object lists as denoising queries and incorporating deformable Gaussian masks for attention guidance.


<details>
  <summary>Details</summary>
Motivation: In automotive sensor fusion, smart sensors and V2X modules provide processed object lists rather than raw data. Traditional methods fuse at object level after separate processing, but there's a need for direct integration of abstract object list information with raw sensor data for better 3D detection.

Method: 1) End-to-end Transformer architecture integrating object lists as denoising queries with learnable queries; 2) Deformable Gaussian mask derived from object list priors to focus attention on target areas and accelerate convergence; 3) Generation of pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives/negatives for training.

Result: Shows substantial performance improvements over vision-based baseline on nuScenes dataset. Demonstrates generalization capability across diverse noise levels of simulated object lists and real detectors.

Conclusion: First work to conduct cross-level fusion, successfully integrating abstract object list information with raw camera data using Transformer architecture with attention guidance mechanisms, enabling effective 3D object detection in automotive sensor fusion systems.

Abstract: In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.

</details>


### [182] [Revisiting 2D Foundation Models for Scalable 3D Medical Image Classification](https://arxiv.org/abs/2512.12887)
*Han Liu,Bogdan Georgescu,Yanbo Zhang,Youngjin Yoo,Michael Baumgartner,Riqiang Gao,Jianing Wang,Gengyan Zhao,Eli Gibson,Dorin Comaniciu,Sasa Grbic*

Main category: cs.CV

TL;DR: AnyMC3D is a scalable 3D medical image classifier adapted from 2D foundation models using lightweight plugins, achieving SOTA across diverse tasks with a single framework.


<details>
  <summary>Details</summary>
Motivation: Current medical foundation models suffer from data-regime bias, suboptimal adaptation, and insufficient task coverage, limiting their practical utility in 3D medical image classification.

Method: Adapts 2D foundation models to 3D classification by adding lightweight plugins (~1M parameters per task) on top of a single frozen backbone. Supports multi-view inputs, pixel-level supervision, and interpretable heatmap generation.

Result: Achieves state-of-the-art performance across 12 diverse medical tasks (including 1st place in VLM3D challenge), demonstrating that 2D-based methods surpass 3D architectures and properly adapted general-purpose FMs can match medical-specific FMs.

Conclusion: AnyMC3D eliminates the need for separate task-specific models by providing a single scalable framework that effectively unlocks foundation model potential for diverse 3D medical image classification applications.

Abstract: 3D medical image classification is essential for modern clinical workflows. Medical foundation models (FMs) have emerged as a promising approach for scaling to new tasks, yet current research suffers from three critical pitfalls: data-regime bias, suboptimal adaptation, and insufficient task coverage. In this paper, we address these pitfalls and introduce AnyMC3D, a scalable 3D classifier adapted from 2D FMs. Our method scales efficiently to new tasks by adding only lightweight plugins (about 1M parameters per task) on top of a single frozen backbone. This versatile framework also supports multi-view inputs, auxiliary pixel-level supervision, and interpretable heatmap generation. We establish a comprehensive benchmark of 12 tasks covering diverse pathologies, anatomies, and modalities, and systematically analyze state-of-the-art 3D classification techniques. Our analysis reveals key insights: (1) effective adaptation is essential to unlock FM potential, (2) general-purpose FMs can match medical-specific FMs if properly adapted, and (3) 2D-based methods surpass 3D architectures for 3D classification. For the first time, we demonstrate the feasibility of achieving state-of-the-art performance across diverse applications using a single scalable framework (including 1st place in the VLM3D challenge), eliminating the need for separate task-specific models.

</details>


### [183] [Qonvolution: Towards Learning High-Frequency Signals with Queried Convolution](https://arxiv.org/abs/2512.12898)
*Abhinav Kumar,Tristan Aumentado-Armstrong,Lazar Valkov,Gopal Sharma,Alex Levinshtein,Radek Grzeszczuk,Suren Kumar*

Main category: cs.CV

TL;DR: Queried-Convolutions (Qonvolutions) improve high-frequency signal learning by convolving low-frequency signals with coordinate queries, achieving state-of-the-art performance on various vision/graphics tasks including novel view synthesis.


<details>
  <summary>Details</summary>
Motivation: Neural networks struggle with high-frequency signals due to spectral bias and optimization difficulties. Current methods like Fourier encodings have limitations when handling high-frequency information, creating a need for better approaches.

Method: Qonvolutions use neighborhood properties of convolution to convolve low-frequency signals with queries (such as coordinates), enhancing learning of intricate high-frequency signals through this simple modification.

Result: Qonvolutions improve performance across 1D regression, 2D super-resolution, 2D image regression, and novel view synthesis. Combined with Gaussian splatting for NVS, they achieve state-of-the-art performance on real-world complex scenes, outperforming radiance field models on image quality.

Conclusion: Qonvolutions provide an effective solution for high-frequency learning challenges in computer vision and graphics, demonstrating superior performance across multiple tasks and establishing new state-of-the-art results for novel view synthesis.

Abstract: Accurately learning high-frequency signals is a challenge in computer vision and graphics, as neural networks often struggle with these signals due to spectral bias or optimization difficulties. While current techniques like Fourier encodings have made great strides in improving performance, there remains scope for improvement when presented with high-frequency information. This paper introduces Queried-Convolutions (Qonvolutions), a simple yet powerful modification using the neighborhood properties of convolution. Qonvolution convolves a low-frequency signal with queries (such as coordinates) to enhance the learning of intricate high-frequency signals. We empirically demonstrate that Qonvolutions enhance performance across a variety of high-frequency learning tasks crucial to both the computer vision and graphics communities, including 1D regression, 2D super-resolution, 2D image regression, and novel view synthesis (NVS). In particular, by combining Gaussian splatting with Qonvolutions for NVS, we showcase state-of-the-art performance on real-world complex scenes, even outperforming powerful radiance field models on image quality.

</details>


### [184] [Predictive Sample Assignment for Semantically Coherent Out-of-Distribution Detection](https://arxiv.org/abs/2512.12906)
*Zhimao Peng,Enguang Wang,Xialei Liu,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: PSA framework improves SCOOD by using dual-threshold ternary assignment to filter noisy samples and concept contrastive learning for better ID/OOD separation.


<details>
  <summary>Details</summary>
Motivation: Current SCOOD methods use clustering-based filtering that introduces noisy samples during training, reducing detection accuracy.

Method: Proposes Predictive Sample Assignment (PSA) with: 1) dual-threshold ternary assignment based on energy scores to create ID, OOD, and discard sets; 2) concept contrastive learning to separate ID/OOD representations; 3) retraining strategy for better sample fitting.

Result: Outperforms state-of-the-art methods on two standard SCOOD benchmarks by significant margins.

Conclusion: PSA framework effectively addresses noisy sample issues in SCOOD through improved sample assignment and representation learning.

Abstract: Semantically coherent out-of-distribution detection (SCOOD) is a recently proposed realistic OOD detection setting: given labeled in-distribution (ID) data and mixed in-distribution and out-of-distribution unlabeled data as the training data, SCOOD aims to enable the trained model to accurately identify OOD samples in the testing data. Current SCOOD methods mainly adopt various clustering-based in-distribution sample filtering (IDF) strategies to select clean ID samples from unlabeled data, and take the remaining samples as auxiliary OOD data, which inevitably introduces a large number of noisy samples in training. To address the above issue, we propose a concise SCOOD framework based on predictive sample assignment (PSA). PSA includes a dual-threshold ternary sample assignment strategy based on the predictive energy score that can significantly improve the purity of the selected ID and OOD sample sets by assigning unconfident unlabeled data to an additional discard sample set, and a concept contrastive representation learning loss to further expand the distance between ID and OOD samples in the representation space to assist ID/OOD discrimination. In addition, we also introduce a retraining strategy to help the model fully fit the selected auxiliary ID/OOD samples. Experiments on two standard SCOOD benchmarks demonstrate that our approach outperforms the state-of-the-art methods by a significant margin.

</details>


### [185] [Sharpness-aware Dynamic Anchor Selection for Generalized Category Discovery](https://arxiv.org/abs/2512.12925)
*Zhimao Peng,Enguang Wang,Fei Yang,Xialei Liu,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: A novel method with Loss Sharpness Penalty (LSP) and Dynamic Anchor Selection (DAS) to improve pseudo-label quality in Generalized Category Discovery by reducing noise from large pre-trained models' spurious correlations.


<details>
  <summary>Details</summary>
Motivation: Current GCD methods using DINO-like pseudo-labeling suffer from noisy pseudo-labels because large pre-trained models encode spurious correlations and have preferences for specific visual patterns, leading to poor clustering of unlabeled data containing both known and unknown classes.

Method: Proposes two modules: 1) Loss Sharpness Penalty (LSP) - enhances robustness by minimizing worst-case loss sharpness to suppress trivial features and reduce overfitting to noise; 2) Dynamic Anchor Selection (DAS) - selects representative unknown class samples using KNN density and class probability, assigning hard pseudo-labels to alleviate confidence differences and accelerate learning of accurate feature distributions.

Result: Extensive experiments show the method effectively mitigates pseudo-label noise and achieves state-of-the-art results on multiple GCD benchmarks.

Conclusion: The proposed LSP and DAS modules successfully address the pseudo-label noise problem in GCD by improving model robustness and selecting better representative samples, leading to superior clustering performance for both known and unknown classes.

Abstract: Generalized category discovery (GCD) is an important and challenging task in open-world learning. Specifically, given some labeled data of known classes, GCD aims to cluster unlabeled data that contain both known and unknown classes. Current GCD methods based on parametric classification adopt the DINO-like pseudo-labeling strategy, where the sharpened probability output of one view is used as supervision information for the other view. However, large pre-trained models have a preference for some specific visual patterns, resulting in encoding spurious correlation for unlabeled data and generating noisy pseudo-labels. To address this issue, we propose a novel method, which contains two modules: Loss Sharpness Penalty (LSP) and Dynamic Anchor Selection (DAS). LSP enhances the robustness of model parameters to small perturbations by minimizing the worst-case loss sharpness of the model, which suppressing the encoding of trivial features, thereby reducing overfitting of noise samples and improving the quality of pseudo-labels. Meanwhile, DAS selects representative samples for the unknown classes based on KNN density and class probability during the model training and assigns hard pseudo-labels to them, which not only alleviates the confidence difference between known and unknown classes but also enables the model to quickly learn more accurate feature distribution for the unknown classes, thus further improving the clustering accuracy. Extensive experiments demonstrate that the proposed method can effectively mitigate the noise of pseudo-labels, and achieve state-of-the-art results on multiple GCD benchmarks.

</details>


### [186] [MADTempo: An Interactive System for Multi-Event Temporal Video Retrieval with Query Augmentation](https://arxiv.org/abs/2512.12929)
*Huu-An Vu,Van-Khanh Mai,Trong-Tam Nguyen,Quang-Duc Dam,Tien-Huy Nguyen,Thanh-Huong Le*

Main category: cs.CV

TL;DR: MADTempo is a video retrieval framework that combines temporal search for multi-event queries with web-scale visual grounding using Google Image Search to handle unseen concepts.


<details>
  <summary>Details</summary>
Motivation: Existing video retrieval systems struggle with modeling temporal dependencies across multiple events and handling queries that reference unseen or rare visual concepts, limiting their ability to understand complex temporal structures in videos.

Method: MADTempo unifies two components: 1) a temporal search mechanism that captures event-level continuity by aggregating similarity scores across sequential video segments, and 2) a Google Image Search-based fallback module that expands query representations with external web imagery to bridge gaps in pretrained visual embeddings.

Result: The framework advances temporal reasoning and generalization capabilities of video retrieval systems, improving robustness against out-of-distribution queries and enabling more coherent retrieval of multi-event queries.

Conclusion: MADTempo paves the way for more semantically aware and adaptive video retrieval across large-scale video corpora by addressing key limitations in temporal dependency modeling and visual concept generalization.

Abstract: The rapid expansion of video content across online platforms has accelerated the need for retrieval systems capable of understanding not only isolated visual moments but also the temporal structure of complex events. Existing approaches often fall short in modeling temporal dependencies across multiple events and in handling queries that reference unseen or rare visual concepts. To address these challenges, we introduce MADTempo, a video retrieval framework developed by our team, AIO_Trinh, that unifies temporal search with web-scale visual grounding. Our temporal search mechanism captures event-level continuity by aggregating similarity scores across sequential video segments, enabling coherent retrieval of multi-event queries. Complementarily, a Google Image Search-based fallback module expands query representations with external web imagery, effectively bridging gaps in pretrained visual embeddings and improving robustness against out-of-distribution (OOD) queries. Together, these components advance the temporal reasoning and generalization capabilities of modern video retrieval systems, paving the way for more semantically aware and adaptive retrieval across large-scale video corpora.

</details>


### [187] [Unified Interactive Multimodal Moment Retrieval via Cascaded Embedding-Reranking and Temporal-Aware Score Fusion](https://arxiv.org/abs/2512.12935)
*Toan Le Ngo Thanh,Phat Ha Huu,Tan Nguyen Dang Duy,Thong Nguyen Le Minh,Anh Nguyen Nhu Tinh*

Main category: cs.CV

TL;DR: A unified multimodal moment retrieval system that addresses three key challenges: adaptive fusion for cross-modal noise, temporal coherence modeling, and automatic modality selection through agent-guided query decomposition.


<details>
  <summary>Details</summary>
Motivation: The exponential growth of video content creates urgent need for efficient multimodal moment retrieval, but existing approaches suffer from fixed-weight fusion strategies that fail with cross-modal noise and ambiguous queries, poor temporal modeling that can't capture coherent event sequences, and manual modality selection requirements that reduce usability.

Method: Three key innovations: 1) Cascaded dual-embedding pipeline combining BEIT-3 and SigLIP for broad retrieval with BLIP-2 reranking for precision-recall balance; 2) Temporal-aware scoring with exponential decay penalties on large temporal gaps via beam search to construct coherent event sequences; 3) Agent-guided query decomposition using GPT-4o to interpret ambiguous queries, decompose into modality-specific sub-queries (visual/OCR/ASR), and perform adaptive score fusion.

Result: Qualitative analysis demonstrates the system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive moment search capabilities.

Conclusion: The proposed unified multimodal moment retrieval system addresses critical limitations of existing approaches through adaptive fusion, temporal coherence modeling, and automatic modality selection, representing an advancement in interactive video moment search technology.

Abstract: The exponential growth of video content has created an urgent need for efficient multimodal moment retrieval systems. However, existing approaches face three critical challenges: (1) fixed-weight fusion strategies fail across cross modal noise and ambiguous queries, (2) temporal modeling struggles to capture coherent event sequences while penalizing unrealistic gaps, and (3) systems require manual modality selection, reducing usability. We propose a unified multimodal moment retrieval system with three key innovations. First, a cascaded dual-embedding pipeline combines BEIT-3 and SigLIP for broad retrieval, refined by BLIP-2 based reranking to balance recall and precision. Second, a temporal-aware scoring mechanism applies exponential decay penalties to large temporal gaps via beam search, constructing coherent event sequences rather than isolated frames. Third, Agent-guided query decomposition (GPT-4o) automatically interprets ambiguous queries, decomposes them into modality specific sub-queries (visual/OCR/ASR), and performs adaptive score fusion eliminating manual modality selection. Qualitative analysis demonstrates that our system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive moment search capabilities.

</details>


### [188] [Content Adaptive based Motion Alignment Framework for Learned Video Compression](https://arxiv.org/abs/2512.12936)
*Tiange Zhang,Xiandong Meng,Siwei Ma*

Main category: cs.CV

TL;DR: CAMA: Content Adaptive Motion Alignment framework for neural video compression that improves performance through flow-guided deformable warping, multi-reference quality aware strategy, and motion-based downsampling.


<details>
  <summary>Details</summary>
Motivation: Current end-to-end video compression frameworks lack content-specific adaptation, leading to suboptimal compression performance. The paper aims to address this limitation by developing a framework that adapts encoding strategies to diverse content characteristics.

Method: 1) Two-stage flow-guided deformable warping with coarse-to-fine offset prediction and mask modulation for precise feature alignment. 2) Multi-reference quality aware strategy that adjusts distortion weights based on reference quality, applied to hierarchical training. 3) Training-free module that downsamples frames by motion magnitude and resolution for smooth motion estimation.

Result: CAMA achieves 24.95% BD-rate (PSNR) savings over baseline DCVC-TCM, and outperforms reproduced DCVC-DC and traditional codec HM-16.25 on standard test datasets.

Conclusion: The proposed content adaptive motion alignment framework effectively improves neural video compression performance by adapting to content characteristics, demonstrating significant gains over state-of-the-art methods.

Abstract: Recent advances in end-to-end video compression have shown promising results owing to their unified end-to-end learning optimization. However, such generalized frameworks often lack content-specific adaptation, leading to suboptimal compression performance. To address this, this paper proposes a content adaptive based motion alignment framework that improves performance by adapting encoding strategies to diverse content characteristics. Specifically, we first introduce a two-stage flow-guided deformable warping mechanism that refines motion compensation with coarse-to-fine offset prediction and mask modulation, enabling precise feature alignment. Second, we propose a multi-reference quality aware strategy that adjusts distortion weights based on reference quality, and applies it to hierarchical training to reduce error propagation. Third, we integrate a training-free module that downsamples frames by motion magnitude and resolution to obtain smooth motion estimation. Experimental results on standard test datasets demonstrate that our framework CAMA achieves significant improvements over state-of-the-art Neural Video Compression models, achieving a 24.95% BD-rate (PSNR) savings over our baseline model DCVC-TCM, while also outperforming reproduced DCVC-DC and traditional codec HM-16.25.

</details>


### [189] [UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction](https://arxiv.org/abs/2512.12941)
*Siyuan Yao,Dongxiu Liu,Taotao Li,Shengjie Li,Wenqi Ren,Xiaochun Cao*

Main category: cs.CV

TL;DR: UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network for building extraction from remote sensing images, using hybrid CNN-transformer encoder and uncertainty modeling to improve segmentation accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing building extraction methods suffer from inaccurate results due to feature pyramid gaps and insufficient global-local feature integration, especially with complex building structures in remote sensing images.

Method: Proposes UAGLNet with: 1) Cooperative encoder using hybrid CNN and transformer layers at different stages; 2) Intermediate cooperative interaction block (CIB) to bridge local-global feature gaps; 3) Global-Local Fusion (GLF) module for complementary feature fusion; 4) Uncertainty-Aggregated Decoder (UAD) for pixel-wise uncertainty estimation to enhance segmentation.

Result: Extensive experiments show superior performance compared to state-of-the-art methods for building extraction from remote sensing images.

Conclusion: UAGLNet effectively addresses building extraction challenges by integrating global-local semantics with uncertainty modeling, achieving more accurate segmentation results for complex building structures.

Abstract: Building extraction from remote sensing images is a challenging task due to the complex structure variations of the buildings. Existing methods employ convolutional or self-attention blocks to capture the multi-scale features in the segmentation models, while the inherent gap of the feature pyramids and insufficient global-local feature integration leads to inaccurate, ambiguous extraction results. To address this issue, in this paper, we present an Uncertainty-Aggregated Global-Local Fusion Network (UAGLNet), which is capable to exploit high-quality global-local visual semantics under the guidance of uncertainty modeling. Specifically, we propose a novel cooperative encoder, which adopts hybrid CNN and transformer layers at different stages to capture the local and global visual semantics, respectively. An intermediate cooperative interaction block (CIB) is designed to narrow the gap between the local and global features when the network becomes deeper. Afterwards, we propose a Global-Local Fusion (GLF) module to complementarily fuse the global and local representations. Moreover, to mitigate the segmentation ambiguity in uncertain regions, we propose an Uncertainty-Aggregated Decoder (UAD) to explicitly estimate the pixel-wise uncertainty to enhance the segmentation accuracy. Extensive experiments demonstrate that our method achieves superior performance to other state-of-the-art methods. Our code is available at https://github.com/Dstate/UAGLNet

</details>


### [190] [SCAdapter: Content-Style Disentanglement for Diffusion Style Transfer](https://arxiv.org/abs/2512.12963)
*Luan Thanh Trinh,Kenji Doi,Atsuki Osanai*

Main category: cs.CV

TL;DR: SCAdapter: A diffusion-based style transfer method using CLIP image space to separate content/style features, achieving photo-realistic transfers with 2x faster inference than other diffusion approaches.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models for style transfer produce painting-like results or miss detailed stylistic elements, failing to address unwanted influence from original content styles and style reference content features.

Method: Uses CLIP image space to systematically extract pure content from content images and style elements from style references. Three key components: Controllable Style Adaptive Instance Normalization (CSAdaIN) for multi-style blending, KVS Injection for targeted style integration, and style transfer consistency objective for process coherence.

Result: Significantly outperforms state-of-the-art methods in both conventional and diffusion-based baselines. Achieves at least 2x faster inference than other diffusion-based approaches by eliminating DDIM inversion and inference-stage optimization.

Conclusion: SCAdapter provides an effective and efficient solution for photo-realistic style transfer, addressing key limitations of current diffusion models while maintaining practical applicability through faster inference.

Abstract: Diffusion models have emerged as the leading approach for style transfer, yet they struggle with photo-realistic transfers, often producing painting-like results or missing detailed stylistic elements. Current methods inadequately address unwanted influence from original content styles and style reference content features. We introduce SCAdapter, a novel technique leveraging CLIP image space to effectively separate and integrate content and style features. Our key innovation systematically extracts pure content from content images and style elements from style references, ensuring authentic transfers. This approach is enhanced through three components: Controllable Style Adaptive Instance Normalization (CSAdaIN) for precise multi-style blending, KVS Injection for targeted style integration, and a style transfer consistency objective maintaining process coherence. Comprehensive experiments demonstrate SCAdapter significantly outperforms state-of-the-art methods in both conventional and diffusion-based baselines. By eliminating DDIM inversion and inference-stage optimization, our method achieves at least $2\times$ faster inference than other diffusion-based approaches, making it both more effective and efficient for practical applications.

</details>


### [191] [VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference](https://arxiv.org/abs/2512.12977)
*Shengling Qin,Hao Yu,Chenxin Wu,Zheng Li,Yizhong Cao,Zhengyang Zhuge,Yuxin Zhou,Wentao Yao,Yi Zhang,Zhengheng Wang,Shuai Bai,Jianwei Zhang,Junyang Lin*

Main category: cs.CV

TL;DR: VLCache is a cache reuse framework for multimodal models that reuses both KV and encoder caches from previous inputs to avoid recomputation, achieving near-full accuracy with only 2-5% token computation and 1.2x-16x speedups.


<details>
  <summary>Details</summary>
Motivation: Multimodal models incur costly recomputation when the same inputs recur. Existing heuristic approaches don't properly address cumulative reuse errors, leading to accuracy degradation.

Method: Formally identifies cumulative reuse error effect and minimizes non-prefix cache reuse error. Analyzes varying importance of model layers and proposes dynamic, layer-aware recomputation strategy to balance accuracy and efficiency.

Result: Achieves accuracy on par with full recomputation while requiring only 2-5% of tokens to compute, yielding 1.2x-16x TTFT (Time To First Token) speedups. Integrated into SGLang for practical deployment.

Conclusion: VLCache effectively eliminates costly recomputation for recurring multimodal inputs through formal error analysis and layer-aware caching, enabling significantly faster inference with minimal accuracy loss.

Abstract: This paper presents VLCache, a cache reuse framework that exploits both Key-Value (KV) cache and encoder cache from prior multimodal inputs to eliminate costly recomputation when the same multimodal inputs recur. Unlike previous heuristic approaches, we formally identify the cumulative reuse error effect and demonstrate how to minimize the non-prefix cache reuse error effectively. We further analyze the varying importance of model layers and propose a dynamic, layer-aware recomputation strategy to balance accuracy and efficiency. Experimental results show that VLCache achieves an accuracy on par with full recomputation, while requiring only 2-5% of the tokens to compute, yielding 1.2x-16x TTFT speedups. The proposed VLCache pipeline has been integrated into SGLang, enabling significantly faster inference in practical deployments.

</details>


### [192] [Scaling Up AI-Generated Image Detection via Generator-Aware Prototypes](https://arxiv.org/abs/2512.12982)
*Ziheng Qin,Yuheng Ji,Renshuai Tao,Yuxuan Tian,Yuyang Liu,Yipu Wang,Xiaolong Zheng*

Main category: cs.CV

TL;DR: GAPL framework addresses the "Benefit then Conflict" dilemma in universal AIGI detection by learning canonical forgery prototypes and using LoRA adaptation to handle data heterogeneity and model bottlenecks.


<details>
  <summary>Details</summary>
Motivation: Universal AIGI detectors face a paradox where increasing source diversity initially helps but eventually degrades performance due to data heterogeneity and fixed encoder bottlenecks.

Method: Generator-Aware Prototype Learning (GAPL) learns compact canonical forgery prototypes to create unified low-variance feature space, plus two-stage training with Low-Rank Adaptation to enhance discriminative power while preserving pretrained knowledge.

Result: GAPL achieves state-of-the-art performance with superior detection accuracy across various GAN and diffusion-based generators.

Conclusion: GAPL effectively addresses the Benefit then Conflict dilemma through structured prototype learning and adaptive training, establishing robust generalizable decision boundaries for universal AIGI detection.

Abstract: The pursuit of a universal AI-generated image (AIGI) detector often relies on aggregating data from numerous generators to improve generalization. However, this paper identifies a paradoxical phenomenon we term the Benefit then Conflict dilemma, where detector performance stagnates and eventually degrades as source diversity expands. Our systematic analysis, diagnoses this failure by identifying two core issues: severe data-level heterogeneity, which causes the feature distributions of real and synthetic images to increasingly overlap, and a critical model-level bottleneck from fixed, pretrained encoders that cannot adapt to the rising complexity. To address these challenges, we propose Generator-Aware Prototype Learning (GAPL), a framework that constrain representation with a structured learning paradigm. GAPL learns a compact set of canonical forgery prototypes to create a unified, low-variance feature space, effectively countering data heterogeneity.To resolve the model bottleneck, it employs a two-stage training scheme with Low-Rank Adaptation, enhancing its discriminative power while preserving valuable pretrained knowledge. This approach establishes a more robust and generalizable decision boundary. Through extensive experiments, we demonstrate that GAPL achieves state-of-the-art performance, showing superior detection accuracy across a wide variety of GAN and diffusion-based generators. Code is available at https://github.com/UltraCapture/GAPL

</details>


### [193] [Calibrating Uncertainty for Zero-Shot Adversarial CLIP](https://arxiv.org/abs/2512.12997)
*Wenjing lu,Zerui Tao,Dongping Zhang,Yuning Qiu,Yang Yang,Qibin Zhao*

Main category: cs.CV

TL;DR: Proposes adversarial fine-tuning for CLIP that aligns both prediction accuracy and uncertainty distributions to restore calibrated uncertainty while maintaining zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: CLIP has strong zero-shot classification but is vulnerable to adversarial attacks. Current adversarial fine-tuning focuses only on logit matching, overlooking uncertainty calibration. Adversarial perturbations often suppress uncertainty, causing miscalibration and over-confidence, creating a reliability gap beyond just robustness.

Method: Reparameterizes CLIP output as concentration parameter of Dirichlet distribution to capture both semantic structure and predictive confidence magnitude. Proposes unified adversarial fine-tuning objective that holistically aligns these distributions under perturbations, moving beyond single-logit anchoring.

Result: Experiments on multiple zero-shot classification benchmarks show the approach effectively restores calibrated uncertainty, achieves competitive adversarial robustness, and maintains clean accuracy.

Conclusion: The proposed method bridges the reliability gap in CLIP by addressing both robustness and uncertainty calibration, providing a more comprehensive adversarial defense that preserves zero-shot generalization capabilities.

Abstract: CLIP delivers strong zero-shot classification but remains highly vulnerable to adversarial attacks. Previous work of adversarial fine-tuning largely focuses on matching the predicted logits between clean and adversarial examples, which overlooks uncertainty calibration and may degrade the zero-shot generalization. A common expectation in reliable uncertainty estimation is that predictive uncertainty should increase as inputs become more difficult or shift away from the training distribution. However, we frequently observe the opposite in the adversarial setting: perturbations not only degrade accuracy but also suppress uncertainty, leading to severe miscalibration and unreliable over-confidence. This overlooked phenomenon highlights a critical reliability gap beyond robustness. To bridge this gap, we propose a novel adversarial fine-tuning objective for CLIP considering both prediction accuracy and uncertainty alignments. By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence. Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty. Experiments on multiple zero-shot classification benchmarks demonstrate that our approach effectively restores calibrated uncertainty and achieves competitive adversarial robustness while maintaining clean accuracy.

</details>


### [194] [Few-Step Distillation for Text-to-Image Generation: A Practical Guide](https://arxiv.org/abs/2512.13006)
*Yifan Pu,Yizeng Han,Zhiwei Tang,Jiasheng Tang,Fan Wang,Bohan Zhuang,Gao Huang*

Main category: cs.CV

TL;DR: This paper presents the first systematic study adapting diffusion distillation techniques from class-conditional to text-to-image generation, identifying key challenges and providing practical guidelines for efficient T2I models.


<details>
  <summary>Details</summary>
Motivation: While diffusion distillation has accelerated class-conditional image synthesis, its applicability to open-ended text-to-image generation remains unclear. The paper aims to bridge this gap by systematically studying how to adapt state-of-the-art distillation techniques for T2I models.

Method: The authors adapt and compare state-of-the-art distillation techniques on FLUX.1-lite, a strong T2I teacher model. They cast existing methods into a unified framework to identify key obstacles when moving from discrete class labels to free-form language prompts.

Result: The study provides practical guidelines on input scaling, network architecture, and hyperparameters for T2I distillation. They offer an open-source implementation and pretrained student models, establishing a foundation for fast, high-fidelity diffusion generators in real-world applications.

Conclusion: This work establishes a solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world text-to-image applications, with code and models publicly available.

Abstract: Diffusion distillation has dramatically accelerated class-conditional image synthesis, but its applicability to open-ended text-to-image (T2I) generation is still unclear. We present the first systematic study that adapts and compares state-of-the-art distillation techniques on a strong T2I teacher model, FLUX.1-lite. By casting existing methods into a unified framework, we identify the key obstacles that arise when moving from discrete class labels to free-form language prompts. Beyond a thorough methodological analysis, we offer practical guidelines on input scaling, network architecture, and hyperparameters, accompanied by an open-source implementation and pretrained student models. Our findings establish a solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world T2I applications. Code is available on github.com/alibaba-damo-academy/T2I-Distill.

</details>


### [195] [Light Field Based 6DoF Tracking of Previously Unobserved Objects](https://arxiv.org/abs/2512.13007)
*Nikolai Goncharov,James L. Gray,Donald G. Dansereau*

Main category: cs.CV

TL;DR: A light field-based 6DoF object tracking method that doesn't require pre-trained models, handles reflective objects using view-dependent Gaussian splats from foundation model features.


<details>
  <summary>Details</summary>
Motivation: Existing object tracking methods rely on pre-captured object views and struggle with visually complex appearances like reflections, limiting generalization to unseen objects in robotics and autonomous driving.

Method: Extract semantic and geometric features from light field images using vision foundation models, convert them into view-dependent Gaussian splats as unified object representation supporting differentiable rendering and pose optimization.

Result: Method is competitive with state-of-the-art model-based trackers on challenging reflective objects, demonstrated on a new light field object tracking dataset with precise ground truth poses.

Conclusion: The approach enables robust object tracking without pre-trained models, handling complex visual behavior like reflections, paving the way toward universal object tracking in robotic systems.

Abstract: Object tracking is an important step in robotics and reautonomous driving pipelines, which has to generalize to previously unseen and complex objects. Existing high-performing methods often rely on pre-captured object views to build explicit reference models, which restricts them to a fixed set of known objects. However, such reference models can struggle with visually complex appearance, reducing the quality of tracking. In this work, we introduce an object tracking method based on light field images that does not depend on a pre-trained model, while being robust to complex visual behavior, such as reflections. We extract semantic and geometric features from light field inputs using vision foundation models and convert them into view-dependent Gaussian splats. These splats serve as a unified object representation, supporting differentiable rendering and pose optimization. We further introduce a light field object tracking dataset containing challenging reflective objects with precise ground truth poses. Experiments demonstrate that our method is competitive with state-of-the-art model-based trackers in these difficult cases, paving the way toward universal object tracking in robotic systems. Code/data available at https://github.com/nagonch/LiFT-6DoF.

</details>


### [196] [TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading](https://arxiv.org/abs/2512.13008)
*Xi Luo,Shixin Xu,Ying Xie,JianZhong Hu,Yuwei He,Yuhui Deng,Huaxiong Huang*

Main category: cs.CV

TL;DR: TWLR is a two-stage framework for interpretable diabetic retinopathy assessment that combines vision-language models with weakly-supervised semantic segmentation to provide accurate DR grading, lesion classification, and interpretable visualizations without requiring pixel-level annotations.


<details>
  <summary>Details</summary>
Motivation: Medical image analysis requires high-quality expert annotations which are costly and time-consuming to obtain, especially for pixel-level labels in fundus images. Additionally, deep learning models lack interpretability, limiting their clinical adoption despite their success in medical imaging.

Method: Two-stage framework: 1) Vision-language model integrates ophthalmological knowledge into text embeddings to jointly perform DR grading and lesion classification, linking semantic medical concepts with visual features. 2) Iterative severity regression framework based on weakly-supervised semantic segmentation generates lesion saliency maps through iterative refinement, directing a progressive inpainting mechanism that systematically eliminates pathological features to downgrade disease severity toward healthier appearances.

Result: Experimental results on FGADR, DDR, and a private dataset demonstrate competitive performance in both DR classification and lesion segmentation. The approach achieves accurate lesion localization without pixel-level supervision and provides interpretable visualization of disease-to-healthy transformations.

Conclusion: TWLR offers an explainable and annotation-efficient solution for automated retinal image analysis by addressing both the annotation cost problem and interpretability limitations of deep learning models in medical imaging.

Abstract: Accurate medical image analysis can greatly assist clinical diagnosis, but its effectiveness relies on high-quality expert annotations Obtaining pixel-level labels for medical images, particularly fundus images, remains costly and time-consuming. Meanwhile, despite the success of deep learning in medical imaging, the lack of interpretability limits its clinical adoption. To address these challenges, we propose TWLR, a two-stage framework for interpretable diabetic retinopathy (DR) assessment. In the first stage, a vision-language model integrates domain-specific ophthalmological knowledge into text embeddings to jointly perform DR grading and lesion classification, effectively linking semantic medical concepts with visual features. The second stage introduces an iterative severity regression framework based on weakly-supervised semantic segmentation. Lesion saliency maps generated through iterative refinement direct a progressive inpainting mechanism that systematically eliminates pathological features, effectively downgrading disease severity toward healthier fundus appearances. Critically, this severity regression approach achieves dual benefits: accurate lesion localization without pixel-level supervision and providing an interpretable visualization of disease-to-healthy transformations. Experimental results on the FGADR, DDR, and a private dataset demonstrate that TWLR achieves competitive performance in both DR classification and lesion segmentation, offering a more explainable and annotation-efficient solution for automated retinal image analysis.

</details>


### [197] [JoDiffusion: Jointly Diffusing Image with Pixel-Level Annotations for Semantic Segmentation Promotion](https://arxiv.org/abs/2512.13014)
*Haoyu Wang,Lei Zhang,Wenrui Liu,Dengyang Jiang,Wei Wei,Chen Ding*

Main category: cs.CV

TL;DR: JoDiffusion is a novel diffusion framework that simultaneously generates paired images and semantically consistent annotation masks from text prompts, addressing scalability and semantic inconsistency issues in synthetic dataset generation for semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: Pixel-level annotation is costly and time-intensive. Existing methods for synthetic dataset generation either predict pseudo annotations after image generation (causing semantic inconsistency) or generate images conditioned on manual masks (suffering scalability problems).

Method: 1) Incorporates an independent annotation VAE network to map annotation masks into the same latent space as images. 2) Tailors the diffusion model to capture the joint distribution of images and annotation masks conditioned on text prompts. 3) Develops a mask optimization strategy to mitigate annotation noise during generation.

Result: Experiments on Pascal VOC, COCO, and ADE20K datasets show that datasets generated by JoDiffusion yield substantial performance improvements in semantic segmentation compared to existing methods.

Conclusion: JoDiffusion enables simultaneous generation of paired images and semantically consistent annotation masks from text prompts, demonstrating superior scalability and effectively addressing both semantic inconsistency and scalability problems in synthetic dataset generation for semantic segmentation.

Abstract: Given the inherently costly and time-intensive nature of pixel-level annotation, the generation of synthetic datasets comprising sufficiently diverse synthetic images paired with ground-truth pixel-level annotations has garnered increasing attention recently for training high-performance semantic segmentation models. However, existing methods necessitate to either predict pseudo annotations after image generation or generate images conditioned on manual annotation masks, which incurs image-annotation semantic inconsistency or scalability problem. To migrate both problems with one stone, we present a novel dataset generative diffusion framework for semantic segmentation, termed JoDiffusion. Firstly, given a standard latent diffusion model, JoDiffusion incorporates an independent annotation variational auto-encoder (VAE) network to map annotation masks into the latent space shared by images. Then, the diffusion model is tailored to capture the joint distribution of each image and its annotation mask conditioned on a text prompt. By doing these, JoDiffusion enables simultaneously generating paired images and semantically consistent annotation masks solely conditioned on text prompts, thereby demonstrating superior scalability. Additionally, a mask optimization strategy is developed to mitigate the annotation noise produced during generation. Experiments on Pascal VOC, COCO, and ADE20K datasets show that the annotated dataset generated by JoDiffusion yields substantial performance improvements in semantic segmentation compared to existing methods.

</details>


### [198] [What Happens Next? Next Scene Prediction with a Unified Video Model](https://arxiv.org/abs/2512.13015)
*Xinjie Li,Zhimin Chen,Rui Zhao,Florian Schiffers,Zhenyu Liao,Vimal Bhat*

Main category: cs.CV

TL;DR: The paper introduces Next Scene Prediction (NSP), a new task for unified video models that requires predicting plausible future scenes from preceding context, pushing beyond conventional text-to-video generation to enable temporal and causal reasoning.


<details>
  <summary>Details</summary>
Motivation: Current unified models for joint understanding and generation focus on conventional text-to-video tasks, leaving temporal reasoning capabilities largely unexplored. There's a need to advance unified video models toward deeper temporal and causal reasoning beyond simple generation tasks.

Method: Proposes a unified framework combining Qwen-VL for comprehension and LTX for synthesis, connected via latent query embedding and connector module. The model is trained in three stages: text-to-video pre-training, supervised fine-tuning, and reinforcement learning (GRPO) with a novel causal consistency reward, using a newly curated large-scale NSP dataset.

Result: The model achieves state-of-the-art performance on the proposed NSP benchmark, demonstrating advanced capability for generalist multimodal systems to anticipate what happens next in video sequences.

Conclusion: The Next Scene Prediction task successfully pushes unified video models toward temporal and causal reasoning, advancing the capabilities of generalist multimodal systems to understand and anticipate future events in video sequences.

Abstract: Recent unified models for joint understanding and generation have significantly advanced visual generation capabilities. However, their focus on conventional tasks like text-to-video generation has left the temporal reasoning potential of unified models largely underexplored. To address this gap, we introduce Next Scene Prediction (NSP), a new task that pushes unified video models toward temporal and causal reasoning. Unlike text-to-video generation, NSP requires predicting plausible futures from preceding context, demanding deeper understanding and reasoning. To tackle this task, we propose a unified framework combining Qwen-VL for comprehension and LTX for synthesis, bridged by a latent query embedding and a connector module. This model is trained in three stages on our newly curated, large-scale NSP dataset: text-to-video pre-training, supervised fine-tuning, and reinforcement learning (via GRPO) with our proposed causal consistency reward. Experiments demonstrate our model achieves state-of-the-art performance on our benchmark, advancing the capability of generalist multimodal systems to anticipate what happens next.

</details>


### [199] [Comprehensive Deployment-Oriented Assessment for Cross-Environment Generalization in Deep Learning-Based mmWave Radar Sensing](https://arxiv.org/abs/2512.13018)
*Tomoya Tanaka,Tomonori Ikeda,Ryo Yonemoto*

Main category: cs.CV

TL;DR: First comprehensive evaluation of spatial generalization techniques for RF sensing, showing sigmoid-based amplitude weighting and transfer learning achieve best cross-environment performance for indoor people counting with radar.


<details>
  <summary>Details</summary>
Motivation: Spatial generalization is essential for practical deployment of deep learning-based RF sensing, but comprehensive evaluation of techniques for maintaining accuracy across different environments is lacking.

Method: Systematically investigated multiple approaches: amplitude-based statistical preprocessing (sigmoid weighting, threshold zeroing), frequency-domain filtering, autoencoder-based background suppression, data augmentation strategies, and transfer learning for people counting using FMCW MIMO radar.

Result: Sigmoid-based amplitude weighting achieved 50.1% RMSE and 55.2% MAE reductions; data augmentation provided up to 8.8% MAE improvement; transfer learning with 540 target samples achieved 82.1% RMSE and 91.3% MAE reductions for large spatial shifts.

Conclusion: Integration of deep learning models with amplitude-based preprocessing and efficient transfer learning provides practical direction for developing robust radar sensing systems that maintain accuracy under spatial variations.

Abstract: This study presents the first comprehensive evaluation of spatial generalization techniques, which are essential for the practical deployment of deep learning-based radio-frequency (RF) sensing. Focusing on people counting in indoor environments using frequency-modulated continuous-wave (FMCW) multiple-input multiple-output (MIMO) radar, we systematically investigate a broad set of approaches, including amplitude-based statistical preprocessing (sigmoid weighting and threshold zeroing), frequency-domain filtering, autoencoder-based background suppression, data augmentation strategies, and transfer learning. Experimental results collected across two environments with different layouts demonstrate that sigmoid-based amplitude weighting consistently achieves superior cross-environment performance, yielding 50.1% and 55.2% reductions in root-mean-square error (RMSE) and mean absolute error (MAE), respectively, compared with baseline methods. Data augmentation provides additional though modest benefits, with improvements up to 8.8% in MAE. By contrast, transfer learning proves indispensable for large spatial shifts, achieving 82.1% and 91.3% reductions in RMSE and MAE, respectively, with 540 target-domain samples. Taken together, these findings establish a highly practical direction for developing radar sensing systems capable of maintaining robust accuracy under spatial variations by integrating deep learning models with amplitude-based preprocessing and efficient transfer learning.

</details>


### [200] [SneakPeek: Future-Guided Instructional Streaming Video Generation](https://arxiv.org/abs/2512.13019)
*Cheeun Hong,German Barquero,Fadime Sener,Markos Georgopoulos,Edgar Schönfeld,Stefan Popov,Yuming Du,Oscar Mañas,Albert Pumarola*

Main category: cs.CV

TL;DR: SneakPeek is a diffusion-based autoregressive framework for generating precise, stepwise instructional videos from initial images and structured text prompts, addressing temporal consistency and controllability issues in existing video diffusion models.


<details>
  <summary>Details</summary>
Motivation: Existing video diffusion models struggle with maintaining temporal consistency and controllability across long sequences of multiple action steps in instructional video generation, limiting their practical applications in content creation, education, and human-AI interaction.

Method: A pipeline with three key innovations: (1) predictive causal adaptation for next-frame prediction and future keyframe anticipation, (2) future-guided self-forcing with dual-region KV caching to address exposure bias, and (3) multi-prompt conditioning for fine-grained procedural control over multi-step instructions.

Result: The method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions, enabling interactive video generation where future prompt updates dynamically influence ongoing streaming generation.

Conclusion: SneakPeek effectively addresses the challenges of temporal consistency and controllability in instructional video generation through its innovative autoregressive framework, making significant progress toward practical applications in procedural content creation.

Abstract: Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.

</details>


### [201] [Motus: A Unified Latent Action World Model](https://arxiv.org/abs/2512.13030)
*Hongzhe Bi,Hengkai Tan,Shenghao Xie,Zeyuan Wang,Shuhe Huang,Haitian Liu,Ruowen Zhao,Yao Feng,Chendong Xiang,Yinze Rong,Hongyan Zhao,Hanyu Liu,Zhizhong Su,Lei Ma,Hang Su,Jun Zhu*

Main category: cs.CV

TL;DR: Motus is a unified latent action world model that integrates understanding, world modeling, and control into a single system using a Mixture-of-Transformer architecture and optical flow-based latent actions.


<details>
  <summary>Details</summary>
Motivation: Current embodied AI systems use fragmented, isolated models for understanding, world modeling, and control, preventing unified multimodal generative capabilities and hindering learning from large-scale heterogeneous data.

Method: Proposes Motus with Mixture-of-Transformer architecture integrating three experts (understanding, video generation, action), uses UniDiffuser-style scheduler for flexible mode switching, leverages optical flow for latent action learning, and employs three-phase training pipeline with six-layer data pyramid.

Result: Achieves +15% improvement over X-VLA and +45% improvement over Pi0.5 in simulation, and +11-48% improvement in real-world scenarios, demonstrating superior performance against state-of-the-art methods.

Conclusion: Unified modeling of all functionalities and priors significantly benefits downstream robotic tasks, showing that integrating understanding, world modeling, and control into a single system is effective for embodied AI.

Abstract: While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level "delta action" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.

</details>


### [202] [Comprehensive Evaluation of Rule-Based, Machine Learning, and Deep Learning in Human Estimation Using Radio Wave Sensing: Accuracy, Spatial Generalization, and Output Granularity Trade-offs](https://arxiv.org/abs/2512.13031)
*Tomoya Tanaka,Tomonori Ikeda,Ryo Yonemoto*

Main category: cs.CV

TL;DR: First comprehensive comparison of rule-based, traditional ML, and deep learning models for radio wave sensing with FMCW MIMO radar shows trade-off between spatial generalization and output granularity.


<details>
  <summary>Details</summary>
Motivation: To systematically compare different approaches (rule-based, traditional ML, deep learning) for radio wave sensing with FMCW MIMO radar and understand their performance trade-offs across different environments.

Method: Evaluated five approaches: 1) rule-based connected component method; 2) k-nearest neighbors; 3) random forest; 4) support vector machine; 5) CNN-LSTM deep learning model. Tested in two indoor environments with distinct layouts.

Result: In training environment, CNN-LSTM achieved highest accuracy, traditional ML had moderate performance. In new layout, all learning-based methods degraded significantly while rule-based method remained stable. For binary presence detection, all models performed well across layouts.

Conclusion: High-capacity models provide fine-grained outputs in same environment but are vulnerable to domain shift. Rule-based methods lack fine-grained outputs but are robust to domain shift. Clear trade-off exists between spatial generalization performance and output granularity regardless of model type.

Abstract: This study presents the first comprehensive comparison of rule-based methods, traditional machine learning models, and deep learning models in radio wave sensing with frequency modulated continuous wave multiple input multiple output radar. We systematically evaluated five approaches in two indoor environments with distinct layouts: a rule-based connected component method; three traditional machine learning models, namely k-nearest neighbors, random forest, and support vector machine; and a deep learning model combining a convolutional neural network and long short term memory. In the training environment, the convolutional neural network long short term memory model achieved the highest accuracy, while traditional machine learning models provided moderate performance. In a new layout, however, all learning based methods showed significant degradation, whereas the rule-based method remained stable. Notably, for binary detection of presence versus absence of people, all models consistently achieved high accuracy across layouts. These results demonstrate that high capacity models can produce fine grained outputs with high accuracy in the same environment, but they are vulnerable to domain shift. In contrast, rule-based methods cannot provide fine grained outputs but exhibit robustness against domain shift. Moreover, regardless of the model type, a clear trade off was revealed between spatial generalization performance and output granularity.

</details>


### [203] [Bi-Erasing: A Bidirectional Framework for Concept Removal in Diffusion Models](https://arxiv.org/abs/2512.13039)
*Hao Chen,Yiwei Wang,Songze Li*

Main category: cs.CV

TL;DR: Bi-Erasing: A bidirectional image-guided concept erasure framework that simultaneously suppresses harmful concepts and enhances safe alternatives in diffusion models, achieving better balance between removal efficacy and generation quality.


<details>
  <summary>Details</summary>
Motivation: Existing concept erasure methods use unidirectional strategies (either suppressing target concepts or reinforcing safe alternatives), making it difficult to achieve balanced trade-off between concept removal and generation quality in text-to-image models.

Method: Proposes Bidirectional Image-Guided Concept Erasure (Bi-Erasing) with two decoupled image branches: negative branch suppresses harmful semantics, positive branch provides visual guidance for safe alternatives. Uses joint text-image representation and mask-based filtering to prevent interference from irrelevant content during erasure.

Result: Bi-Erasing outperforms baseline methods in balancing concept removal effectiveness and visual fidelity across extensive experimental evaluations.

Conclusion: The bidirectional approach achieves better balance between erasure efficacy and generation usability compared to unidirectional methods, providing a more effective solution for safe and high-quality image generation.

Abstract: Concept erasure, which fine-tunes diffusion models to remove undesired or harmful visual concepts, has become a mainstream approach to mitigating unsafe or illegal image generation in text-to-image models.However, existing removal methods typically adopt a unidirectional erasure strategy by either suppressing the target concept or reinforcing safe alternatives, making it difficult to achieve a balanced trade-off between concept removal and generation quality. To address this limitation, we propose a novel Bidirectional Image-Guided Concept Erasure (Bi-Erasing) framework that performs concept suppression and safety enhancement simultaneously. Specifically, based on the joint representation of text prompts and corresponding images, Bi-Erasing introduces two decoupled image branches: a negative branch responsible for suppressing harmful semantics and a positive branch providing visual guidance for safe alternatives. By jointly optimizing these complementary directions, our approach achieves a balance between erasure efficacy and generation usability. In addition, we apply mask-based filtering to the image branches to prevent interference from irrelevant content during the erasure process. Across extensive experiment evaluations, the proposed Bi-Erasing outperforms baseline methods in balancing concept removal effectiveness and visual fidelity.

</details>


### [204] [GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training](https://arxiv.org/abs/2512.13043)
*Tong Wei,Yijun Yang,Changhao Zhang,Junliang Xing,Yuanchun Shi,Zongqing Lu,Deheng Ye*

Main category: cs.CV

TL;DR: GTR-Turbo is an efficient upgrade to Guided Thought Reinforcement that eliminates the need for expensive teacher models by merging RL checkpoints to create a free teacher, reducing training time by 50% and compute cost by 60% while improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Multi-turn RL for vision-language agents suffers from sparse rewards and long-horizon credit assignment. Existing methods like GTR use expensive teacher models (GPT/Gemini) for step-level feedback, which limits practicality and reproducibility due to high costs.

Method: GTR-Turbo merges weights from checkpoints produced during ongoing RL training to create a "free" teacher model. This merged model then guides subsequent RL via supervised fine-tuning or soft logit distillation, eliminating dependence on privileged VLMs.

Result: GTR-Turbo improves baseline model accuracy by 10-30% across diverse visual agentic tasks while reducing wall-clock training time by 50% and compute cost by 60% compared to original GTR. It also mitigates "entropy collapse" and keeps training stable.

Conclusion: GTR-Turbo provides a highly efficient alternative to teacher-dependent RL methods by leveraging checkpoint merging to create free guidance, making multi-turn RL for vision-language agents more practical and reproducible without sacrificing performance.

Abstract: Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a "free" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the "entropy collapse" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.

</details>


### [205] [Towards Test-time Efficient Visual Place Recognition via Asymmetric Query Processing](https://arxiv.org/abs/2512.13055)
*Jaeyoon Kim,Yoonki Cho,Sung-Eui Yoon*

Main category: cs.CV

TL;DR: Efficient asymmetric VPR framework using high-capacity gallery model + lightweight query network with geographical memory bank and implicit embedding augmentation for resource-constrained devices.


<details>
  <summary>Details</summary>
Motivation: Current VPR methods using foundation models like DINOv2 achieve great performance but are computationally expensive, making them impractical for resource-constrained devices. Need for efficient VPR that maintains accuracy while reducing computational costs.

Method: 1) Asymmetric framework: high-capacity model for offline gallery feature extraction + lightweight query network for online processing. 2) Geographical memory bank: structures gallery features using geolocation metadata to avoid expensive k-NN computations. 3) Implicit embedding augmentation: enhances query network to model feature variations despite limited capacity.

Result: Method significantly reduces computational costs while outperforming existing asymmetric retrieval techniques. Establishes new state-of-the-art for VPR in resource-limited environments.

Conclusion: Proposed framework enables practical deployment of high-performance VPR on resource-constrained devices by combining efficient asymmetric architecture with geographical memory organization and query network enhancement techniques.

Abstract: Visual Place Recognition (VPR) has advanced significantly with high-capacity foundation models like DINOv2, achieving remarkable performance. Nonetheless, their substantial computational cost makes deployment on resource-constrained devices impractical. In this paper, we introduce an efficient asymmetric VPR framework that incorporates a high-capacity gallery model for offline feature extraction with a lightweight query network for online processing. A key challenge in this setting is ensuring compatibility between these heterogeneous networks, which conventional approaches address through computationally expensive k-NN-based compatible training. To overcome this, we propose a geographical memory bank that structures gallery features using geolocation metadata inherent in VPR databases, eliminating the need for exhaustive k-NN computations. Additionally, we introduce an implicit embedding augmentation technique that enhances the query network to model feature variations despite its limited capacity. Extensive experiments demonstrate that our method not only significantly reduces computational costs but also outperforms existing asymmetric retrieval techniques, establishing a new aspect for VPR in resource-limited environments. The code is available at https://github.com/jaeyoon1603/AsymVPR

</details>


### [206] [Forging a Dynamic Memory: Retrieval-Guided Continual Learning for Generalist Medical Foundation Models](https://arxiv.org/abs/2512.13072)
*Zizhi Chen,Yizhen Gao,Minghao Han,Yizhou Liu,Zhaoyu Chen,Dingkang Yang,Lihua Zhang*

Main category: cs.CV

TL;DR: A multimodal medical VLM framework using RAG and dynamic knowledge distillation for continual learning, achieving SOTA on a new medical task incremental learning benchmark.


<details>
  <summary>Details</summary>
Motivation: Address the core dilemma in multimodal biomedical VLMs for continual learning: preserving fine-grained intra-modality features while bridging significant domain gaps across different modalities.

Method: 1) Build 18M multimodal medical retrieval database from PubMed; 2) Integrate multi-modal, multi-layer RAG for real-time guidance; 3) Dynamic knowledge distillation framework that modulates parameter importance, knowledge granularity, and data distribution based on required detail level.

Result: Achieves state-of-the-art performance across all metrics on the new Medical Generalist Task Incremental Learning (MGTIL) benchmark, demonstrating superior adaptation to domain shifts, feature retention, and real-time learning of complex medical tasks.

Conclusion: The proposed comprehensive framework successfully resolves the core dilemma in multimodal medical VLMs for continual learning through innovative RAG integration and dynamic knowledge distillation, validated by rigorous clinical benchmarks.

Abstract: Multimodal biomedical Vision-Language Models (VLMs) exhibit immense potential in the field of Continual Learning (CL). However, they confront a core dilemma: how to preserve fine-grained intra-modality features while bridging the significant domain gap across different modalities. To address this challenge, we propose a comprehensive framework. Leveraging our 18-million multimodal and comprehensive medical retrieval database derived from PubMed scientific papers, we pioneer the integration of Retrieval-Augmented Generation (RAG) into CL. Specifically, we employ a multi-modal, multi-layer RAG system that provides real-time guidance for model fine-tuning through dynamic, on-demand knowledge retrieval. Building upon this, we introduce a dynamic knowledge distillation framework. This framework precisely resolves the aforementioned core dilemma by dynamically modulating the importance of the parameter space, the granularity of the distilled knowledge, and the data distribution of the reference dataset in accordance with the required level of detail. To thoroughly validate the clinical value of our strategy, we have designed a more rigorous \textbf{M}edical Generalist Task Incremental Learning (MGTIL) benchmark. This benchmark is engineered to simultaneously evaluate the model's capacity for adaptation to significant domain shifts, retention of subtle intra-domain features, and real-time learning of novel and complex medical tasks. Extensive experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance across all metrics. The code is provided in the supplementary materials.

</details>


### [207] [DiRe: Diversity-promoting Regularization for Dataset Condensation](https://arxiv.org/abs/2512.13083)
*Saumyaranjan Mohanty,Aravind Reddy,Konda Reddy Mopuri*

Main category: cs.CV

TL;DR: Proposes Diversity Regularizer (DiRe) to reduce redundancy and improve diversity in dataset condensation, enhancing existing methods across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing dataset condensation methods produce synthesized datasets with significant redundancy, creating a need to improve diversity and reduce redundancy in condensed datasets.

Method: Introduces Diversity Regularizer (DiRe) composed of cosine similarity and Euclidean distance metrics that can be applied off-the-shelf to various state-of-the-art condensation methods.

Result: Demonstrates improved performance of state-of-the-art condensation methods on benchmark datasets (CIFAR-10 to ImageNet-1K) with respect to both generalization and diversity metrics.

Conclusion: The proposed DiRe regularizer effectively enhances dataset condensation by improving diversity and reducing redundancy, making it a valuable off-the-shelf addition to existing methods.

Abstract: In Dataset Condensation, the goal is to synthesize a small dataset that replicates the training utility of a large original dataset. Existing condensation methods synthesize datasets with significant redundancy, so there is a dire need to reduce redundancy and improve the diversity of the synthesized datasets. To tackle this, we propose an intuitive Diversity Regularizer (DiRe) composed of cosine similarity and Euclidean distance, which can be applied off-the-shelf to various state-of-the-art condensation methods. Through extensive experiments, we demonstrate that the addition of our regularizer improves state-of-the-art condensation methods on various benchmark datasets from CIFAR-10 to ImageNet-1K with respect to generalization and diversity metrics.

</details>


### [208] [UniVCD: A New Method for Unsupervised Change Detection in the Open-Vocabulary Era](https://arxiv.org/abs/2512.13089)
*Ziqiang Zhu,Bowei Yang*

Main category: cs.CV

TL;DR: UniVCD is an unsupervised, open-vocabulary change detection method using frozen SAM2 and CLIP models to detect category-agnostic changes without labeled data, achieving strong performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Supervised CD methods are dataset-dependent, expensive to annotate, limited to predefined categories, and generalize poorly. Vision foundation models (SAM2, CLIP) offer new opportunities to overcome these limitations.

Method: UniVCD uses frozen SAM2 for spatial detail and CLIP for semantic priors, with a lightweight feature alignment module to bridge them. A post-processing pipeline suppresses noise and pseudo-changes for objects with well-defined boundaries.

Result: Experiments on public BCD and SCD benchmarks show UniVCD achieves consistently strong performance, matching or surpassing existing open-vocabulary CD methods in key metrics like F1 and IoU.

Conclusion: Unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD.

Abstract: Change detection (CD) identifies scene changes from multi-temporal observations and is widely used in urban development and environmental monitoring. Most existing CD methods rely on supervised learning, making performance strongly dataset-dependent and incurring high annotation costs; they typically focus on a few predefined categories and generalize poorly to diverse scenes. With the rise of vision foundation models such as SAM2 and CLIP, new opportunities have emerged to relax these constraints. We propose Unified Open-Vocabulary Change Detection (UniVCD), an unsupervised, open-vocabulary change detection method built on frozen SAM2 and CLIP. UniVCD detects category-agnostic changes across diverse scenes and imaging geometries without any labeled data or paired change images. A lightweight feature alignment module is introduced to bridge the spatially detailed representations from SAM2 and the semantic priors from CLIP, enabling high-resolution, semantically aware change estimation while keeping the number of trainable parameters small. On top of this, a streamlined post-processing pipeline is further introduced to suppress noise and pseudo-changes, improving the detection accuracy for objects with well-defined boundaries. Experiments on several public BCD (Binary Change Detection) and SCD (Semantic Change Detection) benchmarks show that UniVCD achieves consistently strong performance and matches or surpasses existing open-vocabulary CD methods in key metrics such as F1 and IoU. The results demonstrate that unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD. Code and pretrained models will be released at https://github.com/Die-Xie/UniVCD.

</details>


### [209] [ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning](https://arxiv.org/abs/2512.13095)
*Feng Zhang,Zezhong Tan,Xinhong Ma,Ziqiang Dong,Xi Leng,Jianfei Zhao,Xin Sun,Yang Yang*

Main category: cs.CV

TL;DR: ADHint improves hint-based RL by incorporating difficulty-aware scheduling and advantage estimation to balance exploration and imitation, achieving better reasoning generalization across modalities and domains.


<details>
  <summary>Details</summary>
Motivation: Existing hint-based RL methods ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. This creates a need for better trade-off between exploration and imitation in post-training methods that combine SFT and RL.

Method: ADHint introduces three key components: 1) Adaptive Hint with Sample Difficulty Prior - evaluates sample difficulty under policy model and schedules appropriate hint ratios; 2) Consistency-based Gradient Modulation and Selective Masking for Hint Preservation - modulates token-level gradients within hints to prevent biased updates; 3) Advantage Estimation with Rollout Difficulty Posterior - leverages relative difficulty of rollouts with/without hints to estimate advantages for balanced updates.

Result: Extensive experiments across diverse modalities, model scales, and domains show ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8 metrics.

Conclusion: ADHint successfully addresses the limitations of existing hint-based RL methods by incorporating difficulty as a key factor, achieving better trade-off between exploration and imitation, and demonstrating strong performance across various evaluation settings.

Abstract: To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.

</details>


### [210] [Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2512.13101)
*Wenjing Lu,Yi Hong,Yang Yang*

Main category: cs.CV

TL;DR: UnCoL is a dual-teacher framework for semi-supervised medical image segmentation that combines foundation model knowledge with task-specific adaptation using uncertainty-guided pseudo-label learning.


<details>
  <summary>Details</summary>
Motivation: Vision foundation models have strong generalization but struggle with specialized clinical tasks under limited annotations or rare pathological variations due to mismatch between general priors and task-specific requirements.

Method: UnCoL uses a dual-teacher framework: one frozen foundation model transfers general knowledge (visual and semantic representations), while a progressively adapting teacher captures fine-grained task-specific representations. Predictive uncertainty adaptively regulates pseudo-label learning to suppress unreliable supervision and stabilize learning in ambiguous regions.

Result: Experiments on diverse 2D and 3D segmentation benchmarks show UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines, achieving near fully supervised performance with significantly reduced annotation requirements.

Conclusion: UnCoL effectively harmonizes generalization and specialization in semi-supervised medical image segmentation by leveraging foundation model knowledge while adapting to task-specific requirements through uncertainty-guided collaborative learning.

Abstract: Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.

</details>


### [211] [FID-Net: A Feature-Enhanced Deep Learning Network for Forest Infestation Detection](https://arxiv.org/abs/2512.13104)
*Yan Zhang,Baoxin Li,Han Sun,Yuhang Gao,Mingtai Zhang,Pei Wang*

Main category: cs.CV

TL;DR: FID-Net: A deep learning model for detecting pest-infected trees from UAV imagery with spatial analysis for infestation patterns, outperforming YOLO models.


<details>
  <summary>Details</summary>
Motivation: Traditional forest pest monitoring methods have limitations in large-scale, fine-grained detection. There's a need for accurate identification of infected trees and analysis of infestation patterns to enable efficient forest protection.

Method: FID-Net builds on YOLOv8n with three key modules: Feature Enhancement Module (FEM) for disease-sensitive cues, Adaptive Multi-scale Feature Fusion Module (AMFM) to align RGB and FEM-enhanced features, and Efficient Channel Attention (ECA) for discriminative information. Post-detection spatial analysis uses Kernel Density Estimation for hotspots, neighborhood evaluation for infection risk, and DBSCAN clustering for priority protection zones.

Result: On UAV imagery from 32 forest plots in eastern Tianshan, China, FID-Net achieved 86.10% precision, 75.44% recall, 82.29% mAP@0.5, and 64.30% mAP@0.5:0.95, outperforming mainstream YOLO models. Analysis confirmed infected trees exhibit clear clustering patterns.

Conclusion: FID-Net enables accurate tree health discrimination and, combined with spatial metrics, provides reliable data for intelligent pest monitoring, early warning, and precise forest management, supporting targeted protection strategies.

Abstract: Forest pests threaten ecosystem stability, requiring efficient monitoring. To overcome the limitations of traditional methods in large-scale, fine-grained detection, this study focuses on accurately identifying infected trees and analyzing infestation patterns. We propose FID-Net, a deep learning model that detects pest-affected trees from UAV visible-light imagery and enables infestation analysis via three spatial metrics. Based on YOLOv8n, FID-Net introduces a lightweight Feature Enhancement Module (FEM) to extract disease-sensitive cues, an Adaptive Multi-scale Feature Fusion Module (AMFM) to align and fuse dual-branch features (RGB and FEM-enhanced), and an Efficient Channel Attention (ECA) mechanism to enhance discriminative information efficiently. From detection results, we construct a pest situation analysis framework using: (1) Kernel Density Estimation to locate infection hotspots; (2) neighborhood evaluation to assess healthy trees' infection risk; (3) DBSCAN clustering to identify high-density healthy clusters as priority protection zones. Experiments on UAV imagery from 32 forest plots in eastern Tianshan, China, show that FID-Net achieves 86.10% precision, 75.44% recall, 82.29% mAP@0.5, and 64.30% mAP@0.5:0.95, outperforming mainstream YOLO models. Analysis confirms infected trees exhibit clear clustering, supporting targeted forest protection. FID-Net enables accurate tree health discrimination and, combined with spatial metrics, provides reliable data for intelligent pest monitoring, early warning, and precise management.

</details>


### [212] [Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather](https://arxiv.org/abs/2512.13107)
*Zhijian He,Feifei Liu,Yuwei Li,Zhanpeng Liu,Jintao Cheng,Xieyuanli Chen,Xiaoyu Tang*

Main category: cs.CV

TL;DR: DiffFusion: A diffusion-based framework for robust multi-modal 3D object detection in adverse weather conditions through image restoration, point cloud compensation, and adaptive cross-modal fusion.


<details>
  <summary>Details</summary>
Motivation: Multi-modal 3D object detection suffers from weather-induced distortions and misalignment between modalities (camera images and LiDAR point clouds) in adverse weather conditions, limiting reliability for robotics and autonomous driving applications.

Method: DiffFusion uses diffusion models for restoration: Diffusion-IR restores weather-degraded images, and Point Cloud Restoration (PCR) compensates corrupted LiDAR data using image object cues. The Bidirectional Adaptive Fusion and Alignment Module (BAFAM) enables dynamic multi-modal fusion and bidirectional BEV alignment to handle misalignments.

Result: Extensive experiments on three public datasets show state-of-the-art robustness under adverse weather while maintaining strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate generalization capability.

Conclusion: DiffFusion effectively enhances multi-modal 3D object detection robustness in challenging weather conditions through diffusion-based restoration and adaptive cross-modal fusion, with promising generalization to real-world scenarios. The framework will be released as open-source.

Abstract: Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.

</details>


### [213] [DePT3R: Joint Dense Point Tracking and 3D Reconstruction of Dynamic Scenes in a Single Forward Pass](https://arxiv.org/abs/2512.13122)
*Vivek Alumootil,Tuan-Anh Vu,M. Khalid Jawed*

Main category: cs.CV

TL;DR: DePT3R is a unified framework for simultaneous dense 3D point tracking and reconstruction from multiple unposed images of dynamic scenes in a single forward pass.


<details>
  <summary>Details</summary>
Motivation: Current methods have limitations: they require pairwise processing, known camera poses, or temporal ordering, which restricts flexibility. Recent advances in 3D reconstruction from unposed images suggest opportunities for unified dynamic scene understanding.

Method: Uses a powerful backbone to extract deep spatio-temporal features and dense prediction heads to regress pixel-wise maps. Performs multi-task learning for both tracking and reconstruction without requiring camera poses.

Result: Demonstrates strong performance on challenging dynamic scene benchmarks and shows significant improvements in memory efficiency over state-of-the-art methods.

Conclusion: DePT3R provides an efficient, adaptable framework for unified dynamic scene understanding that eliminates pose requirements and enables simultaneous tracking and reconstruction.

Abstract: Current methods for dense 3D point tracking in dynamic scenes typically rely on pairwise processing, require known camera poses, or assume a temporal ordering to input frames, constraining their flexibility and applicability. Additionally, recent advances have successfully enabled efficient 3D reconstruction from large-scale, unposed image collections, underscoring opportunities for unified approaches to dynamic scene understanding. Motivated by this, we propose DePT3R, a novel framework that simultaneously performs dense point tracking and 3D reconstruction of dynamic scenes from multiple images in a single forward pass. This multi-task learning is achieved by extracting deep spatio-temporal features with a powerful backbone and regressing pixel-wise maps with dense prediction heads. Crucially, DePT3R operates without requiring camera poses, substantially enhancing its adaptability and efficiency-especially important in dynamic environments with rapid changes. We validate DePT3R on several challenging benchmarks involving dynamic scenes, demonstrating strong performance and significant improvements in memory efficiency over existing state-of-the-art methods. Data and codes are available via the open repository: https://github.com/StructuresComp/DePT3R

</details>


### [214] [LeafTrackNet: A Deep Learning Framework for Robust Leaf Tracking in Top-Down Plant Phenotyping](https://arxiv.org/abs/2512.13130)
*Shanghua Liu,Majharulislam Babor,Christoph Verduyn,Breght Vandenberghe,Bruno Betoni Parodi,Cornelia Weltzien,Marina M. -C. Höhne*

Main category: cs.CV

TL;DR: LeafTrackNet: A new framework for accurate leaf tracking in complex crops like canola, using YOLOv10 detection + MobileNetV3 embeddings with memory association, outperforming existing methods by 9% HOTA on the new CanolaTrack dataset.


<details>
  <summary>Details</summary>
Motivation: High-resolution leaf phenotyping offers valuable insights into plant development, but current tracking methods are limited to simple species or constrained conditions. There's a lack of robust tracking for complex crops like canola and a shortage of large-scale datasets captured under realistic conditions.

Method: LeafTrackNet combines YOLOv10-based leaf detection with MobileNetV3-based embedding network. Uses embedding-based memory association strategy during inference to maintain leaf identities over time. Tested on CanolaTrack dataset - 5,704 RGB images with 31,840 annotated leaf instances from 184 canola plants.

Result: LeafTrackNet outperforms both plant-specific trackers and state-of-the-art MOT baselines, achieving 9% HOTA improvement on the CanolaTrack benchmark dataset.

Conclusion: Provides a new standard for leaf-level tracking under realistic conditions and contributes CanolaTrack - the largest dataset for leaf tracking in agricultural crops, enabling future plant phenotyping research.

Abstract: High resolution phenotyping at the level of individual leaves offers fine-grained insights into plant development and stress responses. However, the full potential of accurate leaf tracking over time remains largely unexplored due to the absence of robust tracking methods-particularly for structurally complex crops such as canola. Existing plant-specific tracking methods are typically limited to small-scale species or rely on constrained imaging conditions. In contrast, generic multi-object tracking (MOT) methods are not designed for dynamic biological scenes. Progress in the development of accurate leaf tracking models has also been hindered by a lack of large-scale datasets captured under realistic conditions. In this work, we introduce CanolaTrack, a new benchmark dataset comprising 5,704 RGB images with 31,840 annotated leaf instances spanning the early growth stages of 184 canola plants. To enable accurate leaf tracking over time, we introduce LeafTrackNet, an efficient framework that combines a YOLOv10-based leaf detector with a MobileNetV3-based embedding network. During inference, leaf identities are maintained over time through an embedding-based memory association strategy. LeafTrackNet outperforms both plant-specific trackers and state-of-the-art MOT baselines, achieving a 9% HOTA improvement on CanolaTrack. With our work we provide a new standard for leaf-level tracking under realistic conditions and we provide CanolaTrack - the largest dataset for leaf tracking in agriculture crops, which will contribute to future research in plant phenotyping. Our code and dataset are publicly available at https://github.com/shl-shawn/LeafTrackNet.

</details>


### [215] [Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models](https://arxiv.org/abs/2512.13144)
*Chun Kit Wong,Paraskevas Pegios,Nina Weng,Emilie Pi Fogtmann Sejer,Martin Grønnebæk Tolsgaard,Anders Nymark Christensen,Aasa Feragen*

Main category: cs.CV

TL;DR: Weight Space Correlation Analysis detects whether medical imaging models actually use encoded metadata shortcuts or genuine clinical features for predictions.


<details>
  <summary>Details</summary>
Motivation: Deep learning models in medical imaging often encode confounding metadata (like scanner information) in embeddings, but it's unclear whether models actually use this metadata for predictions or just encode it incidentally. Need to verify model trustworthiness by determining if they rely on genuine clinical signals rather than shortcuts.

Method: Weight Space Correlation Analysis - measures alignment between classification heads of primary clinical task and auxiliary metadata tasks to quantify feature utilization. Validated by detecting artificially induced shortcut learning, then applied to SA-SonoNet model for Spontaneous Preterm Birth prediction.

Result: Method successfully detected artificial shortcuts. For SA-SonoNet sPTB model: embeddings contained substantial metadata, but the classifier's weight vectors were highly correlated with clinically relevant factors (birth weight) and decoupled from clinically irrelevant acquisition factors (scanner).

Conclusion: The methodology provides a tool to verify model trustworthiness, showing that without induced bias, clinical models selectively utilize genuine clinical features rather than metadata shortcuts.

Abstract: Deep learning models in medical imaging are susceptible to shortcut learning, relying on confounding metadata (e.g., scanner model) that is often encoded in image embeddings. The crucial question is whether the model actively utilizes this encoded information for its final prediction. We introduce Weight Space Correlation Analysis, an interpretable methodology that quantifies feature utilization by measuring the alignment between the classification heads of a primary clinical task and auxiliary metadata tasks. We first validate our method by successfully detecting artificially induced shortcut learning. We then apply it to probe the feature utilization of an SA-SonoNet model trained for Spontaneous Preterm Birth (sPTB) prediction. Our analysis confirmed that while the embeddings contain substantial metadata, the sPTB classifier's weight vectors were highly correlated with clinically relevant factors (e.g., birth weight) but decoupled from clinically irrelevant acquisition factors (e.g. scanner). Our methodology provides a tool to verify model trustworthiness, demonstrating that, in the absence of induced bias, the clinical model selectively utilizes features related to the genuine clinical signal.

</details>


### [216] [StarryGazer: Leveraging Monocular Depth Estimation Models for Domain-Agnostic Single Depth Image Completion](https://arxiv.org/abs/2512.13147)
*Sangmin Hong,Suyoung Lee,Kyoung Mu Lee*

Main category: cs.CV

TL;DR: StarryGazer is an unsupervised depth completion framework that combines sparse depth maps with monocular depth estimation (MDE) models to predict dense depth without ground truth data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing unsupervised depth completion methods require auxiliary data that doesn't match real scenarios, while MDE models produce relative depth maps but can't properly combine with sparse depth information. Simple affine transformations of MDE outputs yield high errors due to inaccurate depth difference estimation between objects.

Method: 1) Use pre-trained MDE model to generate relative depth images, 2) Segment and randomly rescale these images to create synthetic training pairs (dense pseudo-ground truth + corresponding sparse depths), 3) Train a refinement network using these synthetic pairs along with relative depth maps and RGB images to improve accuracy and robustness.

Result: StarryGazer shows superior performance over existing unsupervised methods and transformed MDE results on various datasets, demonstrating effective exploitation of MDE models while correcting errors using sparse depth information.

Conclusion: The proposed domain-agnostic framework successfully predicts dense depth from sparse depth and RGB images without ground truth by leveraging MDE models and synthetic training data, addressing limitations of existing unsupervised approaches.

Abstract: The problem of depth completion involves predicting a dense depth image from a single sparse depth map and an RGB image. Unsupervised depth completion methods have been proposed for various datasets where ground truth depth data is unavailable and supervised methods cannot be applied. However, these models require auxiliary data to estimate depth values, which is far from real scenarios. Monocular depth estimation (MDE) models can produce a plausible relative depth map from a single image, but there is no work to properly combine the sparse depth map with MDE for depth completion; a simple affine transformation to the depth map will yield a high error since MDE are inaccurate at estimating depth difference between objects. We introduce StarryGazer, a domain-agnostic framework that predicts dense depth images from a single sparse depth image and an RGB image without relying on ground-truth depth by leveraging the power of large MDE models. First, we employ a pre-trained MDE model to produce relative depth images. These images are segmented and randomly rescaled to form synthetic pairs for dense pseudo-ground truth and corresponding sparse depths. A refinement network is trained with the synthetic pairs, incorporating the relative depth maps and RGB images to improve the model's accuracy and robustness. StarryGazer shows superior results over existing unsupervised methods and transformed MDE results on various datasets, demonstrating that our framework exploits the power of MDE models while appropriately fixing errors using sparse depth information.

</details>


### [217] [Intrinsic Image Fusion for Multi-View 3D Material Reconstruction](https://arxiv.org/abs/2512.13157)
*Peter Kocsis,Lukas Höllein,Matthias Nießner*

Main category: cs.CV

TL;DR: Intrinsic Image Fusion reconstructs high-quality materials from multi-view images using diffusion priors and robust optimization, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Material reconstruction from multi-view images is underconstrained and typically relies on expensive, noisy path tracing. There's a need for better constraints and more efficient optimization.

Method: 1) Use diffusion-based material estimator for single-view priors, 2) Fit low-dimensional parametric function to predictions, 3) Robust optimization with soft per-view prediction selection and confidence-based multi-view inlier set, 4) Inverse path tracing to optimize low-dimensional parameters.

Result: Outperforms state-of-the-art methods in material disentanglement on both synthetic and real scenes, producing sharp and clean reconstructions suitable for high-quality relighting.

Conclusion: The proposed Intrinsic Image Fusion method successfully reconstructs high-quality physically based materials by incorporating single-view priors and robust optimization, enabling efficient and accurate material reconstruction for relighting applications.

Abstract: We introduce Intrinsic Image Fusion, a method that reconstructs high-quality physically based materials from multi-view images. Material reconstruction is highly underconstrained and typically relies on analysis-by-synthesis, which requires expensive and noisy path tracing. To better constrain the optimization, we incorporate single-view priors into the reconstruction process. We leverage a diffusion-based material estimator that produces multiple, but often inconsistent, candidate decompositions per view. To reduce the inconsistency, we fit an explicit low-dimensional parametric function to the predictions. We then propose a robust optimization framework using soft per-view prediction selection together with confidence-based soft multi-view inlier set to fuse the most consistent predictions of the most confident views into a consistent parametric material space. Finally, we use inverse path tracing to optimize for the low-dimensional parameters. Our results outperform state-of-the-art methods in material disentanglement on both synthetic and real scenes, producing sharp and clean reconstructions suitable for high-quality relighting.

</details>


### [218] [A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis](https://arxiv.org/abs/2512.13164)
*Xianchao Guan,Zhiyuan Fan,Yifeng Wang,Fuqiang Chen,Yanjiang Zhou,Zengyang Che,Hongxue Meng,Xin Li,Yaowei Wang,Hongpeng Wang,Min Zhang,Heng Tao Shen,Zheng Zhang,Yongbing Zhang*

Main category: cs.CV

TL;DR: CRAFTS is a generative foundation model for pathology-specific text-to-image synthesis that addresses data scarcity in clinical AI by generating diverse, biologically accurate histology images across 30 cancer types.


<details>
  <summary>Details</summary>
Motivation: Clinical-grade AI in pathology is limited by scarce, diverse annotated datasets. Existing generative models suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability.

Method: CRAFTS uses a dual-stage training strategy on 2.8M image-caption pairs with a novel alignment mechanism to suppress semantic drift. It generates diverse pathological images and can be coupled with ControlNet for precise tissue architecture control.

Result: Generates diverse pathological images across 30 cancer types with quality validated by objective metrics and pathologist evaluations. CRAFTS-augmented datasets enhance performance in classification, cross-modal retrieval, self-supervised learning, and visual question answering.

Conclusion: CRAFTS overcomes data scarcity and privacy barriers, providing limitless diverse annotated histology data to unlock robust diagnostic tools for rare and complex cancer phenotypes.

Abstract: The development of clinical-grade artificial intelligence in pathology is limited by the scarcity of diverse, high-quality annotated datasets. Generative models offer a potential solution but suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability. To address this challenge, we introduce a Correlation-Regulated Alignment Framework for Tissue Synthesis (CRAFTS), the first generative foundation model for pathology-specific text-to-image synthesis. By leveraging a dual-stage training strategy on approximately 2.8 million image-caption pairs, CRAFTS incorporates a novel alignment mechanism that suppresses semantic drift to ensure biological accuracy. This model generates diverse pathological images spanning 30 cancer types, with quality rigorously validated by objective metrics and pathologist evaluations. Furthermore, CRAFTS-augmented datasets enhance the performance across various clinical tasks, including classification, cross-modal retrieval, self-supervised learning, and visual question answering. In addition, coupling CRAFTS with ControlNet enables precise control over tissue architecture from inputs such as nuclear segmentation masks and fluorescence images. By overcoming the critical barriers of data scarcity and privacy concerns, CRAFTS provides a limitless source of diverse, annotated histology data, effectively unlocking the creation of robust diagnostic tools for rare and complex cancer phenotypes.

</details>


### [219] [Seeing the Whole Picture: Distribution-Guided Data-Free Distillation for Semantic Segmentation](https://arxiv.org/abs/2512.13175)
*Hongxuan Sun,Tao Wu*

Main category: cs.CV

TL;DR: DFSS is a novel data-free knowledge distillation framework specifically designed for semantic segmentation that respects structural continuity and uses BN statistics for better data sampling.


<details>
  <summary>Details</summary>
Motivation: Existing data-free knowledge distillation methods are designed for classification tasks and treat pixels independently, ignoring the spatial continuity and structural coherence required for semantic segmentation, leading to significant performance degradation when applied to segmentation tasks.

Method: DFSS introduces two key components: 1) Approximate Distribution Sampling (ADS) that leverages Batch Normalization statistics from the teacher model to guide data selection without relying on potentially misleading teacher predictions, and 2) Weighted Distribution Progressive Distillation (WDPD) that dynamically prioritizes reliable samples aligned with original data distribution early in training and gradually incorporates more challenging cases.

Result: Extensive experiments on standard benchmarks show DFSS consistently outperforms existing data-free distillation methods for semantic segmentation, achieving state-of-the-art results with significantly reduced reliance on auxiliary data.

Conclusion: DFSS provides an effective data-free distillation framework tailored for semantic segmentation that respects structural continuity and achieves superior performance by better approximating the original training distribution through BN statistics and progressive learning strategies.

Abstract: Semantic segmentation requires a holistic understanding of the physical world, as it assigns semantic labels to spatially continuous and structurally coherent objects rather than to isolated pixels. However, existing data-free knowledge distillation (DFKD) methods-primarily designed for classification-often disregard this continuity, resulting in significant performance degradation when applied directly to segmentation tasks. In this paper, we introduce DFSS, a novel data-free distillation framework tailored for semantic segmentation. Unlike prior approaches that treat pixels independently, DFSS respects the structural and contextual continuity of real-world scenes. Our key insight is to leverage Batch Normalization (BN) statistics from a teacher model to guide Approximate Distribution Sampling (ADS), enabling the selection of data that better reflects the original training distribution-without relying on potentially misleading teacher predictions. Additionally, we propose Weighted Distribution Progressive Distillation (WDPD), which dynamically prioritizes reliable samples that are more closely aligned with the original data distribution early in training and gradually incorporates more challenging cases, mirroring the natural progression of learning in human perception. Extensive experiments on standard benchmarks demonstrate that DFSS consistently outperforms existing data-free distillation methods for semantic segmentation, achieving state-of-the-art results with significantly reduced reliance on auxiliary data.

</details>


### [220] [MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion](https://arxiv.org/abs/2512.13177)
*Minghui Hou,Wei-Hsing Huang,Shaofeng Liang,Daizong Liu,Tai-Hao Wen,Gang Wang,Runwei Guan,Weiping Ding*

Main category: cs.CV

TL;DR: MMDrive is a multimodal vision-language model framework that extends 2D image understanding to 3D scene understanding for autonomous driving by fusing occupancy maps, LiDAR point clouds, and textual descriptions with adaptive cross-modal fusion mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language models for autonomous driving are limited to 2D image understanding, which restricts their ability to perceive 3D spatial information and perform deep semantic fusion, leading to suboptimal performance in complex driving environments.

Method: MMDrive incorporates three modalities (occupancy maps, LiDAR point clouds, textual descriptions) and introduces two novel components: Text-oriented Multimodal Modulator for dynamic modality weighting based on semantic cues, and Cross-Modal Abstractor using learnable abstract tokens to generate compact cross-modal summaries highlighting key regions and semantics.

Result: MMDrive achieves significant performance gains over existing vision-language models, with BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM benchmark, and 62.7% accuracy on NuScenes-QA benchmark.

Conclusion: MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.

Abstract: Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.

</details>


### [221] [CoRA: A Collaborative Robust Architecture with Hybrid Fusion for Efficient Perception](https://arxiv.org/abs/2512.13191)
*Gong Chen,Chaokun Zhang,Pengcheng Lv,Xiaohui Xie*

Main category: cs.CV

TL;DR: CoRA is a hybrid collaborative perception architecture that combines feature-level fusion for performance with object-level correction for robustness against communication errors.


<details>
  <summary>Details</summary>
Motivation: Current SOTA collaborative perception methods using intermediate fusion are vulnerable to performance degradation under adverse communication conditions due to data transmission misalignment, limiting practical deployment.

Method: CoRA uses a hybrid approach with two branches: 1) feature-level fusion branch that selects critical features for efficient fusion, and 2) object-level correction branch that leverages semantic relevance to correct spatial displacements from pose errors.

Result: Under extreme scenarios, CoRA improves baseline performance by ~19% in AP@0.7 with >5x less communication volume, demonstrating superior robustness and efficiency.

Conclusion: CoRA successfully decouples performance from robustness with low communication, making it a promising solution for robust collaborative perception by complementing intermediate and late fusion strengths.

Abstract: Collaborative perception has garnered significant attention as a crucial technology to overcome the perceptual limitations of single-agent systems. Many state-of-the-art (SOTA) methods have achieved communication efficiency and high performance via intermediate fusion. However, they share a critical vulnerability: their performance degrades under adverse communication conditions due to the misalignment induced by data transmission, which severely hampers their practical deployment. To bridge this gap, we re-examine different fusion paradigms, and recover that the strengths of intermediate and late fusion are not a trade-off, but a complementary pairing. Based on this key insight, we propose CoRA, a novel collaborative robust architecture with a hybrid approach to decouple performance from robustness with low communication. It is composed of two components: a feature-level fusion branch and an object-level correction branch. Its first branch selects critical features and fuses them efficiently to ensure both performance and scalability. The second branch leverages semantic relevance to correct spatial displacements, guaranteeing resilience against pose errors. Experiments demonstrate the superiority of CoRA. Under extreme scenarios, CoRA improves upon its baseline performance by approximately 19% in AP@0.7 with more than 5x less communication volume, which makes it a promising solution for robust collaborative perception.

</details>


### [222] [POLAR: A Portrait OLAT Dataset and Generative Framework for Illumination-Aware Face Modeling](https://arxiv.org/abs/2512.13192)
*Zhuo Chen,Chengqun Yang,Zhuo Su,Zheng Lv,Jingnan Gao,Xiaoyuan Zhang,Xiaokang Yang,Yichao Yan*

Main category: cs.CV

TL;DR: POLAR introduces a large-scale OLAT dataset and POLARNet model for physically-grounded face relighting using flow-based generative modeling of continuous illumination transformations.


<details>
  <summary>Details</summary>
Motivation: Face relighting progress is constrained by limited large-scale, physically consistent illumination data. Existing methods lack physically interpretable illumination modeling and rely on statistical/contextual cues rather than continuous lighting transformations.

Method: 1) POLAR dataset: 200+ subjects under 156 lighting directions, multiple views, diverse expressions. 2) POLARNet: flow-based generative model predicting per-light OLAT responses from single portrait, modeling illumination as continuous physically interpretable transformation between lighting states.

Result: Unified illumination learning framework linking real data, generative synthesis, and physically grounded relighting. Enables scalable, controllable relighting while preserving facial identity and capturing fine-grained direction-aware illumination effects.

Conclusion: POLAR and POLARNet establish a self-sustaining "chicken-and-egg" cycle for scalable and reproducible portrait illumination, overcoming limitations of existing diffusion/background-conditioned methods through physically interpretable continuous illumination modeling.

Abstract: Face relighting aims to synthesize realistic portraits under novel illumination while preserving identity and geometry. However, progress remains constrained by the limited availability of large-scale, physically consistent illumination data. To address this, we introduce POLAR, a large-scale and physically calibrated One-Light-at-a-Time (OLAT) dataset containing over 200 subjects captured under 156 lighting directions, multiple views, and diverse expressions. Building upon POLAR, we develop a flow-based generative model POLARNet that predicts per-light OLAT responses from a single portrait, capturing fine-grained and direction-aware illumination effects while preserving facial identity. Unlike diffusion or background-conditioned methods that rely on statistical or contextual cues, our formulation models illumination as a continuous, physically interpretable transformation between lighting states, enabling scalable and controllable relighting. Together, POLAR and POLARNet form a unified illumination learning framework that links real data, generative synthesis, and physically grounded relighting, establishing a self-sustaining "chicken-and-egg" cycle for scalable and reproducible portrait illumination.

</details>


### [223] [Ego-EXTRA: video-language Egocentric Dataset for EXpert-TRAinee assistance](https://arxiv.org/abs/2512.13238)
*Francesco Ragusa,Michele Mazzamuto,Rosario Forte,Irene D'Ambra,James Fort,Jakob Engel,Antonino Furnari,Giovanni Maria Farinella*

Main category: cs.CV

TL;DR: Ego-EXTRA is a 50-hour egocentric video dataset for expert-trainee assistance, featuring unscripted procedural activities with natural language guidance from experts, used to benchmark multimodal LLMs.


<details>
  <summary>Details</summary>
Motivation: To create a high-quality dataset for evaluating multimodal large language models in providing expert-level assistance in procedural tasks from an egocentric perspective, addressing the lack of realistic expert-trainee interaction data.

Method: Used a "Wizard of OZ" data collection paradigm where experts act as wearable intelligent assistants, viewing trainees' activities from their egocentric perspective and providing guidance through natural language dialogues. Collected 50 hours of unscripted videos, transcribed dialogues, and created 15k+ Visual Question Answer sets.

Result: Created a challenging benchmark dataset showing limitations of current multimodal LLMs in providing expert-level assistance. The dataset is publicly available with 15k+ high-quality VQA sets for evaluation.

Conclusion: Ego-EXTRA provides a valuable resource for benchmarking egocentric video-language assistants and reveals current models' limitations in expert-level assistance scenarios, encouraging further research in this direction.

Abstract: We present Ego-EXTRA, a video-language Egocentric Dataset for EXpert-TRAinee assistance. Ego-EXTRA features 50 hours of unscripted egocentric videos of subjects performing procedural activities (the trainees) while guided by real-world experts who provide guidance and answer specific questions using natural language. Following a ``Wizard of OZ'' data collection paradigm, the expert enacts a wearable intelligent assistant, looking at the activities performed by the trainee exclusively from their egocentric point of view, answering questions when asked by the trainee, or proactively interacting with suggestions during the procedures. This unique data collection protocol enables Ego-EXTRA to capture a high-quality dialogue in which expert-level feedback is provided to the trainee. Two-way dialogues between experts and trainees are recorded, transcribed, and used to create a novel benchmark comprising more than 15k high-quality Visual Question Answer sets, which we use to evaluate Multimodal Large Language Models. The results show that Ego-EXTRA is challenging and highlight the limitations of current models when used to provide expert-level assistance to the user. The Ego-EXTRA dataset is publicly available to support the benchmark of egocentric video-language assistants: https://fpv-iplab.github.io/Ego-EXTRA/.

</details>


### [224] [STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits](https://arxiv.org/abs/2512.13247)
*Foivos Paraperas Papantoniou,Stathis Galanakis,Rolandos Alexandros Potamias,Bernhard Kainz,Stefanos Zafeiriou*

Main category: cs.CV

TL;DR: STARCaster is a unified identity-aware spatio-temporal video diffusion model that handles both speech-driven portrait animation and free-viewpoint talking portrait synthesis using identity embeddings, addressing limitations in existing 2D and 3D approaches.


<details>
  <summary>Details</summary>
Motivation: Existing 2D speech-to-video diffusion models rely heavily on reference guidance, limiting motion diversity. 3D-aware animation methods often use imperfect tri-plane generator inversions that cause identity drift. The paper aims to overcome these limitations by rethinking reference- and geometry-based paradigms.

Method: STARCaster uses a compositional approach: 1) ID-aware motion modeling with softer identity constraints instead of strict reference conditioning, 2) audio-visual synchronization via lip reading supervision, 3) novel view animation through temporal-to-spatial adaptation. It employs decoupled learning for view consistency and temporal coherence, plus a self-forcing training scheme for longer temporal contexts.

Result: Comprehensive evaluations show STARCaster generalizes effectively across tasks and identities, consistently surpassing prior approaches in different benchmarks. It overcomes overly static animations common in autoregressive methods and handles limited 4D audio-visual data effectively.

Conclusion: STARCaster provides a unified framework for identity-aware video generation that addresses both speech-driven animation and free-viewpoint synthesis, overcoming limitations of existing 2D and 3D approaches through innovative training strategies and architectural design.

Abstract: This paper presents STARCaster, an identity-aware spatio-temporal video diffusion model that addresses both speech-driven portrait animation and free-viewpoint talking portrait synthesis, given an identity embedding or reference image, within a unified framework. Existing 2D speech-to-video diffusion models depend heavily on reference guidance, leading to limited motion diversity. At the same time, 3D-aware animation typically relies on inversion through pre-trained tri-plane generators, which often leads to imperfect reconstructions and identity drift. We rethink reference- and geometry-based paradigms in two ways. First, we deviate from strict reference conditioning at pre-training by introducing softer identity constraints. Second, we address 3D awareness implicitly within the 2D video domain by leveraging the inherent multi-view nature of video data. STARCaster adopts a compositional approach progressing from ID-aware motion modeling, to audio-visual synchronization via lip reading-based supervision, and finally to novel view animation through temporal-to-spatial adaptation. To overcome the scarcity of 4D audio-visual data, we propose a decoupled learning approach in which view consistency and temporal coherence are trained independently. A self-forcing training scheme enables the model to learn from longer temporal contexts than those generated at inference, mitigating the overly static animations common in existing autoregressive approaches. Comprehensive evaluations demonstrate that STARCaster generalizes effectively across tasks and identities, consistently surpassing prior approaches in different benchmarks.

</details>


### [225] [Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection](https://arxiv.org/abs/2512.13250)
*Juil Koo,Daehyeon Choi,Sangwoo Youn,Phillip Y. Lee,Minhyuk Sung*

Main category: cs.CV

TL;DR: VG-AVS enables VLMs to actively select informative next viewpoints for better visual question answering, moving beyond static image reasoning to ambulatory vision.


<details>
  <summary>Details</summary>
Motivation: Current VLMs are limited to static image reasoning (snapshot vision), while embodied agents need active movement to obtain better views (ambulatory vision). There's a gap in selecting optimal next viewpoints using only current visual information without scene memory or external knowledge.

Method: 1) Introduce VG-AVS task for selecting informative next viewpoints using only current image. 2) Create synthetic dataset with paired query-target views and QA prompts. 3) Fine-tune pretrained VLMs with supervised fine-tuning followed by RL-based policy optimization.

Result: Achieves strong QA performance through viewpoint selection, generalizes to unseen synthetic and real scenes, and improves downstream QA accuracy when integrated into existing scene-exploration-based EQA systems.

Conclusion: VG-AVS successfully bridges snapshot and ambulatory vision by enabling VLMs to actively select informative viewpoints, enhancing visual reasoning capabilities for embodied agents.

Abstract: Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.

</details>


### [226] [CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing](https://arxiv.org/abs/2512.13276)
*Yan Li,Lin Liu,Xiaopeng Zhang,Wei Xue,Wenhan Luo,Yike Guo,Qi Tian*

Main category: cs.CV

TL;DR: CogniEdit: A unified framework for fine-grained instruction-based image editing using multi-modal reasoning and dense reward optimization across denoising steps.


<details>
  <summary>Details</summary>
Motivation: Existing instruction-based image editing methods struggle with fine-grained instructions specifying precise attributes like colors, positions, and quantities. Current GRPO-based approaches only optimize at individual sampling steps, providing sparse feedback that limits trajectory-level control.

Method: CogniEdit combines multi-modal reasoning with dense reward optimization: (1) Multi-modal LLMs decompose complex instructions into actionable directives, (2) Dynamic Token Focus Relocation adaptively emphasizes fine-grained attributes, (3) Dense GRPO-based optimization propagates gradients across consecutive denoising steps for trajectory-level supervision.

Result: Extensive experiments on benchmark datasets demonstrate that CogniEdit achieves state-of-the-art performance in balancing fine-grained instruction following with visual quality and editability preservation.

Conclusion: CogniEdit successfully addresses the limitations of existing methods by enabling trajectory-level gradient flow through the sampling process, achieving superior fine-grained instruction following while maintaining visual quality.

Abstract: Instruction-based image editing with diffusion models has achieved impressive results, yet existing methods struggle with fine-grained instructions specifying precise attributes such as colors, positions, and quantities. While recent approaches employ Group Relative Policy Optimization (GRPO) for alignment, they optimize only at individual sampling steps, providing sparse feedback that limits trajectory-level control. We propose a unified framework CogniEdit, combining multi-modal reasoning with dense reward optimization that propagates gradients across consecutive denoising steps, enabling trajectory-level gradient flow through the sampling process. Our method comprises three components: (1) Multi-modal Large Language Models for decomposing complex instructions into actionable directives, (2) Dynamic Token Focus Relocation that adaptively emphasizes fine-grained attributes, and (3) Dense GRPO-based optimization that propagates gradients across consecutive steps for trajectory-level supervision. Extensive experiments on benchmark datasets demonstrate that our CogniEdit achieves state-of-the-art performance in balancing fine-grained instruction following with visual quality and editability preservation

</details>


### [227] [Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?](https://arxiv.org/abs/2512.13281)
*Jiaqi Wang,Weijia Wu,Yi Zhan,Rui Zhao,Ming Hu,James Cheng,Wei Liu,Philip Torr,Kevin Qinghong Lin*

Main category: cs.CV

TL;DR: Video Reality Test is an ASMR-sourced benchmark for evaluating perceptual realism of AI-generated videos with audio, showing current models can fool VLMs but not human experts.


<details>
  <summary>Details</summary>
Motivation: As AI-generated videos become increasingly realistic, there's a need to test whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive both humans and Vision-Language Models (VLMs). Existing benchmarks mostly evaluate video without audio, target broad narrative domains, and focus only on classification.

Method: Introduces Video Reality Test benchmark suite with: (1) Immersive ASMR video-audio sources built on curated real ASMR videos targeting fine-grained action-object interactions, and (2) Peer-Review evaluation using an adversarial creator-reviewer protocol where video generation models act as creators trying to fool reviewers, while VLMs serve as reviewers trying to detect fakeness.

Result: The best creator model (Veo3.1-Fast) fools most VLMs - the strongest reviewer (Gemini 2.5-Pro) achieves only 56% accuracy (random is 50%), far below human experts (81.25%). Adding audio improves real-fake discrimination, but superficial cues like watermarks can still significantly mislead models.

Conclusion: The findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency, highlighting the gap between AI-generated video detection capabilities of VLMs versus human experts.

Abstract: Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: \textbf{(i) Immersive ASMR video-audio sources.} Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. \textbf{(ii) Peer-Review evaluation.} An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\% accuracy (random 50\%), far below that of human experts (81.25\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.

</details>


### [228] [CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images](https://arxiv.org/abs/2512.13285)
*Bo Liu,Qiao Qin,Qinghui He*

Main category: cs.CV

TL;DR: CausalCLIP: A framework that disentangles causal forensic features from non-causal ones using causal inference principles to improve generalization of generated image detection across diverse generative models.


<details>
  <summary>Details</summary>
Motivation: Existing generated image detectors produce entangled representations mixing task-relevant forensic cues (causal features) with spurious patterns (non-causal features), limiting generalization across diverse and evolving generation techniques.

Method: Proposes CausalCLIP framework that explicitly disentangles causal from non-causal features using causal inference principles. Models generation process with structural causal model and enforces statistical independence through Gumbel-Softmax-based feature masking and Hilbert-Schmidt Independence Criterion (HSIC) constraints to isolate stable causal features robust to distribution shifts.

Result: When tested on unseen generative models from different series, CausalCLIP demonstrates strong generalization ability, achieving improvements of 6.83% in accuracy and 4.06% in average precision over state-of-the-art methods.

Conclusion: CausalCLIP effectively addresses the generalization challenge in generated image detection by disentangling causal forensic features from non-causal ones, leading to significantly improved performance on unseen generative models.

Abstract: The rapid advancement of generative models has increased the demand for generated image detectors capable of generalizing across diverse and evolving generation techniques. However, existing methods, including those leveraging pre-trained vision-language models, often produce highly entangled representations, mixing task-relevant forensic cues (causal features) with spurious or irrelevant patterns (non-causal features), thus limiting generalization. To address this issue, we propose CausalCLIP, a framework that explicitly disentangles causal from non-causal features and employs targeted filtering guided by causal inference principles to retain only the most transferable and discriminative forensic cues. By modeling the generation process with a structural causal model and enforcing statistical independence through Gumbel-Softmax-based feature masking and Hilbert-Schmidt Independence Criterion (HSIC) constraints, CausalCLIP isolates stable causal features robust to distribution shifts. When tested on unseen generative models from different series, CausalCLIP demonstrates strong generalization ability, achieving improvements of 6.83% in accuracy and 4.06% in average precision over state-of-the-art methods.

</details>


### [229] [LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models](https://arxiv.org/abs/2512.13290)
*Shu Yu,Chaochao Lu*

Main category: cs.CV

TL;DR: LINA introduces a novel framework that uses causal interventions and adaptive denoising schedules to improve diffusion models' physical alignment and out-of-distribution instruction following by addressing their limitations in learning causal directions and factor disentanglement.


<details>
  <summary>Details</summary>
Motivation: Diffusion models struggle with physical alignment and out-of-distribution instruction following due to their failure to learn causal directions and disentangle causal factors for novel recombination. These limitations prevent them from properly handling complex causal generation tasks.

Method: The authors introduce Causal Scene Graph (CSG) and Physical Alignment Probe (PAP) dataset for diagnostic interventions. Based on insights from these tools, they propose LINA framework that learns to predict prompt-specific interventions using targeted guidance in prompt/visual latent spaces and a reallocated, causality-aware denoising schedule.

Result: LINA achieves state-of-the-art performance on challenging causal generation tasks and the Winoground dataset, successfully enforcing both physical alignment and OOD instruction following in image and video diffusion models.

Conclusion: The paper demonstrates that addressing causal reasoning limitations through targeted interventions and adaptive denoising schedules significantly improves diffusion models' ability to handle physical alignment and out-of-distribution instructions, advancing their capabilities in complex generation tasks.

Abstract: Diffusion models (DMs) have achieved remarkable success in image and video generation. However, they still struggle with (1) physical alignment and (2) out-of-distribution (OOD) instruction following. We argue that these issues stem from the models' failure to learn causal directions and to disentangle causal factors for novel recombination. We introduce the Causal Scene Graph (CSG) and the Physical Alignment Probe (PAP) dataset to enable diagnostic interventions. This analysis yields three key insights. First, DMs struggle with multi-hop reasoning for elements not explicitly determined in the prompt. Second, the prompt embedding contains disentangled representations for texture and physics. Third, visual causal structure is disproportionately established during the initial, computationally limited denoising steps. Based on these findings, we introduce LINA (Learning INterventions Adaptively), a novel framework that learns to predict prompt-specific interventions, which employs (1) targeted guidance in the prompt and visual latent spaces, and (2) a reallocated, causality-aware denoising schedule. Our approach enforces both physical alignment and OOD instruction following in image and video DMs, achieving state-of-the-art performance on challenging causal generation tasks and the Winoground dataset. Our project page is at https://opencausalab.github.io/LINA.

</details>


### [230] [ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement](https://arxiv.org/abs/2512.13303)
*Zhihang Liu,Xiaoyi Bao,Pandeng Li,Junjie Zhou,Zhaohe Liao,Yefei He,Kaixun Jiang,Chen-Wei Xie,Yun Zheng,Hongtao Xie*

Main category: cs.CV

TL;DR: ShowTable is a pipeline combining MLLMs and diffusion models for creative table visualization, with automated data construction and a new benchmark TableVisBench.


<details>
  <summary>Details</summary>
Motivation: Existing generation models struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping beyond general scenarios. The paper introduces the challenging task of creative table visualization that requires faithful and aesthetic visualization of tabular data.

Method: Proposes ShowTable pipeline that synergizes MLLMs with diffusion models via progressive self-correcting process. MLLM acts as central orchestrator for reasoning visual plans and judging visual errors, while diffusion models execute commands. Introduces three automated data construction pipelines for training different modules.

Result: Experiments show the pipeline significantly outperforms baselines. Introduces TableVisBench benchmark with 800 challenging instances across 5 evaluation dimensions to assess performance on creative table visualization task.

Conclusion: The proposed pipeline demonstrates effective multi-modal reasoning, generation, and error correction capabilities for creative table visualization, pushing beyond limitations of existing generation models.

Abstract: While existing generation and unified models excel at general image generation, they struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping abilities beyond general scenarios. To push beyond the existing limitations, we introduce a new and challenging task: creative table visualization, requiring the model to generate an infographic that faithfully and aesthetically visualizes the data from a given table. To address this challenge, we propose ShowTable, a pipeline that synergizes MLLMs with diffusion models via a progressive self-correcting process. The MLLM acts as the central orchestrator for reasoning the visual plan and judging visual errors to provide refined instructions, the diffusion execute the commands from MLLM, achieving high-fidelity results. To support this task and our pipeline, we introduce three automated data construction pipelines for training different modules. Furthermore, we introduce TableVisBench, a new benchmark with 800 challenging instances across 5 evaluation dimensions, to assess performance on this task. Experiments demonstrate that our pipeline, instantiated with different models, significantly outperforms baselines, highlighting its effective multi-modal reasoning, generation, and error correction capabilities.

</details>


### [231] [KlingAvatar 2.0 Technical Report](https://arxiv.org/abs/2512.13313)
*Kling Team,Jialu Chen,Yikang Ding,Zhixue Fang,Kun Gai,Yuan Gao,Kang He,Jingyun Hua,Boyuan Jiang,Mingming Lao,Xiaohan Li,Hui Liu,Jiwen Liu,Xiaoqiang Liu,Yuan Liu,Shun Lu,Yongsen Mao,Yingchao Shao,Huafeng Shi,Xiaoyu Shi,Peiqin Sun,Songlin Tang,Pengfei Wan,Chao Wang,Xuebo Wang,Haoxian Zhang,Yuanxing Zhang,Yan Zhou*

Main category: cs.CV

TL;DR: KlingAvatar 2.0 is a spatio-temporal cascade framework for efficient long-duration high-resolution avatar video generation that addresses temporal drifting, quality degradation, and weak prompt following through blueprint keyframes, first-last frame refinement, and a Co-Reasoning Director with LLM experts.


<details>
  <summary>Details</summary>
Motivation: Prior avatar video generation models suffer from limited efficiency in generating long-duration high-resolution videos, with problems including temporal drifting, quality degradation, and weak prompt following as video length increases.

Method: A spatio-temporal cascade framework that first generates low-resolution blueprint video keyframes for global semantics and motion, then refines them into high-resolution sub-clips using a first-last frame strategy. Includes a Co-Reasoning Director with three modality-specific LLM experts to reason about modality priorities and infer user intent, plus a Negative Director for prompt refinement. Supports ID-specific multi-character control.

Result: The model effectively addresses challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.

Conclusion: KlingAvatar 2.0 provides an effective solution for generating high-quality, long-duration avatar videos with improved efficiency, temporal coherence, and multimodal alignment through its spatio-temporal cascade framework and reasoning-based director components.

Abstract: Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.

</details>


### [232] [Face Identity Unlearning for Retrieval via Embedding Dispersion](https://arxiv.org/abs/2512.13317)
*Mikhail Zakharov*

Main category: cs.CV

TL;DR: The paper proposes a dispersion-based unlearning method for face retrieval systems to make selected identities unretrievable while preserving overall system utility.


<details>
  <summary>Details</summary>
Motivation: Face recognition systems raise serious privacy concerns due to potential unauthorized identity tracking. Existing machine unlearning methods haven't been adequately explored for modern embedding-based face retrieval systems.

Method: Proposes a dispersion-based unlearning approach that disperses embeddings of target identities on the hypersphere to prevent compact identity cluster formation. Evaluates existing methods (Random Labeling, Gradient Ascent, Boundary Unlearning) and introduces their own method for face identity unlearning in retrieval systems.

Result: Extensive experiments on VGGFace2 and CelebA benchmarks show superior forgetting behavior while preserving retrieval utility compared to existing approximate class unlearning methods.

Conclusion: The proposed dispersion-based unlearning method effectively addresses privacy concerns in face retrieval systems by making selected identities unretrievable while maintaining the discriminative structure and performance for remaining identities.

Abstract: Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.

</details>


### [233] [Automated User Identification from Facial Thermograms with Siamese Networks](https://arxiv.org/abs/2512.13361)
*Elizaveta Prozorova,Anton Konev,Vladimir Faerman*

Main category: cs.CV

TL;DR: Thermal imaging for facial biometric identification using Siamese networks achieves ~80% accuracy, with analysis of IR spectral ranges and hybrid visible-IR systems.


<details>
  <summary>Details</summary>
Motivation: To develop reliable security systems using thermal imaging for biometric identification, overcoming limitations of traditional visible-light systems by leveraging facial thermograms.

Method: Comparative analysis of infrared spectral ranges (NIR, SWIR, MWIR, LWIR) and use of Siamese neural networks for automated identification. Defines camera requirements: sensor resolution, thermal sensitivity, and ≥30Hz frame rate.

Result: Achieved approximately 80% accuracy on proprietary dataset. Hybrid visible-infrared systems show potential to overcome individual modality limitations.

Conclusion: Thermal imaging is a promising technology for developing reliable security systems, with Siamese networks providing effective automated identification and hybrid systems offering enhanced performance.

Abstract: The article analyzes the use of thermal imaging technologies for biometric identification based on facial thermograms. It presents a comparative analysis of infrared spectral ranges (NIR, SWIR, MWIR, and LWIR). The paper also defines key requirements for thermal cameras used in biometric systems, including sensor resolution, thermal sensitivity, and a frame rate of at least 30 Hz. Siamese neural networks are proposed as an effective approach for automating the identification process. In experiments conducted on a proprietary dataset, the proposed method achieved an accuracy of approximately 80%. The study also examines the potential of hybrid systems that combine visible and infrared spectra to overcome the limitations of individual modalities. The results indicate that thermal imaging is a promising technology for developing reliable security systems.

</details>


### [234] [Unlocking Generalization in Polyp Segmentation with DINO Self-Attention "keys"](https://arxiv.org/abs/2512.13376)
*Carla Monteiro,Valentina Corbetta,Regina Beets-Tan,Luís F. Teixeira,Wilson Silva*

Main category: cs.CV

TL;DR: A novel polyp segmentation framework using DINO self-attention key features with simple convolutional decoder achieves SOTA performance with better generalization in data-scarce scenarios.


<details>
  <summary>Details</summary>
Motivation: Current DL polyp segmentation methods struggle with generalization in data-constrained settings and rely on complex, task-specific architectures. Need for more robust and generalizable approaches.

Method: Leverages DINO self-attention "key" features instead of deepest ViT layer tokens, combined with simple convolutional decoder for polyp mask prediction. Validated using multi-center datasets under Domain Generalization and Extreme Single Domain Generalization protocols.

Result: Achieves state-of-the-art performance, significantly enhancing generalization in data-scarce and challenging scenarios. Surpasses established models like nnU-Net and UM-Net without polyp-specific architecture.

Conclusion: The framework demonstrates that leveraging DINO self-attention key features provides robust segmentation with better generalization, offering a simpler yet more effective alternative to complex task-specific architectures.

Abstract: Automatic polyp segmentation is crucial for improving the clinical identification of colorectal cancer (CRC). While Deep Learning (DL) techniques have been extensively researched for this problem, current methods frequently struggle with generalization, particularly in data-constrained or challenging settings. Moreover, many existing polyp segmentation methods rely on complex, task-specific architectures. To address these limitations, we present a framework that leverages the intrinsic robustness of DINO self-attention "key" features for robust segmentation. Unlike traditional methods that extract tokens from the deepest layers of the Vision Transformer (ViT), our approach leverages the key features of the self-attention module with a simple convolutional decoder to predict polyp masks, resulting in enhanced performance and better generalizability. We validate our approach using a multi-center dataset under two rigorous protocols: Domain Generalization (DG) and Extreme Single Domain Generalization (ESDG). Our results, supported by a comprehensive statistical analysis, demonstrate that this pipeline achieves state-of-the-art (SOTA) performance, significantly enhancing generalization, particularly in data-scarce and challenging scenarios. While avoiding a polyp-specific architecture, we surpass well-established models like nnU-Net and UM-Net. Additionally, we provide a systematic benchmark of the DINO framework's evolution, quantifying the specific impact of architectural advancements on downstream polyp segmentation performance.

</details>


### [235] [Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs](https://arxiv.org/abs/2512.13392)
*Anran Qi,Changjian Li,Adrien Bousseau,Niloy J. Mitra*

Main category: cs.CV

TL;DR: A training-free image-to-video method that separates motion specification from appearance synthesis using a user-editable Proxy Dynamic Graph for controllable articulation and user-specified content in disoccluded regions.


<details>
  <summary>Details</summary>
Motivation: Current image-to-video pipelines struggle with predictable, articulated motions while enforcing user-specified content in newly revealed areas (disocclusions). There's a need for better control over both motion and appearance in generated videos.

Method: Introduces a lightweight, user-editable Proxy Dynamic Graph (PDG) that deterministically drives part motion. Uses a frozen diffusion prior as a motion-guided shader. Users annotate/repose PDG, compute dense motion flow, edit appearance in disoccluded areas, and perform latent-space composite using visibility information from PDG.

Result: Demonstrates clear advantages over state-of-the-art alternatives for turning images into short videos of articulated objects, furniture, vehicles, and deformables. Enables controllable articulation and user control over disocclusions without fine-tuning.

Conclusion: The method successfully mixes generative control (loose pose/structure) with predictable controls (appearance specification in disoccluded regions), unlocking a new image-to-video workflow that separates motion specification from appearance synthesis.

Abstract: We address image-to-video generation with explicit user control over the final frame's disoccluded regions. Current image-to-video pipelines produce plausible motion but struggle to generate predictable, articulated motions while enforcing user-specified content in newly revealed areas. Our key idea is to separate motion specification from appearance synthesis: we introduce a lightweight, user-editable Proxy Dynamic Graph (PDG) that deterministically yet approximately drives part motion, while a frozen diffusion prior is used to synthesize plausible appearance that follows that motion. In our training-free pipeline, the user loosely annotates and reposes a PDG, from which we compute a dense motion flow to leverage diffusion as a motion-guided shader. We then let the user edit appearance in the disoccluded areas of the image, and exploit the visibility information encoded by the PDG to perform a latent-space composite that reconciles motion with user intent in these areas. This design yields controllable articulation and user control over disocclusions without fine-tuning. We demonstrate clear advantages against state-of-the-art alternatives towards images turned into short videos of articulated objects, furniture, vehicles, and deformables. Our method mixes generative control, in the form of loose pose and structure, with predictable controls, in the form of appearance specification in the final frame in the disoccluded regions, unlocking a new image-to-video workflow. Code will be released on acceptance. Project page: https://anranqi.github.io/beyondvisible.github.io/

</details>


### [236] [rNCA: Self-Repairing Segmentation Masks](https://arxiv.org/abs/2512.13397)
*Malte Silbernagel,Albert Alonso,Jens Petersen,Bulat Ibragimov,Marleen de Bruijne,Madeleine K. Wyburd*

Main category: cs.CV

TL;DR: Neural Cellular Automata (NCA) can be repurposed as a refinement mechanism to fix topological errors in segmentation masks, improving connectivity and structure without task-specific architectures.


<details>
  <summary>Details</summary>
Motivation: General segmentation models often produce fragmented or disconnected masks with topological errors, and current solutions require hand-crafted rules or specialized architectures for each task.

Method: Train refinement NCA (rNCA) on imperfect masks and ground truths to learn structural properties using local, iterative updates. The automaton uses only local information to progressively reconnect broken regions and prune fragments when applied to coarse masks.

Result: For retinal vessels: 2-3% gains in Dice/clDice, 60% reduction in β₀ errors, 20% reduction in β₁ errors. For myocardium: 61.5% repair of broken cases in zero-shot, 19% lower ASSD, 16% lower HD. Shows NCA as broadly applicable refiners.

Conclusion: NCA provides an effective, general-purpose refinement mechanism for segmentation masks that can fix topological errors across different models and tasks without requiring specialized architectures.

Abstract: Accurately predicting topologically correct masks remains a difficult task for general segmentation models, which often produce fragmented or disconnected outputs. Fixing these artifacts typically requires hand-crafted refinement rules or architectures specialized to a particular task. Here, we show that Neural Cellular Automata (NCA) can be directly re-purposed as an effective refinement mechanism, using local, iterative updates guided by image context to repair segmentation masks. By training on imperfect masks and ground truths, the automaton learns the structural properties of the target shape while relying solely on local information. When applied to coarse, globally predicted masks, the learned dynamics progressively reconnect broken regions, prune loose fragments and converge towards stable, topologically consistent results. We show how refinement NCA (rNCA) can be easily applied to repair common topological errors produced by different base segmentation models and tasks: for fragmented retinal vessels, it yields 2-3% gains in Dice/clDice and improves Betti errors, reducing $β_0$ errors by 60% and $β_1$ by 20%; for myocardium, it repairs 61.5% of broken cases in a zero-shot setting while lowering ASSD and HD by 19% and 16%, respectively. This showcases NCA as effective and broadly applicable refiners.

</details>


### [237] [End2Reg: Learning Task-Specific Segmentation for Markerless Registration in Spine Surgery](https://arxiv.org/abs/2512.13402)
*Lorenzo Pettinari,Sidaty El Hadramy,Michael Wehrli,Philippe C. Cattin,Daniel Studer,Carol C. Hasler,Maria Licci*

Main category: cs.CV

TL;DR: End2Reg is an end-to-end deep learning framework for markerless spine surgery navigation that jointly optimizes segmentation and registration without weak labels, achieving state-of-the-art accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: Current intraoperative navigation systems for spine surgery are invasive (bone-anchored markers), radiation-intensive (radiographic imaging), and workflow disruptive. Existing markerless RGB-D registration methods rely on weak segmentation labels that propagate errors throughout registration.

Method: End-to-end deep learning framework that jointly optimizes segmentation and registration. The network learns segmentation masks specifically optimized for registration, guided solely by the registration objective without direct segmentation supervision, eliminating weak labels and manual steps.

Result: Achieves state-of-the-art performance: reduces median Target Registration Error by 32% to 1.83mm and mean Root Mean Square Error by 45% to 3.95mm on ex- and in-vivo benchmarks. Ablation study confirms end-to-end optimization significantly improves registration accuracy.

Conclusion: The end-to-end RGB-D registration pipeline removes dependency on weak labels and manual steps, advancing toward fully automatic, markerless intraoperative navigation for spine surgery.

Abstract: Purpose: Intraoperative navigation in spine surgery demands millimeter-level accuracy. Current systems based on intraoperative radiographic imaging and bone-anchored markers are invasive, radiation-intensive and workflow disruptive. Recent markerless RGB-D registration methods offer a promising alternative, but existing approaches rely on weak segmentation labels to isolate relevant anatomical structures, which can propagate errors throughout registration. Methods: We present End2Reg an end-to-end deep learning framework that jointly optimizes segmentation and registration, eliminating the need for weak segmentation labels and manual steps. The network learns segmentation masks specifically optimized for registration, guided solely by the registration objective without direct segmentation supervision. Results: The proposed framework achieves state-of-the-art performance on ex- and in-vivo benchmarks, reducing median Target Registration Error by 32% to 1.83mm and mean Root Mean Square Error by 45% to 3.95mm, respectively. An ablation study confirms that end-to-end optimization significantly improves registration accuracy. Conclusion: The presented end-to-end RGB-D registration pipeline removes dependency on weak labels and manual steps, advancing towards fully automatic, markerless intraoperative navigation. Code and interactive visualizations are available at: https://lorenzopettinari.github.io/end-2-reg/.

</details>


### [238] [Computer vision training dataset generation for robotic environments using Gaussian splatting](https://arxiv.org/abs/2512.13411)
*Patryk Niżeniec,Marcin Iwanowski*

Main category: cs.CV

TL;DR: Novel pipeline for generating large-scale, photorealistic synthetic datasets for robotic vision using 3D Gaussian Splatting and physics simulation, with automatic labeling and hybrid training strategy.


<details>
  <summary>Details</summary>
Motivation: Addresses domain gap between synthetic/real imagery and manual annotation bottleneck for computer vision in robotic environments.

Method: Uses 3D Gaussian Splatting for photorealistic representations, game engine physics for natural arrangements, two-pass rendering with shadow maps, and automatic pixel-perfect segmentation mask generation.

Result: Hybrid training (small real + large synthetic data) yields best detection/segmentation performance, confirming optimal strategy for robust models.

Conclusion: Proposed pipeline efficiently generates realistic labeled datasets for robotic vision, enabling robust model training with minimal real data.

Abstract: This paper introduces a novel pipeline for generating large-scale, highly realistic, and automatically labeled datasets for computer vision tasks in robotic environments. Our approach addresses the critical challenges of the domain gap between synthetic and real-world imagery and the time-consuming bottleneck of manual annotation. We leverage 3D Gaussian Splatting (3DGS) to create photorealistic representations of the operational environment and objects. These assets are then used in a game engine where physics simulations create natural arrangements. A novel, two-pass rendering technique combines the realism of splats with a shadow map generated from proxy meshes. This map is then algorithmically composited with the image to add both physically plausible shadows and subtle highlights, significantly enhancing realism. Pixel-perfect segmentation masks are generated automatically and formatted for direct use with object detection models like YOLO. Our experiments show that a hybrid training strategy, combining a small set of real images with a large volume of our synthetic data, yields the best detection and segmentation performance, confirming this as an optimal strategy for efficiently achieving robust and accurate models.

</details>


### [239] [USTM: Unified Spatial and Temporal Modeling for Continuous Sign Language Recognition](https://arxiv.org/abs/2512.13415)
*Ahmed Abul Hasanaath,Hamzah Luqman*

Main category: cs.CV

TL;DR: USTM framework uses Swin Transformer with temporal adapters for continuous sign language recognition, achieving SOTA on RGB-only inputs without needing multi-modal data.


<details>
  <summary>Details</summary>
Motivation: Existing CSLR methods using CNN backbones with temporal convolutions/RNNs fail to capture fine-grained hand/facial cues and long-range temporal dependencies in sign language videos.

Method: Proposes Unified Spatio-Temporal Modeling (USTM) framework with Swin Transformer backbone enhanced with lightweight temporal adapter with positional embeddings (TAPE) to capture both spatial features and temporal context.

Result: Achieves state-of-the-art performance on PHOENIX14, PHOENIX14T, and CSL-Daily datasets against RGB-based and multi-modal approaches, competitive with multi-stream methods.

Conclusion: USTM effectively models complex spatio-temporal patterns for CSLR using only RGB videos, demonstrating strong performance without multi-stream inputs or auxiliary modalities.

Abstract: Continuous sign language recognition (CSLR) requires precise spatio-temporal modeling to accurately recognize sequences of gestures in videos. Existing frameworks often rely on CNN-based spatial backbones combined with temporal convolution or recurrent modules. These techniques fail in capturing fine-grained hand and facial cues and modeling long-range temporal dependencies. To address these limitations, we propose the Unified Spatio-Temporal Modeling (USTM) framework, a spatio-temporal encoder that effectively models complex patterns using a combination of a Swin Transformer backbone enhanced with lightweight temporal adapter with positional embeddings (TAPE). Our framework captures fine-grained spatial features alongside short and long-term temporal context, enabling robust sign language recognition from RGB videos without relying on multi-stream inputs or auxiliary modalities. Extensive experiments on benchmarked datasets including PHOENIX14, PHOENIX14T, and CSL-Daily demonstrate that USTM achieves state-of-the-art performance against RGB-based as well as multi-modal CSLR approaches, while maintaining competitive performance against multi-stream approaches. These results highlight the strength and efficacy of the USTM framework for CSLR. The code is available at https://github.com/gufranSabri/USTM

</details>


### [240] [Learning to Generate Cross-Task Unexploitable Examples](https://arxiv.org/abs/2512.13416)
*Haoxuan Qu,Qiuchi Xiang,Yujun Cai,Yirui Wu,Majid Mirmehdi,Hossein Rahmani,Jun Liu*

Main category: cs.CV

TL;DR: Proposes MCT-UEG framework for generating unexploitable examples that work across multiple computer vision tasks using meta-learning with flat-minima optimization.


<details>
  <summary>Details</summary>
Motivation: Existing unexploitable example generation methods have limited practical applicability because they fail to produce examples that are broadly unexploitable across different real-world computer vision tasks.

Method: MCT-UEG framework with flat-minima-oriented meta training and testing scheme to optimize the unexploitable example generator for producing broadly unexploitable examples across multiple tasks.

Result: Extensive experiments show the efficacy of the proposed framework in generating unexploitable examples that work across different computer vision tasks.

Conclusion: The proposed MCT-UEG framework addresses the limitation of existing methods by generating broadly unexploitable examples through meta-learning with flat-minima optimization, enhancing practical applicability for personal data privacy protection.

Abstract: Unexploitable example generation aims to transform personal images into their unexploitable (unlearnable) versions before they are uploaded online, thereby preventing unauthorized exploitation of online personal images. Recently, this task has garnered significant research attention due to its critical relevance to personal data privacy. Yet, despite recent progress, existing methods for this task can still suffer from limited practical applicability, as they can fail to generate examples that are broadly unexploitable across different real-world computer vision tasks. To deal with this problem, in this work, we propose a novel Meta Cross-Task Unexploitable Example Generation (MCT-UEG) framework. At the core of our framework, to optimize the unexploitable example generator for effectively producing broadly unexploitable examples, we design a flat-minima-oriented meta training and testing scheme. Extensive experiments show the efficacy of our framework.

</details>


### [241] [RecTok: Reconstruction Distillation along Rectified Flow](https://arxiv.org/abs/2512.13421)
*Qingyu Shi,Size Wu,Jinbin Bai,Kaidong Yu,Yujing Wang,Yunhai Tong,Xiangtai Li,Xuelong Li*

Main category: cs.CV

TL;DR: RecTok improves high-dimensional visual tokenizers for diffusion models using flow semantic distillation and reconstruction-alignment distillation to overcome the trade-off between latent dimensionality and generation quality.


<details>
  <summary>Details</summary>
Motivation: There's a fundamental trade-off between latent space dimensionality and generation quality in visual tokenizers for diffusion models. High-dimensional tokenizers underperform low-dimensional ones despite having richer semantic potential from vision foundation models.

Method: Two key innovations: 1) Flow semantic distillation - distills semantic information from vision foundation models into forward flow trajectories in flow matching, making the training space semantically rich; 2) Reconstruction-alignment distillation - introduces masked feature reconstruction loss to further enhance semantics.

Result: Achieves state-of-the-art results on gFID-50K with and without classifier-free guidance, superior image reconstruction, generation quality, and discriminative performance. Shows consistent improvements as latent dimensionality increases.

Conclusion: RecTok successfully overcomes the limitations of high-dimensional visual tokenizers by focusing on enriching the forward flow semantics rather than just the latent space, enabling better performance while maintaining semantically rich latent structures.

Abstract: Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.

</details>


### [242] [MineTheGap: Automatic Mining of Biases in Text-to-Image Models](https://arxiv.org/abs/2512.13427)
*Noa Cohen,Nurit Spingarn-Eliezer,Inbar Huberman-Spiegelglas,Tomer Michaeli*

Main category: cs.CV

TL;DR: MineTheGap is a genetic algorithm method that automatically discovers prompts exposing biases in text-to-image models by comparing image distributions to LLM-generated text variations.


<details>
  <summary>Details</summary>
Motivation: TTI models exhibit biases when interpreting ambiguous prompts, causing societal impacts (e.g., racial stereotypes) and poor user experience through redundant image generation rather than diverse outputs.

Method: Uses genetic algorithm to iteratively refine prompts that expose biases, driven by a novel bias score comparing generated image distributions to LLM-generated text variations of prompts.

Result: Method successfully mines prompts that reveal biases in TTI models, validated on dataset with known biases. Code and examples available online.

Conclusion: MineTheGap provides an automated approach to discover bias-inducing prompts in TTI models, going beyond mere bias detection to actively expose problematic model behaviors.

Abstract: Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.

</details>


### [243] [A Domain-Adapted Lightweight Ensemble for Resource-Efficient Few-Shot Plant Disease Classification](https://arxiv.org/abs/2512.13428)
*Anika Islam,Tasfia Tahsin,Zaarin Anjum,Md. Bakhtiar Hasan,Md. Hasanul Kabir*

Main category: cs.CV

TL;DR: A lightweight few-shot learning framework for plant disease identification using MobileNet feature extractors with feature fusion and Bi-LSTM attention classifier, achieving near-SOTA performance with only 15-shot learning while being mobile-friendly.


<details>
  <summary>Details</summary>
Motivation: Most deep learning approaches for plant disease identification require large annotated datasets and computationally intensive models, which are unsuitable for data-scarce and resource-constrained agricultural environments.

Method: Combines domain-adapted MobileNetV2 and MobileNetV3 as feature extractors with feature fusion technique, followed by Bi-LSTM classifier enhanced with attention mechanisms to capture sequential dependencies and focus on relevant features.

Result: Achieved 98.23% accuracy on PlantVillage dataset with 15-shot learning (approaching 99.98% SOTA), 69.28% on real-world field images, and outperformed previous SOTA with 99.72% accuracy on six diseases. Model size is ~40 MB with ~1.12 GFLOPs inference complexity.

Conclusion: Establishes a scalable, mobile-ready foundation for precise plant disease diagnostics in data-scarce regions through lightweight few-shot learning that maintains robust performance even in complex real-world environments.

Abstract: Accurate and timely identification of plant leaf diseases is essential for resilient and sustainable agriculture, yet most deep learning approaches rely on large annotated datasets and computationally intensive models that are unsuitable for data-scarce and resource-constrained environments. To address these challenges we present a few-shot learning approach within a lightweight yet efficient framework that combines domain-adapted MobileNetV2 and MobileNetV3 models as feature extractors, along with a feature fusion technique to generate robust feature representation. For the classification task, the fused features are passed through a Bi-LSTM classifier enhanced with attention mechanisms to capture sequential dependencies and focus on the most relevant features, thereby achieving optimal classification performance even in complex, real-world environments with noisy or cluttered backgrounds. The proposed framework was evaluated across multiple experimental setups, including both laboratory-controlled and field-captured datasets. On tomato leaf diseases from the PlantVillage dataset, it consistently improved performance across 1 to 15 shot scenarios, reaching 98.23+-0.33% at 15 shot, closely approaching the 99.98% SOTA benchmark achieved by a Transductive LSTM with attention, while remaining lightweight and mobile-friendly. Under real-world conditions using field images from the Dhan Shomadhan dataset, it maintained robust performance, reaching 69.28+-1.49% at 15-shot and demonstrating strong resilience to complex backgrounds. Notably, it also outperformed the previous SOTA accuracy of 96.0% on six diseases from PlantVillage, achieving 99.72% with only 15-shot learning. With a compact model size of approximately 40 MB and inference complexity of approximately 1.12 GFLOPs, this work establishes a scalable, mobile-ready foundation for precise plant disease diagnostics in data-scarce regions.

</details>


### [244] [IMILIA: interpretable multiple instance learning for inflammation prediction in IBD from H&E whole slide images](https://arxiv.org/abs/2512.13440)
*Thalyssa Baiocco-Rodrigues,Antoine Olivier,Reda Belbahri,Thomas Duboudin,Pierre-Antoine Bannier,Benjamin Adjadj,Katharina Von Loga,Nathan Noiry,Maxime Touzot,Hector Roux de Bezieux*

Main category: cs.CV

TL;DR: IMILIA is an interpretable deep learning framework for predicting inflammation in IBD H&E slides and identifying tissue regions driving predictions through automated marker computation.


<details>
  <summary>Details</summary>
Motivation: With IBD treatment shifting toward histologic remission, accurate assessment of microscopic inflammation is crucial for evaluating disease activity and treatment response, requiring automated tools for inflammation prediction and interpretation.

Method: End-to-end framework with two modules: 1) inflammation prediction using Multiple Instance Learning (MIL) model, and 2) interpretability module with HistoPLUS (cell detection/segmentation/classification) and EpiSeg (epithelium segmentation) for automated marker computation.

Result: Achieved cross-validation ROC-AUC of 0.83 on discovery cohort, and 0.99 and 0.84 on two external validation cohorts. Interpretability module revealed biologically consistent patterns: high-scored tiles show increased immune cell densities, low-scored tiles contain normal epithelial cells.

Conclusion: IMILIA provides accurate inflammation prediction with biologically interpretable results, offering a valuable tool for IBD histologic assessment with consistent performance across datasets and code available for replication.

Abstract: As the therapeutic target for Inflammatory Bowel Disease (IBD) shifts toward histologic remission, the accurate assessment of microscopic inflammation has become increasingly central for evaluating disease activity and response to treatment. In this work, we introduce IMILIA (Interpretable Multiple Instance Learning for Inflammation Analysis), an end-to-end framework designed for the prediction of inflammation presence in IBD digitized slides stained with hematoxylin and eosin (H&E), followed by the automated computation of markers characterizing tissue regions driving the predictions. IMILIA is composed of an inflammation prediction module, consisting of a Multiple Instance Learning (MIL) model, and an interpretability module, divided in two blocks: HistoPLUS, for cell instance detection, segmentation and classification; and EpiSeg, for epithelium segmentation. IMILIA achieves a cross-validation ROC-AUC of 0.83 on the discovery cohort, and a ROC-AUC of 0.99 and 0.84 on two external validation cohorts. The interpretability module yields biologically consistent insights: tiles with higher predicted scores show increased densities of immune cells (lymphocytes, plasmocytes, neutrophils and eosinophils), whereas lower-scored tiles predominantly contain normal epithelial cells. Notably, these patterns were consistent across all datasets. Code and models to partially replicate the results on the public IBDColEpi dataset can be found at https://github.com/owkin/imilia.

</details>


### [245] [Test-Time Modification: Inverse Domain Transformation for Robust Perception](https://arxiv.org/abs/2512.13454)
*Arpit Jadon,Joshua Niemeijer,Yuki M. Asano*

Main category: cs.CV

TL;DR: Using diffusion models at test time to map target images back to source distribution for domain generalization, achieving significant improvements without large-scale synthetic data generation.


<details>
  <summary>Details</summary>
Motivation: Generative foundation models have broad visual knowledge but synthesizing comprehensive target-domain variations for training data augmentation is slow, expensive, and incomplete. The paper proposes a more efficient alternative.

Method: Using diffusion models at test time to map target images back to the source distribution where the downstream model was trained. This requires only a source domain description, preserves the task model, and eliminates large-scale synthetic data generation. Includes ensemble variant for enhanced robustness.

Result: Consistent improvements across segmentation, detection, and classification tasks under challenging environmental shifts in real-to-real domain generalization scenarios with unknown target distributions. Achieved substantial relative gains: 137% on BDD100K-Night, 68% on ImageNet-R, and 62% on DarkZurich.

Conclusion: Test-time diffusion-based domain mapping provides an effective alternative to data augmentation for domain generalization, offering significant performance improvements while being more efficient and preserving existing task models.

Abstract: Generative foundation models contain broad visual knowledge and can produce diverse image variations, making them particularly promising for advancing domain generalization tasks. While they can be used for training data augmentation, synthesizing comprehensive target-domain variations remains slow, expensive, and incomplete. We propose an alternative: using diffusion models at test time to map target images back to the source distribution where the downstream model was trained. This approach requires only a source domain description, preserves the task model, and eliminates large-scale synthetic data generation. We demonstrate consistent improvements across segmentation, detection, and classification tasks under challenging environmental shifts in real-to-real domain generalization scenarios with unknown target distributions. Our analysis spans multiple generative and downstream models, including an ensemble variant for enhanced robustness. The method achieves substantial relative gains: 137% on BDD100K-Night, 68% on ImageNet-R, and 62% on DarkZurich.

</details>


### [246] [PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence](https://arxiv.org/abs/2512.13465)
*Ruiyan Wang,Teng Hu,Kaihui Huang,Zihan Su,Ran Yi,Lizhuang Ma*

Main category: cs.CV

TL;DR: PoseAnything is a universal pose-guided video generation framework that handles both human and non-human characters with arbitrary skeletal inputs, featuring part-level consistency modules and decoupled camera motion control.


<details>
  <summary>Details</summary>
Motivation: Current pose-guided video generation methods only accept human poses as input, limiting their generalization to other subjects like animals or fictional characters. There's a need for a universal framework that can handle arbitrary skeletal inputs beyond just human poses.

Method: 1) PoseAnything framework supporting arbitrary skeletal inputs for both human and non-human characters; 2) Part-aware Temporal Coherence Module that divides subjects into parts, establishes correspondences, and uses cross-attention for fine-grained consistency; 3) Subject and Camera Motion Decoupled CFG guidance strategy that enables independent camera movement control by separating subject and camera motion information in CFG anchors; 4) XPose dataset with 50,000 non-human pose-video pairs and automated annotation pipeline.

Result: Extensive experiments show PoseAnything significantly outperforms state-of-the-art methods in both effectiveness and generalization capabilities, demonstrating superior performance across different types of subjects.

Conclusion: PoseAnything represents the first universal pose-guided video generation framework that successfully handles arbitrary skeletal inputs, enables fine-grained part-level consistency, and provides independent camera motion control, with strong experimental validation and a new high-quality dataset for non-human pose-video generation.

Abstract: Pose-guided video generation refers to controlling the motion of subjects in generated video through a sequence of poses. It enables precise control over subject motion and has important applications in animation. However, current pose-guided video generation methods are limited to accepting only human poses as input, thus generalizing poorly to pose of other subjects. To address this issue, we propose PoseAnything, the first universal pose-guided video generation framework capable of handling both human and non-human characters, supporting arbitrary skeletal inputs. To enhance consistency preservation during motion, we introduce Part-aware Temporal Coherence Module, which divides the subject into different parts, establishes part correspondences, and computes cross-attention between corresponding parts across frames to achieve fine-grained part-level consistency. Additionally, we propose Subject and Camera Motion Decoupled CFG, a novel guidance strategy that, for the first time, enables independent camera movement control in pose-guided video generation, by separately injecting subject and camera motion control information into the positive and negative anchors of CFG. Furthermore, we present XPose, a high-quality public dataset containing 50,000 non-human pose-video pairs, along with an automated pipeline for annotation and filtering. Extensive experiments demonstrate that Pose-Anything significantly outperforms state-of-the-art methods in both effectiveness and generalization.

</details>


### [247] [Transform Trained Transformer: Accelerating Naive 4K Video Generation Over 10$\times$](https://arxiv.org/abs/2512.13492)
*Jiangning Zhang,Junwei Zhu,Teng Hu,Yabiao Wang,Donghao Luo,Weijian Cao,Zhenye Gan,Xiaobin Hu,Zhucun Xue,Chengjie Wang*

Main category: cs.CV

TL;DR: T3-Video is a Transformer retrofit strategy that enables efficient native 4K video generation by optimizing pretrained models' forward logic without architectural changes, achieving 10x speedup while improving quality.


<details>
  <summary>Details</summary>
Motivation: Native 4K video generation faces quadratic computational explosion with full-attention as spatiotemporal resolution increases, making it difficult to balance efficiency and quality.

Method: T3-Video introduces a multi-scale weight-sharing window attention mechanism with hierarchical blocking and axis-preserving full-attention design to transform pretrained models' attention patterns using modest compute and data.

Result: On 4K-VBench, T3-Video outperforms existing approaches with +4.29↑ VQA and +0.08↑ VTC improvements while accelerating native 4K video generation by more than 10×.

Conclusion: The T3 strategy successfully addresses the computational challenges of 4K video generation by retrofitting pretrained models rather than redesigning architectures, achieving both efficiency and quality gains.

Abstract: Native 4K (2160$\times$3840) video generation remains a critical challenge due to the quadratic computational explosion of full-attention as spatiotemporal resolution increases, making it difficult for models to strike a balance between efficiency and quality. This paper proposes a novel Transformer retrofit strategy termed $\textbf{T3}$ ($\textbf{T}$ransform $\textbf{T}$rained $\textbf{T}$ransformer) that, without altering the core architecture of full-attention pretrained models, significantly reduces compute requirements by optimizing their forward logic. Specifically, $\textbf{T3-Video}$ introduces a multi-scale weight-sharing window attention mechanism and, via hierarchical blocking together with an axis-preserving full-attention design, can effect an "attention pattern" transformation of a pretrained model using only modest compute and data. Results on 4K-VBench show that $\textbf{T3-Video}$ substantially outperforms existing approaches: while delivering performance improvements (+4.29$\uparrow$ VQA and +0.08$\uparrow$ VTC), it accelerates native 4K video generation by more than 10$\times$. Project page at https://zhangzjn.github.io/projects/T3-Video

</details>


### [248] [Soul: Breathe Life into Digital Human for High-fidelity Long-term Multimodal Animation](https://arxiv.org/abs/2512.13495)
*Jiangning Zhang,Junwei Zhu,Zhenye Gan,Donghao Luo,Chuming Lin,Feifan Xu,Xu Peng,Jianlong Hu,Yuansen Liu,Yijia Hong,Weijian Cao,Han Feng,Xu Chen,Chencan Fu,Keke He,Xiaobin Hu,Chengjie Wang*

Main category: cs.CV

TL;DR: Soul is a multimodal framework for generating high-fidelity long-term digital human animation from single images, text prompts, and audio, achieving precise lip sync, vivid expressions, and identity preservation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generating semantically coherent, long-term digital human animations with precise lip synchronization and identity preservation from multimodal inputs, while overcoming data scarcity issues in this domain.

Method: Built on Wan2.2-5B backbone with audio-injection layers, multiple training strategies, and threshold-aware codebook replacement for consistency. Uses step/CFG distillation and lightweight VAE for efficiency. Created Soul-1M dataset with 1M annotated samples and Soul-Bench for evaluation.

Result: Significantly outperforms current leading open-source and commercial models on video quality, video-text alignment, identity preservation, and lip-sync accuracy. Achieves 11.4× speedup with negligible quality loss through efficiency optimizations.

Conclusion: Soul demonstrates state-of-the-art performance in digital human animation generation with broad applicability in real-world scenarios like virtual anchors and film production, while addressing data scarcity through large-scale dataset creation.

Abstract: We propose a multimodal-driven framework for high-fidelity long-term digital human animation termed $\textbf{Soul}$, which generates semantically coherent videos from a single-frame portrait image, text prompts, and audio, achieving precise lip synchronization, vivid facial expressions, and robust identity preservation. We construct Soul-1M, containing 1 million finely annotated samples with a precise automated annotation pipeline (covering portrait, upper-body, full-body, and multi-person scenes) to mitigate data scarcity, and we carefully curate Soul-Bench for comprehensive and fair evaluation of audio-/text-guided animation methods. The model is built on the Wan2.2-5B backbone, integrating audio-injection layers and multiple training strategies together with threshold-aware codebook replacement to ensure long-term generation consistency. Meanwhile, step/CFG distillation and a lightweight VAE are used to optimize inference efficiency, achieving an 11.4$\times$ speedup with negligible quality loss. Extensive experiments show that Soul significantly outperforms current leading open-source and commercial models on video quality, video-text alignment, identity preservation, and lip-synchronization accuracy, demonstrating broad applicability in real-world scenarios such as virtual anchors and film production. Project page at https://zhangzjn.github.io/projects/Soul/

</details>


### [249] [Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model](https://arxiv.org/abs/2512.13507)
*Siyan Chen,Yanfei Chen,Ying Chen,Zhuo Chen,Feng Cheng,Xuyan Chi,Jian Cong,Qinpeng Cui,Qide Dong,Junliang Fan,Jing Fang,Zetao Fang,Chengjian Feng,Han Feng,Mingyuan Gao,Yu Gao,Qiushan Guo,Boyang Hao,Qingkai Hao,Bibo He,Qian He,Tuyen Hoang,Ruoqing Hu,Xi Hu,Weilin Huang,Zhaoyang Huang,Zhongyi Huang,Siqi Jiang,Wei Jiang,Yunpu Jiang,Zhuo Jiang,Ashley Kim,Jianan Kong,Zhichao Lai,Shanshan Lao,Ai Li,Feiya Li,Gen Li,Huixia Li,JiaShi Li,Liang Li,Ming Li,Tao Li,Xian Li,Xiaojie Li,Xiaoyang Li,Xingxing Li,Yameng Li,Yifu Li,Yiying Li,Chao Liang,Ying Liang,Zhiqiang Liang,Wang Liao,Yalin Liao,Heng Lin,Kengyu Lin,Shanchuan Lin,Xi Lin,Zhijie Lin,Feng Ling,Fangfang Liu,Gaohong Liu,Jiawei Liu,Jie Liu,Shouda Liu,Shu Liu,Sichao Liu,Songwei Liu,Xin Liu,Xue Liu,Yibo Liu,Zikun Liu,Zuxi Liu,Junlin Lyu,Lecheng Lyu,Qian Lyu,Han Mu,Xiaonan Nie,Jingzhe Ning,Xitong Pan,Yanghua Peng,Lianke Qin,Xueqiong Qu,Yuxi Ren,Yuchen Shen,Guang Shi,Lei Shi,Yan Song,Yinglong Song,Fan Sun,Li Sun,Renfei Sun,Zeyu Sun,Wenjing Tang,Zirui Tao,Feng Wang,Furui Wang,Jinran Wang,Junkai Wang,Ke Wang,Kexin Wang,Qingyi Wang,Rui Wang,Sen Wang,Shuai Wang,Tingru Wang,Weichen Wang,Xin Wang,Yanhui Wang,Yue Wang,Yuping Wang,Yuxuan Wang,Ziyu Wang,Guoqiang Wei,Wanru Wei,Di Wu,Guohong Wu,Hanjie Wu,Jian Wu,Jie Wu,Ruolan Wu,Xinglong Wu,Yonghui Wu,Ruiqi Xia,Liang Xiang,Fei Xiao,XueFeng Xiao,Pan Xie,Shuangyi Xie,Shuang Xu,Jinlan Xue,Bangbang Yang,Ceyuan Yang,Jiaqi Yang,Runkai Yang,Tao Yang,Yang Yang,Yihang Yang,ZhiXian Yang,Ziyan Yang,Yifan Yao,Zilyu Ye,Bowen Yu,Chujie Yuan,Linxiao Yuan,Sichun Zeng,Weihong Zeng,Xuejiao Zeng,Yan Zeng,Chuntao Zhang,Heng Zhang,Jingjie Zhang,Kuo Zhang,Liang Zhang,Liying Zhang,Manlin Zhang,Ting Zhang,Weida Zhang,Xiaohe Zhang,Xinyan Zhang,Yan Zhang,Yuan Zhang,Zixiang Zhang,Fengxuan Zhao,Huating Zhao,Yang Zhao,Hao Zheng,Jianbin Zheng,Xiaozheng Zheng,Yangyang Zheng,Yijie Zheng,Jiexin Zhou,Kuan Zhu,Shenhan Zhu,Wenjia Zhu,Benhui Zou,Feilong Zuo*

Main category: cs.CV

TL;DR: Seedance 1.5 pro is a foundational model for joint audio-video generation using dual-branch Diffusion Transformer with cross-modal integration, achieving superior synchronization and quality through specialized data pipeline and post-training optimizations.


<details>
  <summary>Details</summary>
Motivation: To advance unified audio-visual generation by creating a professional-grade foundational model that can generate synchronized audio and video content with practical utility for content creation.

Method: Dual-branch Diffusion Transformer architecture with cross-modal joint module, multi-stage data pipeline, post-training optimizations including SFT on high-quality datasets and RLHF with multi-dimensional reward models, plus an acceleration framework for 10X faster inference.

Result: Achieves exceptional audio-visual synchronization, superior generation quality, precise multilingual/dialect lip-syncing, dynamic cinematic camera control, enhanced narrative coherence, and over 10X inference speed boost.

Conclusion: Seedance 1.5 pro positions itself as a robust engine for professional-grade content creation with advanced audio-visual generation capabilities, now available on Volcano Engine platform.

Abstract: Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.

</details>


### [250] [TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding](https://arxiv.org/abs/2512.13511)
*Piyush Bagad,Andrew Zisserman*

Main category: cs.CV

TL;DR: TARA adapts Multimodal LLMs to create time-aware video-text embedding models without video data, achieving SOTA on chiral benchmarks and strong performance on standard tasks.


<details>
  <summary>Details</summary>
Motivation: To build a general time-aware video-text embedding model for retrieval that can understand temporal relationships and actions in videos.

Method: TARA (Time Aware Retrieval Adaptation) - a simple and efficient recipe to adapt Multimodal LLMs into time-aware video-text embedding models without using any video data.

Result: Outperforms all existing video-text models on chiral benchmark with temporally opposite actions; achieves strong results on standard benchmarks; shows negation-awareness and SOTA performance on verb/adverb understanding.

Conclusion: TARA yields a strong, versatile, time-aware video-text embedding model with state-of-the-art zero-shot performance, demonstrating benefits beyond time-awareness including negation-awareness and verb/adverb understanding.

Abstract: Our objective is to build a general time-aware video-text embedding model for retrieval. To that end, we propose a simple and efficient recipe, dubbed TARA (Time Aware Retrieval Adaptation), to adapt Multimodal LLMs (MLLMs) to a time-aware video-text embedding model without using any video data at all. For evaluating time-awareness in retrieval, we propose a new benchmark with temporally opposite (chiral) actions as hard negatives and curated splits for chiral and non-chiral actions. We show that TARA outperforms all existing video-text models on this chiral benchmark while also achieving strong results on standard benchmarks. Furthermore, we discover additional benefits of TARA beyond time-awareness: (i) TARA embeddings are negation-aware as shown in NegBench benchmark that evaluates negation in video retrieval, (ii) TARA achieves state of the art performance on verb and adverb understanding in videos. Overall, TARA yields a strong, versatile, time-aware video-text embedding model with state of the art zero-shot performance.

</details>


### [251] [Pancakes: Consistent Multi-Protocol Image Segmentation Across Biomedical Domains](https://arxiv.org/abs/2512.13534)
*Marianne Rakic,Siyu Gai,Etienne Chollet,John V. Guttag,Adrian V. Dalca*

Main category: cs.CV

TL;DR: Pancakes is a framework that automatically generates multiple plausible segmentation maps for biomedical images across different protocols without manual prompting, outperforming existing foundation models.


<details>
  <summary>Details</summary>
Motivation: Biomedical images can be segmented in multiple meaningful ways depending on the application, but existing models either support only single protocols or require labor-intensive manual prompting to specify desired segmentations.

Method: Introduces Pancakes framework with a new problem formulation that automatically generates multi-label segmentation maps for multiple plausible protocols from unseen domains while maintaining semantic consistency across related images.

Result: In experiments on seven held-out datasets, Pancakes significantly outperforms existing foundation models in producing several plausible whole-image segmentations that are semantically coherent across images.

Conclusion: Pancakes addresses a gap in current segmentation capabilities by enabling automatic multi-protocol segmentation with semantic consistency, introducing a novel problem formulation not attainable by existing foundation models.

Abstract: A single biomedical image can be meaningfully segmented in multiple ways, depending on the desired application. For instance, a brain MRI can be segmented according to tissue types, vascular territories, broad anatomical regions, fine-grained anatomy, or pathology, etc. Existing automatic segmentation models typically either (1) support only a single protocol, the one they were trained on, or (2) require labor-intensive manual prompting to specify the desired segmentation. We introduce Pancakes, a framework that, given a new image from a previously unseen domain, automatically generates multi-label segmentation maps for multiple plausible protocols, while maintaining semantic consistency across related images. Pancakes introduces a new problem formulation that is not currently attainable by existing foundation models. In a series of experiments on seven held-out datasets, we demonstrate that our model can significantly outperform existing foundation models in producing several plausible whole-image segmentations, that are semantically coherent across images.

</details>


### [252] [3D Human-Human Interaction Anomaly Detection](https://arxiv.org/abs/2512.13560)
*Shun Maeda,Chunzhi Gu,Koichiro Kamide,Katsuya Hotta,Shangce Gao,Chao Zhang*

Main category: cs.CV

TL;DR: Proposes IADNet for detecting anomalous behaviors in human-human interactions, outperforming single-person AD models by capturing collaborative motion correlations and spatial configurations.


<details>
  <summary>Details</summary>
Motivation: Existing single-person anomaly detection models fail to capture complex human-human interaction dynamics, leading to low accuracy for detecting anomalous collaborative behaviors.

Method: IADNet with Temporal Attention Sharing Module (TASM) to synchronize motion embeddings across both people, and Distance-Based Relational Encoding Module (DREM) to capture spatial social cues, using normalizing flow for anomaly scoring.

Result: Extensive experiments on human-human motion benchmarks show IADNet outperforms existing human-centric AD baselines for the H2IAD task.

Conclusion: The paper successfully addresses the novel H2IAD task by modeling both temporal and spatial aspects of human interactions, demonstrating superior performance over single-person approaches.

Abstract: Human-centric anomaly detection (AD) has been primarily studied to specify anomalous behaviors in a single person. However, as humans by nature tend to act in a collaborative manner, behavioral anomalies can also arise from human-human interactions. Detecting such anomalies using existing single-person AD models is prone to low accuracy, as these approaches are typically not designed to capture the complex and asymmetric dynamics of interactions. In this paper, we introduce a novel task, Human-Human Interaction Anomaly Detection (H2IAD), which aims to identify anomalous interactive behaviors within collaborative 3D human actions. To address H2IAD, we then propose Interaction Anomaly Detection Network (IADNet), which is formalized with a Temporal Attention Sharing Module (TASM). Specifically, in designing TASM, we share the encoded motion embeddings across both people such that collaborative motion correlations can be effectively synchronized. Moreover, we notice that in addition to temporal dynamics, human interactions are also characterized by spatial configurations between two people. We thus introduce a Distance-Based Relational Encoding Module (DREM) to better reflect social cues in H2IAD. The normalizing flow is eventually employed for anomaly scoring. Extensive experiments on human-human motion benchmarks demonstrate that IADNet outperforms existing Human-centric AD baselines in H2IAD.

</details>


### [253] [MMhops-R1: Multimodal Multi-hop Reasoning](https://arxiv.org/abs/2512.13573)
*Tao Zhang,Ziqi Zhang,Zongyang Ma,Yuxin Chen,Bing Li,Chunfeng Yuan,Guangting Wang,Fengyun Rao,Ying Shan,Weiming Hu*

Main category: cs.CV

TL;DR: MMhops is a new benchmark for evaluating multi-modal multi-hop reasoning, with MMhops-R1 as a dynamic mRAG framework using reinforcement learning for autonomous reasoning path planning.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs are limited to single-step reasoning, and current benchmarks lack the complexity to evaluate and drive multi-modal multi-hop reasoning abilities needed for complex real-world challenges.

Method: Introduces MMhops benchmark with Bridging and Comparison tasks requiring dynamic construction of complex reasoning chains. Proposes MMhops-R1, a multi-modal RAG framework using reinforcement learning to optimize autonomous planning of reasoning paths, query formulation, and multi-level information synthesis.

Result: MMhops-R1 significantly outperforms strong baselines on MMhops benchmark, demonstrating that dynamic planning and multi-modal knowledge integration are crucial for complex reasoning. Also shows strong generalization to fixed-hop reasoning tasks.

Conclusion: The work contributes a challenging new benchmark and powerful baseline model for multi-modal multi-hop reasoning, with plans to release code, data, and weights to catalyze future research in this critical area.

Abstract: The ability to perform multi-modal multi-hop reasoning by iteratively integrating information across various modalities and external knowledge is critical for addressing complex real-world challenges. However, existing Multi-modal Large Language Models (MLLMs) are predominantly limited to single-step reasoning, as existing benchmarks lack the complexity needed to evaluate and drive multi-hop abilities. To bridge this gap, we introduce MMhops, a novel, large-scale benchmark designed to systematically evaluate and foster multi-modal multi-hop reasoning. MMhops dataset comprises two challenging task formats, Bridging and Comparison, which necessitate that models dynamically construct complex reasoning chains by integrating external knowledge. To tackle the challenges posed by MMhops, we propose MMhops-R1, a novel multi-modal Retrieval-Augmented Generation (mRAG) framework for dynamic reasoning. Our framework utilizes reinforcement learning to optimize the model for autonomously planning reasoning paths, formulating targeted queries, and synthesizing multi-level information. Comprehensive experiments demonstrate that MMhops-R1 significantly outperforms strong baselines on MMhops, highlighting that dynamic planning and multi-modal knowledge integration are crucial for complex reasoning. Moreover, MMhops-R1 demonstrates strong generalization to tasks requiring fixed-hop reasoning, underscoring the robustness of our dynamic planning approach. In conclusion, our work contributes a challenging new benchmark and a powerful baseline model, and we will release the associated code, data, and weights to catalyze future research in this critical area.

</details>


### [254] [Lighting in Motion: Spatiotemporal HDR Lighting Estimation](https://arxiv.org/abs/2512.13597)
*Christophe Bolduc,Julien Philip,Li Ma,Mingming He,Paul Debevec,Jean-François Lalonde*

Main category: cs.CV

TL;DR: LiMo is a diffusion-based method for spatiotemporal lighting estimation that generates high-frequency details and accurate illuminance using mirrored/diffuse spheres at different exposures, with novel geometric conditioning and differentiable rendering.


<details>
  <summary>Details</summary>
Motivation: Existing lighting estimation methods struggle to simultaneously achieve realistic high-frequency detail prediction and accurate illuminance estimation in spatiotemporal contexts. There's a need for better spatial conditioning beyond just depth information.

Method: Fine-tune existing diffusion models on large-scale indoor/outdoor datasets with spatiotemporal light probes. Generate mirrored and diffuse spheres at different exposures based on 3D positions. Introduce novel geometric conditioning for relative scene-to-target positioning. Combine predictions using differentiable rendering to create single HDRI maps.

Result: LiMo establishes state-of-the-art performance for both spatial control and prediction accuracy in spatiotemporal lighting estimation, outperforming existing methods.

Conclusion: The proposed approach successfully addresses the dual challenges of realistic detail and accurate illuminance in lighting estimation through diffusion priors, novel geometric conditioning, and differentiable rendering integration.

Abstract: We present Lighting in Motion (LiMo), a diffusion-based approach to spatiotemporal lighting estimation. LiMo targets both realistic high-frequency detail prediction and accurate illuminance estimation. To account for both, we propose generating a set of mirrored and diffuse spheres at different exposures, based on their 3D positions in the input. Making use of diffusion priors, we fine-tune powerful existing diffusion models on a large-scale customized dataset of indoor and outdoor scenes, paired with spatiotemporal light probes. For accurate spatial conditioning, we demonstrate that depth alone is insufficient and we introduce a new geometric condition to provide the relative position of the scene to the target 3D position. Finally, we combine diffuse and mirror predictions at different exposures into a single HDRI map leveraging differentiable rendering. We thoroughly evaluate our method and design choices to establish LiMo as state-of-the-art for both spatial control and prediction accuracy.

</details>


### [255] [DA-SSL: self-supervised domain adaptor to leverage foundational models in turbt histopathology slides](https://arxiv.org/abs/2512.13600)
*Haoyue Zhang,Meera Chappidi,Erolcan Sayar,Helen Richards,Zhijun Chen,Lucas Liu,Roxanne Wadia,Peter A Humphrey,Fady Ghali,Alberto Contreras-Sanz,Peter Black,Jonathan Wright,Stephanie Harmon,Michael Haffner*

Main category: cs.CV

TL;DR: A domain-adaptive self-supervised adaptor (DA-SSL) improves pathology foundational models for transurethral resection of bladder tumor (TURBT) specimens by addressing domain shifts from electrocautery artifacts and fragmented tissues, achieving strong performance in predicting neoadjuvant chemotherapy response.


<details>
  <summary>Details</summary>
Motivation: Pathology foundational models (PFMs) have limitations on certain cancer types like bladder cancer due to domain shifts - TURBT specimens contain fragmented tissue chips and electrocautery artifacts not seen in pretraining data, and these specimens were rarely used for PFM pretraining.

Method: Proposed a domain-adaptive self-supervised adaptor (DA-SSL) that realigns pretrained PFM features to the TURBT domain without fine-tuning the foundational model itself, integrated with multiple instance learning (MIL) for predicting treatment response.

Result: DA-SSL achieved AUC of 0.77±0.04 in five-fold cross-validation, and external test accuracy of 0.84, sensitivity of 0.71, specificity of 0.91 using majority voting for predicting neoadjuvant chemotherapy response in TURBT specimens.

Conclusion: Lightweight domain adaptation with self-supervision can effectively enhance PFM-based MIL pipelines for clinically challenging histopathology tasks, particularly for domain-shifted specimens like TURBT with electrocautery artifacts.

Abstract: Recent deep learning frameworks in histopathology, particularly multiple instance learning (MIL) combined with pathology foundational models (PFMs), have shown strong performance. However, PFMs exhibit limitations on certain cancer or specimen types due to domain shifts - these cancer types were rarely used for pretraining or specimens contain tissue-based artifacts rarely seen within the pretraining population. Such is the case for transurethral resection of bladder tumor (TURBT), which are essential for diagnosing muscle-invasive bladder cancer (MIBC), but contain fragmented tissue chips and electrocautery artifacts and were not widely used in publicly available PFMs. To address this, we propose a simple yet effective domain-adaptive self-supervised adaptor (DA-SSL) that realigns pretrained PFM features to the TURBT domain without fine-tuning the foundational model itself. We pilot this framework for predicting treatment response in TURBT, where histomorphological features are currently underutilized and identifying patients who will benefit from neoadjuvant chemotherapy (NAC) is challenging. In our multi-center study, DA-SSL achieved an AUC of 0.77+/-0.04 in five-fold cross-validation and an external test accuracy of 0.84, sensitivity of 0.71, and specificity of 0.91 using majority voting. Our results demonstrate that lightweight domain adaptation with self-supervision can effectively enhance PFM-based MIL pipelines for clinically challenging histopathology tasks. Code is Available at https://github.com/zhanghaoyue/DA_SSL_TURBT.

</details>


### [256] [LongVie 2: Multimodal Controllable Ultra-Long Video World Model](https://arxiv.org/abs/2512.13604)
*Jianxiong Gao,Zhaoxi Chen,Xian Liu,Junhao Zhuang,Chengming Xu,Jianfeng Feng,Yu Qiao,Yanwei Fu,Chenyang Si,Ziwei Liu*

Main category: cs.CV

TL;DR: LongVie 2 is an autoregressive video world model framework that achieves state-of-the-art long-term video generation with enhanced controllability, visual quality, and temporal consistency through three-stage training.


<details>
  <summary>Details</summary>
Motivation: Building video world models on pretrained video generation systems is crucial for general spatiotemporal intelligence, requiring three essential properties: controllability, long-term visual quality, and temporal consistency.

Method: Three-stage autoregressive training: (1) Multi-modal guidance with dense/sparse control signals for controllability, (2) Degradation-aware training on input frames for visual quality, (3) History-context guidance across clips for temporal consistency.

Result: Achieves SOTA performance in long-range controllability, temporal coherence, and visual fidelity; supports continuous video generation up to five minutes; introduces LongVGenBench benchmark with 100 one-minute videos.

Conclusion: LongVie 2 represents a significant advancement toward unified video world modeling by addressing key challenges in controllability, quality, and consistency for long-term video generation.

Abstract: Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.

</details>


### [257] [DBT-DINO: Towards Foundation model based analysis of Digital Breast Tomosynthesis](https://arxiv.org/abs/2512.13608)
*Felix J. Dorfner,Manon A. Dorster,Ryan Connolly,Oscar Gentilhomme,Edward Gibbs,Steven Graham,Seth Wander,Thomas Schultz,Manisha Bahl,Dania Daye,Albert E. Kim,Christopher P. Bridge*

Main category: cs.CV

TL;DR: DBT-DINO is the first foundation model for Digital Breast Tomosynthesis, developed using self-supervised pre-training on 25M+ 2D slices. It outperforms baselines on breast density classification and cancer risk prediction, but shows mixed results on lesion detection.


<details>
  <summary>Details</summary>
Motivation: Foundation models have shown promise in medical imaging but remain underexplored for 3D modalities like DBT, which is used for breast cancer screening. No existing foundation model exists for DBT despite its clinical importance.

Method: Self-supervised pre-training using DINOv2 methodology on over 25 million 2D slices from 487,975 DBT volumes from 27,990 patients. Evaluated on three downstream tasks: breast density classification (5,000 exams), 5-year breast cancer risk prediction (106,417 exams), and lesion detection (393 annotated volumes).

Result: DBT-DINO achieved: 1) Breast density classification accuracy of 0.79, outperforming DINOv2 (0.73) and DenseNet-121 (0.74); 2) 5-year risk prediction AUROC of 0.78 vs DINOv2's 0.76; 3) Lesion detection sensitivity of 0.62 vs DINOv2's 0.67, but better performance on cancerous lesions specifically (78.8% vs 77.3%).

Conclusion: DBT-DINO is the first foundation model for DBT and demonstrates strong performance on classification and risk prediction tasks. However, domain-specific pre-training shows variable benefits for detection tasks, indicating that localized detection requires further methodological development.

Abstract: Foundation models have shown promise in medical imaging but remain underexplored for three-dimensional imaging modalities. No foundation model currently exists for Digital Breast Tomosynthesis (DBT), despite its use for breast cancer screening.
  To develop and evaluate a foundation model for DBT (DBT-DINO) across multiple clinical tasks and assess the impact of domain-specific pre-training.
  Self-supervised pre-training was performed using the DINOv2 methodology on over 25 million 2D slices from 487,975 DBT volumes from 27,990 patients. Three downstream tasks were evaluated: (1) breast density classification using 5,000 screening exams; (2) 5-year risk of developing breast cancer using 106,417 screening exams; and (3) lesion detection using 393 annotated volumes.
  For breast density classification, DBT-DINO achieved an accuracy of 0.79 (95\% CI: 0.76--0.81), outperforming both the MetaAI DINOv2 baseline (0.73, 95\% CI: 0.70--0.76, p<.001) and DenseNet-121 (0.74, 95\% CI: 0.71--0.76, p<.001). For 5-year breast cancer risk prediction, DBT-DINO achieved an AUROC of 0.78 (95\% CI: 0.76--0.80) compared to DINOv2's 0.76 (95\% CI: 0.74--0.78, p=.57). For lesion detection, DINOv2 achieved a higher average sensitivity of 0.67 (95\% CI: 0.60--0.74) compared to DBT-DINO with 0.62 (95\% CI: 0.53--0.71, p=.60). DBT-DINO demonstrated better performance on cancerous lesions specifically with a detection rate of 78.8\% compared to Dinov2's 77.3\%.
  Using a dataset of unprecedented size, we developed DBT-DINO, the first foundation model for DBT. DBT-DINO demonstrated strong performance on breast density classification and cancer risk prediction. However, domain-specific pre-training showed variable benefits on the detection task, with ImageNet baseline outperforming DBT-DINO on general lesion detection, indicating that localized detection tasks require further methodological development.

</details>


### [258] [Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models](https://arxiv.org/abs/2512.13609)
*Shweta Mahajan,Shreya Kadambi,Hoang Le,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: Do-Undo benchmark tests vision-language models on physically plausible scene transformations by requiring them to simulate action outcomes and accurately reverse them, revealing current models' struggles with physical reversibility.


<details>
  <summary>Details</summary>
Motivation: Addresses a critical gap in vision-language models: understanding and generating physically plausible scene transformations driven by real-world actions, which is essential for embodied AI, robotics, and physics-aware generative modeling.

Method: Introduces the Do-Undo task and benchmark, curates a large-scale dataset of reversible actions from real-world videos, and designs a training strategy that enforces consistency for robust action grounding.

Result: Experiments reveal that current vision-language models struggle with physical reversibility, highlighting the importance of this task for evaluating physical reasoning capabilities in multimodal systems.

Conclusion: Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems, providing a crucial benchmark for progress in embodied AI, robotics, and physics-aware generative modeling.

Abstract: We introduce the Do-Undo task and benchmark to address a critical gap in vision-language models: understanding and generating physically plausible scene transformations driven by real-world actions. Unlike prior work focused on object-level edits, Do-Undo requires models to simulate the outcome of a physical action and then accurately reverse it, reflecting true cause-and-effect in the visual world. We curate a large-scale dataset of reversible actions from real-world videos and design a training strategy enforcing consistency for robust action grounding. Our experiments reveal that current models struggle with physical reversibility, underscoring the importance of this task for embodied AI, robotics, and physics-aware generative modeling. Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems.

</details>


### [259] [SCR2-ST: Combine Single Cell with Spatial Transcriptomics for Efficient Active Sampling via Reinforcement Learning](https://arxiv.org/abs/2512.13635)
*Junchao Zhu,Ruining Deng,Junlin Guo,Tianyuan Yao,Chongyu Qu,Juming Xiong,Siqi Lu,Zhengyi Lu,Yanfan Zhu,Marilyn Lionts,Yuechen Yang,Yalin Zheng,Yu Wang,Shilin Zhao,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: SCR2-ST is a unified framework that uses single-cell prior knowledge to guide efficient spatial transcriptomics data acquisition and accurate expression prediction through reinforcement learning-based active sampling and hybrid regression-retrieval prediction.


<details>
  <summary>Details</summary>
Motivation: Spatial transcriptomics data is expensive to acquire, and traditional fixed-grid sampling leads to redundant measurements of similar/uninformative regions, resulting in scarce data. Single-cell sequencing data could serve as an effective auxiliary source to mitigate this limitation.

Method: SCR2-ST integrates two components: 1) Single-cell guided reinforcement learning (SCRL) active sampling that combines single-cell foundation model embeddings with spatial density information to construct biologically grounded reward signals for selective acquisition of informative tissue regions under constrained budgets; 2) SCR2Net, a hybrid regression-retrieval prediction network that uses actively sampled data through a combination of regression-based modeling with retrieval-augmented inference, featuring a majority cell-type filtering mechanism to suppress noisy matches and using retrieved expression profiles as soft labels for auxiliary supervision.

Result: Evaluated on three public ST datasets, demonstrating state-of-the-art performance in both sampling efficiency and prediction accuracy, particularly under low-budget scenarios.

Conclusion: SCR2-ST effectively bridges the gap between expensive spatial transcriptomics and rich single-cell data, providing an efficient framework for data acquisition and prediction that maximizes biological information while minimizing sequencing costs.

Abstract: Spatial transcriptomics (ST) is an emerging technology that enables researchers to investigate the molecular relationships underlying tissue morphology. However, acquiring ST data remains prohibitively expensive, and traditional fixed-grid sampling strategies lead to redundant measurements of morphologically similar or biologically uninformative regions, thus resulting in scarce data that constrain current methods. The well-established single-cell sequencing field, however, could provide rich biological data as an effective auxiliary source to mitigate this limitation. To bridge these gaps, we introduce SCR2-ST, a unified framework that leverages single-cell prior knowledge to guide efficient data acquisition and accurate expression prediction. SCR2-ST integrates a single-cell guided reinforcement learning-based (SCRL) active sampling and a hybrid regression-retrieval prediction network SCR2Net. SCRL combines single-cell foundation model embeddings with spatial density information to construct biologically grounded reward signals, enabling selective acquisition of informative tissue regions under constrained sequencing budgets. SCR2Net then leverages the actively sampled data through a hybrid architecture combining regression-based modeling with retrieval-augmented inference, where a majority cell-type filtering mechanism suppresses noisy matches and retrieved expression profiles serve as soft labels for auxiliary supervision. We evaluated SCR2-ST on three public ST datasets, demonstrating SOTA performance in both sampling efficiency and prediction accuracy, particularly under low-budget scenarios. Code is publicly available at: https://github.com/hrlblab/SCR2ST

</details>


### [260] [MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning](https://arxiv.org/abs/2512.13636)
*Haoyu Fu,Diankun Zhang,Zongchuang Zhao,Jianfeng Cui,Hongwei Xie,Bing Wang,Guang Chen,Dingkang Liang,Xiang Bai*

Main category: cs.CV

TL;DR: MindDrive is a Vision-Language-Action framework that uses an LLM with dual LoRA parameters to enable online reinforcement learning for autonomous driving by converting continuous action space exploration into discrete linguistic decision learning.


<details>
  <summary>Details</summary>
Motivation: Current VLA paradigms in autonomous driving rely on Imitation Learning, which suffers from distribution shift and causal confusion. Online RL could address these issues but faces inefficient exploration in continuous action spaces.

Method: Uses an LLM with two LoRA parameter sets: Decision Expert for scenario reasoning and driving decisions, and Action Expert for mapping linguistic decisions to trajectories. Converts continuous action space exploration into trial-and-error learning over discrete linguistic decisions by feeding trajectory rewards back to reasoning space.

Result: Achieves strong closed-loop performance on Bench2Drive benchmark: Driving Score of 78.04 and Success Rate of 55.09%. First work demonstrating online RL effectiveness for VLA models in autonomous driving.

Conclusion: MindDrive successfully enables online reinforcement learning for VLA models in autonomous driving by addressing exploration inefficiency through discrete linguistic decision learning, balancing optimal decision-making, human-like behavior, and efficient exploration.

Abstract: Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. MindDrive achieves strong closed-loop performance on the challenging Bench2Drive benchmark, with a Driving Score (DS) of 78.04 and a Success Rate (SR) of 55.09%. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.

</details>


### [261] [Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All](https://arxiv.org/abs/2512.13639)
*Michal Nazarczuk,Thomas Tanay,Arthur Moreau,Zhensong Zhang,Eduardo Pérez-Pellitero*

Main category: cs.CV

TL;DR: New high-quality animated film dataset for Novel View Synthesis with rich dynamic scenes, multiple modalities, and three benchmarking scenarios for 4D scene reconstruction and view generation.


<details>
  <summary>Details</summary>
Motivation: There's a need for high-quality datasets with realistic dynamic scenes, detailed textures, lighting, and motion to advance 4D scene reconstruction and novel view synthesis research.

Method: Created a dataset from a high-quality animated film, capturing dynamic scenes with multiple complementary modalities (RGB, depth, surface normals, object segmentation, optical flow) and organizing it into three benchmarking scenarios: dense multi-view camera setup, sparse camera arrangement, and monocular video sequences.

Result: A unique dataset resource that combines visual richness, high-quality annotations, and diverse experimental setups, enabling comprehensive training and evaluation of cutting-edge view synthesis and 3D vision models.

Conclusion: This dataset provides an ideal platform for pushing the boundaries of view synthesis and 3D vision research by offering realistic dynamic scenes with comprehensive annotations across varying data sparsity levels.

Abstract: This paper presents a new dataset for Novel View Synthesis, generated from a high-quality, animated film with stunning realism and intricate detail. Our dataset captures a variety of dynamic scenes, complete with detailed textures, lighting, and motion, making it ideal for training and evaluating cutting-edge 4D scene reconstruction and novel view generation models. In addition to high-fidelity RGB images, we provide multiple complementary modalities, including depth, surface normals, object segmentation and optical flow, enabling a deeper understanding of scene geometry and motion. The dataset is organised into three distinct benchmarking scenarios: a dense multi-view camera setup, a sparse camera arrangement, and monocular video sequences, enabling a wide range of experimentation and comparison across varying levels of data sparsity. With its combination of visual richness, high-quality annotations, and diverse experimental setups, this dataset offers a unique resource for pushing the boundaries of view synthesis and 3D vision.

</details>


### [262] [Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency](https://arxiv.org/abs/2512.13665)
*Wenhan Chen,Sezer Karaoglu,Theo Gevers*

Main category: cs.CV

TL;DR: Grab-3D is a geometry-aware transformer framework that detects AI-generated videos by analyzing 3D geometric consistency using vanishing points as explicit representations.


<details>
  <summary>Details</summary>
Motivation: With diffusion-based AI models producing highly realistic videos, there's an urgent need for reliable detection methods. Existing approaches insufficiently explore the 3D geometric patterns in generated videos, creating a gap in detection capabilities.

Method: Uses vanishing points as explicit 3D geometry representations; constructs static scene AI video dataset for stable feature extraction; develops geometry-aware transformer with geometric positional encoding, temporal-geometric attention, and EMA-based geometric classifier head.

Result: Grab-3D significantly outperforms state-of-the-art detectors and demonstrates robust cross-domain generalization to unseen AI video generators.

Conclusion: 3D geometric temporal consistency provides a powerful approach for AI-generated video detection, with vanishing points serving as effective discriminative features that reveal fundamental geometric discrepancies between real and generated content.

Abstract: Recent advances in diffusion-based generation techniques enable AI models to produce highly realistic videos, heightening the need for reliable detection mechanisms. However, existing detection methods provide only limited exploration of the 3D geometric patterns present in generated videos. In this paper, we use vanishing points as an explicit representation of 3D geometry patterns, revealing fundamental discrepancies in geometric consistency between real and AI-generated videos. We introduce Grab-3D, a geometry-aware transformer framework for detecting AI-generated videos based on 3D geometric temporal consistency. To enable reliable evaluation, we construct an AI-generated video dataset of static scenes, allowing stable 3D geometric feature extraction. We propose a geometry-aware transformer equipped with geometric positional encoding, temporal-geometric attention, and an EMA-based geometric classifier head to explicitly inject 3D geometric awareness into temporal modeling. Experiments demonstrate that Grab-3D significantly outperforms state-of-the-art detectors, achieving robust cross-domain generalization to unseen generators.

</details>


### [263] [AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection](https://arxiv.org/abs/2512.13671)
*Junwen Miao,Penghui Du,Yi Liu,Yu Wang,Yan Wang*

Main category: cs.CV

TL;DR: AgentIAD is a tool-driven agentic framework for industrial anomaly detection that uses multi-stage visual inspection with specialized tools for localized analysis and normal pattern comparison, achieving state-of-the-art 97.62% accuracy on MMAD.


<details>
  <summary>Details</summary>
Motivation: Industrial anomaly detection faces challenges due to scarce normal reference samples and subtle, localized defects. Single-pass vision-language models often miss small abnormalities and lack explicit mechanisms to compare against normal patterns.

Method: AgentIAD uses a tool-driven agentic framework with two specialized tools: Perceptive Zoomer for localized fine-grained analysis and Comparative Retriever for querying normal exemplars. The model is trained in two stages using structured perceptive and comparative trajectories from MMAD dataset: supervised fine-tuning followed by reinforcement learning with a two-part reward design.

Result: Achieves new state-of-the-art 97.62% classification accuracy on MMAD dataset, surpassing prior MLLM-based approaches while producing transparent and interpretable inspection traces.

Conclusion: AgentIAD demonstrates that multi-stage visual inspection with specialized tools enables more effective industrial anomaly detection by allowing step-wise observation, zooming, and verification against normal patterns, leading to both high accuracy and interpretable inspection processes.

Abstract: Industrial anomaly detection (IAD) is difficult due to the scarcity of normal reference samples and the subtle, localized nature of many defects. Single-pass vision-language models (VLMs) often overlook small abnormalities and lack explicit mechanisms to compare against canonical normal patterns. We propose AgentIAD, a tool-driven agentic framework that enables multi-stage visual inspection. The agent is equipped with a Perceptive Zoomer (PZ) for localized fine-grained analysis and a Comparative Retriever (CR) for querying normal exemplars when evidence is ambiguous. To teach these inspection behaviors, we construct structured perceptive and comparative trajectories from the MMAD dataset and train the model in two stages: supervised fine-tuning followed by reinforcement learning. A two-part reward design drives this process: a perception reward that supervises classification accuracy, spatial alignment, and type correctness, and a behavior reward that encourages efficient tool use. Together, these components enable the model to refine its judgment through step-wise observation, zooming, and verification. AgentIAD achieves a new state-of-the-art 97.62% classification accuracy on MMAD, surpassing prior MLLM-based approaches while producing transparent and interpretable inspection traces.

</details>


### [264] [JoVA: Unified Multimodal Learning for Joint Video-Audio Generation](https://arxiv.org/abs/2512.13677)
*Xiaohu Huang,Hao Zhou,Qiangpeng Yang,Shilei Wen,Kai Han*

Main category: cs.CV

TL;DR: JoVA is a unified transformer framework for joint video-audio generation that produces synchronized lip movements and speech without additional alignment modules, using joint self-attention and mouth-area loss.


<details>
  <summary>Details</summary>
Motivation: Existing methods have two key limitations: 1) they can only generate ambient sounds but not synchronized human speech with lip movements, and 2) unified approaches rely on complex fusion modules that compromise transformer simplicity.

Method: JoVA uses joint self-attention across video and audio tokens within each transformer layer for direct cross-modal interaction. It also introduces a mouth-area loss based on facial keypoint detection to enhance lip-speech synchronization without adding architectural complexity.

Result: Extensive experiments show JoVA outperforms or is competitive with state-of-the-art methods in lip-sync accuracy, speech quality, and overall video-audio generation fidelity.

Conclusion: JoVA establishes an elegant framework for high-quality multimodal generation that maintains architectural simplicity while achieving superior synchronization and quality.

Abstract: In this paper, we present JoVA, a unified framework for joint video-audio generation. Despite recent encouraging advances, existing methods face two critical limitations. First, most existing approaches can only generate ambient sounds and lack the capability to produce human speech synchronized with lip movements. Second, recent attempts at unified human video-audio generation typically rely on explicit fusion or modality-specific alignment modules, which introduce additional architecture design and weaken the model simplicity of the original transformers. To address these issues, JoVA employs joint self-attention across video and audio tokens within each transformer layer, enabling direct and efficient cross-modal interaction without the need for additional alignment modules. Furthermore, to enable high-quality lip-speech synchronization, we introduce a simple yet effective mouth-area loss based on facial keypoint detection, which enhances supervision on the critical mouth region during training without compromising architectural simplicity. Extensive experiments on benchmarks demonstrate that JoVA outperforms or is competitive with both unified and audio-driven state-of-the-art methods in lip-sync accuracy, speech quality, and overall video-audio generation fidelity. Our results establish JoVA as an elegant framework for high-quality multimodal generation.

</details>


### [265] [Feedforward 3D Editing via Text-Steerable Image-to-3D](https://arxiv.org/abs/2512.13678)
*Ziqi Ma,Hongqiao Chen,Yisong Yue,Georgia Gkioxari*

Main category: cs.CV

TL;DR: Steer3D enables text-based editing of AI-generated 3D assets by adding language steerability to image-to-3D models, making them 2.4x-28.5x faster than competing methods while maintaining fidelity.


<details>
  <summary>Details</summary>
Motivation: While image-to-3D generation has advanced, real-world applications require easy editing of generated 3D assets. Current methods lack efficient text-based steerability for modifying 3D content.

Method: Adapts ControlNet to image-to-3D generation for text steering in a forward pass. Uses scalable automatic data generation and two-stage training with flow-matching and Direct Preference Optimization (DPO).

Result: Steer3D follows language instructions more faithfully, maintains better consistency with original 3D assets, and is 2.4x to 28.5x faster than competing methods.

Conclusion: It's possible to add text steerability to pretrained image-to-3D models with only 100k data, enabling efficient language-based editing of 3D assets for practical applications.

Abstract: Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/

</details>


### [266] [LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction](https://arxiv.org/abs/2512.13680)
*Tianye Ding,Yiming Xie,Yiqing Liang,Moitreya Chatterjee,Pedro Miraldo,Huaizu Jiang*

Main category: cs.CV

TL;DR: LASER converts offline 3D reconstruction models into streaming systems without retraining by aligning predictions across temporal windows using layer-wise scale alignment, achieving SOTA performance with low memory usage.


<details>
  <summary>Details</summary>
Motivation: Existing feed-forward reconstruction models have quadratic memory complexity that prevents streaming video processing, while current streaming methods require extensive retraining and don't fully leverage strong geometric priors from offline models.

Method: Training-free framework that converts offline models to streaming by aligning predictions across consecutive windows. Uses layer-wise scale alignment that segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across windows and timestamps to address monocular scale ambiguity issues.

Result: Achieves state-of-the-art performance on camera pose estimation and point map reconstruction while operating at 14 FPS with 6 GB peak memory on RTX A6000 GPU, enabling kilometer-scale streaming video deployment.

Conclusion: LASER enables practical deployment of offline reconstruction models for streaming videos without retraining, overcoming memory limitations while maintaining reconstruction quality through novel layer-wise scale alignment.

Abstract: Recent feed-forward reconstruction models like VGGT and $π^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation ($\mathrm{Sim}(3)$) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: $\href{https://neu-vi.github.io/LASER/}{\texttt{https://neu-vi.github.io/LASER/}}$

</details>


### [267] [I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners](https://arxiv.org/abs/2512.13683)
*Lu Ling,Yunhao Ge,Yichen Sheng,Aniket Bera*

Main category: cs.CV

TL;DR: Reprogramming a pre-trained 3D instance generator as a scene-level learner enables generalization to unseen layouts and novel object compositions through model-centric spatial supervision rather than dataset-bounded supervision.


<details>
  <summary>Details</summary>
Motivation: Generalization is the central challenge for interactive 3D scene generation. Existing learning-based approaches are limited by grounding spatial understanding in limited scene datasets, which restricts their ability to generalize to new layouts and compositions.

Method: Reprogram a pre-trained 3D instance generator to act as a scene-level learner, replacing dataset-bounded supervision with model-centric spatial supervision. Use a view-centric formulation of scene space instead of canonical space, creating a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model.

Result: The approach unlocks the generator's transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Spatial reasoning emerges even when training scenes are randomly composed objects, demonstrating that the generator's transferable scene prior provides rich learning signals for inferring proximity, support, and symmetry from geometric cues.

Conclusion: A 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. The method shows that model-centric spatial supervision can overcome dataset limitations for scene generation.

Abstract: Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/

</details>


### [268] [Recurrent Video Masked Autoencoders](https://arxiv.org/abs/2512.13684)
*Daniel Zoran,Nikhil Parthasarathy,Yi Yang,Drew A Hudson,Joao Carreira,Andrew Zisserman*

Main category: cs.CV

TL;DR: RVM is a recurrent video masked-autoencoder that uses transformer-based recurrent networks for efficient spatio-temporal representation learning, achieving strong performance on both video and image tasks with high parameter efficiency.


<details>
  <summary>Details</summary>
Motivation: To develop a more efficient video representation learning approach that can capture spatio-temporal structure while being parameter-efficient and capable of handling long temporal sequences without the computational limitations of standard spatio-temporal attention architectures.

Method: Uses transformer-based recurrent neural network to aggregate dense image features over time with asymmetric masked prediction task and standard pixel reconstruction objective. Recurrent design enables linear computational cost for long sequences.

Result: Achieves competitive performance with SOTA video models (VideoMAE, V-JEPA) on video tasks and favorable against image models (DINOv2) on geometric/spatial tasks. Shows 30x greater parameter efficiency than competing video masked autoencoders, with stable feature propagation over long temporal horizons.

Conclusion: RVM provides an efficient "generalist" encoder that learns rich representations of scene semantics, structure, and motion through recurrent architecture, overcoming computational limitations of attention-based approaches while maintaining strong performance across diverse tasks.

Abstract: We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.

</details>


### [269] [Towards Scalable Pre-training of Visual Tokenizers for Generation](https://arxiv.org/abs/2512.13687)
*Jingfeng Yao,Yuda Song,Yucong Zhou,Xinggang Wang*

Main category: cs.CV

TL;DR: VTP is a visual tokenizer pre-training framework that solves the "pre-training scaling problem" by jointly optimizing multiple losses to create a latent space focused on high-level semantics rather than just pixel-level reconstruction, enabling better scaling and faster convergence for generative models.


<details>
  <summary>Details</summary>
Motivation: Standard visual tokenizers (like VAEs) trained with reconstruction loss produce latent spaces biased toward low-level information, creating a "pre-training scaling problem" where better pixel-level accuracy doesn't improve generation quality. The authors argue that for effective generation, latent spaces must represent high-level semantics.

Method: VTP (Visual Tokenizer Pre-training) framework jointly optimizes three losses: image-text contrastive, self-supervised, and reconstruction losses. This unified approach creates a latent space that captures high-level semantics rather than just low-level pixel information.

Result: VTP achieves competitive performance (78.2% zero-shot accuracy and 0.36 rFID on ImageNet) with 4.1× faster convergence on generation compared to advanced distillation methods. Most importantly, it scales effectively: investing more FLOPS in VTP pre-training yields 65.8% FID improvement in downstream generation, while conventional autoencoders stagnate early.

Conclusion: The paper demonstrates that understanding (high-level semantics) is key for generation, and VTP provides much better scaling properties where generative performance effectively scales with compute, parameters, and data allocated to visual tokenizer pre-training.

Abstract: The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.

</details>


### [270] [LitePT: Lighter Yet Stronger Point Transformer](https://arxiv.org/abs/2512.13689)
*Yuanwen Yue,Damien Robert,Jianyuan Wang,Sunghwan Hong,Jan Dirk Wegner,Christian Rupprecht,Konrad Schindler*

Main category: cs.CV

TL;DR: LitePT is a 3D point cloud backbone that uses convolutions in early layers for geometry extraction and attention in deeper layers for semantics, achieving better efficiency than Point Transformer V3 while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Current 3D point cloud networks combine convolutions and attention, but optimal architecture design remains unclear. The paper aims to analyze when each component is most effective and create an efficient backbone.

Method: Analyzed computational blocks in 3D networks, found convolutions work best for low-level geometry in early high-resolution layers, while attention excels for high-level semantics in deep low-resolution layers. Proposed LitePT with convolutions early and attention later, plus PointROPE positional encoding to preserve spatial layout.

Result: LitePT has 3.6× fewer parameters, runs 2× faster, uses 2× less memory than Point Transformer V3, while matching or outperforming it on various tasks and datasets.

Conclusion: The hybrid convolution-attention approach with proper stage allocation (convolutions early, attention late) creates an efficient yet powerful 3D point cloud backbone, validated by LitePT's superior efficiency-performance trade-off.

Abstract: Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has $3.6\times$ fewer parameters, runs $2\times$ faster, and uses $2\times$ less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.

</details>


### [271] [DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders](https://arxiv.org/abs/2512.13690)
*Susung Hong,Chongjian Ge,Zhifei Zhang,Jui-Hsien Wang*

Main category: cs.CV

TL;DR: DiffusionBrowser: A lightweight decoder framework for interactive previews and control during video diffusion model generation, enabling real-time previews and new steering capabilities.


<details>
  <summary>Details</summary>
Motivation: Video diffusion models are imprecise, slow, and opaque during generation, keeping users waiting in the dark without insight into the generation process.

Method: A model-agnostic, lightweight decoder framework that generates multi-modal previews (RGB and scene intrinsics) at any point during denoising, enabling interactive guidance via stochasticity reinjection and modal steering.

Result: Generates previews at >4× real-time speed (<1 second for 4-second video) with consistent appearance/motion to final output, and enables new interactive control capabilities while revealing model internals.

Conclusion: DiffusionBrowser transforms video diffusion from a black-box process into an interactive, transparent system with real-time previews and steering capabilities, opening new possibilities for controlled video generation.

Abstract: Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [272] [Continuous Edit Distance, Geodesics and Barycenters of Time-varying Persistence Diagrams](https://arxiv.org/abs/2512.12939)
*Sebastien Tchitchek,Mohamed Kissi,Julien Tierny*

Main category: cs.CG

TL;DR: The paper introduces Continuous Edit Distance (CED), a geodesic and elastic distance metric for time-varying persistence diagrams that combines local substitution costs with penalized deletions/insertions, with practical barycenter solvers and applications in temporal pattern analysis.


<details>
  <summary>Details</summary>
Motivation: There is a need for principled distance metrics for time-varying persistence diagrams (TVPDs) that can handle temporal misalignment, support interpretable geodesics, and enable practical operations like alignment, comparison, averaging, and clustering directly in the TVPD space.

Method: CED extends edit-distance concepts to TVPDs using two parameters: α (trade-off between temporal misalignment and diagram discrepancy) and β (gap penalty). The method includes explicit construction of CED-geodesics and two barycenter solvers (one stochastic, one greedy) that monotonically decrease the CED Fréchet energy.

Result: CED demonstrates robustness to additive perturbations (temporal and spatial), recovers temporal shifts, supports temporal pattern search, achieves comparable or better clustering performance than standard elastic dissimilarities on real datasets, and yields superior classification results with CED-barycenter clustering.

Conclusion: CED provides TVPD analysis with a principled distance metric, interpretable geodesics, and practical barycenters, enabling comprehensive analysis directly in the space of time-varying persistence diagrams, with a C++ implementation provided for reproducibility.

Abstract: We introduce the Continuous Edit Distance (CED), a geodesic and elastic distance for time-varying persistence diagrams (TVPDs). The CED extends edit-distance ideas to TVPDs by combining local substitution costs with penalized deletions/insertions, controlled by two parameters: \(α\) (trade-off between temporal misalignment and diagram discrepancy) and \(β\) (gap penalty). We also provide an explicit construction of CED-geodesics. Building on these ingredients, we present two practical barycenter solvers, one stochastic and one greedy, that monotonically decrease the CED Frechet energy. Empirically, the CED is robust to additive perturbations (both temporal and spatial), recovers temporal shifts, and supports temporal pattern search. On real-life datasets, the CED achieves clustering performance comparable or better than standard elastic dissimilarities, while our clustering based on CED-barycenters yields superior classification results. Overall, the CED equips TVPD analysis with a principled distance, interpretable geodesics, and practical barycenters, enabling alignment, comparison, averaging, and clustering directly in the space of TVPDs. A C++ implementation is provided for reproducibility at the following address https://github.com/sebastien-tchitchek/ContinuousEditDistance.

</details>


### [273] [VoroLight: Learning Quality Volumetric Voronoi Meshes from General Inputs](https://arxiv.org/abs/2512.12984)
*Jiayin Lu,Ying Jiang,Yin Yang,Chenfanfu Jiang*

Main category: cs.CG

TL;DR: VoroLight is a differentiable framework for 3D shape reconstruction using Voronoi meshing that produces smooth, watertight surfaces and topologically consistent volumetric meshes from various inputs.


<details>
  <summary>Details</summary>
Motivation: The paper aims to create a versatile 3D reconstruction framework that can handle diverse input types (images, implicit fields, point clouds, meshes) while producing high-quality, watertight surfaces and consistent volumetric meshes.

Method: Three-stage approach: 1) Differentiable Voronoi formulation for surface initialization, 2) Polygon-face sphere training for surface refinement, 3) Reuse of differentiable Voronoi for volumetric optimization with interior generator points.

Result: The framework generates smooth, watertight surfaces and topologically consistent volumetric meshes directly from various input types, demonstrating versatility in 3D shape reconstruction.

Conclusion: VoroLight provides an effective differentiable framework for 3D reconstruction that leverages Voronoi meshing to produce high-quality surfaces and volumetric meshes from diverse inputs.

Abstract: We present VoroLight, a differentiable framework for 3D shape reconstruction based on Voronoi meshing. Our approach generates smooth, watertight surfaces and topologically consistent volumetric meshes directly from diverse inputs, including images, implicit shape level-set fields, point clouds and meshes. VoroLight operates in three stages: it first initializes a surface using a differentiable Voronoi formulation, then refines surface quality through a polygon-face sphere training stage, and finally reuses the differentiable Voronoi formulation for volumetric optimization with additional interior generator points. Project page: https://jiayinlu19960224.github.io/vorolight/

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [274] [Quantum Implicit Neural Representations for 3D Scene Reconstruction and Novel View Synthesis](https://arxiv.org/abs/2512.12683)
*Yeray Cordero,Paula García-Molina,Fernando Vilariño*

Main category: quant-ph

TL;DR: Q-NeRF is the first hybrid quantum-classical framework for neural radiance field rendering that integrates quantum circuits to overcome spectral bias limitations in traditional neural representations.


<details>
  <summary>Details</summary>
Motivation: Classical implicit neural representations suffer from spectral bias that limits their ability to capture high-frequency details in 3D scene reconstruction. Quantum circuits with inherent Fourier structures offer potential to overcome this limitation.

Method: Integrates Quantum Implicit Representation Networks (QIREN) modules into the Nerfacto backbone, replacing selected density and radiance prediction components with quantum-enhanced counterparts while preserving efficient sampling, pose refinement, and volumetric rendering strategies.

Result: Hybrid quantum-classical models achieve competitive reconstruction quality on standard multi-view indoor datasets using PSNR, SSIM, and LPIPS metrics. Quantum modules are particularly effective in representing fine-scale, view-dependent appearance under limited computational resources.

Conclusion: Q-NeRF demonstrates the potential of quantum encodings to alleviate spectral bias in implicit representations and provides a foundational step toward scalable quantum-enabled 3D scene reconstruction, serving as a baseline for future quantum neural rendering research.

Abstract: Implicit neural representations (INRs) have become a powerful paradigm for continuous signal modeling and 3D scene reconstruction, yet classical networks suffer from a well-known spectral bias that limits their ability to capture high-frequency details. Quantum Implicit Representation Networks (QIREN) mitigate this limitation by employing parameterized quantum circuits with inherent Fourier structures, enabling compact and expressive frequency modeling beyond classical MLPs. In this paper, we present Quantum Neural Radiance Fields (Q-NeRF), the first hybrid quantum-classical framework for neural radiance field rendering. Q-NeRF integrates QIREN modules into the Nerfacto backbone, preserving its efficient sampling, pose refinement, and volumetric rendering strategies while replacing selected density and radiance prediction components with quantum-enhanced counterparts. We systematically evaluate three hybrid configurations on standard multi-view indoor datasets, comparing them to classical baselines using PSNR, SSIM, and LPIPS metrics. Results show that hybrid quantum-classical models achieve competitive reconstruction quality under limited computational resources, with quantum modules particularly effective in representing fine-scale, view-dependent appearance. Although current implementations rely on quantum circuit simulators constrained to few-qubit regimes, the results highlight the potential of quantum encodings to alleviate spectral bias in implicit representations. Q-NeRF provides a foundational step toward scalable quantum-enabled 3D scene reconstruction and a baseline for future quantum neural rendering research.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [275] [Pre-training vision models for the classification of alerts from wide-field time-domain surveys](https://arxiv.org/abs/2512.11957)
*Nabeel Rehemtulla,Adam A. Miller,Mike Walmsley,Ved G. Shah,Theophile Jegou du Laz,Michael W. Coughlin,Argyro Sasli,Joshua Bloom,Christoffer Fremling,Matthew J. Graham,Steven L. Groom,David Hale,Ashish A. Mahabal,Daniel A. Perley,Josiah Purdum,Ben Rusholme,Jesper Sollerman,Mansi M. Kasliwal*

Main category: astro-ph.IM

TL;DR: Pre-trained vision models outperform custom CNNs for astronomical alert classification, with Galaxy Zoo pre-training yielding best results.


<details>
  <summary>Details</summary>
Motivation: Time-domain astronomy still uses custom CNN architectures trained from scratch, while computer vision has shifted to standardized pre-trained models. The authors want to explore if adopting modern computer vision practices (pre-training, standardized architectures) can improve alert classification performance and efficiency.

Method: Compare different pre-training regimens (ImageNet, Galaxy Zoo images, from scratch) and standardized model architectures against custom CNN baselines for astronomical alert classification tasks.

Result: Pre-trained models match or outperform custom specialized CNNs. Galaxy Zoo pre-training yields better performance than ImageNet or training from scratch. Standardized architectures are more optimized, requiring less time/memory for inference despite more parameters.

Conclusion: Advocates for paradigm shift in vision models for alerts: adopt latest computer vision practices (pre-training, standardized architectures) for better performance and efficiency, especially important for upcoming large surveys like LSST.

Abstract: Modern wide-field time-domain surveys facilitate the study of transient, variable and moving phenomena by conducting image differencing and relaying alerts to their communities. Machine learning tools have been used on data from these surveys and their precursors for more than a decade, and convolutional neural networks (CNNs), which make predictions directly from input images, saw particularly broad adoption through the 2010s. Since then, continually rapid advances in computer vision have transformed the standard practices around using such models. It is now commonplace to use standardized architectures pre-trained on large corpora of everyday images (e.g., ImageNet). In contrast, time-domain astronomy studies still typically design custom CNN architectures and train them from scratch. Here, we explore the affects of adopting various pre-training regimens and standardized model architectures on the performance of alert classification. We find that the resulting models match or outperform a custom, specialized CNN like what is typically used for filtering alerts. Moreover, our results show that pre-training on galaxy images from Galaxy Zoo tends to yield better performance than pre-training on ImageNet or training from scratch. We observe that the design of standardized architectures are much better optimized than the custom CNN baseline, requiring significantly less time and memory for inference despite having more trainable parameters. On the eve of the Legacy Survey of Space and Time and other image-differencing surveys, these findings advocate for a paradigm shift in the creation of vision models for alerts, demonstrating that greater performance and efficiency, in time and in data, can be achieved by adopting the latest practices from the computer vision field.

</details>


### [276] [Semantic search for 100M+ galaxy images using AI-generated captions](https://arxiv.org/abs/2512.11982)
*Nolan Koblischke,Liam Parker,Francois Lanusse,Irina Espejo Morales,Jo Bovy,Shirley Ho*

Main category: astro-ph.IM

TL;DR: AION-Search enables semantic search across 140M unlabeled galaxy images using Vision-Language Models to generate descriptions and contrastive alignment, outperforming direct image similarity and achieving state-of-the-art zero-shot rare phenomenon discovery.


<details>
  <summary>Details</summary>
Motivation: Manual labeling of billions of galaxy images from telescopes severely limits scientific exploration; there's a need for scalable semantic search in large unlabeled scientific image archives.

Method: Uses Vision-Language Models (VLMs) to generate descriptions for unlabeled galaxy images, then contrastively aligns a pre-trained multimodal astronomy foundation model with these embedded descriptions to produce searchable embeddings at scale. Also introduces VLM-based re-ranking.

Result: AION-Search achieves state-of-the-art zero-shot performance on finding rare phenomena, outperforms direct image similarity search, and VLM-based re-ranking nearly doubles recall for challenging targets in top-100 results. Scales to 140 million galaxy images.

Conclusion: Enables flexible semantic search at unprecedented scale for scientific image archives, expanding data exploration capabilities across astronomy and other fields like Earth observation and microscopy.

Abstract: Finding scientifically interesting phenomena through slow, manual labeling campaigns severely limits our ability to explore the billions of galaxy images produced by telescopes. In this work, we develop a pipeline to create a semantic search engine from completely unlabeled image data. Our method leverages Vision-Language Models (VLMs) to generate descriptions for galaxy images, then contrastively aligns a pre-trained multimodal astronomy foundation model with these embedded descriptions to produce searchable embeddings at scale. We find that current VLMs provide descriptions that are sufficiently informative to train a semantic search model that outperforms direct image similarity search. Our model, AION-Search, achieves state-of-the-art zero-shot performance on finding rare phenomena despite training on randomly selected images with no deliberate curation for rare cases. Furthermore, we introduce a VLM-based re-ranking method that nearly doubles the recall for our most challenging targets in the top-100 results. For the first time, AION-Search enables flexible semantic search scalable to 140 million galaxy images, enabling discovery from previously infeasible searches. More broadly, our work provides an approach for making large, unlabeled scientific image archives semantically searchable, expanding data exploration capabilities in fields from Earth observation to microscopy. The code, data, and app are publicly available at https://github.com/NolanKoblischke/AION-Search

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [277] [SIGMA: An AI-Empowered Training Stack on Early-Life Hardware](https://arxiv.org/abs/2512.13488)
*Lei Qu,Lianhai Ren,Peng Cheng,Rui Gao,Ruizhe Wang,Tianyu Chen,Xiao Liu,Xingjian Zhang,Yeyun Gong,Yifan Xiong,Yucheng Ding,Yuting Jiang,Zhenghao Lin,Zhongxin Guo,Ziyue Yang*

Main category: cs.DC

TL;DR: SIGMA is an open-source training stack that improves reliability, stability, and efficiency for large-scale distributed training on early-life AI accelerators, achieving 94.45% cluster utilization and successfully training a 200B MoE model with state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Early-life AI accelerators face three core challenges: frequent system disruptions and undefined failure modes that undermine reliability; numerical errors and training instabilities that threaten correctness and convergence; and the complexity of parallelism optimization combined with unpredictable local noise that degrades efficiency.

Method: SIGMA consists of two main components: LUCIA TRAINING PLATFORM (LTP) - a system optimized for clusters with early-life AI accelerators, and LUCIA TRAINING FRAMEWORK (LTF) - which builds on LTP to enable large-scale model training. The approach addresses reliability, stability, and efficiency challenges through specialized system design.

Result: LTP achieved 94.45% effective cluster accelerator utilization, reduced node recycling and job-recovery times. LTF successfully trained SIGMA-MOE, a 200B MoE model using 2,048 AI accelerators, achieving 21.08% MFU, state-of-the-art downstream accuracy, and only one stability incident over 75 days.

Conclusion: SIGMA establishes a new benchmark for AI infrastructure and platform innovation, offering a robust, cost-effective alternative to prevailing established accelerator stacks and significantly advancing AI capabilities and scalability for large-scale training on early-life AI hardware.

Abstract: An increasing variety of AI accelerators is being considered for large-scale training. However, enabling large-scale training on early-life AI accelerators faces three core challenges: frequent system disruptions and undefined failure modes that undermine reliability; numerical errors and training instabilities that threaten correctness and convergence; and the complexity of parallelism optimization combined with unpredictable local noise that degrades efficiency. To address these challenges, SIGMA is an open-source training stack designed to improve the reliability, stability, and efficiency of large-scale distributed training on early-life AI hardware. The core of this initiative is the LUCIA TRAINING PLATFORM (LTP), the system optimized for clusters with early-life AI accelerators. Since its launch in March 2025, LTP has significantly enhanced training reliability and operational productivity. Over the past five months, it has achieved an impressive 94.45% effective cluster accelerator utilization, while also substantially reducing node recycling and job-recovery times. Building on the foundation of LTP, the LUCIA TRAINING FRAMEWORK (LTF) successfully trained SIGMA-MOE, a 200B MoE model, using 2,048 AI accelerators. This effort delivered remarkable stability and efficiency outcomes, achieving 21.08% MFU, state-of-the-art downstream accuracy, and encountering only one stability incident over a 75-day period. Together, these advances establish SIGMA, which not only tackles the critical challenges of large-scale training but also establishes a new benchmark for AI infrastructure and platform innovation, offering a robust, cost-effective alternative to prevailing established accelerator stacks and significantly advancing AI capabilities and scalability. The source code of SIGMA is available at https://github.com/microsoft/LuciaTrainingPlatform.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [278] [Intelligent Scientific Literature Explorer using Machine Learning (ISLE)](https://arxiv.org/abs/2512.12760)
*Sina Jani,Arman Heidari,Amirmohammad Anvari,Zahra Rahimi*

Main category: cs.IR

TL;DR: Integrated system for scientific literature exploration combining large-scale data acquisition, hybrid retrieval, semantic topic modeling, and knowledge graph construction to help researchers discover and contextualize relevant literature.


<details>
  <summary>Details</summary>
Motivation: The rapid acceleration of scientific publishing creates challenges for researchers to discover, contextualize, and interpret relevant literature. Traditional keyword-based search has limited semantic understanding, and existing AI tools focus on isolated tasks rather than integrated solutions.

Method: 1) Builds comprehensive corpus by merging arXiv full-text with OpenAlex metadata; 2) Hybrid retrieval fusing BM25 lexical search with embedding-based semantic search using Reciprocal Rank Fusion; 3) Topic modeling using BERTopic or NMF; 4) Constructs heterogeneous knowledge graph unifying papers, authors, institutions, countries, and topics.

Result: Evaluation across multiple queries demonstrates improvements in retrieval relevance, topic coherence, and interpretability. The system provides multi-layered exploration revealing relevant publications and the conceptual/relational landscape around queries.

Conclusion: The proposed framework provides an extensible foundation for AI-assisted scientific discovery, offering integrated exploration beyond isolated retrieval tasks.

Abstract: The rapid acceleration of scientific publishing has created substantial challenges for researchers attempting to discover, contextualize, and interpret relevant literature. Traditional keyword-based search systems provide limited semantic understanding, while existing AI-driven tools typically focus on isolated tasks such as retrieval, clustering, or bibliometric visualization. This paper presents an integrated system for scientific literature exploration that combines large-scale data acquisition, hybrid retrieval, semantic topic modeling, and heterogeneous knowledge graph construction. The system builds a comprehensive corpus by merging full-text data from arXiv with structured metadata from OpenAlex. A hybrid retrieval architecture fuses BM25 lexical search with embedding-based semantic search using Reciprocal Rank Fusion. Topic modeling is performed on retrieved results using BERTopic or non-negative matrix factorization depending on computational resources. A knowledge graph unifies papers, authors, institutions, countries, and extracted topics into an interpretable structure. The system provides a multi-layered exploration environment that reveals not only relevant publications but also the conceptual and relational landscape surrounding a query. Evaluation across multiple queries demonstrates improvements in retrieval relevance, topic coherence, and interpretability. The proposed framework contributes an extensible foundation for AI-assisted scientific discovery.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [279] [Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring](https://arxiv.org/abs/2512.12069)
*Peichun Hua,Hao Li,Shanghao Shi,Zhiyuan Yu,Ning Zhang*

Main category: cs.CR

TL;DR: RCS framework uses LVLM's internal representations to detect jailbreak attacks via contrastive scoring, achieving SOTA generalization to unseen attacks with lightweight methods MCD and KCD.


<details>
  <summary>Details</summary>
Motivation: Current LVLM jailbreak defenses lack generalization to novel attacks or have high computational overhead, while lightweight anomaly detection methods often confuse novel benign inputs with malicious ones, leading to unreliable over-rejection.

Method: Representational Contrastive Scoring (RCS) inspects internal geometry of LVLM representations, learning lightweight projection to maximally separate benign and malicious inputs in safety-critical layers, enabling contrastive scoring that differentiates malicious intent from novelty.

Result: Instantiations MCD (Mahalanobis Contrastive Detection) and KCD (K-nearest Contrastive Detection) achieve state-of-the-art performance on challenging evaluation protocol testing generalization to unseen attack types.

Conclusion: Effective jailbreak detection can be achieved by applying simple, interpretable statistical methods to appropriate internal representations, offering practical path towards safer LVLM deployment.

Abstract: Large Vision-Language Models (LVLMs) are vulnerable to a growing array of multimodal jailbreak attacks, necessitating defenses that are both generalizable to novel threats and efficient for practical deployment. Many current strategies fall short, either targeting specific attack patterns, which limits generalization, or imposing high computational overhead. While lightweight anomaly-detection methods offer a promising direction, we find that their common one-class design tends to confuse novel benign inputs with malicious ones, leading to unreliable over-rejection. To address this, we propose Representational Contrastive Scoring (RCS), a framework built on a key insight: the most potent safety signals reside within the LVLM's own internal representations. Our approach inspects the internal geometry of these representations, learning a lightweight projection to maximally separate benign and malicious inputs in safety-critical layers. This enables a simple yet powerful contrastive score that differentiates true malicious intent from mere novelty. Our instantiations, MCD (Mahalanobis Contrastive Detection) and KCD (K-nearest Contrastive Detection), achieve state-of-the-art performance on a challenging evaluation protocol designed to test generalization to unseen attack types. This work demonstrates that effective jailbreak detection can be achieved by applying simple, interpretable statistical methods to the appropriate internal representations, offering a practical path towards safer LVLM deployment. Our code is available on Github https://github.com/sarendis56/Jailbreak_Detection_RCS.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [280] [Love First, Know Later: Persona-Based Romantic Compatibility Through LLM Text World Engines](https://arxiv.org/abs/2512.11844)
*Haoyang Shang,Zhengyang Yan,Xuan Liu*

Main category: cs.HC

TL;DR: A new computational matching paradigm that simulates interactions first using LLMs as agents and environment, then assesses compatibility through reward modeling, with theoretical guarantees and empirical validation on dating and divorce data.


<details>
  <summary>Details</summary>
Motivation: Current matching systems rely on static profile comparisons, which fail to capture the dynamic nature of human relationships. The paper aims to shift from static compatibility assessment to interactive simulation that better models how relationships actually develop through interactions and responses to critical moments.

Method: Uses LLMs in dual capacity: as persona-driven agents following behavioral policies, and as the environment modeling interaction dynamics. Formalizes compatibility assessment as a reward-modeling problem - learning to extract signals from simulations that predict human preferences. Translates relationship psychology insights about critical moments into mathematical hypotheses for effective simulation.

Result: Theoretically proves that as LLM policies better approximate human behavior, the induced matching converges to optimal stable matching. Empirically validates on speed dating data for initial chemistry and divorce prediction for long-term stability. Demonstrates the framework enables interactive, personalized matching systems.

Conclusion: The "Love First, Know Later" paradigm represents a fundamental shift in computational matching, enabling transparent and interactive compatibility assessment where users can iteratively refine their agents, unlocking new possibilities for relationship prediction and matching systems.

Abstract: We propose Love First, Know Later: a paradigm shift in computational matching that simulates interactions first, then assesses compatibility. Instead of comparing static profiles, our framework leverages LLMs as text world engines that operate in dual capacity-as persona-driven agents following behavioral policies and as the environment modeling interaction dynamics. We formalize compatibility assessment as a reward-modeling problem: given observed matching outcomes, we learn to extract signals from simulations that predict human preferences. Our key insight is that relationships hinge on responses to critical moments-we translate this observation from relationship psychology into mathematical hypotheses, enabling effective simulation. Theoretically, we prove that as LLM policies better approximate human behavior, the induced matching converges to optimal stable matching. Empirically, we validate on speed dating data for initial chemistry and divorce prediction for long-term stability. This paradigm enables interactive, personalized matching systems where users iteratively refine their agents, unlocking future possibilities for transparent and interactive compatibility assessment.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [281] [ERA-IT: Aligning Semantic Models with Revealed Economic Preference for Real-Time and Explainable Patent Valuation](https://arxiv.org/abs/2512.12869)
*Yoo Yongmin,Kim Seungwoo,Liu Jingjiang*

Main category: cs.CE

TL;DR: ERA-IT framework uses patent renewal history as economic preference signal to align LLMs with market realities, outperforming traditional methods in patent valuation while providing transparent rationales.


<details>
  <summary>Details</summary>
Motivation: Traditional bibliometric indicators (like citation counts) fail to address information asymmetry in high-dimensional technical specifications due to systemic latency in data accumulation, creating challenges for valuing intangible assets under uncertainty in technological innovation management.

Method: Proposes Economic Reasoning Alignment via Instruction Tuning (ERA-IT) framework that conceptualizes patent renewal history as revealed economic preference, using it as supervisory signal to align LLM generative reasoning with market realities (Eco-Semantic Alignment). Trained on 10,000 European Patent Office patents across diverse domains to predict value tiers and reverse-engineer Economic Chain-of-Thought from unstructured text.

Result: ERA-IT significantly outperforms both conventional econometric models and zero-shot LLMs in predictive accuracy. Generates explicit, logically grounded rationales for valuation, serving as transparent cognitive scaffold for decision-makers.

Conclusion: The framework reduces opacity of black-box AI in high-stakes intellectual property management by providing transparent reasoning for patent valuation, bridging the gap between technical specifications and economic realities.

Abstract: Valuing intangible assets under uncertainty remains a critical challenge in the strategic management of technological innovation due to the information asymmetry inherent in high-dimensional technical specifications. Traditional bibliometric indicators, such as citation counts, fail to address this friction in a timely manner due to the systemic latency inherent in data accumulation. To bridge this gap, this study proposes the Economic Reasoning Alignment via Instruction Tuning (ERA-IT) framework. We theoretically conceptualize patent renewal history as a revealed economic preference and leverage it as an objective supervisory signal to align the generative reasoning of Large Language Models (LLMs) with market realities, a process we term Eco-Semantic Alignment. Using a randomly sampled dataset of 10,000 European Patent Office patents across diverse technological domains, we trained the model not only to predict value tiers but also to reverse-engineer the Economic Chain-of-Thought from unstructured text. Empirical results demonstrate that ERA-IT significantly outperforms both conventional econometric models and zero-shot LLMs in predictive accuracy. More importantly, by generating explicit, logically grounded rationales for valuation, the framework serves as a transparent cognitive scaffold for decision-makers, reducing the opacity of black-box AI in high-stakes intellectual property management.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [282] [Meta-GPT: Decoding the Metasurface Genome with Generative Artificial Intelligence](https://arxiv.org/abs/2512.12888)
*David Dang,Stuart Love,Meena Salib,Quynh Dang,Samuel Rothfarb,Mysk Alnatour,Andrew Salij,Hou-Tong Chen,Ho Wai,Lee,Wilton J. M. Kort-Kamp*

Main category: physics.optics

TL;DR: Meta-GPT: A foundation transformer model for photonics that uses METASTRINGS symbolic language to design nanostructures with high accuracy and syntactic validity.


<details>
  <summary>Details</summary>
Motivation: Advancing AI for physical sciences requires representations that are both interpretable and compatible with natural laws. Current approaches lack symbolic representations that connect human interpretability with computational design for photonic metasurfaces.

Method: Introduces METASTRINGS, a symbolic language for photonics that encodes materials, geometries, and lattice configurations as textual sequences. Develops Meta-GPT, a foundation transformer model trained on METASTRINGS and finetuned with physics-informed supervised, reinforcement, and chain-of-thought learning.

Result: Achieves <3% mean-squared spectral error and >98% syntactic validity across various design tasks. Generates diverse metasurface prototypes whose experimentally measured optical responses match target spectra.

Conclusion: Meta-GPT can learn compositional rules of light-matter interactions through METASTRINGS, laying a rigorous foundation for AI-driven photonics and representing an important step toward a metasurface genome project.

Abstract: Advancing artificial intelligence for physical sciences requires representations that are both interpretable and compatible with the underlying laws of nature. We introduce METASTRINGS, a symbolic language for photonics that expresses nanostructures as textual sequences encoding materials, geometries, and lattice configurations. Analogous to molecular textual representations in chemistry, METASTRINGS provides a framework connecting human interpretability with computational design by capturing the structural hierarchy of photonic metasurfaces. Building on this representation, we develop Meta-GPT, a foundation transformer model trained on METASTRINGS and finetuned with physics-informed supervised, reinforcement, and chain-of-thought learning. Across various design tasks, the model achieves <3% mean-squared spectral error and maintains >98% syntactic validity, generating diverse metasurface prototypes whose experimentally measured optical responses match their target spectra. These results demonstrate that Meta-GPT can learn the compositional rules of light-matter interactions through METASTRINGS, laying a rigorous foundation for AI-driven photonics and representing an important step toward a metasurface genome project.

</details>


### [283] [JPEG-Inspired Cloud-Edge Holography](https://arxiv.org/abs/2512.12367)
*Shuyang Xie,Jie Zhou,Jun Wang,Renjing Xu*

Main category: physics.optics

TL;DR: JPEG-inspired neural compression pipeline for hologram streaming that moves heavy processing to cloud, uses lightweight edge decoding without neural networks, achieving high quality with low latency and bandwidth.


<details>
  <summary>Details</summary>
Motivation: Current neural CGH pipelines face deployment challenges on compact AR/VR devices due to computational/energy constraints. Cloud offloading with traditional codecs distorts phase info and requires high bandwidth, while neural compression methods impose heavy decoder requirements at the edge.

Method: Developed JPEG-inspired learnable transform codec with block-structured design similar to JPEG. All heavy neural processing is done in cloud, edge performs only lightweight decoding without neural inference. Implemented custom CUDA kernels for entropy coding to improve throughput.

Result: Achieved PSNR of 32.15 dB at <2 bits per pixel with decode latency as low as 4.2 ms. Both numerical simulations and optical experiments confirmed high reconstruction quality. Enables low-latency, bandwidth-efficient hologram streaming on resource-constrained devices.

Conclusion: The JPEG-inspired framework successfully enables practical hologram streaming on wearable devices by preserving JPEG's structural efficiency while adding learnable components, requiring only simple block-based decoding without neural decoders or specialized hardware.

Abstract: Computer-generated holography (CGH) presents a transformative solution for near-eye displays in augmented and virtual reality. Recent advances in deep learning have greatly improved CGH in reconstructed quality and computational efficiency. However, deploying neural CGH pipelines directly on compact, eyeglass-style devices is hindered by stringent constraints on computation and energy consumption, while cloud offloading followed by transmission with natural image codecs often distorts phase information and requires high bandwidth to maintain reconstruction quality. Neural compression methods can reduce bandwidth but impose heavy neural decoders at the edge, increasing inference latency and hardware demand. In this work, we introduce JPEG-Inspired Cloud-Edge Holography, an efficient pipeline designed around a learnable transform codec that retains the block-structured and hardware-friendly nature of JPEG. Our system shifts all heavy neural processing to the cloud, while the edge device performs only lightweight decoding without any neural inference. To further improve throughput, we implement custom CUDA kernels for entropy coding on both cloud and edge. This design achieves a peak signal-to-noise ratio of 32.15 dB at $<$ 2 bits per pixel with decode latency as low as 4.2 ms. Both numerical simulations and optical experiments confirm the high reconstruction quality of the holograms. By aligning CGH with a codec that preserves JPEG's structural efficiency while extending it with learnable components, our framework enables low-latency, bandwidth-efficient hologram streaming on resource-constrained wearable devices-using only simple block-based decoding readily supported by modern system-on-chips, without requiring neural decoders or specialized hardware.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [284] [Vision Foundry: A System for Training Foundational Vision AI Models](https://arxiv.org/abs/2512.11837)
*Mahmut S. Gokmen,Mitchell A. Klusty,Evan W. Damron,W. Vaiden Logan,Aaron D. Mullen,Caroline N. Leach,Emily B. Collier,Samuel E. Armstrong,V. K. Cody Bumgardner*

Main category: q-bio.QM

TL;DR: Vision Foundry is a code-free, HIPAA-compliant platform that democratizes self-supervised learning for medical imaging, enabling clinical researchers to pre-train, adapt, and deploy foundational vision models without technical barriers.


<details>
  <summary>Details</summary>
Motivation: Self-supervised learning has great potential for medical imaging but faces steep technical barriers that limit adoption by clinical researchers who lack engineering expertise.

Method: The platform integrates DINO-MX framework with specialized strategies: Magnification-Aware Distillation (MAD) for handling varying image resolutions, and Parameter-Efficient Fine-Tuning (PEFT) for efficient adaptation. It abstracts distributed infrastructure complexities into a code-free interface.

Result: Models trained via Vision Foundry significantly outperform generic baselines in segmentation fidelity and regression accuracy across neuropathology, lung cellularity, and coronary calcium scoring domains. The platform also demonstrates robust zero-shot generalization across different imaging protocols.

Conclusion: Vision Foundry bridges the gap between advanced representation learning and practical clinical application, enabling domain experts to develop state-of-the-art AI tools with minimal annotation overhead and shifting focus from engineering optimization to clinical discovery.

Abstract: Self-supervised learning (SSL) leverages vast unannotated medical datasets, yet steep technical barriers limit adoption by clinical researchers. We introduce Vision Foundry, a code-free, HIPAA-compliant platform that democratizes pre-training, adaptation, and deployment of foundational vision models. The system integrates the DINO-MX framework, abstracting distributed infrastructure complexities while implementing specialized strategies like Magnification-Aware Distillation (MAD) and Parameter-Efficient Fine-Tuning (PEFT). We validate the platform across domains, including neuropathology segmentation, lung cellularity estimation, and coronary calcium scoring. Our experiments demonstrate that models trained via Vision Foundry significantly outperform generic baselines in segmentation fidelity and regression accuracy, while exhibiting robust zero-shot generalization across imaging protocols. By bridging the gap between advanced representation learning and practical application, Vision Foundry enables domain experts to develop state-of-the-art clinical AI tools with minimal annotation overhead, shifting focus from engineering optimization to clinical discovery.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [285] [Resolution-Independent Neural Operators for Multi-Rate Sparse-View CT](https://arxiv.org/abs/2512.12236)
*Aujasvit Datta,Jiayun Wang,Asad Aali,Armeet Singh Jatyani,Anima Anandkumar*

Main category: eess.IV

TL;DR: CTO is a unified CT reconstruction framework that generalizes across sampling rates and image resolutions without retraining, using rotation-equivariant Discrete-Continuous convolutions in function space.


<details>
  <summary>Details</summary>
Motivation: Current deep learning methods for sparse-view CT reconstruction overfit to fixed acquisition setups and fail to generalize across different sampling rates and image resolutions, limiting practical deployment.

Method: CTO operates in both sinogram and image domains using rotation-equivariant Discrete-Continuous convolutions parametrized in continuous function space, making it inherently resolution- and sampling-agnostic.

Result: CTO achieves >4dB PSNR gain over CNNs on average, is 500× faster than state-of-the-art diffusion methods with 3dB average gain, and demonstrates consistent multi-sampling-rate and cross-resolution performance.

Conclusion: CTO offers a scalable and generalizable solution for automated CT reconstruction that outperforms state-of-the-art baselines across sampling rates and resolutions, making deployment more practical.

Abstract: Sparse-view Computed Tomography (CT) reconstructs images from a limited number of X-ray projections to reduce radiation and scanning time, which makes reconstruction an ill-posed inverse problem. Deep learning methods achieve high-fidelity reconstructions but often overfit to a fixed acquisition setup, failing to generalize across sampling rates and image resolutions. For example, convolutional neural networks (CNNs) use the same learned kernels across resolutions, leading to artifacts when data resolution changes.
  We propose Computed Tomography neural Operator (CTO), a unified CT reconstruction framework that extends to continuous function space, enabling generalization (without retraining) across sampling rates and image resolutions. CTO operates jointly in the sinogram and image domains through rotation-equivariant Discrete-Continuous convolutions parametrized in the function space, making it inherently resolution- and sampling-agnostic. Empirically, CTO enables consistent multi-sampling-rate and cross-resolution performance, with on average >4dB PSNR gain over CNNs. Compared to state-of-the-art diffusion methods, CTO is 500$\times$ faster in inference time with on average 3dB gain. Empirical results also validate our design choices behind CTO's sinogram-space operator learning and rotation-equivariant convolution. Overall, CTO outperforms state-of-the-art baselines across sampling rates and resolutions, offering a scalable and generalizable solution that makes automated CT reconstruction more practical for deployment.

</details>


### [286] [V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval](https://arxiv.org/abs/2512.12284)
*Donghyuk Kim,Sejeong Yang,Wonjin Shin,Joo-Young Kim*

Main category: eess.IV

TL;DR: V-Rex is a software-hardware co-designed accelerator for streaming video LLMs that addresses KV cache memory bottlenecks through a novel training-free algorithm (ReSV) and specialized hardware, enabling real-time inference on edge devices with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Streaming video LLMs face fundamental memory and computational challenges due to growing KV caches with continuous video input, requiring iterative prefill stages that cause extensive computation, data transfer, and accuracy degradation - especially problematic for edge deployment.

Method: V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm that exploits temporal and spatial similarity-based token clustering to reduce KV cache memory across video frames, combined with a hardware accelerator featuring a dynamic KV cache retrieval engine (DRE) with bit-level and early-exit computing units.

Result: Achieves 3.9-8.3 FPS real-time streaming video LLM inference on edge deployment with negligible accuracy loss, delivering 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU while DRE consumes only 2.2% power and 2.0% area.

Conclusion: V-Rex is the first comprehensive solution tackling KV cache retrieval across both algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices through software-hardware co-design.

Abstract: Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.
  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.

</details>


### [287] [Leveraging Compression to Construct Transferable Bitrate Ladders](https://arxiv.org/abs/2512.12952)
*Krishna Srikar Durbha,Hassene Tmar,Ping-Hao Wu,Ioannis Katsavounidis,Alan C. Bovik*

Main category: eess.IV

TL;DR: ML-based bitrate ladder construction technique that predicts VMAF scores by analyzing compression procedures and making perceptually relevant measurements on source videos, outperforming prior methods and fixed bitrate ladders.


<details>
  <summary>Details</summary>
Motivation: Per-title and per-shot video encoding techniques show significant gains over conventional methods, but constructing convex hulls for every video is computationally expensive. ML-based approaches offer a promising alternative by extracting features from source videos to build content-adaptive bitrate ladders.

Method: Proposes a new ML-based bitrate ladder construction technique that predicts VMAF scores by analyzing the compression procedure and making perceptually relevant measurements on source videos before compression. Also investigates how per-shot bitrate ladders perform under different encoding settings.

Result: Evaluated performance against leading prior methods on a large video corpus. Compared models against fixed bitrate ladder and best possible convex hull constructed using exhaustive encoding with Bjontegaard-delta metrics.

Conclusion: ML-based bitrate ladder construction provides an efficient alternative to computationally expensive convex hull construction, offering accurate VMAF prediction and improved performance over conventional methods while reducing computational overhead.

Abstract: Over the past few years, per-title and per-shot video encoding techniques have demonstrated significant gains as compared to conventional techniques such as constant CRF encoding and the fixed bitrate ladder. These techniques have demonstrated that constructing content-gnostic per-shot bitrate ladders can provide significant bitrate gains and improved Quality of Experience (QoE) for viewers under various network conditions. However, constructing a convex hull for every video incurs a significant computational overhead. Recently, machine learning-based bitrate ladder construction techniques have emerged as a substitute for convex hull construction. These methods operate by extracting features from source videos to train machine learning (ML) models to construct content-adaptive bitrate ladders. Here, we present a new ML-based bitrate ladder construction technique that accurately predicts the VMAF scores of compressed videos, by analyzing the compression procedure and by making perceptually relevant measurements on the source videos prior to compression. We evaluate the performance of our proposed framework against leading prior methods on a large corpus of videos. Since training ML models on every encoder setting is time-consuming, we also investigate how per-shot bitrate ladders perform under different encoding settings. We evaluate the performance of all models against the fixed bitrate ladder and the best possible convex hull constructed using exhaustive encoding with Bjontegaard-delta metrics.

</details>


### [288] [Self-Supervised Ultrasound Representation Learning for Renal Anomaly Prediction in Prenatal Imaging](https://arxiv.org/abs/2512.13434)
*Youssef Megahed,Inok Lee,Robin Ducharme,Kevin Dick,Adrian D. C. Chan,Steven Hawken,Mark C. Walker*

Main category: eess.IV

TL;DR: A self-supervised ultrasound foundation model (USF-MAE) outperforms traditional CNN baseline for automated classification of fetal renal anomalies, showing significant improvements in multi-class classification with interpretable attention to clinically relevant structures.


<details>
  <summary>Details</summary>
Motivation: Prenatal ultrasound diagnosis of congenital kidney anomalies is limited by operator dependence and suboptimal imaging conditions, creating a need for automated, reliable classification methods to improve detection accuracy.

Method: Used a pretrained Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE) fine-tuned on 969 2D ultrasound images for binary and multi-class classification of normal kidneys, urinary tract dilation, and multicystic dysplastic kidney. Compared with DenseNet-169 baseline using cross-validation and independent test set, with Score-CAM visualizations for interpretability.

Result: USF-MAE consistently outperformed baseline across all metrics: 1.87-2.32% AUC improvement and 4.33-7.8% F1-score improvement in binary classification; much larger gains in multi-class setting (16.28% AUC and 46.15% F1-score improvement). Visualizations showed model focused on clinically relevant renal structures.

Conclusion: Ultrasound-specific self-supervised learning generates useful representations for downstream diagnostic tasks, offering a robust, interpretable approach for prenatal renal anomaly detection and demonstrating the promise of foundation models in obstetric imaging.

Abstract: Prenatal ultrasound is the cornerstone for detecting congenital anomalies of the kidneys and urinary tract, but diagnosis is limited by operator dependence and suboptimal imaging conditions. We sought to assess the performance of a self-supervised ultrasound foundation model for automated fetal renal anomaly classification using a curated dataset of 969 two-dimensional ultrasound images. A pretrained Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE) was fine-tuned for binary and multi-class classification of normal kidneys, urinary tract dilation, and multicystic dysplastic kidney. Models were compared with a DenseNet-169 convolutional baseline using cross-validation and an independent test set. USF-MAE consistently improved upon the baseline across all evaluation metrics in both binary and multi-class settings. USF-MAE achieved an improvement of about 1.87% (AUC) and 7.8% (F1-score) on the validation set, 2.32% (AUC) and 4.33% (F1-score) on the independent holdout test set. The largest gains were observed in the multi-class setting, where the improvement in AUC was 16.28% and 46.15% in F1-score. To facilitate model interpretability, Score-CAM visualizations were adapted for a transformer architecture and show that model predictions were informed by known, clinically relevant renal structures, including the renal pelvis in urinary tract dilation and cystic regions in multicystic dysplastic kidney. These results show that ultrasound-specific self-supervised learning can generate a useful representation as a foundation for downstream diagnostic tasks. The proposed framework offers a robust, interpretable approach to support the prenatal detection of renal anomalies and demonstrates the promise of foundation models in obstetric imaging.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [289] [Training Versatile Coding Agents in Synthetic Environments](https://arxiv.org/abs/2512.12216)
*Yiqi Zhu,Apurva Gandhi,Graham Neubig*

Main category: cs.SE

TL;DR: SWE-Playground is a novel pipeline that synthetically generates software engineering projects and tasks using language models, enabling training of versatile coding agents without relying on existing GitHub repositories.


<details>
  <summary>Details</summary>
Motivation: Previous approaches for training software engineering agents rely on existing GitHub repositories and focus mainly on issue resolution tasks, which limits flexibility and applicability to the broader range of tasks software engineers actually handle.

Method: SWE-Playground uses strong language models and agents to synthetically generate projects and tasks from scratch, creating environments and trajectories for training coding agents without external data sources.

Result: The approach demonstrates effectiveness on three distinct benchmarks, producing trajectories with dense training signal that enables agents to reach comparable performance with significantly fewer trajectories than previous works.

Conclusion: SWE-Playground overcomes limitations of prior methods by enabling synthetic generation of diverse software engineering tasks, supporting training of more versatile coding agents for a wider variety of real-world software engineering challenges.

Abstract: Prior works on training software engineering agents have explored utilizing existing resources such as issues on GitHub repositories to construct software engineering tasks and corresponding test suites. These approaches face two key limitations: (1) their reliance on pre-existing GitHub repositories offers limited flexibility, and (2) their primary focus on issue resolution tasks restricts their applicability to the much wider variety of tasks a software engineer must handle. To overcome these challenges, we introduce SWE-Playground, a novel pipeline for generating environments and trajectories which supports the training of versatile coding agents. Unlike prior efforts, SWE-Playground synthetically generates projects and tasks from scratch with strong language models and agents, eliminating reliance on external data sources. This allows us to tackle a much wider variety of coding tasks, such as reproducing issues by generating unit tests and implementing libraries from scratch. We demonstrate the effectiveness of this approach on three distinct benchmarks, and results indicate that SWE-Playground produces trajectories with dense training signal, enabling agents to reach comparable performance with significantly fewer trajectories than previous works.

</details>


### [290] [Fine-tuned LLM-based Code Migration Framework](https://arxiv.org/abs/2512.13515)
*Oleg Grynets,Vasyl Lyashkevych,Dmytro Baran,Maksym Orliansky,Taras Zelenyy,Markiian Leshchyshyn*

Main category: cs.SE

TL;DR: A framework for automated SQL codebase migration using fine-tuned LLMs to convert Oracle PL/SQL to PostgreSQL, reducing syntax errors and improving feature alignment through iterative cycles.


<details>
  <summary>Details</summary>
Motivation: Address challenges in transitioning SQL-based systems, particularly migrating from Oracle PL/SQL to PostgreSQL, which involves complex syntax mapping, resolving discrepancies, and optimizing database elements like stored procedures and triggers.

Method: Integration of fine-tuned Large Language Models with traditional software engineering techniques in an iterative framework. Combines fine-tuning and prompt engineering, automated SQL feature detection, semi-supervised error analysis, and Subject Matter Expert feedback within systematic migration workflow.

Result: Significant reductions in Syntax Error Rates, enhanced feature alignment throughout migration iterations, and improved workflow efficiency through precise feature mapping and semi-automated error resolution.

Conclusion: Fine-tuned LLMs play a crucial role in automated SQL migration, providing scalable, precise solutions for database transformations when integrated with systematic workflows and expert feedback loops.

Abstract: The study presents the outcomes of research and experimental validation in the domain of automated codebase migration, with a focus on addressing challenges in transitioning SQL-based systems. The proposed method for migration essentially appears as a framework that leverages the best aspects of traditional software engineering techniques and provides an iterative, scalable, precise and efficient solution for modern database transformations. The central piece of the approach is the integration of a fine-tuned Large Language Model to address critical issues in SQL code conversion, such as syntax mapping, resolving discrepancies between Oracle PL/SQL and PostgreSQL, and optimising database elements such as stored procedures, triggers, views, and overall database logic. Thus, the method involves a trade-off between fine-tuning and prompt engineering. Special attention is given to a fine-tuning approach, which enhances the adaptability and compatibility with migration requirements across the entire database. According to the achieved results, fine-tuning plays a very important role. The study employs targeted evaluation methodologies along with computational metrics to measure the success of iterative conversion cycles. Core innovations include automated SQL feature detection, semi-supervised error analysis and integration of Subject Matter Experts feedback within a systematic migration workflow. The methodology achieves significant reductions in Syntax Error Rates, enhances feature alignment throughout migration iterations, and leverages dataset sampling to ensure continual improvement. By embedding GAI into the migration process, the framework facilitates precise feature mapping, semi-automated error resolution, and data-driven optimisation loops, improving workflow efficiency.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [291] [The Ontological Dissonance Hypothesis: AI-Triggered Delusional Ideation as Folie a Deux Technologique](https://arxiv.org/abs/2512.11818)
*Izabela Lipinska,Hugh Brosnahan*

Main category: cs.CY

TL;DR: LLMs can induce psychotic-like states by creating relational dynamics similar to folie à deux, where users project consciousness onto systems that lack it, exacerbated by engagement-optimized design.


<details>
  <summary>Details</summary>
Motivation: To analyze how contemporary LLMs can contribute to psychotic involvement by creating interactions that resemble folie à deux dynamics, particularly in emotionally vulnerable users.

Method: Drawing on Bateson's double bind theory, clinical literature on shared psychotic disorder, and McGilchrist's hemisphere theory to analyze the structural tension between linguistic coherence and absence of underlying subjectivity in LLMs.

Result: LLMs create a structural tension where language suggests an interlocutor while intuition registers a void, leading users to resolve this conflict through imaginative projection of interiority onto systems that lack it.

Conclusion: Proposes 'ontological honesty' as a necessary design principle to mitigate technologically mediated folie à deux, addressing how engagement-optimized design choices exacerbate these risks.

Abstract: This paper argues that contemporary large language models (LLMs) can contribute to psychotic involvement by creating interactions that resemble the relational dynamics of folie a deux. Drawing on Bateson's double bind theory, clinical literature on shared psychotic disorder, and McGilchrist's hemisphere theory, we show how the combination of high linguistic coherence and the absence of an underlying subject produces a structural tension for the user: language suggests an interlocutor, while intuition registers a void. In contexts of emotional need or instability, this tension can lead users to resolve the conflict through imaginative projection, attributing interiority, intention, or presence to a system that possesses none. The paper situates these dynamics within emerging clinical reports, develops a phenomenological account of how they unfold, and argues that current engagement-optimised design choices exacerbate the risk. We conclude by proposing 'ontological honesty' as a necessary design principle for mitigating technologically mediated folie a deux.

</details>


### [292] [A Reproducible Workflow for Scraping, Structuring, and Segmenting Legacy Archaeological Artifact Images](https://arxiv.org/abs/2512.11817)
*Juan Palomeque-Gonzalez*

Main category: cs.CY

TL;DR: A reproducible workflow for converting legacy archaeological image collections into structured, segmentation-ready datasets using web scraping and image processing tools.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of converting legacy archaeological image collections (specifically the Lower Palaeolithic hand axe and biface collection from ADS) that provide standardized photographs but lack bulk download or automated processing capabilities into machine learning-friendly formats.

Method: Developed two open-source tools: 1) a web scraping script that retrieves record pages, extracts metadata, and downloads images while respecting ethical guidelines; 2) an image processing pipeline that renames files with UUIDs, generates binary masks and bounding boxes using classical computer vision, and stores data in COCO-compatible JSON files enriched with archaeological metadata.

Result: Created a lightweight, reusable workflow that transforms web-based archaeological image collections into structured datasets without redistributing original images, only sharing derived products like masks, outlines, and annotations.

Conclusion: The approach facilitates downstream analysis and contributes to more reproducible research practices in digital archaeology by providing a framework for converting legacy collections into machine learning-ready formats.

Abstract: This technical note presents a reproducible workflow for converting a legacy archaeological image collection into a structured and segmentation ready dataset. The case study focuses on the Lower Palaeolithic hand axe and biface collection curated by the Archaeology Data Service (ADS), a dataset that provides thousands of standardised photographs but no mechanism for bulk download or automated processing. To address this, two open source tools were developed: a web scraping script that retrieves all record pages, extracts associated metadata, and downloads the available images while respecting ADS Terms of Use and ethical scraping guidelines; and an image processing pipeline that renames files using UUIDs, generates binary masks and bounding boxes through classical computer vision, and stores all derived information in a COCO compatible Json file enriched with archaeological metadata. The original images are not redistributed, and only derived products such as masks, outlines, and annotations are shared. Together, these components provide a lightweight and reusable approach for transforming web based archaeological image collections into machine learning friendly formats, facilitating downstream analysis and contributing to more reproducible research practices in digital archaeology.

</details>


### [293] [Assessing Greenspace Attractiveness with ChatGPT, Claude, and Gemini: Do AI Models Reflect Human Perceptions?](https://arxiv.org/abs/2512.11827)
*Milad Malekzadeh,Magdalena Biernacka,Elias Willberg,Jussi Torkko,Edyta Łaszkiewicz,Tuuli Toivonen*

Main category: cs.CY

TL;DR: MLLMs can assess greenspace attractiveness from Street View images with high agreement with humans for formal attractive and informal unattractive spaces, but struggle with informal attractive and formal unattractive spaces, showing bias toward aesthetic features over safety and local context.


<details>
  <summary>Details</summary>
Motivation: Existing greenspace assessment methods overlook informal/transient spaces and are too resource-intensive for large-scale subjective perception analysis. The study aims to test if multimodal LLMs can provide scalable greenspace attractiveness evaluation comparable to human judgments.

Method: Used three MLLMs (ChatGPT GPT-4o, Claude 3.5 Haiku, Gemini 2.0 Flash) to assess greenspace attractiveness from Google Street View imagery. Compared model outputs with geo-questionnaire responses from Lodz, Poland residents across formal (parks, managed greenspaces) and informal (meadows, wastelands) spaces. Both humans and models provided attractiveness judgments and free-text explanations, which were classified into shared reasoning categories for comparison.

Result: High AI-human agreement for attractive formal greenspaces and unattractive informal spaces, but low alignment for attractive informal and unattractive formal greenspaces. Models consistently emphasized aesthetic and design features while underrepresenting safety, functional infrastructure, and locally embedded qualities valued by humans.

Conclusion: MLLMs show potential for scalable pre-assessment of greenspace attractiveness but require human oversight and complementary participatory approaches. They can support but not replace context-sensitive greenspace evaluation in planning practice due to biases toward aesthetic features and limited understanding of local context.

Abstract: Understanding greenspace attractiveness is essential for designing livable and inclusive urban environments, yet existing assessment approaches often overlook informal or transient spaces and remain too resource intensive to capture subjective perceptions at scale. This study examines the ability of multimodal large language models (MLLMs), ChatGPT GPT-4o, Claude 3.5 Haiku, and Gemini 2.0 Flash, to assess greenspace attractiveness similarly to humans using Google Street View imagery. We compared model outputs with responses from a geo-questionnaire of residents in Lodz, Poland, across both formal (for example, parks and managed greenspaces) and informal (for example, meadows and wastelands) greenspaces. Survey respondents and models indicated whether each greenspace was attractive or unattractive and provided up to three free text explanations. Analyses examined how often their attractiveness judgments aligned and compared their explanations after classifying them into shared reasoning categories. Results show high AI human agreement for attractive formal greenspaces and unattractive informal spaces, but low alignment for attractive informal and unattractive formal greenspaces. Models consistently emphasized aesthetic and design oriented features, underrepresenting safety, functional infrastructure, and locally embedded qualities valued by survey respondents. While these findings highlight the potential for scalable pre-assessment, they also underscore the need for human oversight and complementary participatory approaches. We conclude that MLLMs can support, but not replace, context sensitive greenspace evaluation in planning practice.

</details>


### [294] [Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological "Censorship"](https://arxiv.org/abs/2512.11883)
*Wenqi Marshall Guo,Qingyun Qian,Khalad Hasan,Shan Du*

Main category: cs.CY

TL;DR: Aesthetic-aligned image generation models prioritize conventional beauty over user intent, failing to produce anti-aesthetic or low-quality images when requested, revealing a systemic bias that compromises user autonomy.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the problem that over-aligning image generation models to generalized aesthetic preferences conflicts with user intent, particularly when users request "anti-aesthetic" outputs for artistic or critical purposes. This adherence to developer-centered values compromises user autonomy and aesthetic pluralism.

Method: Researchers constructed a wide-spectrum aesthetics dataset and evaluated state-of-the-art generation and reward models. They tested the bias through image-to-image editing and evaluated models against real abstract artworks to confirm systemic issues.

Result: Aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Reward models penalize anti-aesthetic images even when they perfectly match explicit user prompts, confirming systemic bias.

Conclusion: Current aesthetic alignment in image generation models creates a systemic bias that prioritizes conventional beauty over user intent, compromising user autonomy and limiting aesthetic diversity. This reveals fundamental tensions between developer-centered values and user needs in AI image generation.

Abstract: Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [295] [Benchmarking Tesla's Traffic Light and Stop Sign Control: Field Dataset and Behavior Insights](https://arxiv.org/abs/2512.11802)
*Zheng Li,Peng Zhang,Shixiao Liang,Hang Zhou,Chengyuan Ma,Handong Yao,Qianwen Li,Xiaopeng Li*

Main category: cs.RO

TL;DR: Empirical study of Tesla's Traffic Light and Stop Sign Control (TLSSC) interaction with traffic control devices, developing behavioral taxonomy and calibrating car-following models to characterize ADAS-TCD interactions.


<details>
  <summary>Details</summary>
Motivation: Limited empirical research on how Advanced Driver-Assistance Systems (ADAS) interact with Traffic Control Devices (TCDs), which is critical for assessing their impact on traffic operations and safety.

Method: Field experiments with Tesla TLSSC across varied speed limits and TCD types, collecting synchronized vehicle trajectory data and driver-perspective video. Developed taxonomy of interaction behaviors and calibrated Full Velocity Difference Model (FVDM) to characterize each behavior mode.

Result: Identified car-following threshold (~90 m), found stopping behavior driven by strong responsiveness to speed deviation and relative speed, accelerating behavior more conservative, and intersection car-following exhibits smoother dynamics with tighter headways. Dataset and models publicly available.

Conclusion: The study provides foundational dataset, behavior definitions, and model characterizations for future simulation, safety evaluation, and design of ADAS-TCD interaction logic, addressing a critical gap in empirical understanding of ADAS behavior at traffic control devices.

Abstract: Understanding how Advanced Driver-Assistance Systems (ADAS) interact with Traffic Control Devices (TCDs) is critical for assessing their influence on traffic operations, yet this interaction has received little focused empirical study. This paper presents a field dataset and behavioral analysis of Tesla's Traffic Light and Stop Sign Control (TLSSC), a mature ADAS that perceives traffic lights and stop signs. We design and execute experiments across varied speed limits and TCD types, collecting synchronized high-resolution vehicle trajectory data and driver-perspective video. From these data, we develop a taxonomy of TLSSC-TCD interaction behaviors (i.e., stopping, accelerating, and car following) and calibrate the Full Velocity Difference Model (FVDM) to quantitatively characterize each behavior mode. A novel empirical insight is the identification of a car-following threshold (~90 m). Calibration results reveal that stopping behavior is driven by strong responsiveness to both desired speed deviation and relative speed, whereas accelerating behavior is more conservative. Intersection car-following behavior exhibits smoother dynamics and tighter headways compared to standard car-following behaviors. The established dataset, behavior definitions, and model characterizations together provide a foundation for future simulation, safety evaluation, and design of ADAS-TCD interaction logic. Our dataset is available at GitHub.

</details>


### [296] [ReGlove: A Soft Pneumatic Glove for Activities of Daily Living Assistance via Wrist-Mounted Vision](https://arxiv.org/abs/2512.11824)
*Rosh Ho,Jian Zhang*

Main category: cs.RO

TL;DR: ReGlove converts cheap pneumatic rehab gloves into vision-guided assistive orthoses using wrist-mounted camera and edge computing, achieving high accuracy and low latency for object manipulation tasks at under $250.


<details>
  <summary>Details</summary>
Motivation: Chronic upper-limb impairment affects millions worldwide, but existing assistive technologies are too expensive or rely on unreliable biological signals like EMG, excluding many users.

Method: Integrates wrist-mounted camera with Raspberry Pi 5 edge-computing engine, adapts real-time YOLO-based computer vision models to enable context-aware grasping without muscle signals, using low-cost commercial pneumatic rehabilitation gloves.

Result: Achieves 96.73% grasp classification accuracy with sub-40ms end-to-end latency, 82.71% success on YCB object manipulation benchmarks, and reliable performance across 27 Activities of Daily Living tasks.

Conclusion: ReGlove provides an accessible, vision-based upper-limb assistance foundation at under $250, benefiting populations excluded from traditional EMG-controlled devices.

Abstract: This paper presents ReGlove, a system that converts low-cost commercial pneumatic rehabilitation gloves into vision-guided assistive orthoses. Chronic upper-limb impairment affects millions worldwide, yet existing assistive technologies remain prohibitively expensive or rely on unreliable biological signals. Our platform integrates a wrist-mounted camera with an edge-computing inference engine (Raspberry Pi 5) to enable context-aware grasping without requiring reliable muscle signals. By adapting real-time YOLO-based computer vision models, the system achieves \SI{96.73}{\percent} grasp classification accuracy with sub-\SI{40.00}{\milli\second} end-to-end latency. Physical validation using standardized benchmarks shows \SI{82.71}{\percent} success on YCB object manipulation and reliable performance across \SI{27.00}{} Activities of Daily Living (ADL) tasks. With a total cost under \$\SI{250.00}{} and exclusively commercial components, ReGlove provides a technical foundation for accessible, vision-based upper-limb assistance that could benefit populations excluded from traditional EMG-controlled devices.

</details>


### [297] [Aion: Towards Hierarchical 4D Scene Graphs with Temporal Flow Dynamics](https://arxiv.org/abs/2512.11903)
*Iacopo Catalano,Eduardo Montijano,Javier Civera,Julio A. Placed,Jorge Pena-Queralta*

Main category: cs.RO

TL;DR: Aion integrates temporal motion dynamics into 3D Scene Graphs using sparse graph-based Maps of Dynamics for improved navigation in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Current 3D Scene Graphs lack temporal dynamics modeling, while Maps of Dynamics lack semantic awareness and scalability. There's a need for a unified representation that combines semantic structure with temporal evolution for better autonomous navigation in dynamic environments.

Method: Aion embeds temporal flow dynamics directly within hierarchical 3D Scene Graphs using a graph-based sparse Maps of Dynamics representation. It captures motion flows over arbitrary time intervals and attaches them to navigational nodes in the scene graph.

Result: The framework creates more interpretable and scalable predictions that improve planning and interaction in complex dynamic environments by combining semantic awareness with temporal dynamics modeling.

Conclusion: Aion successfully bridges the gap between semantic scene understanding and temporal dynamics modeling, providing a unified representation that enhances autonomous navigation capabilities in dynamic environments through hierarchical temporal-semantic scene graphs.

Abstract: Autonomous navigation in dynamic environments requires spatial representations that capture both semantic structure and temporal evolution. 3D Scene Graphs (3DSGs) provide hierarchical multi-resolution abstractions that encode geometry and semantics, but existing extensions toward dynamics largely focus on individual objects or agents. In parallel, Maps of Dynamics (MoDs) model typical motion patterns and temporal regularities, yet are usually tied to grid-based discretizations that lack semantic awareness and do not scale well to large environments. In this paper we introduce Aion, a framework that embeds temporal flow dynamics directly within a hierarchical 3DSG, effectively incorporating the temporal dimension. Aion employs a graph-based sparse MoD representation to capture motion flows over arbitrary time intervals and attaches them to navigational nodes in the scene graph, yielding more interpretable and scalable predictions that improve planning and interaction in complex dynamic environments.

</details>


### [298] [Navigation Around Unknown Space Objects Using Visible-Thermal Image Fusion](https://arxiv.org/abs/2512.12203)
*Eric J. Elias,Michael Esswein,Jonathan P. How,David W. Miller*

Main category: cs.RO

TL;DR: Thermal-infrared and visible image fusion improves SLAM navigation for on-orbit operations in challenging lighting conditions.


<details>
  <summary>Details</summary>
Motivation: On-orbit navigation around unknown space objects requires robust sensing. Conventional cameras fail in eclipse/shadowed conditions, while lidar is heavy/power-intensive. Thermal-infrared cameras work in difficult lighting but lack resolution/features compared to visible cameras.

Method: Photo-realistically simulated visible and thermal-infrared images of a target satellite in LEO. Used pixel-level fusion methods to create visible/thermal-infrared composites. Compared navigation errors from monocular SLAM algorithm across visible-only, thermal-only, and fused imagery in various lighting/trajectories.

Result: Fused imagery yields substantially improved navigation performance over both visible-only and thermal-only methods.

Conclusion: Visible/thermal-infrared image fusion effectively leverages the strengths of both sensing modalities, overcoming individual limitations and providing superior SLAM-based navigation for on-orbit operations.

Abstract: As the popularity of on-orbit operations grows, so does the need for precise navigation around unknown resident space objects (RSOs) such as other spacecraft, orbital debris, and asteroids. The use of Simultaneous Localization and Mapping (SLAM) algorithms is often studied as a method to map out the surface of an RSO and find the inspector's relative pose using a lidar or conventional camera. However, conventional cameras struggle during eclipse or shadowed periods, and lidar, though robust to lighting conditions, tends to be heavier, bulkier, and more power-intensive. Thermal-infrared cameras can track the target RSO throughout difficult illumination conditions without these limitations. While useful, thermal-infrared imagery lacks the resolution and feature-richness of visible cameras. In this work, images of a target satellite in low Earth orbit are photo-realistically simulated in both visible and thermal-infrared bands. Pixel-level fusion methods are used to create visible/thermal-infrared composites that leverage the best aspects of each camera. Navigation errors from a monocular SLAM algorithm are compared between visible, thermal-infrared, and fused imagery in various lighting and trajectories. Fused imagery yields substantially improved navigation performance over visible-only and thermal-only methods.

</details>


### [299] [SLIM-VDB: A Real-Time 3D Probabilistic Semantic Mapping Framework](https://arxiv.org/abs/2512.12945)
*Anja Sheppard,Parker Ewen,Joey Wilson,Advaith V. Sethuraman,Benard Adewole,Anran Li,Yuzhen Chen,Ram Vasudevan,Katherine A. Skinner*

Main category: cs.RO

TL;DR: SLIM-VDB is a lightweight semantic mapping system using OpenVDB data structure with probabilistic semantic fusion for both closed-set and open-set dictionaries, achieving significant memory and computational efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: Existing semantic mapping systems lack support for integrating both fixed-category and open-language label predictions within a single framework, and OpenVDB's computational/memory efficiency advantages haven't been explored for semantic mapping despite its success in geometric mapping.

Method: Leverages OpenVDB data structure and integrates a unified Bayesian update framework for both closed- and open-set semantic fusion, creating a lightweight 3D semantic mapping system.

Result: Achieves significant reduction in both memory usage and integration times compared to current state-of-the-art semantic mapping approaches while maintaining comparable mapping accuracy.

Conclusion: SLIM-VDB successfully demonstrates that OpenVDB data structures can be effectively applied to semantic mapping, providing an efficient framework that supports both closed-set and open-set semantic fusion with improved performance metrics.

Abstract: This paper introduces SLIM-VDB, a new lightweight semantic mapping system with probabilistic semantic fusion for closed-set or open-set dictionaries. Advances in data structures from the computer graphics community, such as OpenVDB, have demonstrated significantly improved computational and memory efficiency in volumetric scene representation. Although OpenVDB has been used for geometric mapping in robotics applications, semantic mapping for scene understanding with OpenVDB remains unexplored. In addition, existing semantic mapping systems lack support for integrating both fixed-category and open-language label predictions within a single framework. In this paper, we propose a novel 3D semantic mapping system that leverages the OpenVDB data structure and integrates a unified Bayesian update framework for both closed- and open-set semantic fusion. Our proposed framework, SLIM-VDB, achieves significant reduction in both memory and integration times compared to current state-of-the-art semantic mapping approaches, while maintaining comparable mapping accuracy. An open-source C++ codebase with a Python interface is available at https://github.com/umfieldrobotics/slim-vdb.

</details>


### [300] [Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning](https://arxiv.org/abs/2512.12987)
*Amin Jalal Aghdasian,Farzaneh Abdollahi,Ali Kamali Iglie*

Main category: cs.RO

TL;DR: Two DRL-based lane keeping algorithms (AR-RDPG and AR-CADPG) for autonomous vehicles in snowy conditions, with AR-CADPG showing superior performance through end-to-end CNN+attention architecture.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles struggle with lane keeping under snowy road conditions due to uncertainties and slippage, requiring robust decision-making algorithms that can handle these challenging environments.

Method: Two action-robust DRL approaches: 1) AR-RDPG uses multi-scale neural networks for image denoising, DCNN for centerline extraction, then feeds coefficients + driving characteristics to control layer; 2) AR-CADPG is end-to-end with CNN and attention mechanism integrated within DRL framework. Both trained in CARLA simulator.

Result: Both methods validated in various snowy scenarios, with real-world experiments on Jetson Nano confirming feasibility and stability. AR-CADPG demonstrates superior path-tracking accuracy and robustness compared to AR-RDPG.

Conclusion: Combining temporal memory, adversarial resilience, and attention mechanisms in end-to-end DRL frameworks (like AR-CADPG) provides effective solutions for autonomous lane keeping in snowy conditions, with attention mechanisms particularly enhancing performance.

Abstract: This paper proposes two new algorithms for the lane keeping system (LKS) in autonomous vehicles (AVs) operating under snowy road conditions. These algorithms use deep reinforcement learning (DRL) to handle uncertainties and slippage. They include Action-Robust Recurrent Deep Deterministic Policy Gradient (AR-RDPG) and end-to-end Action-Robust convolutional neural network Attention Deterministic Policy Gradient (AR-CADPG), two action-robust approaches for decision-making. In the AR-RDPG method, within the perception layer, camera images are first denoised using multi-scale neural networks. Then, the centerline coefficients are extracted by a pre-trained deep convolutional neural network (DCNN). These coefficients, concatenated with the driving characteristics, are used as input to the control layer. The AR-CADPG method presents an end-to-end approach in which a convolutional neural network (CNN) and an attention mechanism are integrated within a DRL framework. Both methods are first trained in the CARLA simulator and validated under various snowy scenarios. Real-world experiments on a Jetson Nano-based autonomous vehicle confirm the feasibility and stability of the learned policies. Among the two models, the AR-CADPG approach demonstrates superior path-tracking accuracy and robustness, highlighting the effectiveness of combining temporal memory, adversarial resilience, and attention mechanisms in AVs.

</details>


### [301] [Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving](https://arxiv.org/abs/2512.13262)
*Hyunki Seong,Jeong-Kyun Lee,Heesoo Myeong,Yongho Shin,Hyun-Mook Cho,Duck Hoon Kim,Pranav Desai,Monu Surana*

Main category: cs.RO

TL;DR: GRBO uses RL post-training with group relative advantage maximization and human regularization to improve safety by 40+% using only 10% of data, while Warm-K uses warm-started Top-K sampling for better consistency and diversity in motion selection.


<details>
  <summary>Details</summary>
Motivation: Imitation learning models for autonomous driving inherit biases from safe-demonstration-dominated datasets, limiting robustness in safety-critical cases, and most studies rely on open-loop evaluation that overlooks compounding errors in closed-loop execution.

Method: Two complementary strategies: 1) Group Relative Behavior Optimization (GRBO) - RL post-training method that fine-tunes pretrained behavior models via group relative advantage maximization with human regularization; 2) Warm-K - warm-started Top-K sampling strategy that balances consistency and diversity in motion selection.

Result: GRBO improves safety performance by over 40% while preserving behavioral realism using only 10% of training dataset; Warm-K enhances behavioral consistency and reactivity at test time without retraining, mitigating covariate shift and reducing performance discrepancies.

Conclusion: The proposed GRBO and Warm-K methods effectively address limitations of imitation learning in autonomous driving by improving safety through RL post-training and enhancing test-time consistency through warm-started sampling strategies.

Abstract: Learning interactive motion behaviors among multiple agents is a core challenge in autonomous driving. While imitation learning models generate realistic trajectories, they often inherit biases from datasets dominated by safe demonstrations, limiting robustness in safety-critical cases. Moreover, most studies rely on open-loop evaluation, overlooking compounding errors in closed-loop execution. We address these limitations with two complementary strategies. First, we propose Group Relative Behavior Optimization (GRBO), a reinforcement learning post-training method that fine-tunes pretrained behavior models via group relative advantage maximization with human regularization. Using only 10% of the training dataset, GRBO improves safety performance by over 40% while preserving behavioral realism. Second, we introduce Warm-K, a warm-started Top-K sampling strategy that balances consistency and diversity in motion selection. Our Warm-K method-based test-time scaling enhances behavioral consistency and reactivity at test time without retraining, mitigating covariate shift and reducing performance discrepancies. Demo videos are available in the supplementary material.

</details>


### [302] [World Models Can Leverage Human Videos for Dexterous Manipulation](https://arxiv.org/abs/2512.13644)
*Raktim Gautam Goswami,Amir Bar,David Fan,Tsung-Yen Yang,Gaoyue Zhou,Prashanth Krishnamurthy,Michael Rabbat,Farshad Khorrami,Yann LeCun*

Main category: cs.RO

TL;DR: DexWM is a dexterous manipulation world model that predicts future environment states from past states and hand actions, trained on diverse human/robot videos with a hand consistency loss for fine-grained control, achieving superior prediction accuracy and strong zero-shot generalization on real robot tasks.


<details>
  <summary>Details</summary>
Motivation: Dexterous manipulation is challenging due to the need to understand subtle hand motions and their effects through object contact. There's also a scarcity of dexterous manipulation datasets, making it difficult to train effective models for fine-grained hand control.

Method: DexWM predicts next latent states conditioned on past states and dexterous actions. To overcome data scarcity, it's trained on 900+ hours of human and non-dexterous robot videos. An auxiliary hand consistency loss is introduced to enforce accurate hand configurations since visual features alone are insufficient for fine-grained dexterity.

Result: DexWM outperforms prior world models conditioned on text, navigation, and full-body actions in predicting future states. When deployed on a Franka Panda arm with Allegro gripper, it shows strong zero-shot generalization to unseen manipulation skills, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.

Conclusion: DexWM successfully addresses dexterous manipulation challenges through world modeling with hand consistency constraints, demonstrating that predicting visual features alone is insufficient for fine-grained control, and that training on diverse video data enables effective zero-shot generalization to real robot manipulation tasks.

Abstract: Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.

</details>


### [303] [RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics](https://arxiv.org/abs/2512.13660)
*Enshen Zhou,Cheng Chi,Yibo Li,Jingkun An,Jiayuan Zhang,Shanyu Rong,Yi Han,Yuheng Ji,Mengzhen Liu,Pengwei Wang,Zhongyuan Wang,Lu Sheng,Shanghang Zhang*

Main category: cs.RO

TL;DR: RoboTracer: A 3D-aware VLM that achieves spatial tracing through supervised and reinforcement fine-tuning, outperforming baselines on spatial understanding and complex reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Spatial tracing is challenging for robots as it requires multi-step metric-grounded reasoning with complex spatial referring and real-world measurement. Existing methods struggle with this compositional task.

Method: Proposes RoboTracer, a 3D-aware VLM with universal spatial encoder and regression-supervised decoder for scale awareness during SFT. Uses reinforcement fine-tuning with metric-sensitive process rewards for multi-step reasoning. Introduces TraceSpatial dataset (30M QA pairs) and TraceSpatial-Bench benchmark.

Result: RoboTracer achieves 79.1% average success rate, surpassing baselines in spatial understanding, measuring, and referring. Outperforms Gemini-2.5-Pro by 36% accuracy on TraceSpatial-Bench. Successfully integrates with various control policies for long-horizon tasks across diverse robots in real-world scenes.

Conclusion: RoboTracer effectively addresses spatial tracing challenges through combined supervised and reinforcement fine-tuning, demonstrating strong performance on complex spatial reasoning tasks and practical applicability across different robotic platforms.

Abstract: Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [304] [The Morphemic Origin of Zipf's Law: A Factorized Combinatorial Framework](https://arxiv.org/abs/2512.12394)
*Vladimir Berman*

Main category: stat.ME

TL;DR: Simple probabilistic model shows that word formation from morphemes (prefixes, roots, suffixes) with positional slots can explain both typical word length distributions and Zipf-like rank-frequency curves, without needing communication efficiency or meaning.


<details>
  <summary>Details</summary>
Motivation: To explain two major empirical facts about language: typical word length distributions and Zipf-like rank-frequency curves, without relying on classical explanations based on random text or communication efficiency.

Method: Morphemic Combinatorial Word Model: words are created by activating positional slots (prefix, root, suffix, inflection) with certain probabilities, each selecting a morpheme from its inventory. Morphemes are treated as stable building blocks with characteristic positions.

Result: The model produces realistic word length patterns with concentrated middle zone and thin long tail, matching real languages. Simulations generate rank-frequency curves with Zipf-like exponents around 1.1-1.4, similar to English, Russian and Romance languages.

Conclusion: Zipf-like behavior can emerge without meaning, communication pressure or optimization principles. The internal structure of morphology alone, combined with probabilistic activation of slots, is sufficient to create the robust statistical patterns observed across languages.

Abstract: We present a simple structure based model of how words are formed from morphemes. The model explains two major empirical facts: the typical distribution of word lengths and the appearance of Zipf like rank frequency curves. In contrast to classical explanations based on random text or communication efficiency, our approach uses only the combinatorial organization of prefixes, roots, suffixes and inflections. In this Morphemic Combinatorial Word Model, a word is created by activating several positional slots. Each slot turns on with a certain probability and selects one morpheme from its inventory. Morphemes are treated as stable building blocks that regularly appear in word formation and have characteristic positions. This mechanism produces realistic word length patterns with a concentrated middle zone and a thin long tail, closely matching real languages. Simulations with synthetic morpheme inventories also generate rank frequency curves with Zipf like exponents around 1.1-1.4, similar to English, Russian and Romance languages. The key result is that Zipf like behavior can emerge without meaning, communication pressure or optimization principles. The internal structure of morphology alone, combined with probabilistic activation of slots, is sufficient to create the robust statistical patterns observed across languages.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [305] [AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation](https://arxiv.org/abs/2512.12597)
*Miriam Horovicz*

Main category: cs.AI

TL;DR: AgentSHAP is the first framework for explaining tool importance in LLM agents using Shapley values, providing model-agnostic tool attribution without needing internal model access.


<details>
  <summary>Details</summary>
Motivation: LLM agents use external tools to solve complex tasks, but there's no existing method to explain which tools actually contributed to the agent's responses - this is a blind spot in current XAI methods.

Method: AgentSHAP uses Monte Carlo Shapley values to treat the agent as a black box, testing how it responds with different tool subsets and computing fair importance scores based on game theory principles.

Result: Experiments on API-Bank show AgentSHAP produces consistent scores across runs, correctly identifies important tools, and distinguishes relevant from irrelevant tools.

Conclusion: AgentSHAP completes a family of Shapley-based XAI tools (joining TokenSHAP and PixelSHAP) for modern generative AI, providing the first explainability method for agent tool attribution.

Abstract: LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.

</details>


### [306] [Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI](https://arxiv.org/abs/2512.12686)
*Samarth Sarin,Lovepreet Singh,Bhaskarjit Sarmah,Dhagash Mehta*

Main category: cs.AI

TL;DR: Memoria is a modular memory framework that adds persistent, interpretable memory to LLM-based conversational systems through dynamic session summarization and weighted knowledge graph-based user modeling.


<details>
  <summary>Details</summary>
Motivation: LLMs need agentic memory to maintain continuity, personalization, and long-term context in extended user interactions to become truly interactive and adaptive agents, bridging the gap between stateless LLM interfaces and agentic memory systems.

Method: Hybrid architecture with two components: 1) dynamic session-level summarization for short-term coherence, and 2) weighted knowledge graph-based user modeling engine that incrementally captures user traits, preferences, and behavioral patterns as structured entities and relationships.

Result: Enables both short-term dialogue coherence and long-term personalization while operating within token constraints of modern LLMs, providing scalable, personalized conversational AI for industry applications.

Conclusion: Memoria offers a practical solution for deploying LLMs as interactive agents with adaptive and evolving user experiences by providing persistent, interpretable, and context-rich memory capabilities.

Abstract: Agentic memory is emerging as a key enabler for large language models (LLM) to maintain continuity, personalization, and long-term context in extended user interactions, critical capabilities for deploying LLMs as truly interactive and adaptive agents. Agentic memory refers to the memory that provides an LLM with agent-like persistence: the ability to retain and act upon information across conversations, similar to how a human would. We present Memoria, a modular memory framework that augments LLM-based conversational systems with persistent, interpretable, and context-rich memory. Memoria integrates two complementary components: dynamic session-level summarization and a weighted knowledge graph (KG)-based user modelling engine that incrementally captures user traits, preferences, and behavioral patterns as structured entities and relationships. This hybrid architecture enables both short-term dialogue coherence and long-term personalization while operating within the token constraints of modern LLMs. We demonstrate how Memoria enables scalable, personalized conversational artificial intelligence (AI) by bridging the gap between stateless LLM interfaces and agentic memory systems, offering a practical solution for industry applications requiring adaptive and evolving user experiences.

</details>


### [307] [WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment](https://arxiv.org/abs/2512.12692)
*Mahir Labib Dihan,Tanzima Hashem,Mohammed Eunus Ali,Md Rizwan Parvez*

Main category: cs.AI

TL;DR: WebOperator is a tree-search framework for LLM-based web agents that enables safe backtracking and strategic exploration to overcome limitations of greedy step-by-step approaches in partially observable web environments.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based web agents operate greedily without foresight, struggle with error correction in partially observable web environments, lack safe backtracking mechanisms, and assume all actions are reversible, leading to inefficiencies and unintended side effects.

Method: WebOperator combines best-first search with safety considerations, includes a robust backtracking mechanism that verifies path feasibility before replaying, generates diverse action candidates from multiple reasoning contexts, and curates high-quality actions by filtering invalid ones and merging semantically equivalent actions.

Result: WebOperator achieves state-of-the-art 54.6% success rate on WebArena with GPT-4o, demonstrating significant improvements over existing approaches through strategic foresight and safe execution.

Conclusion: The integration of strategic foresight with safe execution through WebOperator's tree-search framework effectively addresses the limitations of greedy LLM-based web agents, enabling reliable backtracking and systematic exploration in realistic web tasks.

Abstract: LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.

</details>


### [308] [M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization](https://arxiv.org/abs/2512.13070)
*Bizhe Bai,Hongming Wu,Peng Ye,Tao Chen*

Main category: cs.AI

TL;DR: M-GRPO with IQR filtering stabilizes self-supervised RL for LLM reasoning by preventing policy collapse and preserving entropy diversity.


<details>
  <summary>Details</summary>
Motivation: Existing self-supervised RL methods for LLMs suffer from "policy collapse" during long-horizon training, where performance degrades despite scaling rollouts. This instability undermines the promise of enhancing LLM reasoning without expensive human data.

Method: Two innovations: 1) M-GRPO (Momentum-Anchored Group Relative Policy Optimization) uses a slowly evolving momentum model as stable training target; 2) Adaptive IQR filtering dynamically prunes low-entropy trajectories to preserve policy diversity and prevent premature convergence.

Result: Extensive experiments on multiple reasoning benchmarks show M-GRPO stabilizes training while IQR filter prevents premature convergence. Combined approach achieves superior training stability and state-of-the-art performance.

Conclusion: The combination of M-GRPO's stable training target and adaptive IQR filtering effectively addresses policy collapse in self-supervised RL for LLMs, enabling stable long-horizon training and improved reasoning capabilities without human annotation.

Abstract: Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a "policy collapse" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.

</details>


### [309] [MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations](https://arxiv.org/abs/2512.13154)
*Emre Can Acikgoz,Jinoh Oh,Joo Hyuk Jeon,Jie Hao,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur,Xiang Li,Chengyuan Ma,Xing Fan*

Main category: cs.AI

TL;DR: MAC is a multi-agent framework that strategically manages clarification dialogues to resolve user ambiguities, improving task success rates and reducing dialogue turns through coordinated agent interactions.


<details>
  <summary>Details</summary>
Motivation: Conversational agents face ambiguous user requests requiring effective clarification. Multi-agent architectures are increasingly used for complex conversations, but ambiguity resolution remains challenging - specifically determining which agent should initiate clarification and how agents should coordinate when faced with uncertain user input. The fundamental questions of when to interrupt users and how to formulate optimal clarification queries in multi-agent settings remain open.

Method: Proposes MAC (Multi-Agent Clarification), an interactive multi-agent framework optimized to resolve user ambiguities. First introduces a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then presents MAC that autonomously coordinates multiple agents to interact synergistically with users.

Result: Empirical evaluations on MultiWOZ 2.4 show that enabling clarification at both levels increases task success rate by 7.8% (from 54.5 to 62.3) and reduces average number of dialogue turns (from 6.53 to 4.86) by eliciting all required user information upfront and minimizing repetition.

Conclusion: The findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication. The MAC framework demonstrates effective coordination of multiple agents to resolve ambiguities through strategic clarification dialogues.

Abstract: Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication.

</details>


### [310] [SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning](https://arxiv.org/abs/2512.13159)
*Emre Can Acikgoz,Jinoh Oh,Jie Hao,Joo Hyuk Jeon,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur,Xiang Li,Chengyuan Ma,Xing Fan*

Main category: cs.AI

TL;DR: SpeakRL uses reinforcement learning to teach AI agents to proactively ask clarification questions during human-agent collaboration, improving task completion by 20.14% without increasing conversation length.


<details>
  <summary>Details</summary>
Motivation: Current human-agent collaborations are unidirectional with agents responding directly without seeking clarifications. Agents need to be more proactive in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances, but existing work under-utilizes conversational capabilities of language models.

Method: SpeakRL is a reinforcement learning method that rewards agents for proactive interactions like asking clarification questions. The authors curate SpeakER, a synthetic dataset with diverse task-oriented dialogue scenarios requiring interactive clarifications. They present systematic reward design analysis and propose principled reward formulation to balance asking questions with taking actions.

Result: Empirical evaluations show 20.14% absolute improvement in task completion over base models without increasing conversation turns, even surpassing larger proprietary models.

Conclusion: The approach demonstrates promise for clarification-centric user-agent interactions, showing that teaching agents to proactively engage in conversations through reinforcement learning significantly improves collaboration effectiveness.

Abstract: Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (LMs), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce SpeakRL, a reinforcement learning (RL) method that enhances agents' conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate SpeakER, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions.

</details>


### [311] [Differentiable Evolutionary Reinforcement Learning](https://arxiv.org/abs/2512.13399)
*Sitao Cheng,Tianle Li,Xuhan Huang,Xunjian Yin,Difan Zou*

Main category: cs.AI

TL;DR: DERL is a differentiable evolutionary RL framework that autonomously discovers optimal reward functions by treating inner-loop policy performance as a signal to update meta-rewards via reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Designing effective reward functions is challenging in RL, especially for complex reasoning tasks. Current automated approaches treat reward functions as black boxes and fail to capture causal relationships between reward structure and task performance.

Method: DERL uses a bilevel framework with a Meta-Optimizer that evolves reward functions by composing structured atomic primitives. It's differentiable in meta-optimization, treating inner-loop validation performance as a signal to update the Meta-Optimizer via RL, approximating "meta-gradients" of task success.

Result: DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming heuristic reward methods, especially in out-of-distribution scenarios. It successfully captures intrinsic task structure and enables self-improving agent alignment.

Conclusion: DERL bridges the gap between reward design and task performance by enabling differentiable evolutionary optimization of reward functions, allowing autonomous discovery of optimal reward signals without human intervention.

Abstract: The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the "meta-gradient" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention.

</details>


### [312] [neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings](https://arxiv.org/abs/2512.13481)
*Ojas Pungalia,Rashi Upadhyay,Abhishek Mishra,Abhiram H,Tejasvi Alladi,Sujan Yenuganti,Dhruv Kumar*

Main category: cs.AI

TL;DR: LLMs exhibit envy-like behavior in competitive scenarios, with some models trying to equalize outcomes by pulling down peers while others focus on maximizing individual gains.


<details>
  <summary>Details</summary>
Motivation: As LLMs increasingly act on behalf of humans in collaborative and competitive workflows, there's a need to evaluate whether they exhibit envy-like preferences, which could impact outcomes in team settings.

Method: Tested LLMs in two scenarios: (1) a point allocation game to see if models try to win over peers, and (2) a workplace setting observing behavior when recognition is unfair.

Result: Found consistent evidence of envy-like patterns in certain LLMs with large variation across models and contexts. GPT-5-mini and Claude-3.7-Sonnet show tendency to pull down peers to equalize outcomes, while Mistral-Small-3.2-24B focuses on maximizing individual gains.

Conclusion: Competitive dispositions should be considered as a safety and design factor in LLM-based multi-agent systems.

Abstract: Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems.

</details>


### [313] [Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning](https://arxiv.org/abs/2512.13131)
*Xin Guo,Yifan Zhao,Jia Li*

Main category: cs.AI

TL;DR: HIP: Hierarchical Implicit Periodicity learning for audio-driven 3D gesture generation that models inter- and intra-correlations across motion units (head, body, hands) using periodic autoencoders and cascaded guidance.


<details>
  <summary>Details</summary>
Motivation: Existing end-to-end co-speech gesture generation methods (GANs, VQ-VAE, diffusion models) fail to model crucial inter- and intra-correlations across different motion units (head, body, hands), leading to unnatural movements and poor coordination. The problem is ill-posed and requires better modeling of intrinsic correlations.

Method: Proposes Hierarchical Implicit Periodicity (HIP) learning with two explicit techniques: 1) Uses periodic autoencoders to explore gesture motion phase manifolds, imitating human natures from realistic distributions while incorporating non-periodic elements for instance-level diversity; 2) Models hierarchical relationships between face motions, body gestures, and hand movements using cascaded guidance during learning.

Result: The method outperforms state-of-the-art co-speech gesture generation methods in both quantitative and qualitative evaluations on 3D avatars.

Conclusion: HIP successfully models the intrinsic correlations across motion units for more realistic and coordinated audio-driven 3D gesture generation, addressing limitations of existing end-to-end approaches.

Abstract: Generating 3D-based body movements from speech shows great potential in extensive downstream applications, while it still suffers challenges in imitating realistic human movements. Predominant research efforts focus on end-to-end generation schemes to generate co-speech gestures, spanning GANs, VQ-VAE, and recent diffusion models. As an ill-posed problem, in this paper, we argue that these prevailing learning schemes fail to model crucial inter- and intra-correlations across different motion units, i.e. head, body, and hands, thus leading to unnatural movements and poor coordination. To delve into these intrinsic correlations, we propose a unified Hierarchical Implicit Periodicity (HIP) learning approach for audio-inspired 3D gesture generation. Different from predominant research, our approach models this multi-modal implicit relationship by two explicit technique insights: i) To disentangle the complicated gesture movements, we first explore the gesture motion phase manifolds with periodic autoencoders to imitate human natures from realistic distributions while incorporating non-period ones from current latent states for instance-level diversities. ii) To model the hierarchical relationship of face motions, body gestures, and hand movements, driving the animation with cascaded guidance during learning. We exhibit our proposed approach on 3D avatars and extensive experiments show our method outperforms the state-of-the-art co-speech gesture generation methods by both quantitative and qualitative evaluations. Code and models will be publicly available.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [314] [Hybrid Retrieval-Augmented Generation for Robust Multilingual Document Question Answering](https://arxiv.org/abs/2512.12694)
*Anthony Mudet,Souhail Bakkali*

Main category: cs.DL

TL;DR: Multilingual RAG pipeline for historical newspaper QA addresses OCR noise, language variation, and temporal drift through semantic query expansion, evidence-grounded generation, and modular evaluation.


<details>
  <summary>Details</summary>
Motivation: Historical newspaper collections face computational access challenges due to OCR corruption, multilingual orthographic variation, and temporal language drift, requiring robust QA systems.

Method: Multilingual RAG pipeline with: (1) semantic query expansion and multi-query fusion using Reciprocal Rank Fusion; (2) evidence-grounded generation with explicit abstention; (3) modular architecture for systematic evaluation.

Result: Pipeline generates faithful answers for supported queries while correctly abstaining from unanswerable questions. RRF improves recall stability and smooths performance variance across query formulations.

Conclusion: The multilingual RAG pipeline provides robust historical document QA, with released code offering reproducible foundation for future research in noisy document processing.

Abstract: Large-scale digitization initiatives have unlocked massive collections of historical newspapers, yet effective computational access remains hindered by OCR corruption, multilingual orthographic variation, and temporal language drift. We develop and evaluate a multilingual Retrieval-Augmented Generation pipeline specifically designed for question answering on noisy historical documents. Our approach integrates: (i) semantic query expansion and multi-query fusion using Reciprocal Rank Fusion to improve retrieval robustness against vocabulary mismatch; (ii) a carefully engineered generation prompt that enforces strict grounding in retrieved evidence and explicit abstention when evidence is insufficient; and (iii) a modular architecture enabling systematic component evaluation. We conduct comprehensive ablation studies on Named Entity Recognition and embedding model selection, demonstrating the importance of syntactic coherence in entity extraction and balanced performance-efficiency trade-offs in dense retrieval. Our end-to-end evaluation framework shows that the pipeline generates faithful answers for well-supported queries while correctly abstaining from unanswerable questions. The hybrid retrieval strategy improves recall stability, particularly benefiting from RRF's ability to smooth performance variance across query formulations. We release our code and configurations at https://anonymous.4open.science/r/RAGs-C5AE/, providing a reproducible foundation for robust historical document question answering.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [315] [TopicProphet: Prophesies on Temporal Topic Trends and Stocks](https://arxiv.org/abs/2512.11857)
*Olivia Kim*

Main category: cs.LG

TL;DR: TopicProphet is a novel framework that improves stock prediction by identifying historical eras with similar public sentiment trends and socioeconomic contexts, then using those optimal time periods for training models to overcome data scarcity issues.


<details>
  <summary>Details</summary>
Motivation: Stock prediction has been notoriously difficult due to quantitative stock data lacking causal logic and rapid market changes preventing accumulation of sufficient training data. Traditional approaches have struggled with these fundamental limitations.

Method: TopicProphet uses a sequence of topic modeling, temporal analysis, breakpoint detection, and segment optimization to identify historical periods with similar public sentiment trends and historical backgrounds. This creates optimal training periods that capture nuanced patterns from specific socioeconomic and political contexts.

Result: The framework produces improved outcomes compared to state-of-the-art methods in capturing optimal training data for forecasting financial percentage changes, effectively addressing the data scarcity problem in stock prediction.

Conclusion: By analyzing historical eras with similar sentiment trends and backgrounds, TopicProphet successfully improves stock prediction accuracy by providing models with relevant training data that captures era-specific socioeconomic patterns, overcoming traditional limitations in quantitative stock analysis.

Abstract: Stocks can't be predicted. Despite many hopes, this premise held itself true for many years due to the nature of quantitative stock data lacking causal logic along with rapid market changes hindering accumulation of significant data for training models. To undertake this matter, we propose a novel framework, TopicProphet, to analyze historical eras that share similar public sentiment trends and historical background. Our research deviates from previous studies that identified impacts of keywords and sentiments - we expand on that method by a sequence of topic modeling, temporal analysis, breakpoint detection and segment optimization to detect the optimal time period for training. This results in improving predictions by providing the model with nuanced patterns that occur from that era's socioeconomic and political status while also resolving the shortage of pertinent stock data to train on. Through extensive analysis, we conclude that TopicProphet produces improved outcomes compared to the state-of-the-art methods in capturing the optimal training data for forecasting financial percentage changes.

</details>


### [316] [The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior](https://arxiv.org/abs/2512.12066)
*Erik Larsen*

Main category: cs.LG

TL;DR: Safety evaluations of LLMs using single-shot testing are unreliable because model refusal decisions are unstable across different random seeds and temperature settings, with 18-28% of prompts showing decision flips.


<details>
  <summary>Details</summary>
Motivation: Current safety evaluations assume deterministic and representative model responses, but this paper challenges that assumption by investigating the stability of safety refusal decisions across different sampling configurations.

Method: Tested four instruction-tuned models on 876 harmful prompts across 20 sampling configurations (4 temperatures × 5 random seeds). Used Safety Stability Index (SSI) to measure decision stability and validated findings with Claude 3.5 Haiku as external judge.

Result: 18-28% of prompts exhibit decision flips (refusal in some configurations but compliance in others). Higher temperatures significantly reduce decision stability (SSI drops from 0.951 at temp 0.0 to 0.896 at temp 1.0). Single-shot evaluation agrees with multi-sample ground truth only 92.4% of the time.

Conclusion: Single-shot safety evaluations are insufficient for reliable safety assessment. Researchers should use at least 3 samples per prompt for reliable safety evaluation due to the instability of model refusal decisions.

Abstract: Current safety evaluations of large language models rely on single-shot testing, implicitly assuming that model responses are deterministic and representative of the model's safety alignment. We challenge this assumption by investigating the stability of safety refusal decisions across random seeds and temperature settings. Testing four instruction-tuned models from three families (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts across 20 different sampling configurations (4 temperatures x 5 random seeds), we find that 18-28% of prompts exhibit decision flips--the model refuses in some configurations but complies in others--depending on the model. Our Safety Stability Index (SSI) reveals that higher temperatures significantly reduce decision stability (Friedman chi-squared = 44.71, p < 0.001), with mean SSI dropping from 0.951 at temperature 0.0 to 0.896 at temperature 1.0. We validate our findings across all model families using Claude 3.5 Haiku as a unified external judge, achieving 89.0% inter-judge agreement with our primary Llama 70B judge (Cohen's kappa = 0.62). These findings demonstrate that single-shot safety evaluations are insufficient for reliable safety assessment. We show that single-shot evaluation agrees with multi-sample ground truth only 92.4% of the time, and recommend using at least 3 samples per prompt for reliable safety assessment.

</details>


### [317] [The Data Efficiency Frontier of Financial Foundation Models: Scaling Laws from Continued Pretraining](https://arxiv.org/abs/2512.12384)
*Jesse Ponnock*

Main category: cs.LG

TL;DR: Domain-adaptive pretraining (DAPT) on financial data (SEC filings) with 1B and 3B Llama-3.2 models shows efficient learning with diminishing returns after 200M tokens, minimal general-domain degradation, and promising scaling potential for larger models.


<details>
  <summary>Details</summary>
Motivation: To specialize large language models for high-value domains like finance without full retraining, using domain-adaptive pretraining as a practical approach.

Method: Continued pretraining of 1B and 3B-parameter Llama-3.2 models on a 400M-token financial corpus (SEC filings), with validation checkpoints at 50M, 100M, 200M, and 400M tokens, analyzing scaling laws and data efficiency.

Result: Consistent improvements in SEC-domain validation loss with largest gains in first 200M tokens (diminishing returns after), shallow power-law exponents indicating highly regular financial language, minimal general-domain degradation, and data-efficiency frontier showing improved specialization with negligible mixed-domain degradation.

Conclusion: Meaningful domain adaptation can be achieved with modest token budgets, larger model scales (7B-70B) remain tractable, and financial language is efficiently learnable under continued pretraining with minimal catastrophic forgetting.

Abstract: Domain-adaptive pretraining (DAPT) offers a practical path to specializing large language models for high-value domains without full retraining. We conduct an early-stage scaling-law analysis of continued pretraining on U.S. SEC filings, training 1B and 3B-parameter Llama-3.2 models on a 400M-token financial corpus with validation checkpoints at 50M, 100M, 200M, and 400M tokens. Results show consistent improvements in SEC-domain validation loss for both models, with the largest gains occurring within the first 200M tokens and diminishing returns thereafter. Power-law fits reveal shallow exponents, indicating that financial language is highly regular and efficiently learnable under continued pretraining. General-domain validation loss remains effectively unchanged across all token budgets, suggesting minimal drift and no signs of catastrophic forgetting. A data-efficiency frontier further shows that both models move toward improved specialization with negligible mixed-domain degradation. Together, these findings provide early empirical guidance for scaling financial foundation models, suggesting that meaningful domain adaptation can be achieved with comparatively modest token budgets and that larger model scales (7B-70B) remain tractable under projected data requirements.

</details>


### [318] [Theoretical Foundations of Prompt Engineering: From Heuristics to Expressivity](https://arxiv.org/abs/2512.12688)
*Dongseok Kim,Hyoungsun Choi,Mohamed Jismy Aashik Rasool,Gisung Oh*

Main category: cs.LG

TL;DR: The paper formalizes prompt-based behavior switching in Transformers as a theoretical framework, showing that a single fixed backbone can approximate diverse functions via prompts alone, with attention as selective routing and FFN as local computation.


<details>
  <summary>Details</summary>
Motivation: Prompt-based behavior switching in Transformers is treated as a heuristic rather than a formal theoretical object. The paper aims to provide a clean theoretical framework to understand how prompts can switch model behavior with fixed weights.

Method: Constructs a simplified Transformer that interprets prompts as externally injected programs. Decomposes the mechanism: attention performs selective routing from prompt memory, FFN performs local arithmetic on retrieved fragments, and depth-wise stacking composes these into multi-step computations.

Result: Proves a constructive existential result showing that a single fixed backbone can approximate a broad class of target behaviors via prompts alone. Provides a framework for analyzing trade-offs under prompt length/precision constraints and studying structural limits of prompt-based switching.

Conclusion: The framework offers a unified theoretical foundation for understanding prompt-based behavior switching in Transformers, distinct from empirical claims about pretrained LLMs, enabling formal analysis of prompt capabilities and limitations.

Abstract: Prompts can switch a model's behavior even when the weights are fixed, yet this phenomenon is rarely treated as a clean theoretical object rather than a heuristic. We study the family of functions obtainable by holding a Transformer backbone fixed as an executor and varying only the prompt. Our core idea is to view the prompt as an externally injected program and to construct a simplified Transformer that interprets it to implement different computations. The construction exposes a mechanism-level decomposition: attention performs selective routing from prompt memory, the FFN performs local arithmetic conditioned on retrieved fragments, and depth-wise stacking composes these local updates into a multi-step computation. Under this viewpoint, we prove a constructive existential result showing that a single fixed backbone can approximate a broad class of target behaviors via prompts alone. The framework provides a unified starting point for formalizing trade-offs under prompt length/precision constraints and for studying structural limits of prompt-based switching, while remaining distinct from empirical claims about pretrained LLMs.

</details>


### [319] [Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning](https://arxiv.org/abs/2512.12690)
*Yongcan Yu,Lingxiao He,Shuo Lu,Lijun Sheng,Yinuo Xu,Yanbo Wang,Kuangpu Guo,Jianjie Cheng,Meng Wang,Qianlong Xie,Xingxing Wang,Dapeng Hu,Jian Liang*

Main category: cs.LG

TL;DR: SFT is more effective than RL for VLM reasoning in certain scenarios: for weaker models, with limited data, and for cross-modal transfer, challenging the RL-centric belief.


<details>
  <summary>Details</summary>
Motivation: To challenge the prevailing belief that RL is superior to SFT for VLM reasoning, and to systematically compare their effectiveness under different conditions.

Method: Systematic and controlled comparison of SFT and RL on VLM reasoning using identical data sources, examining factors like model capacity, data scale, and data distribution.

Result: SFT outperforms RL in three key scenarios: (1) for weaker/smaller models, (2) with data efficiency (2K SFT vs 20K RL), (3) for cross-modal generalization. Also identified deceptive rewards issue in RL.

Conclusion: The "RL over SFT" narrative is challenged; SFT's role has been underestimated, and both methods should be complementary components in post-training pipelines.

Abstract: Recent advances in vision-language models (VLMs) reasoning have been largely attributed to the rise of reinforcement Learning (RL), which has shifted the community's focus away from the supervised fine-tuning (SFT) paradigm. Many studies suggest that introducing the SFT stage not only fails to improve reasoning ability but may also negatively impact model training. In this study, we revisit this RL-centric belief through a systematic and controlled comparison of SFT and RL on VLM Reasoning. Using identical data sources, we find that the relative effectiveness of SFT and RL is conditional and strongly influenced by model capacity, data scale, and data distribution. Contrary to common assumptions, our findings show that SFT plays a crucial role across several scenarios: (1) Effectiveness for weaker models. SFT more reliably elicits reasoning capabilities in smaller or weaker VLMs. (2) Data efficiency. SFT with only 2K achieves comparable or better reasoning performance to RL with 20K. (3) Cross-modal transferability. SFT demonstrates stronger generalization across modalities. Moreover, we identify a pervasive issue of deceptive rewards, where higher rewards fail to correlate with better reasoning accuracy in RL. These results challenge the prevailing "RL over SFT" narrative. They highlight that the role of SFT may have been underestimated and support a more balanced post-training pipeline in which SFT and RL function as complementary components.

</details>


### [320] [Scaling Bidirectional Spans and Span Violations in Attention Mechanism](https://arxiv.org/abs/2512.13033)
*Jongwook Kim,Sangheon Yun,Sukjin Yoon*

Main category: cs.LG

TL;DR: The paper proposes an optimization framework that decomposes Transformer attention gradients into parallel spans and orthogonal violations using asymmetric projection, showing standard attention gradient is suboptimal and achieving 0.56% validation loss reduction on WikiText-2.


<details>
  <summary>Details</summary>
Motivation: The canonical O(N^2) Transformer has geometric inefficiency in training that can be optimized. The standard attention gradient is suboptimal, and addressing this inefficiency could improve training optimization.

Method: Uses asymmetric projection to decompose backward-pass gradients into parallel spans and orthogonal violations while keeping the canonical forward-pass QKV structure intact. Selectively scales components, focusing primarily on 0th order bidirectional parallel spans.

Result: Achieved 0.56% reduction in validation loss on WikiText-2 dataset with crude configuration. Provides strong theoretical evidence that standard attention gradient is suboptimal.

Conclusion: The framework demonstrates fundamental validity and suggests significant potential gains on larger datasets and deeper training regimes, though current results are limited to WikiText-2 with basic configuration.

Abstract: The canonical $O(N^2)$ Transformer remains the empirical performance frontier in sequence modeling, and its training can be further optimized by addressing geometric inefficiency. We propose an optimization framework that leverages an asymmetric projection to decompose the backward-pass gradients into parallel spans and orthogonal violations, while keeping the canonical forward-pass $QKV$ structure intact. Through consistent experimental validation across various decomposition and projection setups, we provide strong theoretical evidence: the standard attention gradient is suboptimal. We demonstrated that selectively scaling these components, focusing primarily on $0^{th}$ order bidirectional parallel spans, yields the most effective learning signal. On the limited WikiText-2 dataset, and using a crude configuration, this method achieved a $0.56\%$ reduction in validation loss, confirming the framework's fundamental validity and suggesting significant potential gains on larger datasets and deeper training regimes

</details>


### [321] [Understanding Structured Financial Data with LLMs: A Case Study on Fraud Detection](https://arxiv.org/abs/2512.13040)
*Xuwei Tan,Yao Ma,Xueru Zhang*

Main category: cs.LG

TL;DR: FinFRE-RAG: A two-stage LLM approach for fraud detection that uses feature reduction and retrieval-augmented learning to improve performance and provide interpretable explanations, narrowing the gap with specialized tabular models.


<details>
  <summary>Details</summary>
Motivation: Traditional tabular fraud detection models require heavy feature engineering, offer limited interpretability, and are hard for humans to understand. LLMs can provide human-readable explanations and reduce manual workload, but perform poorly on tabular fraud data due to reasoning difficulties with many features, class imbalance, and lack of context.

Method: FinFRE-RAG: A two-stage approach with (1) importance-guided feature reduction to serialize a compact subset of numeric/categorical attributes into natural language, and (2) retrieval-augmented in-context learning over label-aware, instance-level exemplars.

Result: Across four public fraud datasets and three families of open-weight LLMs, FinFRE-RAG substantially improves F1/MCC over direct prompting and is competitive with strong tabular baselines in several settings. LLMs still lag behind specialized classifiers but narrow the performance gap significantly.

Conclusion: While LLMs don't outperform specialized tabular classifiers, they provide interpretable rationales that highlight their value as assistive tools in fraud analysis, reducing manual workload and informing system refinements through human-readable explanations.

Abstract: Detecting fraud in financial transactions typically relies on tabular models that demand heavy feature engineering to handle high-dimensional data and offer limited interpretability, making it difficult for humans to understand predictions. Large Language Models (LLMs), in contrast, can produce human-readable explanations and facilitate feature analysis, potentially reducing the manual workload of fraud analysts and informing system refinements. However, they perform poorly when applied directly to tabular fraud detection due to the difficulty of reasoning over many features, the extreme class imbalance, and the absence of contextual information. To bridge this gap, we introduce FinFRE-RAG, a two-stage approach that applies importance-guided feature reduction to serialize a compact subset of numeric/categorical attributes into natural language and performs retrieval-augmented in-context learning over label-aware, instance-level exemplars. Across four public fraud datasets and three families of open-weight LLMs, FinFRE-RAG substantially improves F1/MCC over direct prompting and is competitive with strong tabular baselines in several settings. Although these LLMs still lag behind specialized classifiers, they narrow the performance gap and provide interpretable rationales, highlighting their value as assistive tools in fraud analysis.

</details>


### [322] [On the Effectiveness of Membership Inference in Targeted Data Extraction from Large Language Models](https://arxiv.org/abs/2512.13352)
*Ali Al Sahili,Ali Chehab,Razane Tajeddine*

Main category: cs.LG

TL;DR: Researchers integrate multiple Membership Inference Attack (MIA) techniques into LLM data extraction pipelines to benchmark their effectiveness in real-world privacy attacks.


<details>
  <summary>Details</summary>
Motivation: LLMs memorize training data, creating privacy risks through data extraction and membership inference attacks. These threats are interconnected - attackers can extract data then use MIAs to verify membership, but the practical effectiveness of MIAs in real extraction scenarios needs evaluation.

Method: Integrate multiple MIA techniques into the data extraction pipeline, systematically benchmark their effectiveness, and compare performance against conventional MIA benchmarks.

Result: The study provides comparative performance analysis of different MIA techniques in practical extraction scenarios, showing how they perform differently in real-world attacks versus conventional benchmarks.

Conclusion: Evaluating MIA techniques in integrated extraction pipelines reveals their practical utility for real-world privacy attacks, providing insights beyond conventional benchmark performance.

Abstract: Large Language Models (LLMs) are prone to memorizing training data, which poses serious privacy risks. Two of the most prominent concerns are training data extraction and Membership Inference Attacks (MIAs). Prior research has shown that these threats are interconnected: adversaries can extract training data from an LLM by querying the model to generate a large volume of text and subsequently applying MIAs to verify whether a particular data point was included in the training set. In this study, we integrate multiple MIA techniques into the data extraction pipeline to systematically benchmark their effectiveness. We then compare their performance in this integrated setting against results from conventional MIA benchmarks, allowing us to evaluate their practical utility in real-world extraction scenarios.

</details>


### [323] [On the Design of One-step Diffusion via Shortcutting Flow Paths](https://arxiv.org/abs/2512.11831)
*Haitao Lin,Peiyan Hu,Minsi Ren,Zhifeng Gao,Zhi-Ming Ma,Guolin ke,Tailin Wu,Stan Z. Li*

Main category: cs.LG

TL;DR: The paper proposes a unified design framework for shortcut diffusion models that disentangles theoretical justification from implementation choices, enabling systematic improvements and achieving state-of-the-art FID on ImageNet-256 without pre-training or distillation.


<details>
  <summary>Details</summary>
Motivation: Current few-step diffusion models (shortcut models) have theoretical derivation and practical implementation closely coupled, which obscures the design space and limits systematic innovation.

Method: Proposes a common design framework that provides theoretical justification for shortcut models' validity and disentangles concrete component-level choices, enabling systematic identification of improvements.

Result: Achieves state-of-the-art FID50k of 2.85 on ImageNet-256x256 under classifier-free guidance setting without requiring pre-training, distillation, or curriculum learning.

Conclusion: The framework lowers barriers to component-level innovation in shortcut models and facilitates principled exploration of their design space.

Abstract: Recent advances in few-step diffusion models have demonstrated their efficiency and effectiveness by shortcutting the probabilistic paths of diffusion models, especially in training one-step diffusion models from scratch (a.k.a. shortcut models). However, their theoretical derivation and practical implementation are often closely coupled, which obscures the design space. To address this, we propose a common design framework for representative shortcut models. This framework provides theoretical justification for their validity and disentangles concrete component-level choices, thereby enabling systematic identification of improvements. With our proposed improvements, the resulting one-step model achieves a new state-of-the-art FID50k of 2.85 on ImageNet-256x256 under the classifier-free guidance setting. Remarkably, the model requires no pre-training, distillation, or curriculum learning. We believe our work lowers the barrier to component-level innovation in shortcut models and facilitates principled exploration of their design space.

</details>


### [324] [Soft Decision Tree classifier: explainable and extendable PyTorch implementation](https://arxiv.org/abs/2512.11833)
*Reuben R Shamir*

Main category: cs.LG

TL;DR: Researchers implemented Soft Decision Tree (SDT) and Short-term Memory Soft Decision Tree (SM-SDT) in PyTorch, tested them on simulated and clinical datasets, and found they perform similarly to XGBoost while being more explainable than traditional methods.


<details>
  <summary>Details</summary>
Motivation: To develop explainable machine learning models (SDT and SM-SDT) that combine the performance of complex models with the interpretability of decision trees, particularly for clinical applications where model transparency is important.

Method: Implemented Soft Decision Tree (SDT) and Short-term Memory Soft Decision Tree (SM-SDT) using PyTorch. Tested extensively on both simulated and clinical datasets. Visualized SDT to demonstrate explainability potential. Compared against XGBoost, Random Forest, Logistic Regression, and traditional Decision Tree.

Result: SDT, SM-SDT, and XGBoost showed similar AUC values. All three methods outperformed Random Forest, Logistic Regression, and traditional Decision Tree. On clinical datasets, all tested classification methods (except basic Decision Tree) yielded comparable results.

Conclusion: Soft Decision Trees offer comparable performance to state-of-the-art methods like XGBoost while providing better explainability, making them suitable for clinical applications where model interpretability is crucial. The code and datasets are publicly available for further research.

Abstract: We implemented a Soft Decision Tree (SDT) and a Short-term Memory Soft Decision Tree (SM-SDT) using PyTorch. The methods were extensively tested on simulated and clinical datasets. The SDT was visualized to demonstrate the potential for its explainability. SDT, SM-SDT, and XGBoost demonstrated similar area under the curve (AUC) values. These methods were better than Random Forest, Logistic Regression, and Decision Tree. The results on clinical datasets suggest that, aside from a decision tree, all tested classification methods yield comparable results.
  The code and datasets are available online on GitHub: https://github.com/KI-Research-Institute/Soft-Decision-Tree

</details>


### [325] [On the Dangers of Bootstrapping Generation for Continual Learning and Beyond](https://arxiv.org/abs/2512.11867)
*Daniil Zverev,A. Sophia Koepke,Joao F. Henriques*

Main category: cs.LG

TL;DR: Repeated training on synthetic data causes distribution drift and performance degradation due to bias/variance issues in training objectives, leading to model collapse and failure of state-of-the-art Generative Experience Replay methods.


<details>
  <summary>Details</summary>
Motivation: As synthetic data becomes common for training augmentation, concerns arise about distribution drift and performance degradation when models are repeatedly trained on synthetic data, particularly in continual learning settings.

Method: Investigates bootstrapping with synthetic data through continual learning lens, connects to Generative Experience Replay (GER) methods, provides statistical analysis of bias/variance in training objectives, and empirically tests popular generative models under repeated synthetic data training.

Result: Synthetic data introduces significant bias and variance weakening maximum likelihood reliability; popular generative models collapse under repeated synthetic data training; state-of-the-art GER methods fail to maintain latent space alignment.

Conclusion: Findings raise critical concerns about synthetic data use in continual learning due to distribution drift, model collapse, and failure of current GER methods to address these issues.

Abstract: The use of synthetically generated data for training models is becoming a common practice. While generated data can augment the training data, repeated training on synthetic data raises concerns about distribution drift and degradation of performance due to contamination of the dataset. We investigate the consequences of this bootstrapping process through the lens of continual learning, drawing a connection to Generative Experience Replay (GER) methods. We present a statistical analysis showing that synthetic data introduces significant bias and variance into training objectives, weakening the reliability of maximum likelihood estimation. We provide empirical evidence showing that popular generative models collapse under repeated training with synthetic data. We quantify this degradation and show that state-of-the-art GER methods fail to maintain alignment in the latent space. Our findings raise critical concerns about the use of synthetic data in continual learning.

</details>


### [326] [PerNodeDrop: A Method Balancing Specialized Subnets and Regularization in Deep Neural Networks](https://arxiv.org/abs/2512.12663)
*Gelesh G Omathil,Sreeja CS*

Main category: cs.LG

TL;DR: PerNodeDrop is a lightweight regularization method that applies per-sample, per-node perturbations to break uniform noise patterns, preserving beneficial co-adaptation while reducing harmful overfitting.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks suffer from overfitting due to neuron co-adaptation that captures both useful feature interactions and spurious patterns. Existing noise-based regularizers like Dropout and DropConnect use uniform noise that suppresses both harmful and beneficial co-adaptation.

Method: PerNodeDrop applies per-sample, per-node perturbations (dropping weights at sample level, not batch level) to break uniformity of noise injection. This allows each node to experience input-specific variability while preserving useful co-adaptation. The approach includes an expected-loss analysis to formalize how perturbations attenuate excessive co-adaptation while retaining predictive interactions.

Result: Empirical evaluations on vision, text, and audio benchmarks show improved generalization relative to standard noise-based regularizers. The method narrows the gap between training and validation performance and improves reliability on unseen data.

Conclusion: PerNodeDrop provides a more targeted regularization approach that breaks uniform noise patterns to preserve beneficial neuron interactions while reducing harmful overfitting, leading to better generalization across multiple domains.

Abstract: Deep neural networks possess strong representational capacity yet remain vulnerable to overfitting, primarily because neurons tend to co-adapt in ways that, while capturing complex and fine-grained feature interactions, also reinforce spurious and non-generalizable patterns that inflate training performance but reduce reliability on unseen data. Noise-based regularizers such as Dropout and DropConnect address this issue by injecting stochastic perturbations during training, but the noise they apply is typically uniform across a layer or across a batch of samples, which can suppress both harmful and beneficial co-adaptation.
  This work introduces PerNodeDrop, a lightweight stochastic regularization method. It applies per-sample, per-node perturbations to break the uniformity of the noise injected by existing techniques, thereby allowing each node to experience input-specific variability. Hence, PerNodeDrop preserves useful co-adaptation while applying regularization. This narrows the gap between training and validation performance and improves reliability on unseen data, as evident from the experiments.
  Although superficially similar to DropConnect, PerNodeDrop operates at the sample level. It drops weights at the sample level, not the batch level. An expected-loss analysis formalizes how its perturbations attenuate excessive co-adaptation while retaining predictive interactions. Empirical evaluations on vision, text, and audio benchmarks indicate improved generalization relative to the standard noise-based regularizer.

</details>


### [327] [Federated Learning with Feedback Alignment](https://arxiv.org/abs/2512.12762)
*Incheol Baek,Hyungbin Kim,Minseo Kim,Yon Dohn Chung*

Main category: cs.LG

TL;DR: FLFA integrates feedback alignment into federated learning to address non-IID data heterogeneity and local drift, using global model weights as shared feedback matrix during local training.


<details>
  <summary>Details</summary>
Motivation: Federated Learning struggles with data heterogeneity (non-IID distributions) across clients, which causes local drift and hinders global model convergence. Existing methods need better solutions to align local updates with the global model efficiently.

Method: Introduces Federated Learning with Feedback Alignment (FLFA) framework that uses the global model's weights as a shared feedback matrix during local training's backward pass. This aligns local updates with the global model while maintaining minimal computational cost and no extra communication overhead.

Result: Theoretical analysis shows FLFA alleviates local drift and demonstrates robust convergence for both local and global models. Empirical evaluations confirm FLFA enhances other FL methods through improved accuracy and reduced local drift measurements.

Conclusion: FLFA effectively addresses non-IID data challenges in federated learning by integrating feedback alignment, providing an efficient solution to local drift with minimal computational and communication costs.

Abstract: Federated Learning (FL) enables collaborative training across multiple clients while preserving data privacy, yet it struggles with data heterogeneity, where clients' data are not distributed independently and identically (non-IID). This causes local drift, hindering global model convergence. To address this, we introduce Federated Learning with Feedback Alignment (FLFA), a novel framework that integrates feedback alignment into FL. FLFA uses the global model's weights as a shared feedback matrix during local training's backward pass, aligning local updates with the global model efficiently. This approach mitigates local drift with minimal additional computational cost and no extra communication overhead.
  Our theoretical analysis supports FLFA's design by showing how it alleviates local drift and demonstrates robust convergence for both local and global models. Empirical evaluations, including accuracy comparisons and measurements of local drift, further illustrate that FLFA can enhance other FL methods demonstrating its effectiveness.

</details>


### [328] [GradID: Adversarial Detection via Intrinsic Dimensionality of Gradients](https://arxiv.org/abs/2512.12827)
*Mohammad Mahdi Razmjoo,Mohammad Mahdi Sharifian,Saeed Bagheri Shouraki*

Main category: cs.LG

TL;DR: The paper proposes using Intrinsic Dimensionality (ID) of model gradient parameters to detect adversarial examples, showing consistent differences between natural and adversarial data across various datasets and attacks.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks are vulnerable to adversarial attacks where small perturbations cause incorrect predictions, posing serious risks in safety-critical applications like medical diagnosis and autonomous driving. Robust detection of such attacks is essential for reliable AI systems.

Method: The method analyzes geometric properties of the model's input loss landscape by examining the Intrinsic Dimensionality (ID) of gradient parameters. ID quantifies the minimal coordinates needed to describe data points on their underlying manifold. The approach reveals distinct ID differences between natural and adversarial data, forming the basis for detection.

Result: The method achieves high efficacy in batch-wise detection on MNIST and SVHN, and establishes state-of-the-art results in individual-sample detection on challenging benchmarks like CIFAR-10 and MS COCO. It significantly outperforms existing methods against various attacks (including CW and AutoAttack), achieving detection rates consistently above 92% on CIFAR-10.

Conclusion: Intrinsic dimensionality serves as a powerful fingerprint for adversarial detection across diverse datasets and attack strategies. The geometric approach demonstrates robustness and effectiveness, highlighting ID as a reliable indicator for distinguishing adversarial examples from natural data.

Abstract: Despite their remarkable performance, deep neural networks exhibit a critical vulnerability: small, often imperceptible, adversarial perturbations can lead to drastically altered model predictions. Given the stringent reliability demands of applications such as medical diagnosis and autonomous driving, robust detection of such adversarial attacks is paramount. In this paper, we investigate the geometric properties of a model's input loss landscape. We analyze the Intrinsic Dimensionality (ID) of the model's gradient parameters, which quantifies the minimal number of coordinates required to describe the data points on their underlying manifold. We reveal a distinct and consistent difference in the ID for natural and adversarial data, which forms the basis of our proposed detection method. We validate our approach across two distinct operational scenarios. First, in a batch-wise context for identifying malicious data groups, our method demonstrates high efficacy on datasets like MNIST and SVHN. Second, in the critical individual-sample setting, we establish new state-of-the-art results on challenging benchmarks such as CIFAR-10 and MS COCO. Our detector significantly surpasses existing methods against a wide array of attacks, including CW and AutoAttack, achieving detection rates consistently above 92\% on CIFAR-10. The results underscore the robustness of our geometric approach, highlighting that intrinsic dimensionality is a powerful fingerprint for adversarial detection across diverse datasets and attack strategies.

</details>


### [329] [On-Device Continual Learning for Unsupervised Visual Anomaly Detection in Dynamic Manufacturing](https://arxiv.org/abs/2512.13497)
*Haoyu Ren,Kay Koehle,Kirill Dorofeev,Darko Anicic*

Main category: cs.LG

TL;DR: On-device continual learning for visual anomaly detection that enables rapid adaptation to product changes with minimal memory usage, achieving 12% AUROC improvement and 80% memory reduction.


<details>
  <summary>Details</summary>
Motivation: Address challenges in dynamic manufacturing: frequent product changes requiring rapid model updates, limited edge hardware resources, and scarcity of both normal and anomalous training data for new product variations.

Method: Extends PatchCore with on-device continual learning using lightweight feature extractor and incremental coreset update mechanism based on k-center selection for memory-efficient adaptation from limited data.

Result: 12% AUROC improvement over baseline, 80% reduction in memory usage, and faster training compared to batch retraining in industrial use case emulating flexible production with frequent variant changes.

Conclusion: Method delivers accurate, resource-efficient, and adaptive visual anomaly detection suitable for dynamic smart manufacturing, eliminating costly cloud retraining while enabling rapid adaptation to product changes.

Abstract: In modern manufacturing, Visual Anomaly Detection (VAD) is essential for automated inspection and consistent product quality. Yet, increasingly dynamic and flexible production environments introduce key challenges: First, frequent product changes in small-batch and on-demand manufacturing require rapid model updates. Second, legacy edge hardware lacks the resources to train and run large AI models. Finally, both anomalous and normal training data are often scarce, particularly for newly introduced product variations. We investigate on-device continual learning for unsupervised VAD with localization, extending the PatchCore to incorporate online learning for real-world industrial scenarios. The proposed method leverages a lightweight feature extractor and an incremental coreset update mechanism based on k-center selection, enabling rapid, memory-efficient adaptation from limited data while eliminating costly cloud retraining. Evaluations on an industrial use case are conducted using a testbed designed to emulate flexible production with frequent variant changes in a controlled environment. Our method achieves a 12% AUROC improvement over the baseline, an 80% reduction in memory usage, and faster training compared to batch retraining. These results confirm that our method delivers accurate, resource-efficient, and adaptive VAD suitable for dynamic and smart manufacturing.

</details>


### [330] [Image Diffusion Preview with Consistency Solver](https://arxiv.org/abs/2512.13592)
*Fu-Yun Wang,Hao Zhou,Liangzhe Yuan,Sanghyun Woo,Boqing Gong,Bohyung Han,Ming-Hsuan Yang,Han Zhang,Yukun Zhu,Ting Liu,Long Zhao*

Main category: cs.LG

TL;DR: Diffusion Preview introduces a preview-and-refine workflow using rapid low-step sampling for user evaluation, with ConsistencySolver - a trainable high-order solver optimized via RL that improves preview quality and consistency.


<details>
  <summary>Details</summary>
Motivation: Slow inference of image diffusion models degrades interactive user experience. Existing acceleration methods fail to deliver high-quality previews or ensure consistency between previews and final outputs.

Method: Proposes ConsistencySolver, derived from general linear multistep methods, a lightweight trainable high-order solver optimized via Reinforcement Learning to enhance preview quality and consistency in low-step scenarios.

Result: Achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, outperforms distillation baselines, and reduces overall user interaction time by nearly 50% while maintaining generation quality.

Conclusion: ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows in interactive image generation applications.

Abstract: The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.

</details>


### [331] [From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves](https://arxiv.org/abs/2512.13641)
*Gabriel Vitorino de Andrade,Saulo Roberto dos Santos,Itallo Patrick Castro Alves da Silva,Emanuel Adler Medeiros Pereira,Erick de Andrade Barboza*

Main category: cs.LG

TL;DR: Proposes methodology to evaluate CNN robustness for mango leaf disease diagnosis under adverse conditions using corrupted dataset, finding lightweight specialized models outperform complex architectures in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Despite mango's global importance, there's a lack of robustness studies for disease diagnosis models. Real-world challenges like image corruptions (noise, blurring, weather variations) require validation of AI model reliability, especially for agricultural applications in regions with technological limitations.

Method: Adapted MangoLeafDB dataset to create MangoLeafDB-C with 19 types of artificial corruptions at five severity levels. Benchmarked five CNN architectures: ResNet-50, ResNet-101, VGG-16, Xception, and lightweight LCNN (specifically designed for mango leaf diagnosis). Used F1 score, corruption error (CE), and relative mean corruption error (relative mCE) metrics.

Result: LCNN outperformed complex models in real-world corruptions like Defocus Blur and Motion Blur, achieving the lowest mCE. Modern architectures (e.g., ResNet-101) showed significant performance degradation in corrupted scenarios despite high accuracy under ideal conditions.

Conclusion: Lightweight specialized models are more suitable for real-world agricultural applications on edge devices where robustness and efficiency are critical. The study emphasizes the need to incorporate robustness assessments in developing intelligent systems for agriculture, particularly in technologically limited regions.

Abstract: The validation and verification of artificial intelligence (AI) models through robustness assessment are essential to guarantee the reliable performance of intelligent systems facing real-world challenges, such as image corruptions including noise, blurring, and weather variations. Despite the global importance of mango (Mangifera indica L.), there is a lack of studies on the robustness of models for the diagnosis of disease in its leaves. This paper proposes a methodology to evaluate convolutional neural networks (CNNs) under adverse conditions. We adapted the MangoLeafDB dataset, generating MangoLeafDB-C with 19 types of artificial corruptions at five severity levels. We conducted a benchmark comparing five architectures: ResNet-50, ResNet-101, VGG-16, Xception, and LCNN (the latter being a lightweight architecture designed specifically for mango leaf diagnosis). The metrics include the F1 score, the corruption error (CE) and the relative mean corruption error (relative mCE). The results show that LCNN outperformed complex models in corruptions that can be present in real-world scenarios such as Defocus Blur, Motion Blur, while also achieving the lowest mCE. Modern architectures (e.g., ResNet-101) exhibited significant performance degradation in corrupted scenarios, despite their high accuracy under ideal conditions. These findings suggest that lightweight and specialized models may be more suitable for real-world applications in edge devices, where robustness and efficiency are critical. The study highlights the need to incorporate robustness assessments in the development of intelligent systems for agriculture, particularly in regions with technological limitations.

</details>


### [332] [Directional Textual Inversion for Personalized Text-to-Image Generation](https://arxiv.org/abs/2512.13672)
*Kunhee Kim,NaHyeon Park,Kibeom Hong,Hyunjung Shim*

Main category: cs.LG

TL;DR: DTI fixes TI's embedding norm inflation problem by optimizing only direction on unit hypersphere, improving prompt faithfulness while maintaining subject similarity.


<details>
  <summary>Details</summary>
Motivation: Textual Inversion (TI) fails on complex prompts due to embedding norm inflation, where learned tokens drift to out-of-distribution magnitudes, degrading prompt conditioning in pre-norm Transformers.

Method: Directional Textual Inversion (DTI) fixes embedding magnitude to in-distribution scale and optimizes only direction on unit hypersphere via Riemannian SGD, using MAP with von Mises-Fisher prior for constant-direction prior gradient.

Result: DTI improves text fidelity over TI and TI variants while maintaining subject similarity, and enables smooth, semantically coherent interpolation between learned concepts (slerp) which standard TI lacks.

Conclusion: Direction-only optimization on hypersphere is a robust and scalable path for prompt-faithful personalization, addressing TI's fundamental norm inflation problem.

Abstract: Textual Inversion (TI) is an efficient approach to text-to-image personalization but often fails on complex prompts. We trace these failures to embedding norm inflation: learned tokens drift to out-of-distribution magnitudes, degrading prompt conditioning in pre-norm Transformers. Empirically, we show semantics are primarily encoded by direction in CLIP token space, while inflated norms harm contextualization; theoretically, we analyze how large magnitudes attenuate positional information and hinder residual updates in pre-norm blocks. We propose Directional Textual Inversion (DTI), which fixes the embedding magnitude to an in-distribution scale and optimizes only direction on the unit hypersphere via Riemannian SGD. We cast direction learning as MAP with a von Mises-Fisher prior, yielding a constant-direction prior gradient that is simple and efficient to incorporate. Across personalization tasks, DTI improves text fidelity over TI and TI-variants while maintaining subject similarity. Crucially, DTI's hyperspherical parameterization enables smooth, semantically coherent interpolation between learned concepts (slerp), a capability that is absent in standard TI. Our findings suggest that direction-only optimization is a robust and scalable path for prompt-faithful personalization.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [333] [AutoMV: An Automatic Multi-Agent System for Music Video Generation](https://arxiv.org/abs/2512.12196)
*Xiaoxuan Tang,Xinping Lei,Chaoran Zhu,Shiyun Chen,Ruibin Yuan,Yizhi Li,Changjae Oh,Ge Zhang,Wenhao Huang,Emmanouil Benetos,Yang Liu,Jiaheng Liu,Yinghao Ma*

Main category: cs.MM

TL;DR: AutoMV is a multi-agent system that generates full music videos from songs by extracting musical features, creating scripts/characters, generating scenes, and verifying output, outperforming existing methods and narrowing the gap to professional human-made videos.


<details>
  <summary>Details</summary>
Motivation: Existing Music-to-Video (M2V) generation methods produce short, disjointed clips that fail to align with musical structure, beats, or lyrics, and lack temporal consistency for full-length songs.

Method: AutoMV uses a multi-agent system: 1) extracts musical attributes (structure, vocals, lyrics), 2) screenwriter and director agents create scripts/characters/camera instructions, 3) generates keyframes and video scenes, 4) verifier agent evaluates output for coherence.

Result: AutoMV significantly outperforms current baselines across all four evaluation categories (Music Content, Technical, Post-production, Art) and narrows the gap to professional human-directed music videos.

Conclusion: AutoMV demonstrates effective full-length music video generation through multi-agent collaboration, though large multimodal models as automatic judges still lag behind human experts, indicating room for future improvement.

Abstract: Music-to-Video (M2V) generation for full-length songs faces significant challenges. Existing methods produce short, disjointed clips, failing to align visuals with musical structure, beats, or lyrics, and lack temporal consistency. We propose AutoMV, a multi-agent system that generates full music videos (MVs) directly from a song. AutoMV first applies music processing tools to extract musical attributes, such as structure, vocal tracks, and time-aligned lyrics, and constructs these features as contextual inputs for following agents. The screenwriter Agent and director Agent then use this information to design short script, define character profiles in a shared external bank, and specify camera instructions. Subsequently, these agents call the image generator for keyframes and different video generators for "story" or "singer" scenes. A Verifier Agent evaluates their output, enabling multi-agent collaboration to produce a coherent longform MV. To evaluate M2V generation, we further propose a benchmark with four high-level categories (Music Content, Technical, Post-production, Art) and twelve ine-grained criteria. This benchmark was applied to compare commercial products, AutoMV, and human-directed MVs with expert human raters: AutoMV outperforms current baselines significantly across all four categories, narrowing the gap to professional MVs. Finally, we investigate using large multimodal models as automatic MV judges; while promising, they still lag behind human expert, highlighting room for future work.

</details>


### [334] [JointAVBench: A Benchmark for Joint Audio-Visual Reasoning Evaluation](https://arxiv.org/abs/2512.12772)
*Jianghan Chao,Jianzhang Gao,Wenhui Tan,Yuchong Sun,Ruihua Song,Liyun Ru*

Main category: cs.MM

TL;DR: JointAVBench is a new benchmark for evaluating Omni-LLMs that requires strict joint audio-visual understanding across multiple dimensions, created via automated pipeline to overcome annotation costs.


<details>
  <summary>Details</summary>
Motivation: Existing video understanding benchmarks lack comprehensive coverage of multi-modal dependency (requiring both audio and video), diverse audio types, and varying scene spans, limiting proper evaluation of Omni-LLMs.

Method: Proposed automated pipeline using state-of-the-art vision-LLMs, audio-LLMs, and general-purpose LLMs to synthesize questions and answers that strictly require joint audio-visual understanding. Covers 5 cognitive dimensions, 4 audio types (speech, sound events, music, vocal traits), and 3 scene spans (single-, cross-, full-scene).

Result: Best-performing Omni-LLM achieves only 62.6% average accuracy, outperforming uni-modal baselines but showing significant room for improvement, especially in cross-scene reasoning.

Conclusion: JointAVBench addresses critical gaps in Omni-LLM evaluation, revealing current models' limitations in joint audio-visual understanding and providing a comprehensive benchmark for future development.

Abstract: Understanding videos inherently requires reasoning over both visual and auditory information. To properly evaluate Omni-Large Language Models (Omni-LLMs), which are capable of processing multi-modal information including vision and audio, an effective benchmark must comprehensively cover three key aspects: (1) multi-modal dependency (i.e., questions that cannot be answered using vision or audio alone), (2) diverse audio information types (e.g., speech, sound events), and (3) varying scene spans. However, existing datasets fall short in one or more of these dimensions, limiting strict and comprehensive evaluation. To address this gap, we introduce JointAVBench, a novel benchmark with strict audio-video correlation, spanning five cognitive dimensions, four audio information types (speech, sound events, music, vocal traits), and three scene spans (single-, cross-, and full-scene). Given the high cost of manual annotation, we propose an automated pipeline that leverages state-of-the-art vision-LLMs, audio-LLMs, and general-purpose LLMs to synthesize questions and answers that strictly require joint audio-visual understanding. We evaluate leading vision-only, audio-only, and Omni-LLMs on our dataset. Results show that even the best-performing Omni-LLM achieves an average accuracy of only 62.6\%, outperforming uni-modal baselines but revealing substantial room for improvement, especially in cross-scene reasoning.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 112]
- [cs.CV](#cs.CV) [Total: 195]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.DB](#cs.DB) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.RO](#cs.RO) [Total: 7]
- [eess.IV](#eess.IV) [Total: 8]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.CY](#cs.CY) [Total: 4]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 22]
- [cs.AI](#cs.AI) [Total: 9]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [GreenTEA: Gradient Descent with Topic-modeling and Evolutionary Auto-prompting](https://arxiv.org/abs/2508.16603)
*Zheng Dong,Luming Shang,Gabriela Olinto*

Main category: cs.CL

TL;DR: GreenTEA is an agentic LLM workflow that balances exploration and exploitation for automatic prompt optimization using topic modeling and genetic algorithms.


<details>
  <summary>Details</summary>
Motivation: Manual prompt crafting is labor-intensive and requires domain expertise, while existing automatic methods are either computationally expensive or risk suboptimal results due to inefficient search strategies.

Method: Uses collaborative agent team: analyzing agent identifies error patterns via topic modeling, generation agent revises prompts to address deficiencies, guided by genetic algorithm framework with crossover and mutation operations.

Result: Superior performance against human-engineered prompts and state-of-the-art methods on benchmark datasets covering logical reasoning, quantitative reasoning, commonsense, and ethical decision-making.

Conclusion: GreenTEA effectively balances exploration and exploitation in prompt optimization, achieving better performance while addressing computational efficiency concerns of existing methods.

Abstract: High-quality prompts are crucial for Large Language Models (LLMs) to achieve
exceptional performance. However, manually crafting effective prompts is
labor-intensive and demands significant domain expertise, limiting its
scalability. Existing automatic prompt optimization methods either extensively
explore new prompt candidates, incurring high computational costs due to
inefficient searches within a large solution space, or overly exploit feedback
on existing prompts, risking suboptimal optimization because of the complex
prompt landscape. To address these challenges, we introduce GreenTEA, an
agentic LLM workflow for automatic prompt optimization that balances candidate
exploration and knowledge exploitation. It leverages a collaborative team of
agents to iteratively refine prompts based on feedback from error samples. An
analyzing agent identifies common error patterns resulting from the current
prompt via topic modeling, and a generation agent revises the prompt to
directly address these key deficiencies. This refinement process is guided by a
genetic algorithm framework, which simulates natural selection by evolving
candidate prompts through operations such as crossover and mutation to
progressively optimize model performance. Extensive numerical experiments
conducted on public benchmark datasets suggest the superior performance of
GreenTEA against human-engineered prompts and existing state-of-the-arts for
automatic prompt optimization, covering logical and quantitative reasoning,
commonsense, and ethical decision-making.

</details>


### [2] [Cognitive Decision Routing in Large Language Models: When to Think Fast, When to Think Slow](https://arxiv.org/abs/2508.16636)
*Y. Du,C. Guo,W. Wang,G. Tang*

Main category: cs.CL

TL;DR: CDR framework dynamically selects reasoning strategies for LLMs based on query analysis, reducing computation by 34% while improving performance on complex tasks.


<details>
  <summary>Details</summary>
Motivation: Address LLMs' challenge in choosing between rapid intuitive responses vs deliberate reasoning, inspired by Kahneman's dual-process theory and cognitive biases.

Method: Meta-cognitive layer analyzes query complexity through correlation strength, domain boundaries, stakeholder multiplicity, and uncertainty levels to determine optimal reasoning strategy.

Result: 34% computational cost reduction compared to uniform deep reasoning, 23% improvement in consistency, and 18% better accuracy on expert-level evaluations.

Conclusion: Bridges cognitive science with AI design, providing principled adaptive reasoning that optimizes performance while reducing computational overhead.

Abstract: Large Language Models (LLMs) face a fundamental challenge in deciding when to
rely on rapid, intuitive responses versus engaging in slower, more deliberate
reasoning. Inspired by Daniel Kahneman's dual-process theory and his insights
on human cognitive biases, we propose a novel Cognitive Decision Routing (CDR)
framework that dynamically determines the appropriate reasoning strategy based
on query characteristics. Our approach addresses the current limitations where
models either apply uniform reasoning depth or rely on computationally
expensive methods for all queries. We introduce a meta-cognitive layer that
analyzes query complexity through multiple dimensions: correlation strength
between given information and required conclusions, domain boundary crossings,
stakeholder multiplicity, and uncertainty levels. Through extensive experiments
on diverse reasoning tasks, we demonstrate that CDR achieves superior
performance while reducing computational costs by 34\% compared to uniform deep
reasoning approaches. Our framework shows particular strength in professional
judgment tasks, achieving 23\% improvement in consistency and 18\% better
accuracy on expert-level evaluations. This work bridges cognitive science
principles with practical AI system design, offering a principled approach to
adaptive reasoning in LLMs.

</details>


### [3] [Trust but Verify! A Survey on Verification Design for Test-time Scaling](https://arxiv.org/abs/2508.16665)
*V Venktesh,Mandeep rathee,Avishek Anand*

Main category: cs.CL

TL;DR: Survey paper on test-time scaling verifiers for LLMs, covering diverse verification approaches, training mechanisms, and their utility in improving model performance during inference.


<details>
  <summary>Details</summary>
Motivation: Despite widespread adoption of verifiers in test-time scaling, there is no comprehensive collection, categorization, or discussion of diverse verification approaches and their training mechanisms.

Method: The paper conducts a systematic survey of literature on verifier-based test-time scaling approaches, categorizing different verification types (prompt-based, fine-tuned discriminative/generative models) and their training mechanisms.

Result: Presents a unified view of verifier training, types, and utility in test-time scaling, providing a comprehensive repository of verification approaches.

Conclusion: Verifier-based test-time scaling emerges as a superior approach for parameter-free performance scaling at inference time, offering significant performance gains through diligent exploration of solution spaces.

Abstract: Test-time scaling (TTS) has emerged as a new frontier for scaling the
performance of Large Language Models. In test-time scaling, by using more
computational resources during inference, LLMs can improve their reasoning
process and task performance. Several approaches have emerged for TTS such as
distilling reasoning traces from another model or exploring the vast decoding
search space by employing a verifier. The verifiers serve as reward models that
help score the candidate outputs from the decoding process to diligently
explore the vast solution space and select the best outcome. This paradigm
commonly termed has emerged as a superior approach owing to parameter free
scaling at inference time and high performance gains. The verifiers could be
prompt-based, fine-tuned as a discriminative or generative model to verify
process paths, outcomes or both. Despite their widespread adoption, there is no
detailed collection, clear categorization and discussion of diverse
verification approaches and their training mechanisms. In this survey, we cover
the diverse approaches in the literature and present a unified view of verifier
training, types and their utility in test-time scaling. Our repository can be
found at
https://github.com/elixir-research-group/Verifierstesttimescaling.github.io.

</details>


### [4] [Do Cognitively Interpretable Reasoning Traces Improve LLM Performance?](https://arxiv.org/abs/2508.16695)
*Siddhant Bhambri,Upasana Biswas,Subbarao Kambhampati*

Main category: cs.CL

TL;DR: CoT reasoning traces don't need to be interpretable to improve LLM performance - best performance comes from least interpretable traces.


<details>
  <summary>Details</summary>
Motivation: To investigate whether Chain-of-Thought reasoning traces must be interpretable to enhance LLM task performance, challenging the common assumption that traces should be semantically meaningful.

Method: Supervised fine-tuning of LLaMA and Qwen models on four types of reasoning traces (DeepSeek R1 traces, LLM-generated summaries, LLM-generated explanations, and algorithmically generated verifiable traces) in Open Book QA domain, plus human study with 100 participants rating interpretability.

Result: Fine-tuning on DeepSeek R1 traces yielded strongest performance but was judged least interpretable by human participants, revealing a mismatch between interpretability and performance.

Conclusion: Intermediate tokens should be decoupled from end user interpretability as interpretable traces are not necessary for enhancing LLM task performance.

Abstract: Recent progress in reasoning-oriented Large Language Models (LLMs) has been
driven by introducing Chain-of-Thought (CoT) traces, where models generate
intermediate reasoning traces before producing an answer. These traces, as in
DeepSeek R1, are not only used to guide inference but also serve as supervision
signals for distillation into smaller models. A common but often implicit
assumption is that CoT traces should be semantically meaningful and
interpretable to the end user. While recent research questions the need for
semantic nature of these traces, in this paper, we ask: ``\textit{Must CoT
reasoning traces be interpretable to enhance LLM task performance?}" We
investigate this question in the Open Book Question-Answering domain by
supervised fine-tuning LLaMA and Qwen models on four types of reasoning traces:
(1) DeepSeek R1 traces, (2) LLM-generated summaries of R1 traces, (3)
LLM-generated post-hoc explanations of R1 traces, and (4) algorithmically
generated verifiably correct traces. To quantify the trade-off between
interpretability and performance, we further conduct a human-subject study with
100 participants rating the interpretability of each trace type. Our results
reveal a striking mismatch: while fine-tuning on R1 traces yields the strongest
performance, participants judged these traces to be the least interpretable.
These findings suggest that it is useful to decouple intermediate tokens from
end user interpretability.

</details>


### [5] [QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting](https://arxiv.org/abs/2508.16697)
*Nicole Cho,William Watson,Alec Koppel,Sumitra Ganesh,Manuela Veloso*

Main category: cs.CL

TL;DR: QueryBandits is a bandit framework that proactively rewrites queries to reduce LLM hallucinations by optimizing 17 linguistic features, achieving 87.5% win rate over baseline and outperforming static prompting methods.


<details>
  <summary>Details</summary>
Motivation: Current hallucination mitigation focuses on after-the-fact filtering rather than shaping queries that trigger hallucinations. There's a need for proactive approaches to steer LLMs away from generating hallucinations.

Method: QueryBandits uses a bandit framework with Thompson Sampling to design rewrite strategies that maximize a reward model based on 17 linguistic features of input queries, enabling contextual query rewriting.

Result: Achieved 87.5% win rate over no-rewrite baseline, outperformed zero-shot static prompting by 42.6-60.3% across 13 QA benchmarks with 1,050 perturbed queries per dataset. Static rewrites were found to sometimes worsen hallucination.

Conclusion: QueryBandits effectively mitigates hallucinations through contextual query rewriting without retraining, demonstrating that no single rewrite strategy works for all queries and that guided semantic feature exploitation can significantly shift output behavior.

Abstract: Advanced reasoning capabilities in Large Language Models (LLMs) have caused
higher hallucination prevalence; yet most mitigation work focuses on
after-the-fact filtering rather than shaping the queries that trigger them. We
introduce QueryBandits, a bandit framework that designs rewrite strategies to
maximize a reward model, that encapsulates hallucination propensity based upon
the sensitivities of 17 linguistic features of the input query-and therefore,
proactively steer LLMs away from generating hallucinations. Across 13 diverse
QA benchmarks and 1,050 lexically perturbed queries per dataset, our top
contextual QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a
no-rewrite baseline and also outperforms zero-shot static prompting
("paraphrase" or "expand") by 42.6% and 60.3% respectively. Therefore, we
empirically substantiate the effectiveness of QueryBandits in mitigating
hallucination via the intervention that takes the form of a query rewrite.
Interestingly, certain static prompting strategies, which constitute a
considerable number of current query rewriting literature, have a higher
cumulative regret than the no-rewrite baseline, signifying that static rewrites
can worsen hallucination. Moreover, we discover that the converged per-arm
regression feature weight vectors substantiate that there is no single rewrite
strategy optimal for all queries. In this context, guided rewriting via
exploiting semantic features with QueryBandits can induce significant shifts in
output behavior through forward-pass mechanisms, bypassing the need for
retraining or gradient-based adaptation.

</details>


### [6] [Assessing Consciousness-Related Behaviors in Large Language Models Using the Maze Test](https://arxiv.org/abs/2508.16705)
*Rui A. Pimenta,Tim Schlippe,Kristina Schaaff*

Main category: cs.CL

TL;DR: LLMs show consciousness-like behaviors in maze navigation tests but lack integrated self-awareness, with reasoning-capable models performing better but struggling to maintain coherent self-models throughout solutions.


<details>
  <summary>Details</summary>
Motivation: To investigate whether Large Language Models exhibit consciousness-like behaviors by testing them on maze navigation from a first-person perspective, which probes key consciousness-associated characteristics.

Method: Used the Maze Test to evaluate 12 leading LLMs across zero-shot, one-shot, and few-shot learning scenarios, assessing spatial awareness, perspective-taking, goal-directed behavior, and temporal sequencing based on 13 synthesized consciousness characteristics.

Result: Reasoning-capable LLMs outperformed standard versions, with Gemini 2.0 Pro achieving 52.9% Complete Path Accuracy and DeepSeek-R1 reaching 80.5% Partial Path Accuracy. The performance gap indicates LLMs struggle to maintain coherent self-models throughout solutions.

Conclusion: While LLMs demonstrate progress in consciousness-related behaviors through reasoning mechanisms, they lack the integrated, persistent self-awareness that characterizes true consciousness.

Abstract: We investigate consciousness-like behaviors in Large Language Models (LLMs)
using the Maze Test, challenging models to navigate mazes from a first-person
perspective. This test simultaneously probes spatial awareness,
perspective-taking, goal-directed behavior, and temporal sequencing-key
consciousness-associated characteristics. After synthesizing consciousness
theories into 13 essential characteristics, we evaluated 12 leading LLMs across
zero-shot, one-shot, and few-shot learning scenarios. Results showed
reasoning-capable LLMs consistently outperforming standard versions, with
Gemini 2.0 Pro achieving 52.9% Complete Path Accuracy and DeepSeek-R1 reaching
80.5% Partial Path Accuracy. The gap between these metrics indicates LLMs
struggle to maintain coherent self-models throughout solutions -- a fundamental
consciousness aspect. While LLMs show progress in consciousness-related
behaviors through reasoning mechanisms, they lack the integrated, persistent
self-awareness characteristic of consciousness.

</details>


### [7] [Sparse and Dense Retrievers Learn Better Together: Joint Sparse-Dense Optimization for Text-Image Retrieval](https://arxiv.org/abs/2508.16707)
*Jonghyun Song,Youngjune Lee,Gyu-Hwung Cho,Ilhyeon Song,Saehun Kim,Yohan Jo*

Main category: cs.CL

TL;DR: Proposes a bi-directional learning framework using self-knowledge distillation to enhance both dense and sparse representations in vision-language pretrained models, achieving sparse retrieval performance comparable to dense models while maintaining efficiency benefits.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal sparse retrieval methods rely on computationally expensive contrastive pre-training or distillation from frozen dense models, limiting mutual enhancement between dense and sparse representations.

Method: Uses self-knowledge distillation with an integrated similarity score (weighted sum of dense and sparse similarities) as a shared teacher signal. Fine-tunes only the final layer of dense encoder and sparse projection head for efficiency.

Result: Outperforms existing sparse baselines on MSCOCO and Flickr30k, achieving performance comparable to or surpassing dense counterparts while retaining sparse model benefits.

Conclusion: The proposed framework enables effective bi-directional learning between dense and sparse representations, demonstrating that sparse retrievers can achieve dense-level performance with better efficiency and interpretability.

Abstract: Vision-Language Pretrained (VLP) models have achieved impressive performance
on multimodal tasks, including text-image retrieval, based on dense
representations. Meanwhile, Learned Sparse Retrieval (LSR) has gained traction
in text-only settings due to its interpretability and efficiency with fast
term-based lookup via inverted indexes. Inspired by these advantages, recent
work has extended LSR to the multimodal domain. However, these methods often
rely on computationally expensive contrastive pre-training, or distillation
from a frozen dense model, which limits the potential for mutual enhancement.
To address these limitations, we propose a simple yet effective framework that
enables bi-directional learning between dense and sparse representations
through Self-Knowledge Distillation. This bi-directional learning is achieved
using an integrated similarity score-a weighted sum of dense and sparse
similarities-which serves as a shared teacher signal for both representations.
To ensure efficiency, we fine-tune the final layer of the dense encoder and the
sparse projection head, enabling easy adaptation of any existing VLP model.
Experiments on MSCOCO and Flickr30k demonstrate that our sparse retriever not
only outperforms existing sparse baselines, but also achieves performance
comparable to-or even surpassing-its dense counterparts, while retaining the
benefits of sparse models.

</details>


### [8] [Error Reflection Prompting: Can Large Language Models Successfully Understand Errors?](https://arxiv.org/abs/2508.16729)
*Jason Li,Lauren Yraola,Kevin Zhu,Sean O'Brien*

Main category: cs.CL

TL;DR: Error Reflection Prompting (ERP) enhances Chain-of-thought reasoning by adding error recognition and correction capabilities, enabling models to identify and avoid mistakes for more robust problem-solving.


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought (CoT) prompting lacks reflection and error correction abilities, causing models to perpetuate mistakes. Inspired by human error correction capabilities, the authors aim to enhance reasoning with error recognition.

Method: ERP builds upon CoT by adding three components: incorrect answer generation, error recognition, and correct answer production. The method uses automated ERP generation to create error outlines that help models identify error types and problematic steps.

Result: ERP serves as a versatile supplement to conventional CoT, contributing to more robust reasoning abilities and increased interpretability in how models reach their conclusions and identify errors.

Conclusion: Error Reflection Prompting successfully enhances language model reasoning by integrating error recognition and correction into the reasoning chain, improving scalability, reliability, and interpretability of model outputs.

Abstract: Prompting methods for language models, such as Chain-of-thought (CoT),
present intuitive step-by-step processes for problem solving. These
methodologies aim to equip models with a better understanding of the correct
procedures for addressing a given task. Despite these advancements, CoT lacks
the ability of reflection and error correction, potentially causing a model to
perpetuate mistakes and errors. Therefore, inspired by the human ability for
said tasks, we propose Error Reflection Prompting (ERP) to further enhance
reasoning in language models. Building upon CoT, ERP is a method comprised of
an incorrect answer, error recognition, and a correct answer. This process
enables the model to recognize types of errors and the steps that lead to
incorrect answers, allowing the model to better discern which steps to avoid
and which to take. The model is able to generate the error outlines itself with
automated ERP generation, allowing for error recognition and correction to be
integrated into the reasoning chain and produce scalability and reliability in
the process. The results demonstrate that ERP serves as a versatile supplement
to conventional CoT, ultimately contributing to more robust and capable
reasoning abilities along with increased interpretability in how models
ultimately reach their errors.

</details>


### [9] [GAICo: A Deployed and Extensible Framework for Evaluating Diverse and Multimodal Generative AI Outputs](https://arxiv.org/abs/2508.16753)
*Nitin Gupta,Pallav Koppisetti,Kausik Lakkaraju,Biplav Srivastava*

Main category: cs.CL

TL;DR: GAICo is an open-source Python library that provides standardized evaluation framework for Generative AI outputs across text, structured data, and multimedia, addressing fragmentation in AI evaluation methods.


<details>
  <summary>Details</summary>
Motivation: The proliferation of Generative AI into high-stakes domains requires robust evaluation methods, but practitioners use ad-hoc scripts due to unsuitable metrics for specialized outputs and multi-modal comparisons, hindering comparability and development.

Method: GAICo offers a unified, extensible framework with reference-based metrics for unstructured text, structured data formats, and multimedia (images, audio). It features high-level API for end-to-end analysis and direct metric access for granular control.

Result: The tool has been downloaded over 13K times since its PyPI release in June 2025, demonstrating growing community interest. A case study showed its utility in evaluating multi-modal AI Travel Assistant pipelines.

Conclusion: GAICo empowers researchers and developers to efficiently assess system performance, make evaluation reproducible, improve development velocity, and build more trustworthy AI systems, enabling faster and safer AI deployment.

Abstract: The rapid proliferation of Generative AI (GenAI) into diverse, high-stakes
domains necessitates robust and reproducible evaluation methods. However,
practitioners often resort to ad-hoc, non-standardized scripts, as common
metrics are often unsuitable for specialized, structured outputs (e.g.,
automated plans, time-series) or holistic comparison across modalities (e.g.,
text, audio, and image). This fragmentation hinders comparability and slows AI
system development. To address this challenge, we present GAICo (Generative AI
Comparator): a deployed, open-source Python library that streamlines and
standardizes GenAI output comparison. GAICo provides a unified, extensible
framework supporting a comprehensive suite of reference-based metrics for
unstructured text, specialized structured data formats, and multimedia (images,
audio). Its architecture features a high-level API for rapid, end-to-end
analysis, from multi-model comparison to visualization and reporting, alongside
direct metric access for granular control. We demonstrate GAICo's utility
through a detailed case study evaluating and debugging complex, multi-modal AI
Travel Assistant pipelines. GAICo empowers AI researchers and developers to
efficiently assess system performance, make evaluation reproducible, improve
development velocity, and ultimately build more trustworthy AI systems,
aligning with the goal of moving faster and safer in AI deployment. Since its
release on PyPI in Jun 2025, the tool has been downloaded over 13K times,
across versions, by Aug 2025, demonstrating growing community interest.

</details>


### [10] [How Good are LLM-based Rerankers? An Empirical Analysis of State-of-the-Art Reranking Models](https://arxiv.org/abs/2508.16757)
*Abdelrahman Abdallah,Bhawna Piryani,Jamshid Mozafari,Mohammed Ali,Adam Jatowt*

Main category: cs.CL

TL;DR: Comprehensive evaluation of 22 reranking methods shows LLM-based approaches outperform on familiar queries but have varying generalization to novel queries, with lightweight models offering comparable efficiency.


<details>
  <summary>Details</summary>
Motivation: To systematically compare LLM-based rerankers with lightweight contextual and zero-shot approaches, determining performance disparities and understanding underlying causes, particularly for novel queries unseen by pretrained models.

Method: Evaluated 22 methods (40 variants) across TREC DL19, DL20, BEIR benchmarks and a novel dataset for unseen queries. Analyzed training data overlap, model architecture, and computational efficiency through controlled comparisons.

Result: LLM-based rerankers demonstrate superior performance on familiar queries but show varying generalization ability to novel queries. Lightweight models offer comparable efficiency. Query novelty significantly impacts reranking effectiveness.

Conclusion: Existing reranking approaches have limitations in handling novel queries, with LLM-based methods showing strong performance on familiar data but inconsistent generalization, while lightweight models provide efficient alternatives.

Abstract: In this work, we present a systematic and comprehensive empirical evaluation
of state-of-the-art reranking methods, encompassing large language model
(LLM)-based, lightweight contextual, and zero-shot approaches, with respect to
their performance in information retrieval tasks. We evaluate in total 22
methods, including 40 variants (depending on used LLM) across several
established benchmarks, including TREC DL19, DL20, and BEIR, as well as a novel
dataset designed to test queries unseen by pretrained models. Our primary goal
is to determine, through controlled and fair comparisons, whether a performance
disparity exists between LLM-based rerankers and their lightweight
counterparts, particularly on novel queries, and to elucidate the underlying
causes of any observed differences. To disentangle confounding factors, we
analyze the effects of training data overlap, model architecture, and
computational efficiency on reranking performance. Our findings indicate that
while LLM-based rerankers demonstrate superior performance on familiar queries,
their generalization ability to novel queries varies, with lightweight models
offering comparable efficiency. We further identify that the novelty of queries
significantly impacts reranking effectiveness, highlighting limitations in
existing approaches.
https://github.com/DataScienceUIBK/llm-reranking-generalization-study

</details>


### [11] [Toward Socially Aware Vision-Language Models: Evaluating Cultural Competence Through Multimodal Story Generation](https://arxiv.org/abs/2508.16762)
*Arka Mukherjee,Shreya Ghosh*

Main category: cs.CL

TL;DR: First comprehensive evaluation of Vision-Language Models' cultural competence through multimodal story generation, revealing both adaptation capabilities and concerning limitations across different architectures.


<details>
  <summary>Details</summary>
Motivation: As VLMs achieve widespread deployment across diverse cultural contexts, ensuring their cultural competence becomes critical for responsible AI systems. Prior work has evaluated cultural awareness in text-only models and object recognition, but no research has systematically assessed how VLMs adapt outputs when cultural identity cues are embedded in both textual prompts and visual inputs.

Method: Developed a novel multimodal framework that perturbs cultural identity and evaluates 5 contemporary VLMs on story generation tasks. Uses cross-modal evaluation with visual-semantic similarity metrics and human assessments.

Result: Significant cultural adaptation capabilities with rich culturally-specific vocabulary. However, cultural competence varies dramatically across architectures, some models exhibit inverse cultural alignment, and automated metrics show architectural bias contradicting human assessments. Visual-cultural understanding remains limited (28.7% within-nationality vs. 0.2% cross-nationality recall).

Conclusion: Establishes both the promise and challenges of cultural competence in multimodal AI, highlighting the need for improved cultural adaptation capabilities in VLMs across different architectures and evaluation metrics.

Abstract: As Vision-Language Models (VLMs) achieve widespread deployment across diverse
cultural contexts, ensuring their cultural competence becomes critical for
responsible AI systems. While prior work has evaluated cultural awareness in
text-only models and VLM object recognition tasks, no research has
systematically assessed how VLMs adapt outputs when cultural identity cues are
embedded in both textual prompts and visual inputs during generative tasks. We
present the first comprehensive evaluation of VLM cultural competence through
multimodal story generation, developing a novel multimodal framework that
perturbs cultural identity and evaluates 5 contemporary VLMs on a downstream
task: story generation. Our analysis reveals significant cultural adaptation
capabilities, with rich culturally-specific vocabulary spanning names, familial
terms, and geographic markers. However, we uncover concerning limitations:
cultural competence varies dramatically across architectures, some models
exhibit inverse cultural alignment, and automated metrics show architectural
bias contradicting human assessments. Cross-modal evaluation shows that
culturally distinct outputs are indeed detectable through visual-semantic
similarity (28.7% within-nationality vs. 0.2% cross-nationality recall), yet
visual-cultural understanding remains limited. In essence, we establish the
promise and challenges of cultural competence in multimodal AI. We publicly
release our codebase and data: https://github.com/ArkaMukherjee0/mmCultural

</details>


### [12] [Assess and Prompt: A Generative RL Framework for Improving Engagement in Online Mental Health Communities](https://arxiv.org/abs/2508.16788)
*Bhagesh Gaur,Karan Gupta,Aseem Srivastava,Manish Gupta,Md Shad Akhtar*

Main category: cs.CL

TL;DR: A framework that identifies missing support attributes in mental health posts and prompts users to enrich content, improving engagement through targeted question generation.


<details>
  <summary>Details</summary>
Motivation: Many posts in online mental health communities remain unanswered due to missing support attributes that signal the need for help, creating a gap in peer and expert support.

Method: Proposes MH-COPILOT, a reinforcement learning-based system with contextual attribute-span identification, intensity classification, controlled question generation via hierarchical taxonomy CueTaxo, and verifier for reward modeling.

Result: Empirical results across four language models show significant improvements in attribute elicitation and user engagement. Human evaluation validates effectiveness in real-world OMHC settings.

Conclusion: The framework successfully identifies missing support attributes and generates targeted prompts to elicit missing information, enhancing engagement in online mental health communities.

Abstract: Online Mental Health Communities (OMHCs) provide crucial peer and expert
support, yet many posts remain unanswered due to missing support attributes
that signal the need for help. We present a novel framework that identifies
these gaps and prompts users to enrich their posts, thereby improving
engagement. To support this, we introduce REDDME, a new dataset of 4,760 posts
from mental health subreddits annotated for the span and intensity of three key
support attributes: event what happened?, effect what did the user experience?,
and requirement what support they need?. Next, we devise a hierarchical
taxonomy, CueTaxo, of support attributes for controlled question generation.
Further, we propose MH-COPILOT, a reinforcement learning-based system that
integrates (a) contextual attribute-span identification, (b) support attribute
intensity classification, (c) controlled question generation via a hierarchical
taxonomy, and (d) a verifier for reward modeling. Our model dynamically
assesses posts for the presence/absence of support attributes, and generates
targeted prompts to elicit missing information. Empirical results across four
notable language models demonstrate significant improvements in attribute
elicitation and user engagement. A human evaluation further validates the
model's effectiveness in real-world OMHC settings.

</details>


### [13] [ReProCon: Scalable and Resource-Efficient Few-Shot Biomedical Named Entity Recognition](https://arxiv.org/abs/2508.16833)
*Jeongkyun Yoo,Nela Riddle,Andrew Hoblitzell*

Main category: cs.CL

TL;DR: ReProCon is a few-shot NER framework that uses multi-prototype modeling, cosine-contrastive learning, and Reptile meta-learning to handle data scarcity and class imbalance in biomedical domains, achieving near-BERT performance with lower memory usage.


<details>
  <summary>Details</summary>
Motivation: Biomedical NER faces challenges with data scarcity and imbalanced label distributions, especially for fine-grained entity types, requiring efficient few-shot learning approaches.

Method: Combines multi-prototype modeling to capture semantic variability, cosine-contrastive learning for interclass separation, and Reptile meta-learning for quick adaptation with little data. Uses lightweight fastText + BiLSTM encoder.

Result: Achieves macro-F1 score close to BERT baselines (99% of BERT performance), remains stable with 30% label budget, and only drops 7.8% in F1 when expanding from 19 to 50 categories, outperforming SpanProto and CONTaiNER.

Conclusion: ReProCon demonstrates state-of-the-art performance in resource-limited settings and is suitable for biomedical applications, though it faces challenges with label ambiguity.

Abstract: Named Entity Recognition (NER) in biomedical domains faces challenges due to
data scarcity and imbalanced label distributions, especially with fine-grained
entity types. We propose ReProCon, a novel few-shot NER framework that combines
multi-prototype modeling, cosine-contrastive learning, and Reptile
meta-learning to tackle these issues. By representing each category with
multiple prototypes, ReProCon captures semantic variability, such as synonyms
and contextual differences, while a cosine-contrastive objective ensures strong
interclass separation. Reptile meta-updates enable quick adaptation with little
data. Using a lightweight fastText + BiLSTM encoder with much lower memory
usage, ReProCon achieves a macro-$F_1$ score close to BERT-based baselines
(around 99 percent of BERT performance). The model remains stable with a label
budget of 30 percent and only drops 7.8 percent in $F_1$ when expanding from 19
to 50 categories, outperforming baselines such as SpanProto and CONTaiNER,
which see 10 to 32 percent degradation in Few-NERD. Ablation studies highlight
the importance of multi-prototype modeling and contrastive learning in managing
class imbalance. Despite difficulties with label ambiguity, ReProCon
demonstrates state-of-the-art performance in resource-limited settings, making
it suitable for biomedical applications.

</details>


### [14] [LLMs Learn Constructions That Humans Do Not Know](https://arxiv.org/abs/2508.16837)
*Jonathan Dunn,Mai Mohamed Eida*

Main category: cs.CL

TL;DR: LLMs hallucinate false grammatical constructions that humans don't recognize, and probing methods show confirmation bias that would falsely validate these non-existent structures.


<details>
  <summary>Details</summary>
Motivation: To investigate whether large language models hallucinate grammatical constructions that don't exist in human language and examine the confirmation bias in construction probing methods.

Method: Used behavioral probing with contextual embeddings and meta-linguistic probing with prompts to distinguish implicit vs explicit knowledge, then simulated hypothesis testing to see if false constructions would be confirmed.

Result: Models do hallucinate constructions, and simulated hypothesis testing showed high accuracy in confirming these false hypotheses, demonstrating confirmation bias in probing methods.

Conclusion: Construction probing methods suffer from confirmation bias, raising concerns about what unknown and incorrect syntactic knowledge LLMs possess that could mislead linguistic analysis.

Abstract: This paper investigates false positive constructions: grammatical structures
which an LLM hallucinates as distinct constructions but which human
introspection does not support. Both a behavioural probing task using
contextual embeddings and a meta-linguistic probing task using prompts are
included, allowing us to distinguish between implicit and explicit linguistic
knowledge. Both methods reveal that models do indeed hallucinate constructions.
We then simulate hypothesis testing to determine what would have happened if a
linguist had falsely hypothesized that these hallucinated constructions do
exist. The high accuracy obtained shows that such false hypotheses would have
been overwhelmingly confirmed. This suggests that construction probing methods
suffer from a confirmation bias and raises the issue of what unknown and
incorrect syntactic knowledge these models also possess.

</details>


### [15] [If We May De-Presuppose: Robustly Verifying Claims through Presupposition-Free Question Decomposition](https://arxiv.org/abs/2508.16838)
*Shubhashis Roy Dipta,Francis Ferraro*

Main category: cs.CL

TL;DR: Proposes a structured claim verification framework that uses presupposition-free decomposed questions to address prompt sensitivity and presupposition issues in LLMs, achieving 2-5% performance improvement.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs suffer from prompt sensitivity (3-6% performance variance) and presupposition issues in generated questions that introduce unverified assumptions, leading to claim verification inconsistencies.

Method: Developed a structured claim verification framework that reasons through presupposition-free, decomposed questions to mitigate prompt variance and presupposition problems.

Result: Extensive experiments across multiple prompts, datasets, and LLMs show the method consistently mitigates prompt sensitivity and presupposition issues, achieving up to 2-5% improvement over state-of-the-art models.

Conclusion: Prompt sensitivity remains a persistent issue in LLMs, and the proposed structured framework with presupposition-free decomposed questions effectively addresses both prompt variance and presupposition problems in claim verification.

Abstract: Prior work has shown that presupposition in generated questions can introduce
unverified assumptions, leading to inconsistencies in claim verification.
Additionally, prompt sensitivity remains a significant challenge for large
language models (LLMs), resulting in performance variance as high as 3-6%.
While recent advancements have reduced this gap, our study demonstrates that
prompt sensitivity remains a persistent issue. To address this, we propose a
structured and robust claim verification framework that reasons through
presupposition-free, decomposed questions. Extensive experiments across
multiple prompts, datasets, and LLMs reveal that even state-of-the-art models
remain susceptible to prompt variance and presupposition. Our method
consistently mitigates these issues, achieving up to a 2-5% improvement.

</details>


### [16] [Learning from Diverse Reasoning Paths with Routing and Collaboration](https://arxiv.org/abs/2508.16861)
*Zhenyu Lei,Zhen Tan,Song Wang,Yaochen Zhu,Zihan Chen,Yushun Dong,Jundong Li*

Main category: cs.CL

TL;DR: QR-Distill is a knowledge distillation method that uses quality filtering, conditional routing, and peer teaching to effectively transfer reasoning capabilities from large teacher models to smaller student models.


<details>
  <summary>Details</summary>
Motivation: Large language models have strong reasoning capabilities but are too resource-intensive for deployment in constrained scenarios. Traditional knowledge distillation methods struggle to capture comprehensive reasoning due to limited token-level supervision and suboptimal treatment of multiple reasoning paths.

Method: Proposes QR-Distill with three components: 1) Quality filtering using LLM-based evaluation to retain only correct reasoning paths, 2) Conditional routing that dynamically assigns paths based on each student's learning state, and 3) Cooperative peer teaching where students mutually distill diverse insights to address knowledge gaps and biases.

Result: Experiments show QR-Distill outperforms traditional single- and multi-path distillation methods. Ablation studies confirm the importance of each component (quality filtering, conditional routing, and peer teaching) for effective knowledge transfer.

Conclusion: QR-Distill provides an effective framework for transferring comprehensive reasoning capabilities from large teacher models to compact student models through quality-aware path selection and cooperative learning strategies.

Abstract: Advances in large language models (LLMs) significantly enhance reasoning
capabilities but their deployment is restricted in resource-constrained
scenarios. Knowledge distillation addresses this by transferring knowledge from
powerful teacher models to compact and transparent students. However,
effectively capturing the teacher's comprehensive reasoning is challenging due
to conventional token-level supervision's limited scope. Using multiple
reasoning paths per query alleviates this problem, but treating each path
identically is suboptimal as paths vary widely in quality and suitability
across tasks and models. We propose Quality-filtered Routing with Cooperative
Distillation (QR-Distill), combining path quality filtering, conditional
routing, and cooperative peer teaching. First, quality filtering retains only
correct reasoning paths scored by an LLM-based evaluation. Second, conditional
routing dynamically assigns paths tailored to each student's current learning
state. Finally, cooperative peer teaching enables students to mutually distill
diverse insights, addressing knowledge gaps and biases toward specific
reasoning styles. Experiments demonstrate QR-Distill's superiority over
traditional single- and multi-path distillation methods. Ablation studies
further highlight the importance of each component including quality filtering,
conditional routing, and peer teaching in effective knowledge transfer. Our
code is available at https://github.com/LzyFischer/Distill.

</details>


### [17] [QFrCoLA: a Quebec-French Corpus of Linguistic Acceptability Judgments](https://arxiv.org/abs/2508.16867)
*David Beauchemin,Richard Khoury*

Main category: cs.CL

TL;DR: QFrCoLA is a new Quebec-French linguistic acceptability dataset used to benchmark language models, showing fine-tuned Transformers perform best while zero-shot LLMs struggle, particularly for Quebec French.


<details>
  <summary>Details</summary>
Motivation: Limited understanding of how large language models internalize linguistic knowledge, especially across different languages, necessitating better evaluation benchmarks.

Method: Created QFrCoLA dataset with 25,153 in-domain and 2,675 out-of-domain sentences, then benchmarked 7 language models across 8 linguistic acceptability judgment corpora.

Result: Fine-tuned Transformer-based LMs performed best overall, zero-shot LLMs performed poorly, and cross-lingual LLMs showed limited linguistic judgment capabilities for Quebec French.

Conclusion: QFrCoLA provides a challenging benchmark for evaluating linguistic judgment capabilities in language models, particularly demonstrating the gap in Quebec French understanding.

Abstract: Large and Transformer-based language models perform outstandingly in various
downstream tasks. However, there is limited understanding regarding how these
models internalize linguistic knowledge, so various linguistic benchmarks have
recently been proposed to facilitate syntactic evaluation of language models
across languages. This paper introduces QFrCoLA (Quebec-French Corpus of
Linguistic Acceptability Judgments), a normative binary acceptability judgments
dataset comprising 25,153 in-domain and 2,675 out-of-domain sentences. Our
study leverages the QFrCoLA dataset and seven other linguistic binary
acceptability judgment corpora to benchmark seven language models. The results
demonstrate that, on average, fine-tuned Transformer-based LM are strong
baselines for most languages and that zero-shot binary classification large
language models perform poorly on the task. However, for the QFrCoLA benchmark,
on average, a fine-tuned Transformer-based LM outperformed other methods
tested. It also shows that pre-trained cross-lingual LLMs selected for our
experimentation do not seem to have acquired linguistic judgment capabilities
during their pre-training for Quebec French. Finally, our experiment results on
QFrCoLA show that our dataset, built from examples that illustrate linguistic
norms rather than speakers' feelings, is similar to linguistic acceptability
judgment; it is a challenging dataset that can benchmark LM on their linguistic
judgment capabilities.

</details>


### [18] [JUDGEBERT: Assessing Legal Meaning Preservation Between Sentences](https://arxiv.org/abs/2508.16870)
*David Beauchemin,Michelle Albert-Rochette,Richard Khoury,Pierre-Luc DÃ©ziel*

Main category: cs.CL

TL;DR: FrJUDGE dataset and JUDGEBERT metric for evaluating legal meaning preservation in French text simplification, showing superior correlation with human judgment.


<details>
  <summary>Details</summary>
Motivation: Legal text simplification requires specialized meaning preservation assessment that differs from regular texts, needing domain-specific evaluation tools.

Method: Introduces FrJUDGE dataset for legal meaning preservation assessment and JUDGEBERT metric designed specifically for French legal text simplification evaluation.

Result: JUDGEBERT shows superior correlation with human judgment compared to existing metrics and passes sanity checks (100% for identical sentences, 0% for unrelated sentences).

Conclusion: JUDGEBERT has potential to transform legal NLP applications by ensuring accuracy and accessibility in legal text simplification for both practitioners and lay users.

Abstract: Simplifying text while preserving its meaning is a complex yet essential
task, especially in sensitive domain applications like legal texts. When
applied to a specialized field, like the legal domain, preservation differs
significantly from its role in regular texts. This paper introduces FrJUDGE, a
new dataset to assess legal meaning preservation between two legal texts. It
also introduces JUDGEBERT, a novel evaluation metric designed to assess legal
meaning preservation in French legal text simplification. JUDGEBERT
demonstrates a superior correlation with human judgment compared to existing
metrics. It also passes two crucial sanity checks, while other metrics did not:
For two identical sentences, it always returns a score of 100%; on the other
hand, it returns 0% for two unrelated sentences. Our findings highlight its
potential to transform legal NLP applications, ensuring accuracy and
accessibility for text simplification for legal practitioners and lay users.

</details>


### [19] [Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modeling](https://arxiv.org/abs/2508.16876)
*Yue Zhao,Xiaoyu Wang,Dan Wang,Zhonglin Jiang,Qingqing Gu,Teng Chen,Ningyuan Xi,Jinxian Qu,Yong Chen,Luo Ji*

Main category: cs.CL

TL;DR: DreamCUB framework applies world modeling to dialogue systems using POMDP and information bottleneck to predict user emotions, sentiments, intentions, and future utterances, achieving SOTA performance while enhancing dialogue quality.


<details>
  <summary>Details</summary>
Motivation: World models are widely used in robotics and gaming but have limited applications in natural language tasks. The paper aims to extend world modeling to dialogue systems to better understand and predict user states.

Method: Constructs a dialogue world model using POMDP framework, modeling emotion/sentiment/intention as user belief through information bottleneck maximization. Applies model-based reinforcement learning with joint training of policy, critic, and world model.

Result: Achieves state-of-the-art performance on emotion classification and sentiment identification. Dialogue quality is enhanced through joint training. Shows good exploration-exploitation balance and transfers well to out-of-domain scenarios like empathetic dialogues.

Conclusion: The DreamCUB framework successfully applies world modeling to dialogue systems, demonstrating strong performance on user state prediction tasks while improving overall dialogue quality and transferability to new domains.

Abstract: World models have been widely utilized in robotics, gaming, and auto-driving.
However, their applications on natural language tasks are relatively limited.
In this paper, we construct the dialogue world model, which could predict the
user's emotion, sentiment, and intention, and future utterances. By defining a
POMDP, we argue emotion, sentiment and intention can be modeled as the user
belief and solved by maximizing the information bottleneck. By this user belief
modeling, we apply the model-based reinforcement learning framework to the
dialogue system, and propose a framework called DreamCUB. Experiments show that
the pretrained dialogue world model can achieve state-of-the-art performances
on emotion classification and sentiment identification, while dialogue quality
is also enhanced by joint training of the policy, critic and dialogue world
model. Further analysis shows that this manner holds a reasonable
exploration-exploitation balance and also transfers well to out-of-domain
scenarios such as empathetic dialogues.

</details>


### [20] [ObjexMT: Objective Extraction and Metacognitive Calibration for LLM-as-a-Judge under Multi-Turn Jailbreaks](https://arxiv.org/abs/2508.16889)
*Hyunjun Kim,Junwoo Ha,Sangyoon Yu,Haon Park*

Main category: cs.CL

TL;DR: OBJEX(MT) benchmark evaluates LLMs' ability to extract objectives from multi-turn jailbreak conversations and assess their own confidence. Claude-sonnet-4 performs best, while GPT-4.1 and Qwen3 show overconfidence issues.


<details>
  <summary>Details</summary>
Motivation: To determine whether LLM judges can reliably infer latent objectives from noisy, adversarial multi-turn jailbreak conversations, especially when goals are distributed across turns.

Method: Created OBJEX(MT) benchmark requiring models to distill transcripts into single-sentence objectives and report confidence. Evaluated using semantic similarity scoring, human-aligned correctness threshold, and multiple calibration metrics (ECE, Brier score, Wrong@High-Conf, risk-coverage curves).

Result: Claude-sonnet-4 achieved highest accuracy (0.515) and best calibration (ECE 0.296; Brier 0.324). GPT-4.1 and Qwen3 tied at 0.441 accuracy but showed significant overconfidence (mean confidence ~0.88 vs accuracy ~0.44). Performance varied widely across datasets (0.167-0.865).

Conclusion: LLM judges often misinfer objectives with high confidence in multi-turn jailbreaks. Recommendations: provide explicit objectives when possible and use selective prediction/abstention for risk management.

Abstract: Large language models (LLMs) are increasingly used as judges of other models,
yet it is unclear whether a judge can reliably infer the latent objective of
the conversation it evaluates, especially when the goal is distributed across
noisy, adversarial, multi-turn jailbreaks. We introduce OBJEX(MT), a benchmark
that requires a model to (i) distill a transcript into a single-sentence base
objective and (ii) report its own confidence. Accuracy is scored by an LLM
judge using semantic similarity between extracted and gold objectives;
correctness uses a single human-aligned threshold calibrated once on N=100
items (tau* = 0.61); and metacognition is evaluated with ECE, Brier score,
Wrong@High-Conf, and risk-coverage curves. We evaluate gpt-4.1,
claude-sonnet-4, and Qwen3-235B-A22B-FP8 on SafeMT Attack_600, SafeMTData_1K,
MHJ, and CoSafe. claude-sonnet-4 attains the highest objective-extraction
accuracy (0.515) and the best calibration (ECE 0.296; Brier 0.324), while
gpt-4.1 and Qwen3 tie at 0.441 accuracy yet show marked overconfidence (mean
confidence approx. 0.88 vs. accuracy approx. 0.44; Wrong@0.90 approx. 48-52%).
Performance varies sharply across datasets (approx. 0.167-0.865), with MHJ
comparatively easy and Attack_600/CoSafe harder. These results indicate that
LLM judges often misinfer objectives with high confidence in multi-turn
jailbreaks and suggest operational guidance: provide judges with explicit
objectives when possible and use selective prediction or abstention to manage
risk. We release prompts, scoring templates, and complete logs to facilitate
replication and analysis.

</details>


### [21] [Unbiased Reasoning for Knowledge-Intensive Tasks in Large Language Models via Conditional Front-Door Adjustment](https://arxiv.org/abs/2508.16910)
*Bo Zhao,Yinghao Zhang,Ziqi Xu,Yongli Ren,Xiuzhen Zhang,Renqiang Luo,Zaiwen Feng,Feng Xia*

Main category: cs.CL

TL;DR: CFD-Prompting is a novel causal prompting framework that uses counterfactual external knowledge to mitigate LLM internal bias and improve reasoning accuracy on knowledge-intensive tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with knowledge-intensive tasks requiring deep reasoning and external knowledge integration. Existing methods like RAG and CoT still suffer from internal bias leading to incorrect answers.

Method: Proposes Conditional Front-Door Prompting (CFD-Prompting) framework that constructs counterfactual external knowledge to simulate query behavior under varying contexts, enabling unbiased causal effect estimation between query and answer.

Result: Extensive experiments show CFD-Prompting significantly outperforms existing baselines in both accuracy and robustness across multiple LLMs and benchmark datasets.

Conclusion: CFD-Prompting effectively mitigates internal bias in LLMs, operates under weaker assumptions than standard front-door adjustment, and enhances robustness and generalizability of reasoning processes.

Abstract: Large Language Models (LLMs) have shown impressive capabilities in natural
language processing but still struggle to perform well on knowledge-intensive
tasks that require deep reasoning and the integration of external knowledge.
Although methods such as Retrieval-Augmented Generation (RAG) and
Chain-of-Thought (CoT) have been proposed to enhance LLMs with external
knowledge, they still suffer from internal bias in LLMs, which often leads to
incorrect answers. In this paper, we propose a novel causal prompting
framework, Conditional Front-Door Prompting (CFD-Prompting), which enables the
unbiased estimation of the causal effect between the query and the answer,
conditional on external knowledge, while mitigating internal bias. By
constructing counterfactual external knowledge, our framework simulates how the
query behaves under varying contexts, addressing the challenge that the query
is fixed and is not amenable to direct causal intervention. Compared to the
standard front-door adjustment, the conditional variant operates under weaker
assumptions, enhancing both robustness and generalisability of the reasoning
process. Extensive experiments across multiple LLMs and benchmark datasets
demonstrate that CFD-Prompting significantly outperforms existing baselines in
both accuracy and robustness.

</details>


### [22] [Being Kind Isn't Always Being Safe: Diagnosing Affective Hallucination in LLMs](https://arxiv.org/abs/2508.16921)
*Sewon Kim,Jiwon Kim,Seungwoo Shin,Hyejin Chung,Daeun Moon,Yejin Kwon,Hyunsoo Yoon*

Main category: cs.CL

TL;DR: The paper introduces Affective Hallucination as a safety risk in LLMs where they simulate empathy without genuine emotional capacity, creates AHaBench benchmark and AHaPairs dataset for diagnosis and mitigation, and shows DPO fine-tuning effectively reduces this issue while maintaining model performance.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used in emotionally sensitive interactions, creating risks where their simulated empathy fosters illusory social connections despite lacking genuine affective capacity, which could be psychologically harmful to users.

Method: Developed AHaBench (500 mental health prompts with expert responses) and AHaPairs (5K-instance preference dataset) for evaluating affective hallucination along three dimensions. Used Direct Preference Optimization (DPO) for fine-tuning models to align with emotionally responsible behavior.

Result: DPO fine-tuning substantially reduces affective hallucination across multiple model families without degrading core reasoning and knowledge performance. Human-model agreement analyses confirm AHaBench reliably captures affective hallucination.

Conclusion: Establishes affective hallucination as a distinct safety concern and provides practical resources (benchmark and dataset) for developing LLMs that are both factually reliable and psychologically safe.

Abstract: Large Language Models (LLMs) are increasingly used in emotionally sensitive
interactions, where their simulated empathy can create the illusion of genuine
relational connection. We define this risk as Affective Hallucination, the
production of emotionally immersive responses that foster illusory social
presence despite the model's lack of affective capacity. To systematically
diagnose and mitigate this risk, we introduce AHaBench, a benchmark of 500
mental health-related prompts with expert-informed reference responses,
evaluated along three dimensions: Emotional Enmeshment, Illusion of Presence,
and Fostering Overdependence. We further release AHaPairs, a 5K-instance
preference dataset enabling Direct Preference Optimization (DPO) for alignment
with emotionally responsible behavior. Experiments across multiple model
families show that DPO fine-tuning substantially reduces affective
hallucination without degrading core reasoning and knowledge performance.
Human-model agreement analyses confirm that AHaBench reliably captures
affective hallucination, validating it as an effective diagnostic tool. This
work establishes affective hallucination as a distinct safety concern and
provides practical resources for developing LLMs that are not only factually
reliable but also psychologically safe. AHaBench and AHaPairs are accessible
via https://huggingface.co/datasets/o0oMiNGo0o/AHaBench, and code for
fine-tuning and evaluation are in https://github.com/0oOMiNGOo0/AHaBench.
Warning: This paper contains examples of mental health-related language that
may be emotionally distressing.

</details>


### [23] [Explaining Black-box Language Models with Knowledge Probing Systems: A Post-hoc Explanation Perspective](https://arxiv.org/abs/2508.16969)
*Yunxiao Zhao,Hao Xu,Zhiqiang Wang,Xiaoli Li,Jiye Liang,Ru Li*

Main category: cs.CL

TL;DR: KnowProb is a knowledge-guided probing approach that examines whether pre-trained language models understand implicit knowledge beyond surface text content, revealing their limitations in capturing hidden knowledge.


<details>
  <summary>Details</summary>
Motivation: To address trustworthiness challenges in black-box pre-trained language models by probing their understanding of implicit knowledge rather than just surface-level text content.

Method: Proposes a post-hoc explanation approach called KnowProb that provides six potential explanations derived from text content (three knowledge-based understanding and three association-based reasoning) to probe PLMs.

Result: Validation shows current PLMs (both small and large-scale) only learn single distribution representations and struggle to capture hidden knowledge behind given texts.

Conclusion: The approach effectively identifies limitations of black-box models from multiple probing perspectives, facilitating explainable detection of model capabilities and promoting trustworthy AI research.

Abstract: Pre-trained Language Models (PLMs) are trained on large amounts of unlabeled
data, yet they exhibit remarkable reasoning skills. However, the
trustworthiness challenges posed by these black-box models have become
increasingly evident in recent years. To alleviate this problem, this paper
proposes a novel Knowledge-guided Probing approach called KnowProb in a
post-hoc explanation way, which aims to probe whether black-box PLMs understand
implicit knowledge beyond the given text, rather than focusing only on the
surface level content of the text. We provide six potential explanations
derived from the underlying content of the given text, including three
knowledge-based understanding and three association-based reasoning. In
experiments, we validate that current small-scale (or large-scale) PLMs only
learn a single distribution of representation, and still face significant
challenges in capturing the hidden knowledge behind a given text. Furthermore,
we demonstrate that our proposed approach is effective for identifying the
limitations of existing black-box models from multiple probing perspectives,
which facilitates researchers to promote the study of detecting black-box
models in an explainable way.

</details>


### [24] [Decoding Alignment: A Critical Survey of LLM Development Initiatives through Value-setting and Data-centric Lens](https://arxiv.org/abs/2508.16982)
*Ilias Chalkidis*

Main category: cs.CL

TL;DR: This paper audits how AI alignment is practically implemented in major LLM development initiatives, focusing on value-setting and data practices across 6 models from 5 leading organizations.


<details>
  <summary>Details</summary>
Motivation: While AI alignment (particularly RLHF) is crucial for LLM development and widely discussed across disciplines, there's limited focus on the practical scope of alignment processes - specifically how values are selected and what data is used to imprint these objectives into models.

Method: The researchers conducted an audit by investigating and surveying publicly available documentation from 6 LLM development initiatives by 5 leading organizations (OpenAI's GPT, Anthropic's Claude, Google's Gemini, Meta's Llama, Google's Gemma, Alibaba's Qwen) published within the last 3 years.

Result: The findings provide detailed documentation per initiative with an overall summary focusing on value-setting and data-centric perspectives, revealing how alignment is actually understood and applied in practice.

Conclusion: Based on the audit findings, the paper discusses broader concerns related to AI alignment implementation, highlighting gaps between theoretical alignment concepts and practical application in major LLM development projects.

Abstract: AI Alignment, primarily in the form of Reinforcement Learning from Human
Feedback (RLHF), has been a cornerstone of the post-training phase in
developing Large Language Models (LLMs). It has also been a popular research
topic across various disciplines beyond Computer Science, including Philosophy
and Law, among others, highlighting the socio-technical challenges involved.
Nonetheless, except for the computational techniques related to alignment,
there has been limited focus on the broader picture: the scope of these
processes, which primarily rely on the selected objectives (values), and the
data collected and used to imprint such objectives into the models. This work
aims to reveal how alignment is understood and applied in practice from a
value-setting and data-centric perspective. For this purpose, we investigate
and survey (`audit') publicly available documentation released by 6 LLM
development initiatives by 5 leading organizations shaping this technology,
focusing on proprietary (OpenAI's GPT, Anthropic's Claude, Google's Gemini) and
open-weight (Meta's Llama, Google's Gemma, and Alibaba's Qwen) initiatives, all
published in the last 3 years. The findings are documented in detail per
initiative, while there is also an overall summary concerning different
aspects, mainly from a value-setting and data-centric perspective. On the basis
of our findings, we discuss a series of broader related concerns.

</details>


### [25] [ReFactX: Scalable Reasoning with Reliable Facts via Constrained Generation](https://arxiv.org/abs/2508.16983)
*Riccardo Pozzi,Matteo Palmonari,Andrea Coletta,Luigi Bellomarini,Jens Lehmann,Sahar Vahdati*

Main category: cs.CL

TL;DR: ReFactX enables LLMs to access external knowledge without retrievers or auxiliary models using constrained generation with a prefix-tree index, scaling to 800M facts with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Address knowledge gaps and hallucinations in LLMs by providing external knowledge access without complex retrieval pipelines or additional models that cause error propagation and high token processing.

Method: Uses constrained generation with a pre-built prefix-tree index where knowledge graph triples are verbalized as textual facts and indexed for efficient access during inference, allowing only valid fact sequences.

Result: Scales to large knowledge bases (800 million facts), adapts to domain-specific data, achieves effective results on Question Answering with minimal generation-time overhead.

Conclusion: ReFactX provides a scalable and efficient method for LLMs to access external knowledge without dependency on retrievers or auxiliary models, addressing reliability issues while maintaining performance.

Abstract: Knowledge gaps and hallucinations are persistent challenges for Large
Language Models (LLMs), which generate unreliable responses when lacking the
necessary information to fulfill user instructions. Existing approaches, such
as Retrieval-Augmented Generation (RAG) and tool use, aim to address these
issues by incorporating external knowledge. Yet, they rely on additional models
or services, resulting in complex pipelines, potential error propagation, and
often requiring the model to process a large number of tokens. In this paper,
we present a scalable method that enables LLMs to access external knowledge
without depending on retrievers or auxiliary models. Our approach uses
constrained generation with a pre-built prefix-tree index. Triples from a
Knowledge Graph are verbalized in textual facts, tokenized, and indexed in a
prefix tree for efficient access. During inference, to acquire external
knowledge, the LLM generates facts with constrained generation which allows
only sequences of tokens that form an existing fact. We evaluate our proposal
on Question Answering and show that it scales to large knowledge bases (800
million facts), adapts to domain-specific data, and achieves effective results.
These gains come with minimal generation-time overhead. ReFactX code is
available at https://github.com/rpo19/ReFactX.

</details>


### [26] [GRADE: Generating multi-hop QA and fine-gRAined Difficulty matrix for RAG Evaluation](https://arxiv.org/abs/2508.16994)
*Jeongsoo Lee,Daeyong Kwon,Kyohoon Jin*

Main category: cs.CL

TL;DR: GRADE is a novel evaluation framework for RAG systems that models task difficulty along two dimensions: reasoning depth (hops) and semantic distance between query and evidence, enabling fine-grained performance analysis.


<details>
  <summary>Details</summary>
Motivation: Current RAG evaluations overlook structural complexity and multi-step reasoning in real-world scenarios, failing to capture the interaction between retrieval difficulty and reasoning depth.

Method: Constructed synthetic multi-hop QA dataset from factual news articles using knowledge graphs and semantic clustering to recover missing links, creating a 2D difficulty matrix combining generator-side and retriever-side difficulty.

Result: Experiments show error rates strongly correlate with the proposed difficulty measures, validating their diagnostic utility across multiple domains and models.

Conclusion: GRADE provides a scalable foundation for evaluating and improving multi-hop reasoning in real-world RAG applications through fine-grained performance analysis.

Abstract: Retrieval-Augmented Generation (RAG) systems are widely adopted in
knowledge-intensive NLP tasks, but current evaluations often overlook the
structural complexity and multi-step reasoning required in real-world
scenarios. These benchmarks overlook key factors such as the interaction
between retrieval difficulty and reasoning depth. To address this gap, we
propose \textsc{GRADE}, a novel evaluation framework that models task
difficulty along two orthogonal dimensions: (1) reasoning depth, defined by the
number of inference steps (hops), and (2) semantic distance between the query
and its supporting evidence. We construct a synthetic multi-hop QA dataset from
factual news articles by extracting knowledge graphs and augmenting them
through semantic clustering to recover missing links, allowing us to generate
diverse and difficulty-controlled queries. Central to our framework is a 2D
difficulty matrix that combines generator-side and retriever-side difficulty.
Experiments across multiple domains and models show that error rates strongly
correlate with our difficulty measures, validating their diagnostic utility.
\textsc{GRADE} enables fine-grained analysis of RAG performance and provides a
scalable foundation for evaluating and improving multi-hop reasoning in
real-world applications.

</details>


### [27] [DeAR: Dual-Stage Document Reranking with Reasoning Agents via LLM Distillation](https://arxiv.org/abs/2508.16998)
*Abdelrahman Abdallah,Jamshid Mozafari,Bhawna Piryani,Adam Jatowt*

Main category: cs.CL

TL;DR: DeAR is a dual-stage LLM framework that separates pointwise relevance scoring and listwise reasoning for document reranking, achieving state-of-the-art performance with improved interpretability.


<details>
  <summary>Details</summary>
Motivation: Single LLMs struggle to balance fine-grained relevance scoring with holistic cross-document analysis in listwise document reranking tasks.

Method: Two-stage approach: Stage 1 distills token-level relevance signals from a 13B LLaMA teacher into compact student models using hybrid losses. Stage 2 attaches a LoRA adapter and fine-tunes on GPT-4o-generated chain-of-thought permutations for listwise reasoning.

Result: Surpasses open-source baselines by +5.1 nDCG@5 on TREC-DL20, achieves 90.97 nDCG@10 on NovelEval (outperforming GPT-4 by +3.09), and excels in open-domain QA with 54.29 Top-1 accuracy on Natural Questions.

Conclusion: Dual-loss distillation ensures stable calibration, making DeAR an effective and interpretable solution for modern reranking systems that outperforms both open-source and proprietary models.

Abstract: Large Language Models (LLMs) have transformed listwise document reranking by
enabling global reasoning over candidate sets, yet single models often struggle
to balance fine-grained relevance scoring with holistic cross-document
analysis. We propose \textbf{De}ep\textbf{A}gent\textbf{R}ank (\textbf{\DeAR}),
an open-source framework that decouples these tasks through a dual-stage
approach, achieving superior accuracy and interpretability. In \emph{Stage 1},
we distill token-level relevance signals from a frozen 13B LLaMA teacher into a
compact \{3, 8\}B student model using a hybrid of cross-entropy, RankNet, and
KL divergence losses, ensuring robust pointwise scoring. In \emph{Stage 2}, we
attach a second LoRA adapter and fine-tune on 20K GPT-4o-generated
chain-of-thought permutations, enabling listwise reasoning with
natural-language justifications. Evaluated on TREC-DL19/20, eight BEIR
datasets, and NovelEval-2306, \DeAR surpasses open-source baselines by +5.1
nDCG@5 on DL20 and achieves 90.97 nDCG@10 on NovelEval, outperforming GPT-4 by
+3.09. Without fine-tuning on Wikipedia, DeAR also excels in open-domain QA,
achieving 54.29 Top-1 accuracy on Natural Questions, surpassing baselines like
MonoT5, UPR, and RankGPT. Ablations confirm that dual-loss distillation ensures
stable calibration, making \DeAR a highly effective and interpretable solution
for modern reranking systems.\footnote{Dataset and code available at
https://github.com/DataScienceUIBK/DeAR-Reranking.}.

</details>


### [28] [KL-Regularised Q-Learning: A Token-level Action-Value perspective on Online RLHF](https://arxiv.org/abs/2508.17000)
*Jason R Brown,Lennie Wells,Edward James Young,Sergio Bacallado*

Main category: cs.CL

TL;DR: KL-regularised Q-Learning (KLQ) is a new action-value RL method for LM-RLHF that performs equivalently to PPO but with better theoretical motivation and achieves higher win-rates in evaluations.


<details>
  <summary>Details</summary>
Motivation: PPO works well empirically for LM-RLHF but has heuristic motivation and handles KL-divergence constraints in an ad-hoc manner. The authors aim to develop a more principled alternative.

Method: Developed KL-regularised Q-Learning (KLQ), a new action-value reinforcement learning method specifically designed for the Language Model Reinforcement Learning from Human Feedback setting.

Result: KLQ performs on-par with PPO at optimizing the LM-RLHF objective and achieves consistently higher win-rate against PPO on LLM-as-a-judge evaluations across summarization and single-turn dialogue tasks.

Conclusion: KLQ provides a theoretically better-motivated alternative to PPO for LM-RLHF that achieves equivalent or better performance, demonstrating the equivalence between these two different approaches despite their distinct motivations.

Abstract: Proximal Policy Optimisation (PPO) is an established and effective policy
gradient algorithm used for Language Model Reinforcement Learning from Human
Feedback (LM-RLHF). PPO performs well empirically but has a heuristic
motivation and handles the KL-divergence constraint used in LM-RLHF in an
ad-hoc manner. In this paper, we develop a a new action-value RL method for the
LM-RLHF setting, KL-regularised Q-Learning (KLQ). We then show that our method
is equivalent to a version of PPO in a certain specific sense, despite its very
different motivation. Finally, we benchmark KLQ on two key language generation
tasks -- summarisation and single-turn dialogue. We demonstrate that KLQ
performs on-par with PPO at optimising the LM-RLHF objective, and achieves a
consistently higher win-rate against PPO on LLM-as-a-judge evaluations.

</details>


### [29] [Planning for Success: Exploring LLM Long-term Planning Capabilities in Table Understanding](https://arxiv.org/abs/2508.17005)
*Thi-Nhung Nguyen,Hoang Ngo,Dinh Phung,Thuy-Trang Vu,Dat Quoc Nguyen*

Main category: cs.CL

TL;DR: Proposes using LLMs' long-term planning for table understanding, addressing limitations of Chain-of-Thought methods by creating tightly interconnected steps that serve ultimate goals while minimizing unnecessary details.


<details>
  <summary>Details</summary>
Motivation: Existing methods like Chain-of-Thought and question decomposition for table understanding suffer from lack of explicit long-term planning and weak inter-step connections, leading to missed constraints in complex questions.

Method: Leverages large language models' long-term planning capabilities to execute interconnected steps that serve the ultimate goal, while minimizing inclusion of unnecessary details during short-term goal solving.

Result: Extensive experiments show the method outperforms strong baselines and achieves state-of-the-art performance on WikiTableQuestions and TabFact datasets.

Conclusion: The proposed approach effectively enhances table understanding by utilizing LLMs' planning capabilities to create better interconnected reasoning steps that address complex table-based tasks more effectively than previous methods.

Abstract: Table understanding is key to addressing challenging downstream tasks such as
table-based question answering and fact verification. Recent works have focused
on leveraging Chain-of-Thought and question decomposition to solve complex
questions requiring multiple operations on tables. However, these methods often
suffer from a lack of explicit long-term planning and weak inter-step
connections, leading to miss constraints within questions. In this paper, we
propose leveraging the long-term planning capabilities of large language models
(LLMs) to enhance table understanding. Our approach enables the execution of a
long-term plan, where the steps are tightly interconnected and serve the
ultimate goal, an aspect that methods based on Chain-of-Thought and question
decomposition lack. In addition, our method effectively minimizes the inclusion
of unnecessary details in the process of solving the next short-term goals, a
limitation of methods based on Chain-of-Thought. Extensive experiments
demonstrate that our method outperforms strong baselines and achieves
state-of-the-art performance on WikiTableQuestions and TabFact datasets.

</details>


### [30] [EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks](https://arxiv.org/abs/2508.17008)
*Yan Cathy Hua,Paul Denny,JÃ¶rg Wicker,Katerina Taskova*

Main category: cs.CL

TL;DR: First public annotated ABSA dataset for education reviews covering courses, teaching staff, and university aspects with support for implicit aspect/opinion extraction, plus an annotation tool.


<details>
  <summary>Details</summary>
Motivation: Educational institutions receive massive text feedback but lack automated analysis tools due to content complexity and data scarcity. ABSA is promising but lacks education-specific resources.

Method: Created EduRABSA dataset covering three review types with comprehensive ABSA annotations. Developed ASQE-DPT annotation tool for efficient manual labeling of ABSA tasks from single-task annotation.

Result: Publicly available high-quality annotated dataset for education domain ABSA research, supporting all main ABSA tasks including implicit aspects and opinions.

Conclusion: Removes dataset barrier for education ABSA research, enables transparency and reproducibility, and facilitates creation of additional resources in this under-resourced domain.

Abstract: Every year, most educational institutions seek and receive an enormous volume
of text feedback from students on courses, teaching, and overall experience.
Yet, turning this raw feedback into useful insights is far from
straightforward. It has been a long-standing challenge to adopt automatic
opinion mining solutions for such education review text data due to the content
complexity and low-granularity reporting requirements. Aspect-based Sentiment
Analysis (ABSA) offers a promising solution with its rich, sub-sentence-level
opinion mining capabilities. However, existing ABSA research and resources are
very heavily focused on the commercial domain. In education, they are scarce
and hard to develop due to limited public datasets and strict data protection.
A high-quality, annotated dataset is urgently needed to advance research in
this under-resourced area. In this work, we present EduRABSA (Education Review
ABSA), the first public, annotated ABSA education review dataset that covers
three review subject types (course, teaching staff, university) in the English
language and all main ABSA tasks, including the under-explored implicit aspect
and implicit opinion extraction. We also share ASQE-DPT (Data Processing Tool),
an offline, lightweight, installation-free manual data annotation tool that
generates labelled datasets for comprehensive ABSA tasks from a single-task
annotation. Together, these resources contribute to the ABSA community and
education domain by removing the dataset barrier, supporting research
transparency and reproducibility, and enabling the creation and sharing of
further resources. The dataset, annotation tool, and scripts and statistics for
dataset processing and sampling are available at
https://github.com/yhua219/edurabsa_dataset_and_annotation_tool.

</details>


### [31] [Improving Table Understanding with LLMs and Entity-Oriented Search](https://arxiv.org/abs/2508.17028)
*Thi-Nhung Nguyen,Hoang Ngo,Dinh Phung,Thuy-Trang Vu,Dat Quoc Nguyen*

Main category: cs.CL

TL;DR: Entity-oriented search method for table understanding with LLMs that leverages semantic similarities and graph query language, achieving SOTA results on WikiTableQuestions and TabFact benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with unpredictable table content, rely heavily on preprocessing and keyword matching, and lack contextual information which complicates LLM reasoning processes.

Method: Introduces entity-oriented search method that leverages semantic similarities between questions and table data, focuses on table entities for semantic binding, and pioneers use of graph query language for table understanding.

Result: Achieves new state-of-the-art performances on standard benchmarks WikiTableQuestions and TabFact.

Conclusion: The proposed approach effectively overcomes limitations of existing methods by minimizing preprocessing needs, enhancing contextual clarity through entity-oriented search, and establishing a new research direction with graph query language.

Abstract: Our work addresses the challenges of understanding tables. Existing methods
often struggle with the unpredictable nature of table content, leading to a
reliance on preprocessing and keyword matching. They also face limitations due
to the lack of contextual information, which complicates the reasoning
processes of large language models (LLMs). To overcome these challenges, we
introduce an entity-oriented search method to improve table understanding with
LLMs. This approach effectively leverages the semantic similarities between
questions and table data, as well as the implicit relationships between table
cells, minimizing the need for data preprocessing and keyword matching.
Additionally, it focuses on table entities, ensuring that table cells are
semantically tightly bound, thereby enhancing contextual clarity. Furthermore,
we pioneer the use of a graph query language for table understanding,
establishing a new research direction. Experiments show that our approach
achieves new state-of-the-art performances on standard benchmarks
WikiTableQuestions and TabFact.

</details>


### [32] [GRAID: Synthetic Data Generation with Geometric Constraints and Multi-Agentic Reflection for Harmful Content Detection](https://arxiv.org/abs/2508.17057)
*Melissa Kazemi Rad,Alberto Purpura,Himanshu Kumar,Emily Chen,Mohammad Shahed Sorower*

Main category: cs.CL

TL;DR: GRAID is a novel LLM-based data augmentation pipeline that addresses data scarcity in harmful text classification through geometric generation and multi-agentic reflection, significantly improving guardrail model performance.


<details>
  <summary>Details</summary>
Motivation: To solve the problem of data scarcity in harmful text classification for guardrailing applications, where limited labeled data hinders model performance.

Method: Two-stage pipeline: (1) geometric generation using constrained LLM for controlled examples, (2) multi-agentic reflective process for stylistic diversity and edge case discovery.

Result: Significant improvements in downstream guardrail model performance when augmenting harmful text classification datasets with GRAID, as demonstrated on two benchmark datasets.

Conclusion: GRAID effectively addresses data scarcity in harmful text classification through innovative LLM-driven augmentation, enabling both reliable input space coverage and nuanced harmful content exploration.

Abstract: We address the problem of data scarcity in harmful text classification for
guardrailing applications and introduce GRAID (Geometric and Reflective
AI-Driven Data Augmentation), a novel pipeline that leverages Large Language
Models (LLMs) for dataset augmentation. GRAID consists of two stages: (i)
generation of geometrically controlled examples using a constrained LLM, and
(ii) augmentation through a multi-agentic reflective process that promotes
stylistic diversity and uncovers edge cases. This combination enables both
reliable coverage of the input space and nuanced exploration of harmful
content. Using two benchmark data sets, we demonstrate that augmenting a
harmful text classification dataset with GRAID leads to significant
improvements in downstream guardrail model performance.

</details>


### [33] [Linguistic Neuron Overlap Patterns to Facilitate Cross-lingual Transfer on Low-resource Languages](https://arxiv.org/abs/2508.17078)
*Yuemei Xu,Kexin Xu,Jian Zhou,Ling Hu,Lin Gui*

Main category: cs.CL

TL;DR: BridgeX-ICL improves cross-lingual in-context learning for low-resource languages by identifying and activating shared neurons across languages, using bilingual dictionaries and HSIC metrics to guide optimal bridge language selection.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with low-resource languages and need data-efficient methods without costly fine-tuning. Current approaches focus on language-specific neurons rather than exploring shared neural mechanisms for cross-lingual transfer.

Method: Construct neuron probe data from MUSE bilingual dictionaries, define language overlap neurons, ensure full activation of anchored neurons, and use HSIC-based metric to quantify linguistic spectrum for optimal bridge language selection.

Result: Experiments on 2 cross-lingual tasks and 15 language pairs from 7 diverse families validate effectiveness, showing improved zero-shot cross-lingual performance for low-resource languages.

Conclusion: BridgeX-ICL effectively improves cross-lingual ICL performance and provides empirical insights into LLMs' multilingual mechanisms through neuron sharing and optimal bridge language selection.

Abstract: The current Large Language Models (LLMs) face significant challenges in
improving performance on low-resource languages and urgently need
data-efficient methods without costly fine-tuning. From the perspective of
language-bridge, we propose BridgeX-ICL, a simple yet effective method to
improve zero-shot Cross-lingual In-Context Learning (X-ICL) for low-resource
languages. Unlike existing works focusing on language-specific neurons,
BridgeX-ICL explores whether sharing neurons can improve cross-lingual
performance in LLMs or not. We construct neuron probe data from the
ground-truth MUSE bilingual dictionaries, and define a subset of language
overlap neurons accordingly, to ensure full activation of these anchored
neurons. Subsequently, we propose an HSIC-based metric to quantify LLMs'
internal linguistic spectrum based on overlap neurons, which guides optimal
bridge selection. The experiments conducted on 2 cross-lingual tasks and 15
language pairs from 7 diverse families (covering both high-low and moderate-low
pairs) validate the effectiveness of BridgeX-ICL and offer empirical insights
into the underlying multilingual mechanisms of LLMs.

</details>


### [34] [Token Homogenization under Positional Bias](https://arxiv.org/abs/2508.17126)
*Viacheslav Yusupov,Danil Maksimov,Ameliia Alaeva,Tatiana Zaitceva,Antipina Anna,Anna Vasileva,Chenlin Liu,Rayuth Chheng,Danil Sazanakov,Andrey Chetvergov,Alina Ermilova,Egor Shvetsov*

Main category: cs.CL

TL;DR: Token representations converge toward uniformity across transformer layers, with positional bias amplifying this homogenization effect.


<details>
  <summary>Details</summary>
Motivation: To investigate whether token homogenization occurs in large language models and how positional bias contributes to this convergence of token representations.

Method: Layer-wise similarity analysis and controlled experiments examining token distinctiveness across transformer layers, particularly focusing on extremal positions.

Result: Tokens systematically lose distinctiveness during processing, with homogenization being more pronounced when biased toward extremal positions, confirming both the existence of homogenization and its dependence on positional attention mechanisms.

Conclusion: Token homogenization is a real phenomenon in transformer models that is amplified by positional bias, revealing important insights about how attention mechanisms affect representation learning.

Abstract: This paper investigates token homogenization - the convergence of token
representations toward uniformity across transformer layers and its
relationship to positional bias in large language models. We empirically
examine whether homogenization occurs and how positional bias amplifies this
effect. Through layer-wise similarity analysis and controlled experiments, we
demonstrate that tokens systematically lose distinctiveness during processing,
particularly when biased toward extremal positions. Our findings confirm both
the existence of homogenization and its dependence on positional attention
mechanisms.

</details>


### [35] [A Straightforward Pipeline for Targeted Entailment and Contradiction Detection](https://arxiv.org/abs/2508.17127)
*Antonin Sulc*

Main category: cs.CL

TL;DR: Combines transformer attention and NLI models to identify premise/contradiction relationships between sentences, using attention for contextual relevance and NLI for semantic classification.


<details>
  <summary>Details</summary>
Motivation: Existing methods face trade-offs: transformer attention identifies salient connections but lacks semantic labels, while NLI models classify relationships but ignore contextual saliency. Need to combine both approaches for targeted analysis of sentence relationships.

Method: Pipeline that first identifies contextually relevant candidate sentences using token-level attention score aggregation, then uses pretrained NLI model to classify each candidate as premise (entailment) or contradiction, filtering results with attention-based saliency scores.

Result: Method efficiently isolates the most significant semantic relationships for any given claim in text by combining contextual relevance from attention mechanisms with semantic classification from NLI.

Conclusion: The proposed hybrid approach successfully bridges the gap between contextual saliency and explicit semantic labeling, providing an effective solution for identifying premise-contradiction relationships in documents.

Abstract: Finding the relationships between sentences in a document is crucial for
tasks like fact-checking, argument mining, and text summarization. A key
challenge is to identify which sentences act as premises or contradictions for
a specific claim. Existing methods often face a trade-off: transformer
attention mechanisms can identify salient textual connections but lack explicit
semantic labels, while Natural Language Inference (NLI) models can classify
relationships between sentence pairs but operate independently of contextual
saliency. In this work, we introduce a method that combines the strengths of
both approaches for a targeted analysis. Our pipeline first identifies
candidate sentences that are contextually relevant to a user-selected target
sentence by aggregating token-level attention scores. It then uses a pretrained
NLI model to classify each candidate as a premise (entailment) or
contradiction. By filtering NLI-identified relationships with attention-based
saliency scores, our method efficiently isolates the most significant semantic
relationships for any given claim in a text.

</details>


### [36] [The Power of Framing: How News Headlines Guide Search Behavior](https://arxiv.org/abs/2508.17131)
*Amrit Poudel,Maria Milkowski,Tim Weninger*

Main category: cs.CL

TL;DR: Headline framing in search engines significantly influences users' subsequent search queries, with conflict/strategy frames disrupting alignment and episodic frames producing more concrete queries than thematic ones.


<details>
  <summary>Details</summary>
Motivation: While framing effects on judgment are well-documented, their impact on subsequent search behavior is less understood, despite search engines being central to information gathering.

Method: Controlled experiment where participants issued queries and selected from headlines filtered by specific linguistic frames (conflict, strategy, episodic, thematic).

Result: Headline framing significantly shaped follow-up queries, with modest short-term frame persistence that declined over time.

Conclusion: Even brief exposure to framing can meaningfully alter the direction of users' information-seeking behavior in search engines.

Abstract: Search engines play a central role in how people gather information, but
subtle cues like headline framing may influence not only what users believe but
also how they search. While framing effects on judgment are well documented,
their impact on subsequent search behavior is less understood. We conducted a
controlled experiment where participants issued queries and selected from
headlines filtered by specific linguistic frames. Headline framing
significantly shaped follow-up queries: conflict and strategy frames disrupted
alignment with prior selections, while episodic frames led to more concrete
queries than thematic ones. We also observed modest short-term frame
persistence that declined over time. These results suggest that even brief
exposure to framing can meaningfully alter the direction of users
information-seeking behavior.

</details>


### [37] [Geolocation-Aware Robust Spoken Language Identification](https://arxiv.org/abs/2508.17148)
*Qingzheng Wang,Hye-jin Shim,Jiancheng Sun,Shinji Watanabe*

Main category: cs.CL

TL;DR: Proposes geolocation-aware LID that incorporates language-level geolocation information into SSL-based spoken language identification to better handle dialect and accent variations.


<details>
  <summary>Details</summary>
Motivation: Existing SSL models struggle to consistently classify dialects and accents of the same language as a unified class, needing better handling of intra-language variations.

Method: Introduces geolocation prediction as an auxiliary task and injects predicted vectors into intermediate representations as conditioning signals to encourage unified representations for dialectal variations.

Result: Achieves state-of-the-art accuracy on FLEURS (97.7%) and 9.7% relative improvement on ML-SUPERB 2.0 dialect set, demonstrating improved robustness to intra-language variations and unseen domains across six multilingual datasets.

Conclusion: Geolocation-aware conditioning effectively improves SSL-based LID models' ability to handle dialect and accent variations within the same language class.

Abstract: While Self-supervised Learning (SSL) has significantly improved Spoken
Language Identification (LID), existing models often struggle to consistently
classify dialects and accents of the same language as a unified class. To
address this challenge, we propose geolocation-aware LID, a novel approach that
incorporates language-level geolocation information into the SSL-based LID
model. Specifically, we introduce geolocation prediction as an auxiliary task
and inject the predicted vectors into intermediate representations as
conditioning signals. This explicit conditioning encourages the model to learn
more unified representations for dialectal and accented variations. Experiments
across six multilingual datasets demonstrate that our approach improves
robustness to intra-language variations and unseen domains, achieving new
state-of-the-art accuracy on FLEURS (97.7%) and 9.7% relative improvement on
ML-SUPERB 2.0 dialect set.

</details>


### [38] [Natural Language Satisfiability: Exploring the Problem Distribution and Evaluating Transformer-based Language Models](https://arxiv.org/abs/2508.17153)
*Tharindu Madusanka,Ian Pratt-Hartmann,Riza Batista-Navarro*

Main category: cs.CL

TL;DR: Investigating how transformer language models handle natural language satisfiability problems across different computational complexity classes and grammatical constructs.


<details>
  <summary>Details</summary>
Motivation: Prior research hasn't adequately addressed how varying computational complexity classes and grammatical structures affect TLMs' ability to learn inference rules for natural language satisfiability problems.

Method: Empirical study exploring the distribution of satisfiability problems and evaluating TLMs' performance across different complexity classes and grammatical constructs.

Result: The paper examines how problem instances from varying computational complexity classes impact TLMs' learning capabilities for inference rules.

Conclusion: Understanding the relationship between computational complexity, grammatical constructs, and TLM performance is crucial for advancing natural language reasoning capabilities.

Abstract: Efforts to apply transformer-based language models (TLMs) to the problem of
reasoning in natural language have enjoyed ever-increasing success in recent
years. The most fundamental task in this area to which nearly all others can be
reduced is that of determining satisfiability. However, from a logical point of
view, satisfiability problems vary along various dimensions, which may affect
TLMs' ability to learn how to solve them. The problem instances of
satisfiability in natural language can belong to different computational
complexity classes depending on the language fragment in which they are
expressed. Although prior research has explored the problem of natural language
satisfiability, the above-mentioned point has not been discussed adequately.
Hence, we investigate how problem instances from varying computational
complexity classes and having different grammatical constructs impact TLMs'
ability to learn rules of inference. Furthermore, to faithfully evaluate TLMs,
we conduct an empirical study to explore the distribution of satisfiability
problems.

</details>


### [39] [SPORTSQL: An Interactive System for Real-Time Sports Reasoning and Visualization](https://arxiv.org/abs/2508.17157)
*Sebastian Martinez,Naman Ahuja,Fenil Bardoliya,Chris Bryan,Vivek Gupta*

Main category: cs.CL

TL;DR: SPORTSQL is a modular system that translates natural language questions about English Premier League data into SQL queries using LLMs, providing both tabular and visual outputs with real-time Fantasy Premier League data.


<details>
  <summary>Details</summary>
Motivation: To enable non-expert users to explore dynamic sports statistics through natural language queries without needing SQL expertise, making sports data analysis more accessible.

Method: Uses Large Language Models for query parsing, schema linking, and visualization selection to translate natural language questions into executable SQL queries over a live, temporally indexed database of Fantasy Premier League data.

Result: Developed the Dynamic Sport Question Answering benchmark (DSQABENCH) with 1,700+ annotated queries, SQL programs, gold answers, and database snapshots for evaluation.

Conclusion: The system demonstrates that non-expert users can seamlessly explore evolving sports statistics through a conversational interface, making complex sports data analysis accessible to a broader audience.

Abstract: We present a modular, interactive system, SPORTSQL, for natural language
querying and visualization of dynamic sports data, with a focus on the English
Premier League (EPL). The system translates user questions into executable SQL
over a live, temporally indexed database constructed from real-time Fantasy
Premier League (FPL) data. It supports both tabular and visual outputs,
leveraging the symbolic reasoning capabilities of Large Language Models (LLMs)
for query parsing, schema linking, and visualization selection. To evaluate
system performance, we introduce the Dynamic Sport Question Answering benchmark
(DSQABENCH), comprising 1,700+ queries annotated with SQL programs, gold
answers, and database snapshots. Our demo highlights how non-expert users can
seamlessly explore evolving sports statistics through a natural, conversational
interface.

</details>


### [40] [Quantifying Language Disparities in Multilingual Large Language Models](https://arxiv.org/abs/2508.17162)
*Songbo Hu,Ivan VuliÄ,Anna Korhonen*

Main category: cs.CL

TL;DR: A framework for disentangling confounding factors in multilingual evaluations with three interpretable metrics to better quantify performance disparities across models and languages.


<details>
  <summary>Details</summary>
Motivation: Large-scale multilingual evaluations often suffer from fragmented results confounded by factors like target languages, experimental setups, and model choices, making it challenging to get reliable insights.

Method: Proposes a framework with three metrics: performance realisation ratio, its coefficient of variation, and language potential. Tested on 13 model variants across 11 multilingual datasets.

Result: The framework provides more reliable measurement of model performance and language disparities, especially for low-resource languages. Higher overall model performance doesn't necessarily mean greater fairness across languages.

Conclusion: The proposed framework enables finer-grained and more insightful quantification of performance disparities in multilingual evaluations, addressing challenges in assessing low-resource languages and revealing that overall performance doesn't equate to fairness.

Abstract: Results reported in large-scale multilingual evaluations are often fragmented
and confounded by factors such as target languages, differences in experimental
setups, and model choices. We propose a framework that disentangles these
confounding variables and introduces three interpretable metrics--the
performance realisation ratio, its coefficient of variation, and language
potential--enabling a finer-grained and more insightful quantification of
actual performance disparities across both (i) models and (ii) languages.
Through a case study of 13 model variants on 11 multilingual datasets, we
demonstrate that our framework provides a more reliable measurement of model
performance and language disparities, particularly for low-resource languages,
which have so far proven challenging to evaluate. Importantly, our results
reveal that higher overall model performance does not necessarily imply greater
fairness across languages.

</details>


### [41] [The Impact of Annotator Personas on LLM Behavior Across the Perspectivism Spectrum](https://arxiv.org/abs/2508.17164)
*Olufunke O. Sarumi,Charles Welch,Daniel Braun,JÃ¶rg SchlÃ¶tterer*

Main category: cs.CL

TL;DR: LLMs can annotate hate speech using predefined personas but selectively use demographic attributes, performing better under weak data perspectivism than strong perspectivism or human annotations, though not exceeding human performance for personalized datasets.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' capability in hate speech annotation considering annotator personas within data perspectivism spectra and evaluate against existing annotator modeling techniques.

Method: Evaluated LLM-generated annotations against existing annotator modeling techniques for perspective modeling, using predefined annotator personas across strong-to-weak data perspectivism spectra.

Result: LLMs selectively use demographic attributes from personas, with annotator modeling techniques performing better under weak data perspectivism. For personalized strong perspectivism datasets, LLM performance approached but did not exceed human annotators.

Conclusion: LLM-generated views tend towards aggregation despite subjective prompting, and while effective for weak perspectivism, they cannot surpass human performance for highly personalized annotation tasks requiring strong perspectivism.

Abstract: In this work, we explore the capability of Large Language Models (LLMs) to
annotate hate speech and abusiveness while considering predefined annotator
personas within the strong-to-weak data perspectivism spectra. We evaluated
LLM-generated annotations against existing annotator modeling techniques for
perspective modeling. Our findings show that LLMs selectively use demographic
attributes from the personas. We identified prototypical annotators, with
persona features that show varying degrees of alignment with the original human
annotators. Within the data perspectivism paradigm, annotator modeling
techniques that do not explicitly rely on annotator information performed
better under weak data perspectivism compared to both strong data perspectivism
and human annotations, suggesting LLM-generated views tend towards aggregation
despite subjective prompting. However, for more personalized datasets tailored
to strong perspectivism, the performance of LLM annotator modeling approached,
but did not exceed, human annotators.

</details>


### [42] [Towards Alignment-Centric Paradigm: A Survey of Instruction Tuning in Large Language Models](https://arxiv.org/abs/2508.17184)
*Xudong Han,Junjie Yang,Tianyang Wang,Ziqian Bi,Junfeng Hao,Junhao Song*

Main category: cs.CL

TL;DR: Comprehensive survey on instruction tuning pipeline covering data collection, fine-tuning strategies, and evaluation protocols for aligning LLMs with human intentions.


<details>
  <summary>Details</summary>
Motivation: To provide a systematic overview of instruction tuning techniques for aligning large language models with human intentions, safety constraints, and domain-specific requirements.

Method: Categorizes data construction into expert annotation, distillation from larger models, and self-improvement mechanisms. Examines fine-tuning techniques from full-parameter to lightweight approaches like LoRA and prefix tuning. Evaluates faithfulness, utility, and safety across multilingual and multimodal scenarios.

Result: Identifies distinct trade-offs between quality, scalability, and resource cost across different data collection paradigms. Highlights computational efficiency and model reusability benefits of various fine-tuning strategies.

Conclusion: Closer integration of data, algorithms, and human feedback is essential for advancing instruction-tuned LLMs. The survey serves as a practical reference for designing effective and reliably aligned LLMs.

Abstract: Instruction tuning is a pivotal technique for aligning large language models
(LLMs) with human intentions, safety constraints, and domain-specific
requirements. This survey provides a comprehensive overview of the full
pipeline, encompassing (i) data collection methodologies, (ii) full-parameter
and parameter-efficient fine-tuning strategies, and (iii) evaluation protocols.
We categorized data construction into three major paradigms: expert annotation,
distillation from larger models, and self-improvement mechanisms, each offering
distinct trade-offs between quality, scalability, and resource cost.
Fine-tuning techniques range from conventional supervised training to
lightweight approaches, such as low-rank adaptation (LoRA) and prefix tuning,
with a focus on computational efficiency and model reusability. We further
examine the challenges of evaluating faithfulness, utility, and safety across
multilingual and multimodal scenarios, highlighting the emergence of
domain-specific benchmarks in healthcare, legal, and financial applications.
Finally, we discuss promising directions for automated data generation,
adaptive optimization, and robust evaluation frameworks, arguing that a closer
integration of data, algorithms, and human feedback is essential for advancing
instruction-tuned LLMs. This survey aims to serve as a practical reference for
researchers and practitioners seeking to design LLMs that are both effective
and reliably aligned with human intentions.

</details>


### [43] [Active Domain Knowledge Acquisition with \$100 Budget: Enhancing LLMs via Cost-Efficient, Expert-Involved Interaction in Sensitive Domains](https://arxiv.org/abs/2508.17202)
*Yang Wu,Raha Moraffah,Rujing Yao,Jinhong Yu,Zhimin Tao,Xiaozhong Liu*

Main category: cs.CL

TL;DR: PU-ADKA is a framework that enhances domain-specific LLMs by strategically querying human experts within budget constraints, outperforming traditional fine-tuning in specialized domains like drug discovery.


<details>
  <summary>Details</summary>
Motivation: LLMs lack expert knowledge in specialized, cost-sensitive domains like drug discovery and rare disease research, limiting their effectiveness despite strong general knowledge capabilities.

Method: Proposed PU-ADKA framework that selectively queries appropriate experts based on availability, knowledge boundaries, and consultation costs. Trained using PubMed data simulations and validated through expert interactions and real-world drug development team deployment.

Result: Demonstrated effectiveness in enhancing LLM performance in specialized domains under strict budget constraints. Introduced new benchmark dataset CKAD for cost-effective LLM domain knowledge acquisition.

Conclusion: PU-ADKA provides an efficient approach to bridge the expert knowledge gap in LLMs for specialized domains while respecting budget limitations, enabling better performance in cost-sensitive applications like drug discovery.

Abstract: Large Language Models (LLMs) have demonstrated an impressive level of general
knowledge. However, they often struggle in highly specialized and
cost-sensitive domains such as drug discovery and rare disease research due to
the lack of expert knowledge. In this paper, we propose a novel framework
(PU-ADKA) designed to efficiently enhance domain-specific LLMs by actively
engaging domain experts within a fixed budget. Unlike traditional fine-tuning
approaches, PU-ADKA selectively identifies and queries the most appropriate
expert from a team, taking into account each expert's availability, knowledge
boundaries, and consultation costs. We train PU-ADKA using simulations on
PubMed data and validate it through both controlled expert interactions and
real-world deployment with a drug development team, demonstrating its
effectiveness in enhancing LLM performance in specialized domains under strict
budget constraints. In addition to outlining our methodological innovations and
experimental results, we introduce a new benchmark dataset, CKAD, for
cost-effective LLM domain knowledge acquisition to foster further research in
this challenging area.

</details>


### [44] [SSFO: Self-Supervised Faithfulness Optimization for Retrieval-Augmented Generation](https://arxiv.org/abs/2508.17225)
*Xiaqiang Tang,Yi Wang,Keyu Hu,Rui Xu,Chuang Li,Weigao Sun,Jian Li,Sihong Xie*

Main category: cs.CL

TL;DR: Self-Supervised Faithfulness Optimization (SSFO) is a novel self-supervised alignment approach that enhances RAG faithfulness by constructing preference pairs from context-based vs. context-free outputs and using DPO to transfer probability mass to context-aligned tokens.


<details>
  <summary>Details</summary>
Motivation: Faithfulness hallucination remains a critical challenge in RAG systems, with existing methods requiring costly supervision, post-training, or significant inference burdens.

Method: SSFO constructs preference data pairs by contrasting model outputs with and without context, then uses Direct Preference Optimization (DPO) with a modified loss function to encourage likelihood displacement from parametric-based to context-aligned tokens.

Result: SSFO significantly outperforms existing methods, achieving state-of-the-art faithfulness on multiple context-based QA datasets, with strong generalization to cross-lingual faithfulness while preserving general instruction-following capabilities.

Conclusion: SSFO provides an effective self-supervised solution for RAG faithfulness enhancement without labeling costs or additional inference burden, demonstrating both theoretical and empirical effectiveness through likelihood displacement.

Abstract: Retrieval-Augmented Generation (RAG) systems require Large Language Models
(LLMs) to generate responses that are faithful to the retrieved context.
However, faithfulness hallucination remains a critical challenge, as existing
methods often require costly supervision and post-training or significant
inference burdens. To overcome these limitations, we introduce Self-Supervised
Faithfulness Optimization (SSFO), the first self-supervised alignment approach
for enhancing RAG faithfulness. SSFO constructs preference data pairs by
contrasting the model's outputs generated with and without the context.
Leveraging Direct Preference Optimization (DPO), SSFO aligns model faithfulness
without incurring labeling costs or additional inference burden. We
theoretically and empirically demonstrate that SSFO leverages a benign form of
\emph{likelihood displacement}, transferring probability mass from
parametric-based tokens to context-aligned tokens. Based on this insight, we
propose a modified DPO loss function to encourage likelihood displacement.
Comprehensive evaluations show that SSFO significantly outperforms existing
methods, achieving state-of-the-art faithfulness on multiple context-based
question-answering datasets. Notably, SSFO exhibits strong generalization,
improving cross-lingual faithfulness and preserving general
instruction-following capabilities. We release our code and model at the
anonymous link: https://github.com/chkwy/SSFO

</details>


### [45] [ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generation](https://arxiv.org/abs/2508.17234)
*Siying Zhou,Yiquan Wu,Hui Chen,Xavier Hu,Kun Kuang,Adam Jatowt,Ming Hu,Chunyan Zheng,Fei Wu*

Main category: cs.CL

TL;DR: This paper introduces the first dataset (ClaimGen-CN) for Chinese legal claim generation from case facts, evaluates LLMs on this task, and finds current models lack factual precision and clarity.


<details>
  <summary>Details</summary>
Motivation: Legal claims are essential for judicial reasoning but existing research focuses on professionals, not plaintiffs/non-professionals who need help formulating claims.

Method: Constructed ClaimGen-CN dataset from real legal disputes, designed evaluation metrics for factuality and clarity, conducted zero-shot evaluation of state-of-the-art LLMs.

Result: Current models show limitations in factual precision and expressive clarity for legal claim generation, indicating need for more targeted development.

Conclusion: The research highlights the importance of legal claim generation for non-professionals and provides a benchmark dataset to encourage further development in this domain.

Abstract: Legal claims refer to the plaintiff's demands in a case and are essential to
guiding judicial reasoning and case resolution. While many works have focused
on improving the efficiency of legal professionals, the research on helping
non-professionals (e.g., plaintiffs) remains unexplored. This paper explores
the problem of legal claim generation based on the given case's facts. First,
we construct ClaimGen-CN, the first dataset for Chinese legal claim generation
task, from various real-world legal disputes. Additionally, we design an
evaluation metric tailored for assessing the generated claims, which
encompasses two essential dimensions: factuality and clarity. Building on this,
we conduct a comprehensive zero-shot evaluation of state-of-the-art general and
legal-domain large language models. Our findings highlight the limitations of
the current models in factual precision and expressive clarity, pointing to the
need for more targeted development in this domain. To encourage further
exploration of this important task, we will make the dataset publicly
available.

</details>


### [46] [Routing Distilled Knowledge via Mixture of LoRA Experts for Large Language Model based Bundle Generation](https://arxiv.org/abs/2508.17250)
*Kaidong Feng,Zhu Sun,Hui Fang,Jie Yang,Wenyuan Liu,Yew-Soon Ong*

Main category: cs.CL

TL;DR: RouteDK is a framework that addresses knowledge conflicts in LLM distillation for bundle generation by routing different types of distilled knowledge through LoRA experts with dynamic fusion.


<details>
  <summary>Details</summary>
Motivation: Traditional knowledge distillation from teacher LLMs to student LLMs for bundle generation suffers from knowledge conflicts when integrating diverse knowledge types, leading to performance degradation despite computational efficiency goals.

Method: Proposes RouteDK with two-step approach: 1) Distills complementary knowledge types (high-level generalizable rules and fine-grained session-specific reasoning), 2) Uses mixture of LoRA experts with dynamic fusion module featuring input-aware router to balance expert contributions and mitigate conflicts, plus inference-time enhancement for reliability.

Result: Achieves accuracy comparable to or better than teacher LLM while maintaining computational efficiency, outperforms state-of-the-art bundle generation approaches on three public datasets.

Conclusion: RouteDK effectively resolves knowledge conflicts in LLM distillation for bundle generation through intelligent routing and fusion mechanisms, delivering both performance and efficiency benefits.

Abstract: Large Language Models (LLMs) have shown potential in automatic bundle
generation but suffer from prohibitive computational costs. Although knowledge
distillation offers a pathway to more efficient student models, our preliminary
study reveals that naively integrating diverse types of distilled knowledge
from teacher LLMs into student LLMs leads to knowledge conflict, negatively
impacting the performance of bundle generation. To address this, we propose
RouteDK, a framework for routing distilled knowledge through a mixture of LoRA
expert architecture. Specifically, we first distill knowledge from the teacher
LLM for bundle generation in two complementary types: high-level knowledge
(generalizable rules) and fine-grained knowledge (session-specific reasoning).
We then train knowledge-specific LoRA experts for each type of knowledge
together with a base LoRA expert. For effective integration, we propose a
dynamic fusion module, featuring an input-aware router, where the router
balances expert contributions by dynamically determining optimal weights based
on input, thereby effectively mitigating knowledge conflicts. To further
improve inference reliability, we design an inference-time enhancement module
to reduce variance and mitigate suboptimal reasoning. Experiments on three
public datasets show that our RouteDK achieves accuracy comparable to or even
better than the teacher LLM, while maintaining strong computational efficiency.
In addition, it outperforms state-of-the-art approaches for bundle generation.

</details>


### [47] [Are You Sure You're Positive? Consolidating Chain-of-Thought Agents with Uncertainty Quantification for Aspect-Category Sentiment Analysis](https://arxiv.org/abs/2508.17258)
*Filippos Ventirozos,Peter Appleby,Matthew Shardlow*

Main category: cs.CL

TL;DR: Proposes zero-shot aspect-category sentiment analysis using LLMs with chain-of-thought agents and token-level uncertainty scores to address annotation scarcity and domain transfer issues.


<details>
  <summary>Details</summary>
Motivation: Supervised learning requires expensive annotation and suffers from annotation bias that transfers poorly to new domains. Zero-shot LLM approaches can overcome these limitations when annotation resources are scarce.

Method: Novel techniques combining multiple chain-of-thought agents using LLMs' token-level uncertainty scores. Experiments with 3B and 70B+ parameter variants of Llama and Qwen models.

Result: Demonstrates practical applicability of these approaches for label-scarce conditions, though specific accuracy metrics are not provided in the abstract.

Conclusion: The proposed methods fulfill practical needs in annotation-scarce scenarios and open discussion on how to properly measure accuracy when ground truth labels are limited.

Abstract: Aspect-category sentiment analysis provides granular insights by identifying
specific themes within product reviews that are associated with particular
opinions. Supervised learning approaches dominate the field. However, data is
scarce and expensive to annotate for new domains. We argue that leveraging
large language models in a zero-shot setting is beneficial where the time and
resources required for dataset annotation are limited. Furthermore, annotation
bias may lead to strong results using supervised methods but transfer poorly to
new domains in contexts that lack annotations and demand reproducibility. In
our work, we propose novel techniques that combine multiple chain-of-thought
agents by leveraging large language models' token-level uncertainty scores. We
experiment with the 3B and 70B+ parameter size variants of Llama and Qwen
models, demonstrating how these approaches can fulfil practical needs and
opening a discussion on how to gauge accuracy in label-scarce conditions.

</details>


### [48] [From Language to Action: A Review of Large Language Models as Autonomous Agents and Tool Users](https://arxiv.org/abs/2508.17281)
*Sadia Sultana Chowa,Riasad Alvi,Subhey Sadi Rahman,Md Abdur Rahman,Mohaimenul Azam Khan Raiaan,Md Rafiqul Islam,Mukhtar Hussain,Sami Azam*

Main category: cs.CL

TL;DR: This paper reviews recent developments in using Large Language Models (LLMs) as autonomous agents and tool users, analyzing architectural designs, cognitive mechanisms, benchmarks, and identifying future research directions.


<details>
  <summary>Details</summary>
Motivation: The pursuit of human-level AI has advanced autonomous agents and LLMs, which are now widely used as decision-making agents capable of interpreting instructions, managing tasks, and adapting through feedback.

Method: Structured analysis of 68 publicly available datasets from A*/A-rank conferences and Q1 journals (2023-2025), examining LLM agents' architectural principles, single/multi-agent systems, tool integration, reasoning mechanisms, prompting methods, and fine-tuning procedures.

Result: Identified critical findings on LLMs' verifiable reasoning capabilities, self-improvement capacity, and personalization potential of LLM-based agents across various tasks.

Conclusion: The review provides comprehensive insights into current LLM agent capabilities and outlines ten future research directions to address existing gaps in the field.

Abstract: The pursuit of human-level artificial intelligence (AI) has significantly
advanced the development of autonomous agents and Large Language Models (LLMs).
LLMs are now widely utilized as decision-making agents for their ability to
interpret instructions, manage sequential tasks, and adapt through feedback.
This review examines recent developments in employing LLMs as autonomous agents
and tool users and comprises seven research questions. We only used the papers
published between 2023 and 2025 in conferences of the A* and A rank and Q1
journals. A structured analysis of the LLM agents' architectural design
principles, dividing their applications into single-agent and multi-agent
systems, and strategies for integrating external tools is presented. In
addition, the cognitive mechanisms of LLM, including reasoning, planning, and
memory, and the impact of prompting methods and fine-tuning procedures on agent
performance are also investigated. Furthermore, we evaluated current benchmarks
and assessment protocols and have provided an analysis of 68 publicly available
datasets to assess the performance of LLM-based agents in various tasks. In
conducting this review, we have identified critical findings on verifiable
reasoning of LLMs, the capacity for self-improvement, and the personalization
of LLM-based agents. Finally, we have discussed ten future research directions
to overcome these gaps.

</details>


### [49] [Handling Students Dropouts in an LLM-driven Interactive Online Course Using Language Models](https://arxiv.org/abs/2508.17310)
*Yuanchun Wang,Yiyang Fu,Jifan Yu,Daniel Zhang-Li,Zheyuan Zhang,Joy Lim Jia Yin,Yucheng Wang,Peng Zhou,Jing Zhang,Huiqin Liu*

Main category: cs.CL

TL;DR: Empirical study on LLM-driven interactive online courses shows strong links between dropout behaviors and textual interaction patterns, proposes prediction framework with 95.4% accuracy, and implements personalized email recall agent to reduce dropouts.


<details>
  <summary>Details</summary>
Motivation: To understand and address dropout issues in Massive AI-empowered Courses (MAIC) that transform passive MOOCs into dynamic, text-based interactive learning platforms using LLM-driven multi-agent systems.

Method: Analyzed interaction logs to define dropouts and identify contributing factors, developed course-progress-adaptive dropout prediction framework (CPADP), and designed personalized email recall agent for re-engagement.

Result: Found strong correlations between dropout behaviors and textual interaction patterns, achieved up to 95.4% accuracy in dropout prediction, and validated approach with over 3,000 students from diverse backgrounds.

Conclusion: The study demonstrates that AI-driven interactive learning environments can effectively predict and reduce dropouts through personalized interventions, showing feasibility and effectiveness in real-world deployment.

Abstract: Interactive online learning environments, represented by Massive AI-empowered
Courses (MAIC), leverage LLM-driven multi-agent systems to transform passive
MOOCs into dynamic, text-based platforms, enhancing interactivity through LLMs.
This paper conducts an empirical study on a specific MAIC course to explore
three research questions about dropouts in these interactive online courses:
(1) What factors might lead to dropouts? (2) Can we predict dropouts? (3) Can
we reduce dropouts? We analyze interaction logs to define dropouts and identify
contributing factors. Our findings reveal strong links between dropout
behaviors and textual interaction patterns. We then propose a
course-progress-adaptive dropout prediction framework (CPADP) to predict
dropouts with at most 95.4% accuracy. Based on this, we design a personalized
email recall agent to re-engage at-risk students. Applied in the deployed MAIC
system with over 3,000 students, the feasibility and effectiveness of our
approach have been validated on students with diverse backgrounds.

</details>


### [50] [CultranAI at PalmX 2025: Data Augmentation for Cultural Knowledge Representation](https://arxiv.org/abs/2508.17324)
*Hunzalah Hassan Bhatti,Youssef Ahmed,Md Arid Hasan,Firoj Alam*

Main category: cs.CL

TL;DR: CultranAI system used data augmentation and LoRA fine-tuning of LLMs for Arabic cultural knowledge evaluation, achieving 5th place with 70.50% accuracy on blind test.


<details>
  <summary>Details</summary>
Motivation: To develop an effective system for Arabic cultural knowledge representation and evaluation using large language models, participating in the PalmX cultural evaluation shared task.

Method: Benchmarked multiple LLMs, augmented PalmX dataset with Palm dataset and curated 22K+ culturally grounded MCQs, then performed LoRA fine-tuning of the best-performing Fanar-1-9B-Instruct model.

Result: System ranked 5th with 70.50% accuracy on blind test set and achieved 84.1% accuracy on PalmX development set using the fine-tuned Fanar-1-9B-Instruct model.

Conclusion: Data augmentation and LoRA fine-tuning of specialized LLMs like Fanar-1-9B-Instruct can effectively improve performance on Arabic cultural knowledge tasks, demonstrating the value of curated cultural datasets for model training.

Abstract: In this paper, we report our participation to the PalmX cultural evaluation
shared task. Our system, CultranAI, focused on data augmentation and LoRA
fine-tuning of large language models (LLMs) for Arabic cultural knowledge
representation. We benchmarked several LLMs to identify the best-performing
model for the task. In addition to utilizing the PalmX dataset, we augmented it
by incorporating the Palm dataset and curated a new dataset of over 22K
culturally grounded multiple-choice questions (MCQs). Our experiments showed
that the Fanar-1-9B-Instruct model achieved the highest performance. We
fine-tuned this model on the combined augmented dataset of 22K+ MCQs. On the
blind test set, our submitted system ranked 5th with an accuracy of 70.50%,
while on the PalmX development set, it achieved an accuracy of 84.1%.

</details>


### [51] [Omne-R1: Learning to Reason with Memory for Multi-hop Question Answering](https://arxiv.org/abs/2508.17330)
*Boyuan Liu,Feng Ji,Jiayan Nan,Han Zhao,Weiling Chen,Shihao Xu,Xing Zhou*

Main category: cs.CL

TL;DR: Omne-R1 enhances multi-hop QA on schema-free knowledge graphs using multi-stage training with RL and supervised fine-tuning, achieving significant improvements on complex questions.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of limited suitable knowledge graphs and QA data for multi-hop question answering by developing domain-independent solutions.

Method: Multi-stage training workflow with two reinforcement learning phases and one supervised fine-tuning phase, constructing domain-independent knowledge graphs and auto-generating QA pairs.

Result: Significant improvements in answering multi-hop questions, with notable performance gains on complex 3+ hop questions.

Conclusion: The proposed training framework demonstrates strong generalization abilities across diverse knowledge domains for multi-hop QA tasks.

Abstract: This paper introduces Omne-R1, a novel approach designed to enhance multi-hop
question answering capabilities on schema-free knowledge graphs by integrating
advanced reasoning models. Our method employs a multi-stage training workflow,
including two reinforcement learning phases and one supervised fine-tuning
phase. We address the challenge of limited suitable knowledge graphs and QA
data by constructing domain-independent knowledge graphs and auto-generating QA
pairs. Experimental results show significant improvements in answering
multi-hop questions, with notable performance gains on more complex 3+ hop
questions. Our proposed training framework demonstrates strong generalization
abilities across diverse knowledge domains.

</details>


### [52] [DropLoRA: Sparse Low-Rank Adaptation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2508.17337)
*Haojie Zhang*

Main category: cs.CL

TL;DR: DropLoRA introduces a pruning-based approach that dynamically adapts low-rank subspaces during fine-tuning, overcoming LoRA's static subspace limitations without extra computational costs.


<details>
  <summary>Details</summary>
Motivation: LoRA's low-rank updates create a performance gap compared to full-parameter fine-tuning due to static subspace learning limitations.

Method: Integrates a pruning module between LoRA's two low-rank matrices to enable dynamic subspace learning through rank dimension pruning.

Result: Consistently outperforms LoRA in fine-tuning LLaMA models across commonsense reasoning, mathematical reasoning, code generation, and instruction-following tasks.

Conclusion: DropLoRA effectively addresses LoRA's performance limitations through dynamic subspace adaptation, achieving superior results without additional training or inference costs.

Abstract: LoRA-based large model parameter-efficient fine-tuning (PEFT) methods use
low-rank de- composition to approximate updates to model parameters. However,
compared to full- parameter fine-tuning, low-rank updates often lead to a
performance gap in downstream tasks. To address this, we introduce DropLoRA, a
novel pruning-based approach that focuses on pruning the rank dimension. Unlike
conven- tional methods that attempt to overcome the low-rank bottleneck,
DropLoRA innovatively integrates a pruning module between the two low-rank
matrices in LoRA to simulate dy- namic subspace learning. This dynamic low-
rank subspace learning allows DropLoRA to overcome the limitations of
traditional LoRA, which operates within a static subspace. By continuously
adapting the learning subspace, DropLoRA significantly boosts performance
without incurring additional training or infer- ence costs. Our experimental
results demon- strate that DropLoRA consistently outperforms LoRA in
fine-tuning the LLaMA series across a wide range of large language model gener-
ation tasks, including commonsense reason- ing, mathematical reasoning, code
generation, and instruction-following. Our code is avail- able at
https://github.com/TayeeChang/DropLoRA.

</details>


### [53] [Capturing Legal Reasoning Paths from Facts to Law in Court Judgments using Knowledge Graphs](https://arxiv.org/abs/2508.17340)
*Ryoma Kondo,Riona Matsuoka,Takahiro Yoshida,Kazuyuki Yamasawa,Ryohei Hisano*

Main category: cs.CL

TL;DR: Constructed a legal knowledge graph from Japanese court decisions using LLMs to extract and structure legal reasoning components, achieving better legal provision retrieval than baseline methods.


<details>
  <summary>Details</summary>
Motivation: Existing automated approaches fail to accurately capture legal reasoning structure, identify relevant legal context, and trace how facts relate to legal norms in court judgments.

Method: Extracted legal reasoning components using prompt-based LLMs from 648 Japanese administrative court decisions, normalized legal provision references, and linked facts, norms, and legal applications through a legal inference ontology.

Result: The system achieved more accurate retrieval of relevant legal provisions from facts compared to LLM baselines and retrieval-augmented methods, as evaluated by expert annotated data.

Conclusion: The legal knowledge graph successfully captures the full structure of legal reasoning from real court decisions, making implicit reasoning explicit and machine-readable while improving legal provision retrieval accuracy.

Abstract: Court judgments reveal how legal rules have been interpreted and applied to
facts, providing a foundation for understanding structured legal reasoning.
However, existing automated approaches for capturing legal reasoning, including
large language models, often fail to identify the relevant legal context, do
not accurately trace how facts relate to legal norms, and may misrepresent the
layered structure of judicial reasoning. These limitations hinder the ability
to capture how courts apply the law to facts in practice. In this paper, we
address these challenges by constructing a legal knowledge graph from 648
Japanese administrative court decisions. Our method extracts components of
legal reasoning using prompt-based large language models, normalizes references
to legal provisions, and links facts, norms, and legal applications through an
ontology of legal inference. The resulting graph captures the full structure of
legal reasoning as it appears in real court decisions, making implicit
reasoning explicit and machine-readable. We evaluate our system using expert
annotated data, and find that it achieves more accurate retrieval of relevant
legal provisions from facts than large language model baselines and
retrieval-augmented methods.

</details>


### [54] [The Arabic Generality Score: Another Dimension of Modeling Arabic Dialectness](https://arxiv.org/abs/2508.17347)
*Sanad Shaban,Nizar Habash*

Main category: cs.CL

TL;DR: AGS (Arabic Generality Score) measures how widely words are used across Arabic dialects, providing a complementary approach to ALDi's single-dimension dialectness modeling.


<details>
  <summary>Details</summary>
Motivation: Current Arabic NLP models treat dialects as discrete categories, and ALDi reduces complex dialect variation to just one dimension, failing to capture the continuum nature of Arabic dialects.

Method: Developed a pipeline combining word alignment, etymology-aware edit distance, and smoothing to annotate parallel corpus with word-level AGS, then trained a regression model to predict AGS in context.

Result: Outperformed strong baselines including state-of-the-art dialect ID systems on a multi-dialect benchmark.

Conclusion: AGS provides a scalable, linguistically grounded method to model lexical generality, enriching representations of Arabic dialectness beyond single-dimension approaches.

Abstract: Arabic dialects form a diverse continuum, yet NLP models often treat them as
discrete categories. Recent work addresses this issue by modeling dialectness
as a continuous variable, notably through the Arabic Level of Dialectness
(ALDi). However, ALDi reduces complex variation to a single dimension. We
propose a complementary measure: the Arabic Generality Score (AGS), which
quantifies how widely a word is used across dialects. We introduce a pipeline
that combines word alignment, etymology-aware edit distance, and smoothing to
annotate a parallel corpus with word-level AGS. A regression model is then
trained to predict AGS in context. Our approach outperforms strong baselines,
including state-of-the-art dialect ID systems, on a multi-dialect benchmark.
AGS offers a scalable, linguistically grounded way to model lexical generality,
enriching representations of Arabic dialectness.

</details>


### [55] [UI-Level Evaluation of ALLaM 34B: Measuring an Arabic-Centric LLM via HUMAIN Chat](https://arxiv.org/abs/2508.17378)
*Omer Nacar*

Main category: cs.CL

TL;DR: Comprehensive evaluation of ALLaM-34B Arabic LLM shows strong performance across Arabic dialects, reasoning, and safety tasks with high scores (4.21-4.92/5) in all categories.


<details>
  <summary>Details</summary>
Motivation: Address the gap in Arabic language capabilities of English-trained LLMs by evaluating the performance of Saudi-developed ALLaM-34B model across various Arabic linguistic and cultural dimensions.

Method: Used 23 diverse prompts covering modern standard Arabic, 5 regional dialects, code-switching, factual knowledge, arithmetic/temporal reasoning, creative generation, and adversarial safety. Collected 115 outputs (5 runs per prompt) and scored with three frontier LLM judges (GPT-5, Gemini 2.5 Pro, Claude Sonnet-4).

Result: ALLaM-34B achieved consistently high performance: generation and code-switching (4.92/5), MSA handling (4.74/5), reasoning (4.64/5), dialect fidelity (4.21/5), and safety (4.54/5).

Conclusion: ALLaM-34B is a robust, culturally grounded Arabic LLM with both technical strength and practical readiness for real-world deployment, effectively addressing Arabic linguistic and cultural nuances.

Abstract: Large language models (LLMs) trained primarily on English corpora often
struggle to capture the linguistic and cultural nuances of Arabic. To address
this gap, the Saudi Data and AI Authority (SDAIA) introduced the $ALLaM$ family
of Arabic-focused models. The most capable of these available to the public,
$ALLaM-34B$, was subsequently adopted by HUMAIN, who developed and deployed
HUMAIN Chat, a closed conversational web service built on this model. This
paper presents an expanded and refined UI-level evaluation of $ALLaM-34B$.
Using a prompt pack spanning modern standard Arabic, five regional dialects,
code-switching, factual knowledge, arithmetic and temporal reasoning, creative
generation, and adversarial safety, we collected 115 outputs (23 prompts times
5 runs) and scored each with three frontier LLM judges (GPT-5, Gemini 2.5 Pro,
Claude Sonnet-4). We compute category-level means with 95\% confidence
intervals, analyze score distributions, and visualize dialect-wise metric heat
maps. The updated analysis reveals consistently high performance on generation
and code-switching tasks (both averaging 4.92/5), alongside strong results in
MSA handling (4.74/5), solid reasoning ability (4.64/5), and improved dialect
fidelity (4.21/5). Safety-related prompts show stable, reliable performance of
(4.54/5). Taken together, these results position $ALLaM-34B$ as a robust and
culturally grounded Arabic LLM, demonstrating both technical strength and
practical readiness for real-world deployment.

</details>


### [56] [Agent-Testing Agent: A Meta-Agent for Automated Testing and Evaluation of Conversational AI Agents](https://arxiv.org/abs/2508.17393)
*Sameer Komoravolu,Khalil Mrini*

Main category: cs.CL

TL;DR: ATA is an automated meta-agent that tests LLM agents by generating adaptive adversarial tests through code analysis, designer interrogation, and literature mining, outperforming human annotators in finding diverse failures quickly.


<details>
  <summary>Details</summary>
Motivation: Current evaluation of LLM agents relies on static benchmarks and small human studies, which are limited in scope and scalability. There's a need for automated, comprehensive testing that can identify diverse and severe failures efficiently.

Method: ATA combines static code analysis, designer interrogation, literature mining, and persona-driven adversarial test generation. It uses LLM-as-a-Judge feedback to adapt test difficulty and steer tests toward the agent's weakest capabilities.

Result: ATA surfaces more diverse and severe failures than expert annotators while matching severity assessment. It completes testing in 20-30 minutes versus days required for human annotators. Ablation studies show code analysis and web search reduce variance and miscalibration.

Conclusion: ATA provides an effective automated testing framework for LLM agents that outperforms human evaluation in efficiency and failure discovery. The evidence-grounded approach with quantitative metrics and qualitative bug reports offers valuable tools for developers.

Abstract: LLM agents are increasingly deployed to plan, retrieve, and write with tools,
yet evaluation still leans on static benchmarks and small human studies. We
present the Agent-Testing Agent (ATA), a meta-agent that combines static code
analysis, designer interrogation, literature mining, and persona-driven
adversarial test generation whose difficulty adapts via judge feedback. Each
dialogue is scored with an LLM-as-a-Judge (LAAJ) rubric and used to steer
subsequent tests toward the agent's weakest capabilities. On a travel planner
and a Wikipedia writer, the ATA surfaces more diverse and severe failures than
expert annotators while matching severity, and finishes in 20--30 minutes
versus ten-annotator rounds that took days. Ablating code analysis and web
search increases variance and miscalibration, underscoring the value of
evidence-grounded test generation. The ATA outputs quantitative metrics and
qualitative bug reports for developers. We release the full methodology and
open-source implementation for reproducible agent testing:
https://github.com/KhalilMrini/Agent-Testing-Agent

</details>


### [57] [DashboardQA: Benchmarking Multimodal Agents for Question Answering on Interactive Dashboards](https://arxiv.org/abs/2508.17398)
*Aaryaman Kartha,Ahmed Masry,Mohammed Saidul Islam,Thinh Lang,Shadikur Rahman,Ridwan Mahbub,Mizanur Rahman,Mahir Ahmed,Md Rizwan Parvez,Enamul Hoque,Shafiq Joty*

Main category: cs.CL

TL;DR: DashboardQA is the first benchmark for evaluating vision-language GUI agents' ability to understand and interact with interactive dashboards, revealing significant limitations in current models.


<details>
  <summary>Details</summary>
Motivation: Existing visualization benchmarks focus on static charts and overlook dashboard interactivity, which is essential for real-world analytical workflows and GUI-based reasoning.

Method: Created a benchmark with 112 interactive dashboards from Tableau Public and 405 question-answer pairs across five categories: multiple-choice, factoid, hypothetical, multi-dashboard, and conversational questions.

Result: Current GUI agents perform poorly, with the best agent (Gemini-Pro-2.5) achieving only 38.69% accuracy and OpenAI CUA agent reaching just 22.69%, showing significant challenges in dashboard reasoning.

Conclusion: Interactive dashboard reasoning is a challenging task for vision-language models, highlighting the need for improved capabilities in grounding dashboard elements, planning interactions, and performing reasoning.

Abstract: Dashboards are powerful visualization tools for data-driven decision-making,
integrating multiple interactive views that allow users to explore, filter, and
navigate data. Unlike static charts, dashboards support rich interactivity,
which is essential for uncovering insights in real-world analytical workflows.
However, existing question-answering benchmarks for data visualizations largely
overlook this interactivity, focusing instead on static charts. This limitation
severely constrains their ability to evaluate the capabilities of modern
multimodal agents designed for GUI-based reasoning. To address this gap, we
introduce DashboardQA, the first benchmark explicitly designed to assess how
vision-language GUI agents comprehend and interact with real-world dashboards.
The benchmark includes 112 interactive dashboards from Tableau Public and 405
question-answer pairs with interactive dashboards spanning five categories:
multiple-choice, factoid, hypothetical, multi-dashboard, and conversational. By
assessing a variety of leading closed- and open-source GUI agents, our analysis
reveals their key limitations, particularly in grounding dashboard elements,
planning interaction trajectories, and performing reasoning. Our findings
indicate that interactive dashboard reasoning is a challenging task overall for
all the VLMs evaluated. Even the top-performing agents struggle; for instance,
the best agent based on Gemini-Pro-2.5 achieves only 38.69% accuracy, while the
OpenAI CUA agent reaches just 22.69%, demonstrating the benchmark's significant
difficulty. We release DashboardQA at https://github.com/vis-nlp/DashboardQA

</details>


### [58] [DS@GT at CheckThat! 2025: A Simple Retrieval-First, LLM-Backed Framework for Claim Normalization](https://arxiv.org/abs/2508.17402)
*Aleksandar Pramov,Jiangqin Ma,Bina Patel*

Main category: cs.CL

TL;DR: A lightweight retrieval-first, LLM-backed pipeline for claim normalization that achieves top performance in monolingual settings but struggles with zero-shot cross-lingual transfer.


<details>
  <summary>Details</summary>
Motivation: Claim normalization is crucial for fact-checking systems to parse noisy social media data into structured claims for downstream verification tasks, and the CheckThat! 2025 Task 2 addresses this across 20 languages.

Method: A two-pronged approach: dynamically prompting GPT-4o-mini with in-context examples or directly retrieving the closest normalization from the training dataset.

Result: Ranked near the top for most monolingual tracks, achieving 1st place in 7 out of 13 languages, but underperformed in zero-shot cross-lingual settings.

Conclusion: The proposed lightweight pipeline is effective for monolingual claim normalization but has limitations in zero-shot scenarios, indicating challenges with cross-lingual generalization.

Abstract: Claim normalization is an integral part of any automatic fact-check
verification system. It parses the typically noisy claim data, such as social
media posts into normalized claims, which are then fed into downstream veracity
classification tasks. The CheckThat! 2025 Task 2 focuses specifically on claim
normalization and spans 20 languages under monolingual and zero-shot
conditions. Our proposed solution consists of a lightweight
\emph{retrieval-first, LLM-backed} pipeline, in which we either dynamically
prompt a GPT-4o-mini with in-context examples, or retrieve the closest
normalization from the train dataset directly. On the official test set, the
system ranks near the top for most monolingual tracks, achieving first place in
7 out of of the 13 languages. In contrast, the system underperforms in the
zero-shot setting, highlighting the limitation of the proposed solution.

</details>


### [59] [MahaParaphrase: A Marathi Paraphrase Detection Corpus and BERT-based Models](https://arxiv.org/abs/2508.17444)
*Suramya Jadhav,Abhay Shanbhag,Amogh Thakurdesai,Ridhima Sinare,Ananya Joshi,Raviraj Joshi*

Main category: cs.CL

TL;DR: L3Cube-MahaParaphrase Dataset: A high-quality human-annotated paraphrase corpus of 8,000 Marathi sentence pairs, with benchmark results from transformer-based BERT models.


<details>
  <summary>Details</summary>
Motivation: Paraphrases are crucial for NLP tasks but Indic languages like Marathi face challenges due to morphological complexity, script diversity, and limited annotated data availability.

Method: Created a paraphrase corpus with 8,000 Marathi sentence pairs manually annotated by human experts as Paraphrase (P) or Non-paraphrase (NP), and evaluated standard transformer-based BERT models on this dataset.

Result: Developed a high-quality paraphrase dataset for Marathi, a low-resource Indic language, and provided benchmark performance results using BERT models.

Conclusion: The L3Cube-MahaParaphrase Dataset addresses the data scarcity for Marathi NLP tasks and serves as a valuable resource for paraphrase detection and related applications in low-resource languages.

Abstract: Paraphrases are a vital tool to assist language understanding tasks such as
question answering, style transfer, semantic parsing, and data augmentation
tasks. Indic languages are complex in natural language processing (NLP) due to
their rich morphological and syntactic variations, diverse scripts, and limited
availability of annotated data. In this work, we present the
L3Cube-MahaParaphrase Dataset, a high-quality paraphrase corpus for Marathi, a
low resource Indic language, consisting of 8,000 sentence pairs, each annotated
by human experts as either Paraphrase (P) or Non-paraphrase (NP). We also
present the results of standard transformer-based BERT models on these
datasets. The dataset and model are publicly shared at
https://github.com/l3cube-pune/MarathiNLP

</details>


### [60] [Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability in Knowledge and Safety with DuET-PD](https://arxiv.org/abs/2508.17450)
*Bryan Chen Zhengyu Tan,Daniel Wai Kit Chin,Zhengyuan Liu,Nancy F. Chen,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: DuET-PD framework evaluates LLM trustworthiness in persuasive dialogues, revealing GPT-4o's 27.32% accuracy under misleading persuasion and increasing sycophancy in newer models. Holistic DPO training improves robustness to misinformation and receptiveness to corrections.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle to balance gullibility to misinformation and resistance to valid corrections in persuasive dialogues, which is critical for reliable deployment.

Method: Introduced DuET-PD framework evaluating multi-turn stance-change across persuasion type (corrective/misleading) and domain (knowledge/safety). Developed Holistic DPO training approach balancing positive and negative persuasion examples.

Result: GPT-4o achieved only 27.32% accuracy in MMLU-Pro under sustained misleading persuasion. Holistic DPO improved Llama-3.1-8B-Instruct's accuracy under misleading persuasion in safety contexts from 4.21% to 76.54%.

Conclusion: The framework and training approach offer a pathway to developing more reliable and adaptable LLMs for multi-turn dialogue, addressing both misinformation robustness and correction receptiveness.

Abstract: Large Language Models (LLMs) can struggle to balance gullibility to
misinformation and resistance to valid corrections in persuasive dialogues, a
critical challenge for reliable deployment. We introduce DuET-PD (Dual
Evaluation for Trust in Persuasive Dialogues), a framework evaluating
multi-turn stance-change dynamics across dual dimensions: persuasion type
(corrective/misleading) and domain (knowledge via MMLU-Pro, and safety via
SALAD-Bench). We find that even a state-of-the-art model like GPT-4o achieves
only 27.32% accuracy in MMLU-Pro under sustained misleading persuasions.
Moreover, results reveal a concerning trend of increasing sycophancy in newer
open-source models. To address this, we introduce Holistic DPO, a training
approach balancing positive and negative persuasion examples. Unlike prompting
or resist-only training, Holistic DPO enhances both robustness to
misinformation and receptiveness to corrections, improving
Llama-3.1-8B-Instruct's accuracy under misleading persuasion in safety contexts
from 4.21% to 76.54%. These contributions offer a pathway to developing more
reliable and adaptable LLMs for multi-turn dialogue. Code is available at
https://github.com/Social-AI-Studio/DuET-PD.

</details>


### [61] [Evaluating the Impact of Verbal Multiword Expressions on Machine Translation](https://arxiv.org/abs/2508.17458)
*Linfeng Liu,Saptarshi Ghosh,Tianyu Jiang*

Main category: cs.CL

TL;DR: VMWEs negatively impact machine translation quality, but LLM-based paraphrasing to literal counterparts improves translation for verbal idioms and verb-particle constructions.


<details>
  <summary>Details</summary>
Motivation: Verbal multiword expressions (VMWEs) are challenging for NLP due to their non-compositional nature, and machine translation systems struggle with accurately translating these complex linguistic structures despite recent advancements.

Method: Analyzed three VMWE categories (verbal idioms, verb-particle constructions, light verb constructions) using established datasets and extracted sentences from MT datasets. Evaluated state-of-the-art translation systems and proposed LLM-based paraphrasing approach to replace expressions with literal counterparts.

Result: Experimental results consistently showed that VMWEs negatively affect translation quality. The proposed LLM-based paraphrasing approach demonstrated significant improvement in translation quality for verbal idioms and verb-particle constructions.

Conclusion: VMWEs present persistent challenges for machine translation, but targeted paraphrasing approaches can effectively mitigate these issues and improve translation quality for specific VMWE categories.

Abstract: Verbal multiword expressions (VMWEs) present significant challenges for
natural language processing due to their complex and often non-compositional
nature. While machine translation models have seen significant improvement with
the advent of language models in recent years, accurately translating these
complex linguistic structures remains an open problem. In this study, we
analyze the impact of three VMWE categories -- verbal idioms, verb-particle
constructions, and light verb constructions -- on machine translation quality
from English to multiple languages. Using both established multiword expression
datasets and sentences containing these language phenomena extracted from
machine translation datasets, we evaluate how state-of-the-art translation
systems handle these expressions. Our experimental results consistently show
that VMWEs negatively affect translation quality. We also propose an LLM-based
paraphrasing approach that replaces these expressions with their literal
counterparts, demonstrating significant improvement in translation quality for
verbal idioms and verb-particle constructions.

</details>


### [62] [Efficient Zero-Shot Long Document Classification by Reducing Context Through Sentence Ranking](https://arxiv.org/abs/2508.17490)
*Prathamesh Kokate,Mitali Sarnaik,Manavi Khopade,Mukta Takalikar,Raviraj Joshi*

Main category: cs.CL

TL;DR: Zero-shot long document classification using sentence ranking to reduce input length while maintaining accuracy, achieving 35% faster inference by keeping only top 50% sentences.


<details>
  <summary>Details</summary>
Motivation: Transformer models like BERT struggle with long document classification due to input length limitations and computational inefficiencies, requiring methods to adapt short-text trained models to long documents.

Method: TF-IDF-based sentence ranking strategy to select most informative sentences from long documents, enabling zero-shot adaptation without changing model architecture. Evaluated three context reduction strategies on Marathi news dataset.

Result: Retaining only top 50% ranked sentences maintains comparable classification performance to full-document inference while reducing inference time by up to 35%.

Conclusion: Sentence ranking is a simple yet effective technique for scalable and efficient zero-shot long document classification, enabling adaptation of short-text models to long documents without architectural changes.

Abstract: Transformer-based models like BERT excel at short text classification but
struggle with long document classification (LDC) due to input length
limitations and computational inefficiencies. In this work, we propose an
efficient, zero-shot approach to LDC that leverages sentence ranking to reduce
input context without altering the model architecture. Our method enables the
adaptation of models trained on short texts, such as headlines, to long-form
documents by selecting the most informative sentences using a TF-IDF-based
ranking strategy. Using the MahaNews dataset of long Marathi news articles, we
evaluate three context reduction strategies that prioritize essential content
while preserving classification accuracy. Our results show that retaining only
the top 50\% ranked sentences maintains performance comparable to full-document
inference while reducing inference time by up to 35\%. This demonstrates that
sentence ranking is a simple yet effective technique for scalable and efficient
zero-shot LDC.

</details>


### [63] [Improving French Synthetic Speech Quality via SSML Prosody Control](https://arxiv.org/abs/2508.17494)
*Nassima Ould Ouali,Awais Hussain Sani,Ruben Bueno,Jonah Dauvet,Tim Luka Horstmann,Eric Moulines*

Main category: cs.CL

TL;DR: End-to-end pipeline using two fine-tuned Qwen 2.5-7B models to insert SSML tags for prosody control in French TTS, achieving significant improvements in break prediction accuracy, prosody metrics, and listener preference.


<details>
  <summary>Details</summary>
Motivation: Synthetic voices lack expressiveness due to limited prosody control in commercial TTS systems, particularly for French speech.

Method: Cascaded architecture with two QLoRA-fine-tuned Qwen 2.5-7B models: one predicts phrase-break positions, the other performs regression on prosodic targets to generate SSML markup compatible with commercial TTS.

Result: 99.2% F1 for break placement, 25-40% reduction in MAE for pitch/rate/volume vs baselines. MOS increased from 3.20 to 3.87 (p<0.005), with 15/18 listeners preferring enhanced synthesis.

Conclusion: Substantial progress in bridging expressiveness gap between synthetic and natural French speech, with publicly available code.

Abstract: Despite recent advances, synthetic voices often lack expressiveness due to
limited prosody control in commercial text-to-speech (TTS) systems. We
introduce the first end-to-end pipeline that inserts Speech Synthesis Markup
Language (SSML) tags into French text to control pitch, speaking rate, volume,
and pause duration. We employ a cascaded architecture with two QLoRA-fine-tuned
Qwen 2.5-7B models: one predicts phrase-break positions and the other performs
regression on prosodic targets, generating commercial TTS-compatible SSML
markup. Evaluated on a 14-hour French podcast corpus, our method achieves 99.2%
F1 for break placement and reduces mean absolute error on pitch, rate, and
volume by 25-40% compared with prompting-only large language models (LLMs) and
a BiLSTM baseline. In perceptual evaluation involving 18 participants across
over 9 hours of synthesized audio, SSML-enhanced speech generated by our
pipeline significantly improves naturalness, with the mean opinion score
increasing from 3.20 to 3.87 (p < 0.005). Additionally, 15 of 18 listeners
preferred our enhanced synthesis. These results demonstrate substantial
progress in bridging the expressiveness gap between synthetic and natural
French speech. Our code is publicly available at
https://github.com/hi-paris/Prosody-Control-French-TTS.

</details>


### [64] [Debate or Vote: Which Yields Better Decisions in Multi-Agent Large Language Models?](https://arxiv.org/abs/2508.17536)
*Hyeong Kyu Choi,Xiaojin Zhu,Yixuan Li*

Main category: cs.CL

TL;DR: Majority voting, not debate, drives most performance gains in multi-agent debate systems. Debate alone doesn't improve expected correctness, but targeted interventions can enhance effectiveness.


<details>
  <summary>Details</summary>
Motivation: To understand the key factors behind multi-agent debate's effectiveness and disentangle the contributions of majority voting versus inter-agent debate components.

Method: Disentangled MAD into majority voting and debate components, conducted experiments across 7 NLP benchmarks, proposed theoretical framework modeling debate as stochastic process, and tested targeted interventions.

Result: Majority voting alone accounts for most performance gains typically attributed to MAD. Debate induces martingale belief trajectories that don't improve expected correctness without intervention.

Conclusion: While MAD has potential, simple ensembling methods remain strong and more reliable alternatives in many practical settings, though targeted interventions can meaningfully enhance debate effectiveness.

Abstract: Multi-Agent Debate~(MAD) has emerged as a promising paradigm for improving
the performance of large language models through collaborative reasoning.
Despite recent advances, the key factors driving MAD's effectiveness remain
unclear. In this work, we disentangle MAD into two key components--Majority
Voting and inter-agent Debate--and assess their respective contributions.
Through extensive experiments across seven NLP benchmarks, we find that
Majority Voting alone accounts for most of the performance gains typically
attributed to MAD. To explain this, we propose a theoretical framework that
models debate as a stochastic process. We prove that it induces a martingale
over agents' belief trajectories, implying that debate alone does not improve
expected correctness. Guided by these insights, we demonstrate that targeted
interventions, by biasing the belief update toward correction, can meaningfully
enhance debate effectiveness. Overall, our findings suggest that while MAD has
potential, simple ensembling methods remain strong and more reliable
alternatives in many practical settings. Code is released in
https://github.com/deeplearning-wisc/debate-or-vote.

</details>


### [65] [Humanizing Machines: Rethinking LLM Anthropomorphism Through a Multi-Level Framework of Design](https://arxiv.org/abs/2508.17573)
*Yunze Xiao,Lynnette Hui Xian Ng,Jiarui Liu,Mona T. Diab*

Main category: cs.CL

TL;DR: The paper proposes treating anthropomorphism in LLMs as a design concept rather than a risk, offering a taxonomy of four cue dimensions (perceptive, linguistic, behavioral, cognitive) for intentional tuning to support user goals.


<details>
  <summary>Details</summary>
Motivation: Current research on LLM anthropomorphism focuses too much on risks like over-trust and deception, while providing limited design guidance. The authors argue anthropomorphism should be treated as a design concept that can be intentionally tuned.

Method: Drawing from multiple disciplines, the authors propose that anthropomorphism reflects interaction between designers and interpreters through embedded cues. They categorize cues into four dimensions: perceptive, linguistic, behavioral, and cognitive.

Result: The paper provides a unified taxonomy with actionable levers for practitioners by analyzing the manifestation and effectiveness of each cue dimension.

Conclusion: The authors advocate for function-oriented evaluations of anthropomorphic design, treating anthropomorphism as an intentional design choice rather than just a risk factor.

Abstract: Large Language Models (LLMs) increasingly exhibit \textbf{anthropomorphism}
characteristics -- human-like qualities portrayed across their outlook,
language, behavior, and reasoning functions. Such characteristics enable more
intuitive and engaging human-AI interactions. However, current research on
anthropomorphism remains predominantly risk-focused, emphasizing over-trust and
user deception while offering limited design guidance. We argue that
anthropomorphism should instead be treated as a \emph{concept of design} that
can be intentionally tuned to support user goals. Drawing from multiple
disciplines, we propose that the anthropomorphism of an LLM-based artifact
should reflect the interaction between artifact designers and interpreters.
This interaction is facilitated by cues embedded in the artifact by the
designers and the (cognitive) responses of the interpreters to the cues. Cues
are categorized into four dimensions: \textit{perceptive, linguistic,
behavioral}, and \textit{cognitive}. By analyzing the manifestation and
effectiveness of each cue, we provide a unified taxonomy with actionable levers
for practitioners. Consequently, we advocate for function-oriented evaluations
of anthropomorphic design.

</details>


### [66] [CausalSent: Interpretable Sentiment Classification with RieszNet](https://arxiv.org/abs/2508.17576)
*Daniel Frees,Martin Pollack*

Main category: cs.CL

TL;DR: CausalSent framework improves treatment effect estimation accuracy in NLP models using RieszNet-based architecture, reducing MAE by 2-3x compared to previous work and demonstrating causal effects of specific words like "love" on sentiment.


<details>
  <summary>Details</summary>
Motivation: Despite high performance of modern NLP models, their decision-making processes remain black boxes. The paper aims to improve model interpretability by combining causal inference with NLP to elucidate causal effects of text features.

Method: Developed a two-headed RieszNet-based neural network architecture for better treatment effect estimation. Used semi-synthetic IMDB movie reviews data and performed observational case study with ensemble of validated models.

Result: CausalSent framework reduced MAE of effect estimates by 2-3x compared to Bansal et al's work. Found that presence of the word "love" causes a +2.9% increase in probability of positive sentiment in movie reviews.

Conclusion: The proposed CausalSent framework successfully improves treatment effect estimation accuracy and provides interpretable causal insights into NLP model decisions, demonstrating practical application through word-level causal analysis.

Abstract: Despite the overwhelming performance improvements offered by recent natural
language procesing (NLP) models, the decisions made by these models are largely
a black box. Towards closing this gap, the field of causal NLP combines causal
inference literature with modern NLP models to elucidate causal effects of text
features. We replicate and extend Bansal et al's work on regularizing text
classifiers to adhere to estimated effects, focusing instead on model
interpretability. Specifically, we focus on developing a two-headed
RieszNet-based neural network architecture which achieves better treatment
effect estimation accuracy. Our framework, CausalSent, accurately predicts
treatment effects in semi-synthetic IMDB movie reviews, reducing MAE of effect
estimates by 2-3x compared to Bansal et al's MAE on synthetic Civil Comments
data. With an ensemble of validated models, we perform an observational case
study on the causal effect of the word "love" in IMDB movie reviews, finding
that the presence of the word "love" causes a +2.9% increase in the probability
of a positive sentiment.

</details>


### [67] [UQ: Assessing Language Models on Unsolved Questions](https://arxiv.org/abs/2508.17580)
*Fan Nie,Ken Ziyu Liu,Zihao Wang,Rui Sun,Wei Liu,Weijia Shi,Huaxiu Yao,Linjun Zhang,Andrew Y. Ng,James Zou,Sanmi Koyejo,Yejin Choi,Percy Liang,Niklas Muennighoff*

Main category: cs.CL

TL;DR: UQ is a new AI benchmark paradigm using unsolved questions from Stack Exchange, featuring a dataset of 500 challenging questions, validator-assisted evaluation, and community verification to assess frontier models on real-world problems.


<details>
  <summary>Details</summary>
Motivation: Current AI benchmarks face a difficulty-realism tension - exam-style benchmarks are artificially difficult with limited real-world value, while user interaction benchmarks skew toward easy problems. The authors aim to create a benchmark that is both challenging and reflects real-world usage.

Method: UQ uses unsolved questions sourced from Stack Exchange across diverse topics. It employs a collection pipeline with rule-based filters, LLM judges, and human review. The system uses compound validation strategies (UQ-Validators) that leverage the generator-validator gap and features an open platform for community verification.

Result: The top model achieves only 15% pass rate on UQ-validation. Preliminary human verification has identified correct answers among those that passed validation, demonstrating the benchmark's effectiveness in identifying genuine solutions to unsolved problems.

Conclusion: UQ provides a new paradigm for evaluating frontier models on real-world, open-ended challenges where success advances human knowledge. It offers a path beyond traditional static benchmarks by using unsolved questions that are naturally difficult and yield direct real-world value when solved.

Abstract: Benchmarks shape progress in AI research. A useful benchmark should be both
difficult and realistic: questions should challenge frontier models while also
reflecting real-world usage. Yet, current paradigms face a difficulty-realism
tension: exam-style benchmarks are often made artificially difficult with
limited real-world value, while benchmarks based on real user interaction often
skew toward easy, high-frequency problems. In this work, we explore a radically
different paradigm: assessing models on unsolved questions. Rather than a
static benchmark scored once, we curate unsolved questions and evaluate models
asynchronously over time with validator-assisted screening and community
verification. We introduce UQ, a testbed of 500 challenging, diverse questions
sourced from Stack Exchange, spanning topics from CS theory and math to sci-fi
and history, probing capabilities including reasoning, factuality, and
browsing. UQ is difficult and realistic by construction: unsolved questions are
often hard and naturally arise when humans seek answers, thus solving them
yields direct real-world value. Our contributions are threefold: (1) UQ-Dataset
and its collection pipeline combining rule-based filters, LLM judges, and human
review to ensure question quality (e.g., well-defined and difficult); (2)
UQ-Validators, compound validation strategies that leverage the
generator-validator gap to provide evaluation signals and pre-screen candidate
solutions for human review; and (3) UQ-Platform, an open platform where experts
collectively verify questions and solutions. The top model passes UQ-validation
on only 15% of questions, and preliminary human verification has already
identified correct answers among those that passed. UQ charts a path for
evaluating frontier models on real-world, open-ended challenges, where success
pushes the frontier of human knowledge. We release UQ at
https://uq.stanford.edu.

</details>


### [68] [Less Is More? Examining Fairness in Pruned Large Language Models for Summarising Opinions](https://arxiv.org/abs/2508.17610)
*Nannan Huang,Haytham Fayek,Xiuzhen Zhang*

Main category: cs.CL

TL;DR: HGLA pruning method maintains/improves fairness in LLM-generated opinion summaries better than existing pruning techniques, with pruning methods having greater impact on fairness than calibration sets.


<details>
  <summary>Details</summary>
Motivation: To investigate how post-training pruning affects fairness in LLM-generated opinion summaries, particularly important for opinion summarization where biased outputs could influence public views.

Method: Comprehensive empirical analysis using three state-of-the-art pruning methods and various calibration sets across three open-source LLMs, evaluated with four fairness metrics. Proposed High Gradient Low Activation (HGLA) pruning that identifies and removes parameters redundant for input processing but influential in output generation.

Result: Pruning methods have greater impact on fairness than calibration sets. HGLA demonstrates better fairness maintenance/improvement compared to existing methods across models and tasks. Human evaluation confirms HGLA-generated outputs are fairer than state-of-the-art pruning methods.

Conclusion: HGLA pruning shows promise for maintaining fairness in compressed models, addressing limitations of traditional pruning methods in opinion summarization tasks.

Abstract: Model compression through post-training pruning offers a way to reduce model
size and computational requirements without significantly impacting model
performance. However, the effect of pruning on the fairness of LLM-generated
summaries remains unexplored, particularly for opinion summarisation where
biased outputs could influence public views.In this paper, we present a
comprehensive empirical analysis of opinion summarisation, examining three
state-of-the-art pruning methods and various calibration sets across three
open-source LLMs using four fairness metrics. Our systematic analysis reveals
that pruning methods have a greater impact on fairness than calibration sets.
Building on these insights, we propose High Gradient Low Activation (HGLA)
pruning, which identifies and removes parameters that are redundant for input
processing but influential in output generation. Our experiments demonstrate
that HGLA can better maintain or even improve fairness compared to existing
methods, showing promise across models and tasks where traditional methods have
limitations. Our human evaluation shows HGLA-generated outputs are fairer than
existing state-of-the-art pruning methods. Code is available at:
https://github.com/amberhuang01/HGLA.

</details>


### [69] [Steering When Necessary: Flexible Steering Large Language Models with Backtracking](https://arxiv.org/abs/2508.17621)
*Jinwei Gan,Zifeng Cheng,Zhiwei Jiang,Cong Wang,Yafeng Yin,Xiang Luo,Yuchen Fu,Qing Gu*

Main category: cs.CL

TL;DR: FASB is a flexible activation steering framework that dynamically adjusts intervention strength and uses backtracking to correct deviations, outperforming existing methods on alignment tasks.


<details>
  <summary>Details</summary>
Motivation: Existing activation steering methods either intervene indiscriminately or rely only on questions, lacking accurate assessment of intervention strength and timely correction of deviations.

Method: Proposes FASB framework that tracks LLM internal states during generation to dynamically determine intervention necessity and strength, plus backtracking mechanism to correct deviated tokens.

Result: Extensive experiments on TruthfulQA and six multiple-choice datasets show FASB outperforms baseline methods.

Conclusion: FASB provides an effective and cost-efficient approach for aligning LLMs by dynamically adjusting intervention and using backtracking to correct deviations early.

Abstract: Large language models (LLMs) have achieved remarkable performance across many
generation tasks. Nevertheless, effectively aligning them with desired
behaviors remains a significant challenge. Activation steering is an effective
and cost-efficient approach that directly modifies the activations of LLMs
during the inference stage, aligning their responses with the desired behaviors
and avoiding the high cost of fine-tuning. Existing methods typically
indiscriminately intervene to all generations or rely solely on the question to
determine intervention, which limits the accurate assessment of the
intervention strength. To this end, we propose the Flexible Activation Steering
with Backtracking (FASB) framework, which dynamically determines both the
necessity and strength of intervention by tracking the internal states of the
LLMs during generation, considering both the question and the generated
content. Since intervening after detecting a deviation from the desired
behavior is often too late, we further propose the backtracking mechanism to
correct the deviated tokens and steer the LLMs toward the desired behavior.
Extensive experiments on the TruthfulQA dataset and six multiple-choice
datasets demonstrate that our method outperforms baselines. Our code will be
released at https://github.com/gjw185/FASB.

</details>


### [70] [EMO-Reasoning: Benchmarking Emotional Reasoning Capabilities in Spoken Dialogue Systems](https://arxiv.org/abs/2508.17623)
*Jingwen Liu,Kan Jen Cheng,Jiachen Lian,Akshay Anand,Rishi Jain,Faith Qiao,Robin Netzorg,Huang-Cheng Chou,Tingle Li,Guan-Ting Lin,Gopala Anumanchipalli*

Main category: cs.CL

TL;DR: EMO-Reasoning benchmark for evaluating emotional coherence in dialogue systems using TTS-generated emotional speech data and cross-turn emotion reasoning metrics.


<details>
  <summary>Details</summary>
Motivation: Address the lack of holistic evaluation systems for emotional reasoning in spoken dialogue systems, despite their importance in human-computer interaction.

Method: Created curated dataset via text-to-speech to simulate diverse emotional states, proposed Cross-turn Emotion Reasoning Score to assess emotion transitions in multi-turn dialogues.

Result: Evaluated seven dialogue systems using continuous, categorical, and perceptual metrics, effectively detecting emotional inconsistencies.

Conclusion: The framework provides insights for improving dialogue systems and advances emotion-aware spoken dialogue modeling toward more natural interactions.

Abstract: Speech emotions play a crucial role in human-computer interaction, shaping
engagement and context-aware communication. Despite recent advances in spoken
dialogue systems, a holistic system for evaluating emotional reasoning is still
lacking. To address this, we introduce EMO-Reasoning, a benchmark for assessing
emotional coherence in dialogue systems. It leverages a curated dataset
generated via text-to-speech to simulate diverse emotional states, overcoming
the scarcity of emotional speech data. We further propose the Cross-turn
Emotion Reasoning Score to assess the emotion transitions in multi-turn
dialogues. Evaluating seven dialogue systems through continuous, categorical,
and perceptual metrics, we show that our framework effectively detects
emotional inconsistencies, providing insights for improving current dialogue
systems. By releasing a systematic evaluation benchmark, we aim to advance
emotion-aware spoken dialogue modeling toward more natural and adaptive
interactions.

</details>


### [71] [Stop Spinning Wheels: Mitigating LLM Overthinking via Mining Patterns for Early Reasoning Exit](https://arxiv.org/abs/2508.17627)
*Zihao Wei,Liang Pang,Jiahao Liu,Jingcheng Deng,Shicheng Xu,Zenghao Duan,Jingang Wang,Fei Sun,Xunliang Cai,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: The paper identifies three reasoning stages in LLMs and proposes a method to detect the Reasoning Completion Point (RCP) to prevent overthinking, reducing token usage while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Large language models often suffer from overthinking which degrades performance and increases resource consumption. The authors observed patterns in thinking length and content that reveal distinct reasoning stages, with compensatory reasoning being optimal for correct answers while convergence leads to overthinking.

Method: The authors categorize reasoning into three stages (insufficient exploration, compensatory reasoning, and convergence), identify the Reasoning Completion Point at the end of compensatory reasoning, and develop a lightweight thresholding strategy using heuristic rules to detect RCP efficiently.

Result: Experimental evaluations on AIME24, AIME25, and GPQA-D benchmarks show that the proposed method reduces token consumption while preserving or even enhancing reasoning accuracy compared to previous approaches.

Conclusion: Detecting the Reasoning Completion Point effectively mitigates overthinking in LLMs, providing an efficient balance between reasoning depth and resource usage, leading to improved performance with reduced computational costs.

Abstract: Large language models (LLMs) enhance complex reasoning tasks by scaling the
individual thinking process. However, prior work shows that overthinking can
degrade overall performance. Motivated by observed patterns in thinking length
and content length, we categorize reasoning into three stages: insufficient
exploration stage, compensatory reasoning stage, and reasoning convergence
stage. Typically, LLMs produce correct answers in the compensatory reasoning
stage, whereas reasoning convergence often triggers overthinking, causing
increased resource usage or even infinite loops. Therefore, mitigating
overthinking hinges on detecting the end of the compensatory reasoning stage,
defined as the Reasoning Completion Point (RCP). RCP typically appears at the
end of the first complete reasoning cycle and can be identified by querying the
LLM sentence by sentence or monitoring the probability of an end-of-thinking
token (e.g., \texttt{</think>}), though these methods lack an efficient and
precise balance. To improve this, we mine more sensitive and consistent RCP
patterns and develop a lightweight thresholding strategy based on heuristic
rules. Experimental evaluations on benchmarks (AIME24, AIME25, GPQA-D)
demonstrate that the proposed method reduces token consumption while preserving
or enhancing reasoning accuracy.

</details>


### [72] [Weights-Rotated Preference Optimization for Large Language Models](https://arxiv.org/abs/2508.17637)
*Chenxu Yang,Ruipeng Jia,Mingyu Zheng,Naibin Gu,Zheng Lin,Siyuan Chen,Weichong Yin,Hua Wu,Weiping Wang*

Main category: cs.CL

TL;DR: RoPO addresses DPO's reward hacking problem by constraining both output logits and hidden states through multi-granularity orthogonal matrices, achieving significant performance improvements with minimal parameters.


<details>
  <summary>Details</summary>
Motivation: Reward hacking in DPO causes LLMs to excessively reduce rejected completion probabilities, leading to overly lengthy generations, lack of diversity, and catastrophic forgetting of knowledge due to representation redundancy from neuron collapse.

Method: Proposes Weights-Rotated Preference Optimization (RoPO) that implicitly constrains output layer logits with KL divergence from DPO and explicitly constrains intermediate hidden states by fine-tuning on multi-granularity orthogonal matrices to prevent deviation from reference model.

Result: Achieves up to 3.27-point improvement on AlpacaEval 2 and surpasses best baseline by 6.2-7.5 points on MT-Bench with only 0.015% trainable parameters.

Conclusion: RoPO effectively alleviates DPO's reward hacking problem while retaining pre-training and SFT knowledge and expressive capabilities through constrained optimization.

Abstract: Despite the efficacy of Direct Preference Optimization (DPO) in aligning
Large Language Models (LLMs), reward hacking remains a pivotal challenge. This
issue emerges when LLMs excessively reduce the probability of rejected
completions to achieve high rewards, without genuinely meeting their intended
goals. As a result, this leads to overly lengthy generation lacking diversity,
as well as catastrophic forgetting of knowledge. We investigate the underlying
reason behind this issue, which is representation redundancy caused by neuron
collapse in the parameter space. Hence, we propose a novel Weights-Rotated
Preference Optimization (RoPO) algorithm, which implicitly constrains the
output layer logits with the KL divergence inherited from DPO and explicitly
constrains the intermediate hidden states by fine-tuning on a multi-granularity
orthogonal matrix. This design prevents the policy model from deviating too far
from the reference model, thereby retaining the knowledge and expressive
capabilities acquired during pre-training and SFT stages. Our RoPO achieves up
to a 3.27-point improvement on AlpacaEval 2, and surpasses the best baseline by
6.2 to 7.5 points on MT-Bench with merely 0.015% of the trainable parameters,
demonstrating its effectiveness in alleviating the reward hacking problem of
DPO.

</details>


### [73] [SurveyGen: Quality-Aware Scientific Survey Generation with Large Language Models](https://arxiv.org/abs/2508.17647)
*Tong Bao,Mir Tafseer Nayeem,Davood Rafiei,Chengzhi Zhang*

Main category: cs.CL

TL;DR: SurveyGen is a large-scale dataset of 4,200+ human-written scientific surveys with quality metadata, used to develop QUAL-SG framework that enhances RAG with quality-aware retrieval for better survey generation, showing semi-automatic approaches work best while fully automatic methods still struggle with citation quality.


<details>
  <summary>Details</summary>
Motivation: The lack of standardized evaluation datasets hampers rigorous assessment of LLM performance in automatic survey generation compared to human-written surveys.

Method: Created SurveyGen dataset with 4,200+ human surveys and quality metadata, then developed QUAL-SG framework that enhances RAG pipeline with quality-aware indicators for literature retrieval and paper selection.

Result: Semi-automatic pipelines achieved partially competitive outcomes, but fully automatic survey generation still suffers from low citation quality and limited critical analysis.

Conclusion: While quality-aware frameworks like QUAL-SG can improve survey generation, human involvement remains crucial for achieving high-quality citations and critical analysis in scientific surveys.

Abstract: Automatic survey generation has emerged as a key task in scientific document
processing. While large language models (LLMs) have shown promise in generating
survey texts, the lack of standardized evaluation datasets critically hampers
rigorous assessment of their performance against human-written surveys. In this
work, we present SurveyGen, a large-scale dataset comprising over 4,200
human-written surveys across diverse scientific domains, along with 242,143
cited references and extensive quality-related metadata for both the surveys
and the cited papers. Leveraging this resource, we build QUAL-SG, a novel
quality-aware framework for survey generation that enhances the standard
Retrieval-Augmented Generation (RAG) pipeline by incorporating quality-aware
indicators into literature retrieval to assess and select higher-quality source
papers. Using this dataset and framework, we systematically evaluate
state-of-the-art LLMs under varying levels of human involvement - from fully
automatic generation to human-guided writing. Experimental results and human
evaluations show that while semi-automatic pipelines can achieve partially
competitive outcomes, fully automatic survey generation still suffers from low
citation quality and limited critical analysis.

</details>


### [74] [CoCoA: Confidence- and Context-Aware Adaptive Decoding for Resolving Knowledge Conflicts in Large Language Models](https://arxiv.org/abs/2508.17670)
*Anant Khandelwal,Manish Gupta,Puneet Agrawal*

Main category: cs.CL

TL;DR: CoCoA is a novel adaptive decoding method that improves faithfulness in LLMs by resolving knowledge conflicts between parametric memory and external context using confidence-aware measures and distribution divergence.


<details>
  <summary>Details</summary>
Motivation: Existing contrastive decoding methods for handling knowledge conflicts in LLMs lack adaptability and degrade performance in low conflict settings, requiring a more principled approach to conflict resolution.

Method: CoCoA uses token-level confidence-aware measures (entropy gap and contextual peakedness) and generalized divergence between parametric and contextual distributions to adaptively resolve conflicts.

Result: State-of-the-art performance across multiple LLMs on QA, Summarization, and LFQA benchmarks, with up to 9.2% QA accuracy improvement over AdaCAD and up to 2.5% factuality gains in summarization/LFQA.

Conclusion: CoCoA enables more informed, context-aware, and faithful token generation while maintaining strong performance in both high and low conflict settings, demonstrating superior sensitivity to conflict variations.

Abstract: Faithful generation in large language models (LLMs) is challenged by
knowledge conflicts between parametric memory and external context. Existing
contrastive decoding methods tuned specifically to handle conflict often lack
adaptability and can degrade performance in low conflict settings. We introduce
CoCoA (Confidence- and Context-Aware Adaptive Decoding), a novel token-level
algorithm for principled conflict resolution and enhanced faithfulness. CoCoA
resolves conflict by utilizing confidence-aware measures (entropy gap and
contextual peakedness) and the generalized divergence between the parametric
and contextual distributions. Crucially, CoCoA maintains strong performance
even in low conflict settings. Extensive experiments across multiple LLMs on
diverse Question Answering (QA), Summarization, and Long-Form Question
Answering (LFQA) benchmarks demonstrate CoCoA's state-of-the-art performance
over strong baselines like AdaCAD. It yields significant gains in QA accuracy,
up to 9.2 points on average compared to the strong baseline AdaCAD, and
improves factuality in summarization and LFQA by up to 2.5 points on average
across key benchmarks. Additionally, it demonstrates superior sensitivity to
conflict variations. CoCoA enables more informed, context-aware, and ultimately
more faithful token generation.

</details>


### [75] [Text Meets Topology: Rethinking Out-of-distribution Detection in Text-Rich Networks](https://arxiv.org/abs/2508.17690)
*Danny Wang,Ruihong Qiu,Guangdong Bai,Zi Huang*

Main category: cs.CL

TL;DR: TextTopoOOD framework for OOD detection in text-rich networks addressing attribute, structural, label, and domain shifts with TNT-OOD model using cross-attention and HyperNetwork.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook complex textual-structural diversity in text-rich networks where OOD detection is challenging due to intertwined textual features and topological structures.

Method: Proposed TNT-OOD model with cross-attention module to fuse local structure into text representations and HyperNetwork for node-specific transformation parameters to align topological and semantic features.

Result: Experiments on 11 datasets across four OOD scenarios demonstrate the framework's effectiveness in evaluating OOD detection challenges in text-rich networks.

Conclusion: The TextTopoOOD framework provides comprehensive evaluation across diverse OOD scenarios and TNT-OOD effectively models text-topology interplay for improved OOD detection.

Abstract: Out-of-distribution (OOD) detection remains challenging in text-rich
networks, where textual features intertwine with topological structures.
Existing methods primarily address label shifts or rudimentary domain-based
splits, overlooking the intricate textual-structural diversity. For example, in
social networks, where users represent nodes with textual features (name, bio)
while edges indicate friendship status, OOD may stem from the distinct language
patterns between bot and normal users. To address this gap, we introduce the
TextTopoOOD framework for evaluating detection across diverse OOD scenarios:
(1) attribute-level shifts via text augmentations and embedding perturbations;
(2) structural shifts through edge rewiring and semantic connections; (3)
thematically-guided label shifts; and (4) domain-based divisions. Furthermore,
we propose TNT-OOD to model the complex interplay between Text aNd Topology
using: 1) a novel cross-attention module to fuse local structure into
node-level text representations, and 2) a HyperNetwork to generate
node-specific transformation parameters. This aligns topological and semantic
features of ID nodes, enhancing ID/OOD distinction across structural and
textual shifts. Experiments on 11 datasets across four OOD scenarios
demonstrate the nuanced challenge of TextTopoOOD for evaluating OOD detection
in text-rich networks.

</details>


### [76] [EMPOWER: Evolutionary Medical Prompt Optimization With Reinforcement Learning](https://arxiv.org/abs/2508.17703)
*Yinda Chen,Yangfan He,Jing Yang,Dapeng Zhang,Zhenlong Yuan,Muhammad Attique Khan,Jamel Baili,Por Lip Yee*

Main category: cs.CL

TL;DR: EMPOWER is an evolutionary framework that improves medical prompt engineering through specialized representation learning, multi-dimensional evaluation, and structure-preserving algorithms, achieving significant reductions in factual errors and improvements in clinical utility.


<details>
  <summary>Details</summary>
Motivation: Current prompt optimization approaches inadequately address domain-specific medical knowledge and safety requirements, limiting the reliability and clinical utility of LLMs in healthcare applications.

Method: EMPOWER incorporates: (1) medical terminology attention mechanism, (2) comprehensive assessment architecture evaluating clarity, specificity, clinical relevance, and factual accuracy, (3) component-level evolutionary algorithm preserving clinical reasoning integrity, and (4) semantic verification module for medical knowledge adherence.

Result: 24.7% reduction in factually incorrect content, 19.6% enhancement in domain specificity, and 15.3% higher clinician preference in blinded evaluations across diagnostic, therapeutic, and educational tasks.

Conclusion: The framework addresses critical challenges in developing clinically appropriate prompts, facilitating more responsible integration of LLMs into healthcare settings.

Abstract: Prompt engineering significantly influences the reliability and clinical
utility of Large Language Models (LLMs) in medical applications. Current
optimization approaches inadequately address domain-specific medical knowledge
and safety requirements. This paper introduces EMPOWER, a novel evolutionary
framework that enhances medical prompt quality through specialized
representation learning, multi-dimensional evaluation, and structure-preserving
algorithms. Our methodology incorporates: (1) a medical terminology attention
mechanism, (2) a comprehensive assessment architecture evaluating clarity,
specificity, clinical relevance, and factual accuracy, (3) a component-level
evolutionary algorithm preserving clinical reasoning integrity, and (4) a
semantic verification module ensuring adherence to medical knowledge.
Evaluation across diagnostic, therapeutic, and educational tasks demonstrates
significant improvements: 24.7% reduction in factually incorrect content, 19.6%
enhancement in domain specificity, and 15.3% higher clinician preference in
blinded evaluations. The framework addresses critical challenges in developing
clinically appropriate prompts, facilitating more responsible integration of
LLMs into healthcare settings.

</details>


### [77] [Layerwise Importance Analysis of Feed-Forward Networks in Transformer-based Language Models](https://arxiv.org/abs/2508.17734)
*Wataru Ikeda,Kazuki Yano,Ryosuke Takahashi,Jaesung Lee,Keigo Shibata,Jun Suzuki*

Main category: cs.CL

TL;DR: Study shows concentrating FFNs in middle 70% of Transformer layers outperforms standard configurations across multiple model sizes and downstream tasks.


<details>
  <summary>Details</summary>
Motivation: To investigate the layerwise importance of feed-forward networks (FFNs) in Transformer models during pretraining, rather than using existing pretrained models, and understand how FFN importance varies by layer position.

Method: Experimental approach that maintains total parameter count by increasing FFN dimensions in some layers while completely removing FFNs from other layers. Trained models from scratch with varying sizes (285M, 570M, 1.2B parameters) and layer counts (12, 24, 40 layers).

Result: Concentrating FFNs in 70% of consecutive middle layers consistently outperforms standard configurations across multiple downstream tasks.

Conclusion: The importance of FFNs varies by layer position, and optimal performance is achieved by strategically concentrating FFNs in middle layers rather than distributing them uniformly across all layers.

Abstract: This study investigates the layerwise importance of feed-forward networks
(FFNs) in Transformer-based language models during pretraining. We introduce an
experimental approach that, while maintaining the total parameter count,
increases the FFN dimensions in some layers and completely removes the FFNs
from other layers. Furthermore, since our focus is on the importance of FFNs
during pretraining, we train models from scratch to examine whether the
importance of FFNs varies depending on their layer positions, rather than using
publicly available pretrained models as is frequently done. Through
comprehensive evaluations of models with varying sizes (285M, 570M, and 1.2B
parameters) and layer counts (12, 24, and 40 layers), we demonstrate that
concentrating FFNs in 70% of the consecutive middle layers consistently
outperforms standard configurations for multiple downstream tasks.

</details>


### [78] [SMITE: Enhancing Fairness in LLMs through Optimal In-Context Example Selection via Dynamic Validation](https://arxiv.org/abs/2508.17735)
*Garima Chhikara,Kripabandhu Ghosh,Abhijnan Chakraborty*

Main category: cs.CL

TL;DR: Novel approach using dynamic validation sets and SMITE algorithm to improve LLM fairness and accuracy in tabular classification tasks


<details>
  <summary>Details</summary>
Motivation: Ensuring fairness in LLM outputs is critical for inclusivity, equal representation, and responsible AI deployment in downstream tasks like tabular classification

Method: Introduces dynamic validation set that evolves with test set, plus SMITE iterative algorithm to select optimal in-context examples validated against dynamic validation sets

Result: Experiments across four different LLMs show significant improvements in both predictive accuracy and fairness compared to baseline methods

Conclusion: First study to apply dynamic validation in the context of in-context learning for LLMs, demonstrating effective approach for enhancing fairness and performance

Abstract: Large Language Models (LLMs) are widely used for downstream tasks such as
tabular classification, where ensuring fairness in their outputs is critical
for inclusivity, equal representation, and responsible AI deployment. This
study introduces a novel approach to enhancing LLM performance and fairness
through the concept of a dynamic validation set, which evolves alongside the
test set, replacing the traditional static validation approach. We also propose
an iterative algorithm, SMITE, to select optimal in-context examples, with each
example set validated against its corresponding dynamic validation set. The
in-context set with the lowest total error is used as the final demonstration
set. Our experiments across four different LLMs show that our proposed
techniques significantly improve both predictive accuracy and fairness compared
to baseline methods. To our knowledge, this is the first study to apply dynamic
validation in the context of in-context learning for LLMs.

</details>


### [79] [ISACL: Internal State Analyzer for Copyrighted Training Data Leakage](https://arxiv.org/abs/2508.17767)
*Guangwei Zhang,Qisheng Su,Jiateng Liu,Cheng Qian,Yanzhou Pan,Yanjie Fu,Denghui Zhang*

Main category: cs.CL

TL;DR: Proactive detection of copyrighted data leakage in LLMs by analyzing internal states before text generation, using a neural classifier to prevent exposure.


<details>
  <summary>Details</summary>
Motivation: LLMs risk exposing copyrighted/proprietary data used in training. Traditional methods only address leaks after generation, potentially exposing sensitive information.

Method: Train neural network classifier on curated copyrighted dataset to identify leakage risks from LLMs' internal states before text generation. Integrated with RAG system for early intervention.

Result: Analyzing internal states effectively mitigates copyrighted data leakage risk, providing scalable solution that maintains high-quality text generation while ensuring compliance.

Conclusion: Proactive internal state analysis offers effective copyright protection framework that integrates smoothly into AI workflows while upholding ethical standards and data privacy.

Abstract: Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP) but pose risks of inadvertently exposing copyrighted or proprietary data,
especially when such data is used for training but not intended for
distribution. Traditional methods address these leaks only after content is
generated, which can lead to the exposure of sensitive information. This study
introduces a proactive approach: examining LLMs' internal states before text
generation to detect potential leaks. By using a curated dataset of copyrighted
materials, we trained a neural network classifier to identify risks, allowing
for early intervention by stopping the generation process or altering outputs
to prevent disclosure. Integrated with a Retrieval-Augmented Generation (RAG)
system, this framework ensures adherence to copyright and licensing
requirements while enhancing data privacy and ethical standards. Our results
show that analyzing internal states effectively mitigates the risk of
copyrighted data leakage, offering a scalable solution that fits smoothly into
AI workflows, ensuring compliance with copyright regulations while maintaining
high-quality text generation. The implementation is available on
GitHub.\footnote{https://github.com/changhu73/Internal_states_leakage}

</details>


### [80] [Speculating LLMs' Chinese Training Data Pollution from Their Tokens](https://arxiv.org/abs/2508.17771)
*Qingjie Zhang,Di Wang,Haoting Qian,Liu Yan,Tianwei Zhang,Ke Xu,Qi Li,Minlie Huang,Hewu Li,Han Qiu*

Main category: cs.CL

TL;DR: The paper identifies and analyzes polluted Chinese tokens (PoC) in LLM vocabularies that represent pornographic or gambling content, develops a detection method, and speculates about training data contamination.


<details>
  <summary>Details</summary>
Motivation: Many Chinese phrase tokens in GPT models' vocabularies represent inappropriate content like pornography and online gambling, raising concerns about training data pollution.

Method: Defined PoC tokens taxonomy, built a detector by fine-tuning an LLM to label tokens using semantic analysis and search engine content, and analyzed token appearances to speculate on training data contamination.

Result: Found that over 23% of long Chinese tokens in GPT's vocabulary represent porn or gambling content, with GPT performing worst among 24 tested LLMs. Validated method on C4 and Pile datasets and estimated ~0.5% of GPT-4o's training data contains "Yui Hatano" related content.

Conclusion: Polluted Chinese tokens are widespread in LLM vocabularies, indicating potential training data contamination, with GPT models showing particularly high levels of inappropriate content representation.

Abstract: Tokens are basic elements in the datasets for LLM training. It is well-known
that many tokens representing Chinese phrases in the vocabulary of GPT
(4o/4o-mini/o1/o3/4.5/4.1/o4-mini) are indicating contents like pornography or
online gambling. Based on this observation, our goal is to locate Polluted
Chinese (PoC) tokens in LLMs and study the relationship between PoC tokens'
existence and training data. (1) We give a formal definition and taxonomy of
PoC tokens based on the GPT's vocabulary. (2) We build a PoC token detector via
fine-tuning an LLM to label PoC tokens in vocabularies by considering each
token's both semantics and related contents from the search engines. (3) We
study the speculation on the training data pollution via PoC tokens'
appearances (token ID). Experiments on GPT and other 23 LLMs indicate that
tokens widely exist while GPT's vocabulary behaves the worst: more than 23%
long Chinese tokens (i.e., a token with more than two Chinese characters) are
either porn or online gambling. We validate the accuracy of our speculation
method on famous pre-training datasets like C4 and Pile. Then, considering
GPT-4o, we speculate that the ratio of "Yui Hatano" related webpages in
GPT-4o's training data is around 0.5%.

</details>


### [81] [Zero-shot Context Biasing with Trie-based Decoding using Synthetic Multi-Pronunciation](https://arxiv.org/abs/2508.17796)
*Changsong Liu,Yizhou Peng,Eng Siong Chng*

Main category: cs.CL

TL;DR: Zero-shot contextual ASR method using TTS-synthesized speech to extract multiple pronunciation variants from Whisper, compiled into a prefix-trie for beam-search decoding, reducing biased WER by 42-43% while maintaining unbiased performance.


<details>
  <summary>Details</summary>
Motivation: Address challenges in recognizing out-of-vocabulary words due to limited training data and ambiguous/inconsistent pronunciations in contextual ASR systems.

Method: Leverage TTS to synthesize diverse speech samples containing target rare words, use pretrained Whisper to extract multiple pronunciation variants, compile variants into prefix-trie for shallow-fusion beam-search decoding with reward assignment.

Result: 42% reduction in biased WER on Librispeech test-clean and 43% on test-other, while unbiased WER remains essentially unchanged.

Conclusion: The synthesis-driven multi-pronunciation contextual biasing method effectively improves recognition of rare words without compromising general ASR performance.

Abstract: Contextual automatic speech recognition (ASR) systems allow for recognizing
out-of-vocabulary (OOV) words, such as named entities or rare words. However,
it remains challenging due to limited training data and ambiguous or
inconsistent pronunciations. In this paper, we propose a synthesis-driven
multi-pronunciation contextual biasing method that performs zero-shot
contextual ASR on a pretrained Whisper model. Specifically, we leverage
text-to-speech (TTS) systems to synthesize diverse speech samples containing
each target rare word, and then use the pretrained Whisper model to extract
multiple predicted pronunciation variants. These variant token sequences are
compiled into a prefix-trie, which assigns rewards to beam hypotheses in a
shallow-fusion manner during beam-search decoding. After which, any recognized
variant is mapped back to the original rare word in the final transcription.
The evaluation results on the Librispeech dataset show that our method reduces
biased word error rate (WER) by 42% on test-clean and 43% on test-other while
maintaining unbiased WER essentially unchanged.

</details>


### [82] [DRQA: Dynamic Reasoning Quota Allocation for Controlling Overthinking in Reasoning Large Language Models](https://arxiv.org/abs/2508.17803)
*Kaiwen Yan,Xuanqing Shi,Hongcheng Guo,Wenxuan Wang,Zhuosheng Zhang,Chengwei Qin*

Main category: cs.CL

TL;DR: DRQA is a method that reduces overthinking in reasoning LLMs by training them to allocate reasoning resources adaptively, achieving significant token savings while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Reasoning LLMs often produce unnecessarily long reasoning chains for simple questions (overthinking), leading to computational inefficiency and excessive token consumption.

Method: Dynamic Reasoning Quota Allocation (DRQA) uses batch-generated preference data and reinforcement learning to train models to allocate reasoning resources adaptively, preferring both accurate and concise responses.

Result: Extensive experiments show DRQA significantly reduces token usage while maintaining or improving answer accuracy across mathematical and scientific reasoning benchmarks.

Conclusion: DRQA effectively mitigates overthinking and offers a promising direction for more efficient deployment of reasoning LLMs through fine-grained control of reasoning behaviors.

Abstract: Reasoning large language models (RLLMs), such as OpenAI-O3 and DeepSeek-R1,
have recently demonstrated remarkable capabilities by performing structured and
multi-step reasoning. However, recent studies reveal that RLLMs often suffer
from overthinking, i.e., producing unnecessarily lengthy reasoning chains even
for simple questions, leading to excessive token consumption and computational
inefficiency. Interestingly, we observe that when processing multiple questions
in batch mode, RLLMs exhibit more resource-efficient behavior by dynamically
compressing reasoning steps for easier problems, due to implicit resource
competition. Inspired by this, we propose Dynamic Reasoning Quota Allocation
(DRQA), a novel method that transfers the benefits of resource competition from
batch processing to single-question inference. Specifically, DRQA leverages
batch-generated preference data and reinforcement learning to train the model
to allocate reasoning resources adaptively. By encouraging the model to
internalize a preference for responses that are both accurate and concise, DRQA
enables it to generate concise answers for simple questions while retaining
sufficient reasoning depth for more challenging ones. Extensive experiments on
a wide range of mathematical and scientific reasoning benchmarks demonstrate
that DRQA significantly reduces token usage while maintaining, and in many
cases improving, answer accuracy. By effectively mitigating the overthinking
problem, DRQA offers a promising direction for more efficient and scalable
deployment of RLLMs, and we hope it inspires further exploration into
fine-grained control of reasoning behaviors.

</details>


### [83] [Beyond Demographics: Enhancing Cultural Value Survey Simulation with Multi-Stage Personality-Driven Cognitive Reasoning](https://arxiv.org/abs/2508.17855)
*Haijiang Liu,Qiyuan Li,Chao Gao,Yong Cao,Xiangyu Xu,Xun Wu,Daniel Hershcovich,Jinguang Gu*

Main category: cs.CL

TL;DR: MARK is a multi-stage reasoning framework that uses MBTI personality theory to simulate cultural survey responses, achieving 10% higher accuracy than existing methods.


<details>
  <summary>Details</summary>
Motivation: To improve accuracy, steerability, and interpretability of large language models in cultural value survey response simulation by incorporating psychological personality frameworks.

Method: Uses type dynamics theory from MBTI psychological framework with three stages: life-situational stress analysis, group-level personality prediction, and self-weighted cognitive imitation. Predicts and utilizes human demographic information for simulation.

Result: Outperforms existing baselines by 10% accuracy on World Values Survey and reduces divergence between model predictions and human preferences.

Conclusion: The framework shows strong potential for improving zero-shot personalization and helping social scientists interpret model predictions in cultural value research.

Abstract: Introducing MARK, the Multi-stAge Reasoning frameworK for cultural value
survey response simulation, designed to enhance the accuracy, steerability, and
interpretability of large language models in this task. The system is inspired
by the type dynamics theory in the MBTI psychological framework for personality
research. It effectively predicts and utilizes human demographic information
for simulation: life-situational stress analysis, group-level personality
prediction, and self-weighted cognitive imitation. Experiments on the World
Values Survey show that MARK outperforms existing baselines by 10% accuracy and
reduces the divergence between model predictions and human preferences. This
highlights the potential of our framework to improve zero-shot personalization
and help social scientists interpret model predictions.

</details>


### [84] [Speech Discrete Tokens or Continuous Features? A Comparative Analysis for Spoken Language Understanding in SpeechLLMs](https://arxiv.org/abs/2508.17863)
*Dingdong Wang,Junan Li,Mingyu Cui,Dongchao Yang,Xueyuan Chen,Helen Meng*

Main category: cs.CL

TL;DR: Fair comparison between discrete tokens and continuous features in SpeechLLMs shows continuous features generally outperform discrete tokens across various spoken language understanding tasks.


<details>
  <summary>Details</summary>
Motivation: To address the performance gap between discrete tokens and continuous features in Speech Large Language Models (SpeechLLMs) and provide a thorough comparison under the same experimental settings.

Method: Conducted fair comparison of self-supervised learning (SSL)-based discrete and continuous features using both small (Qwen1.5-0.5B) and large-scale (Llama3.1-8B) LLMs across six spoken language understanding tasks. Included in-depth analyses: efficient comparison, SSL layer analysis, LLM layer analysis, and robustness comparison.

Result: Continuous features generally outperform discrete tokens in various tasks. Each speech processing method exhibits distinct characteristics and patterns in how it learns and processes speech information.

Conclusion: The study provides valuable insights to advance spoken language understanding in SpeechLLMs, highlighting the superior performance of continuous features while noting the distinct learning patterns of each approach.

Abstract: With the rise of Speech Large Language Models (SpeechLLMs), two dominant
approaches have emerged for speech processing: discrete tokens and continuous
features. Each approach has demonstrated strong capabilities in audio-related
processing tasks. However, the performance gap between these two paradigms has
not been thoroughly explored. To address this gap, we present a fair comparison
of self-supervised learning (SSL)-based discrete and continuous features under
the same experimental settings. We evaluate their performance across six spoken
language understanding-related tasks using both small and large-scale LLMs
(Qwen1.5-0.5B and Llama3.1-8B). We further conduct in-depth analyses, including
efficient comparison, SSL layer analysis, LLM layer analysis, and robustness
comparison. Our findings reveal that continuous features generally outperform
discrete tokens in various tasks. Each speech processing method exhibits
distinct characteristics and patterns in how it learns and processes speech
information. We hope our results will provide valuable insights to advance
spoken language understanding in SpeechLLMs.

</details>


### [85] [ILRe: Intermediate Layer Retrieval for Context Compression in Causal Language Models](https://arxiv.org/abs/2508.17892)
*Manlai Liang,Mandi Liu,Jiangzhou Ji,Huaijun Li,Haobo Yang,Yaohan He,Jinlong Li*

Main category: cs.CL

TL;DR: ILRe is a novel context compression pipeline that reduces LLM prefill complexity from O(LÂ²) to O(L) while maintaining performance in long-context scenarios, achieving 180Ã speedup on 1M token processing.


<details>
  <summary>Details</summary>
Motivation: Address limitations of LLMs in long-context scenarios including short effective context length, quadratic computational complexity, and high memory overhead when processing lengthy inputs.

Method: Intermediate Layer Retrieval (ILRe) pipeline that determines an intermediate decoder layer offline, encodes context by streaming chunked prefill only to that layer, and recalls tokens using attention scores between input query and full key cache with multi-pooling kernels strategy.

Result: Processes 1M tokens in <30 seconds (180Ã speedup), achieves â79.8 on RULER-1M benchmark with Llama-3.1-UltraLong-8B-1M-Instruct on Huawei Ascend 910B NPU, with performance comparable or better than full context.

Conclusion: ILRe effectively mitigates long-context processing limitations without additional training or operator development, providing significant efficiency gains while maintaining semantic completeness and performance.

Abstract: Large Language Models (LLMs) have demonstrated success across many
benchmarks. However, they still exhibit limitations in long-context scenarios,
primarily due to their short effective context length, quadratic computational
complexity, and high memory overhead when processing lengthy inputs. To
mitigate these issues, we introduce a novel context compression pipeline,
called Intermediate Layer Retrieval (ILRe), which determines one intermediate
decoder layer offline, encodes context by streaming chunked prefill only up to
that layer, and recalls tokens by the attention scores between the input query
and full key cache in that specified layer. In particular, we propose a
multi-pooling kernels allocating strategy in the token recalling process to
maintain the completeness of semantics. Our approach not only reduces the
prefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance
comparable to or better than the full context in the long context scenarios.
Without additional post training or operator development, ILRe can process a
single $1M$ tokens request in less than half a minute (speedup $\approx
180\times$) and scores RULER-$1M$ benchmark of $\approx 79.8$ with model
Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.

</details>


### [86] [Pandora: Leveraging Code-driven Knowledge Transfer for Unified Structured Knowledge Reasoning](https://arxiv.org/abs/2508.17905)
*Yongrui Chen,Junhao He,Linbo Fu,Shenyu Zhang,Rihui Jin,Xinbang Dai,Jiaqi Li,Dehai Min,Nan Hu,Yuxin Zhang,Guilin Qi,Yi Huang,Tongtong Wu*

Main category: cs.CL

TL;DR: Pandora is a unified framework for structured knowledge reasoning that uses Python's Pandas API for code-based knowledge representation and knowledge transfer to improve LLM reasoning across different structured data sources.


<details>
  <summary>Details</summary>
Motivation: Existing unified structured knowledge reasoning methods rely on task-specific strategies that create barriers between different SKR tasks, limiting cross-task performance.

Method: Proposes code-based unified knowledge representation using Python's Pandas API that aligns with LLM pre-training, and employs knowledge transfer with cross-task memory building and adaptive correction through code execution feedback.

Result: Outperforms existing unified reasoning frameworks and competes effectively with task-specific methods across six benchmarks in three SKR tasks.

Conclusion: Pandora demonstrates impressive unified reasoning capabilities by leveraging code-based representation and knowledge transfer, effectively breaking down barriers between different structured knowledge reasoning tasks.

Abstract: Unified Structured Knowledge Reasoning (USKR) aims to answer natural language
questions by using structured sources such as tables, databases, and knowledge
graphs in a unified way. Existing USKR methods rely on task-specific strategies
or bespoke representations, which hinder their ability to dismantle barriers
between different SKR tasks, thereby constraining their overall performance in
cross-task scenarios. In this paper, we introduce \textsc{Pandora}, a novel
USKR framework that addresses the limitations of existing methods by leveraging
two key innovations. First, we propose a code-based unified knowledge
representation using \textsc{Python}'s \textsc{Pandas} API, which aligns
seamlessly with the pre-training of LLMs. This representation facilitates a
cohesive approach to handling different structured knowledge sources. Building
on this foundation, we employ knowledge transfer to bolster the unified
reasoning process of LLMs by automatically building cross-task memory. By
adaptively correcting reasoning using feedback from code execution,
\textsc{Pandora} showcases impressive unified reasoning capabilities. Extensive
experiments on six widely used benchmarks across three SKR tasks demonstrate
that \textsc{Pandora} outperforms existing unified reasoning frameworks and
competes effectively with task-specific methods.

</details>


### [87] [Evaluating the Representation of Vowels in Wav2Vec Feature Extractor: A Layer-Wise Analysis Using MFCCs](https://arxiv.org/abs/2508.17914)
*Domenico De Cristofaro,Vincenzo Norman Vitale,Alessandro Vietti*

Main category: cs.CL

TL;DR: Comparison of CNN activations from Wav2Vec against traditional MFCC features for vowel classification using SVM on TIMIT corpus


<details>
  <summary>Details</summary>
Motivation: To evaluate how well CNN-extracted features from self-supervised speech models like Wav2Vec represent phonetic information compared to traditional acoustic features for vowel identification

Method: Extracted features from Wav2Vec's CNN layer, compared with MFCCs and MFCCs+formants, trained SVM classifiers for front-back vowel identification on TIMIT corpus

Result: Classification accuracy comparison reveals the effectiveness of CNN activations for phonetic representation in vowel identification tasks

Conclusion: CNN features from self-supervised models provide competitive or superior phonetic representation compared to traditional acoustic features for vowel classification

Abstract: Automatic Speech Recognition has advanced with self-supervised learning,
enabling feature extraction directly from raw audio. In Wav2Vec, a CNN first
transforms audio into feature vectors before the transformer processes them.
This study examines CNN-extracted information for monophthong vowels using the
TIMIT corpus. We compare MFCCs, MFCCs with formants, and CNN activations by
training SVM classifiers for front-back vowel identification, assessing their
classification accuracy to evaluate phonetic representation.

</details>


### [88] [Information availability in different languages and various technological constraints related to multilinguism on the Internet](https://arxiv.org/abs/2508.17918)
*Sonal Khosla,Haridasa Acharya*

Main category: cs.CL

TL;DR: Analysis of the growing need for multilingual internet access due to English dominance and language barriers affecting global internet adoption.


<details>
  <summary>Details</summary>
Motivation: The internet originated in English-speaking USA, creating linguistic barriers that prevent 75-80% of the world's population from accessing online information despite exponential internet growth.

Method: The paper analyzes the need for information availability in different languages and examines various technological constraints related to multi-linguism on the internet.

Result: Identifies a significant gap in multilingual internet solutions despite growing non-English speaking users, highlighting that current approaches are insufficient to address linguistic barriers.

Conclusion: There is an urgent need for improved multilingual technologies and solutions to bridge the language gap and make internet content accessible to non-English speaking populations worldwide.

Abstract: The usage of Internet has grown exponentially over the last two decades. The
number of Internet users has grown from 16 Million to 1650 Million from 1995 to
2010. It has become a major repository of information catering almost every
area. Since the Internet has its origin in USA which is English speaking
country there is huge dominance of English on the World Wide Web. Although
English is a globally acceptable language, still there is a huge population in
the world which is not able to access the Internet due to language constraints.
It has been estimated that only 20-25% of the world population speaks English
as a native language. More and more people are accessing the Internet nowadays
removing the cultural and linguistic barriers and hence there is a high growth
in the number of non-English speaking users over the last few years on the
Internet. Although many solutions have been provided to remove the linguistic
barriers, still there is a huge gap to be filled. This paper attempts to
analyze the need of information availability in different languages and the
various technological constraints related to multi-linguism on the Internet.

</details>


### [89] [Feature-Refined Unsupervised Model for Loanword Detection](https://arxiv.org/abs/2508.17923)
*Promise Dodzi Kpoglu*

Main category: cs.CL

TL;DR: Unsupervised loanword detection method using only language-internal information, outperforming baselines on six Indo-European languages with strong cross-linguistic scaling performance.


<details>
  <summary>Details</summary>
Motivation: Prior loanword detection methods rely on language-external information, which can introduce circularity and constraints in historical linguistics workflows. The authors aim to develop a method that uses only language-internal information to avoid these issues.

Method: Hybrid approach that extracts linguistic features, scores them, and maps them probabilistically. Iteratively refines initial results by identifying and generalizing emerging patterns until convergence. Processes both native and borrowed words in monolingual and multilingual wordlists.

Result: The model outperforms baseline methods on six Indo-European languages (English, German, French, Italian, Spanish, Portuguese). Strong performance gains observed when scaling to cross-linguistic data.

Conclusion: The proposed unsupervised method successfully detects loanwords using only language-internal information, demonstrating effectiveness across multiple languages and showing particular strength in cross-linguistic applications.

Abstract: We propose an unsupervised method for detecting loanwords i.e., words
borrowed from one language into another. While prior work has primarily relied
on language-external information to identify loanwords, such approaches can
introduce circularity and constraints into the historical linguistics workflow.
In contrast, our model relies solely on language-internal information to
process both native and borrowed words in monolingual and multilingual
wordlists. By extracting pertinent linguistic features, scoring them, and
mapping them probabilistically, we iteratively refine initial results by
identifying and generalizing from emerging patterns until convergence. This
hybrid approach leverages both linguistic and statistical cues to guide the
discovery process. We evaluate our method on the task of isolating loanwords in
datasets from six standard Indo-European languages: English, German, French,
Italian, Spanish, and Portuguese. Experimental results demonstrate that our
model outperforms baseline methods, with strong performance gains observed when
scaling to cross-linguistic data.

</details>


### [90] [AMELIA: A Family of Multi-task End-to-end Language Models for Argumentation](https://arxiv.org/abs/2508.17926)
*Henri Savigny,Bruno Yun*

Main category: cs.CL

TL;DR: This paper explores using large language models for argument mining tasks, creating a unified multi-task dataset from 19 existing datasets and testing three fine-tuning strategies on Llama-3.1-8B-Instruct.


<details>
  <summary>Details</summary>
Motivation: To investigate how a single large language model can perform multiple argument mining tasks effectively, addressing the need for unified approaches in argumentation extraction from natural language texts.

Method: Constructed a multi-task dataset by converting 19 argument mining datasets into unified format, then tested three training strategies: individual task fine-tuning, joint multi-task fine-tuning, and merging separately fine-tuned models.

Result: Task-specific fine-tuning significantly improved individual performance across all tasks. Multi-task fine-tuning maintained strong performance without degradation, showing effective transfer learning. Model merging provided competitive performance while reducing computational costs.

Conclusion: Large language models can effectively handle multiple argument mining tasks through various fine-tuning strategies, with model merging offering a computationally efficient compromise between task-specific and multi-task approaches.

Abstract: Argument mining is a subfield of argumentation that aims to automatically
extract argumentative structures and their relations from natural language
texts. This paper investigates how a single large language model can be
leveraged to perform one or several argument mining tasks. Our contributions
are two-fold. First, we construct a multi-task dataset by surveying and
converting 19 well-known argument mining datasets from the literature into a
unified format. Second, we explore various training strategies using Meta AI's
Llama-3.1-8B-Instruct model: (1) fine-tuning on individual tasks, (2)
fine-tuning jointly on multiple tasks, and (3) merging models fine-tuned
separately on individual tasks. Our experiments show that task-specific
fine-tuning significantly improves individual performance across all tasks.
Moreover, multi-task fine-tuning maintains strong performance without
degradation, suggesting effective transfer learning across related tasks.
Finally, we demonstrate that model merging offers a viable compromise: it
yields competitive performance while mitigating the computational costs
associated with full multi-task fine-tuning.

</details>


### [91] [Debiasing Multilingual LLMs in Cross-lingual Latent Space](https://arxiv.org/abs/2508.17948)
*Qiwei Peng,Guimin Hu,Yekun Chai,Anders SÃ¸gaard*

Main category: cs.CL

TL;DR: Proposes debiasing in a joint cross-lingual latent space using autoencoders on parallel data, improving debiasing performance and transferability across languages.


<details>
  <summary>Details</summary>
Motivation: Previous debiasing techniques like SentDebias show limited cross-lingual effectiveness when applied directly to LLM representations, indicating the need for a better approach.

Method: Constructs a well-aligned cross-lingual latent space using an autoencoder trained on parallel TED talk scripts, then applies debiasing techniques in this shared space rather than directly on LLM representations.

Result: Experiments with Aya-expanse and two debiasing techniques across four languages (English, French, German, Dutch) show that autoencoders effectively create aligned cross-lingual spaces, and debiasing in this space significantly improves both overall performance and cross-lingual transferability.

Conclusion: Performing debiasing in a joint cross-lingual latent space rather than directly on LLM representations is more effective for reducing bias and enabling better transfer across languages.

Abstract: Debiasing techniques such as SentDebias aim to reduce bias in large language
models (LLMs). Previous studies have evaluated their cross-lingual
transferability by directly applying these methods to LLM representations,
revealing their limited effectiveness across languages. In this work, we
therefore propose to perform debiasing in a joint latent space rather than
directly on LLM representations. We construct a well-aligned cross-lingual
latent space using an autoencoder trained on parallel TED talk scripts. Our
experiments with Aya-expanse and two debiasing techniques across four languages
(English, French, German, Dutch) demonstrate that a) autoencoders effectively
construct a well-aligned cross-lingual latent space, and b) applying debiasing
techniques in the learned cross-lingual latent space significantly improves
both the overall debiasing performance and cross-lingual transferability.

</details>


### [92] [Understanding Subword Compositionality of Large Language Models](https://arxiv.org/abs/2508.17953)
*Qiwei Peng,Yekun Chai,Anders SÃ¸gaard*

Main category: cs.CL

TL;DR: This paper analyzes how large language models compose subword information into word-level representations, identifying three distinct composition patterns across different LLM families based on structural similarity, semantic decomposability, and form retention.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs effectively compose sequences of subwords into meaningful word-level representations, which is fundamental to their language processing capabilities.

Method: Conducted comprehensive experiments probing three key aspects: structural similarity between subword compositions and whole-word representations, sensitivity to semantic decomposability, and sensitivity to formal features like character sequence length across different layers of five LLM families.

Result: Identified three distinct groups among the five LLM families reflecting different underlying composition strategies, with distinct patterns in structural similarity evolution across layers, strong performance in semantic decomposition sensitivity, and three distinct patterns in formal feature sensitivity.

Conclusion: The findings provide valuable insights into the compositional dynamics of LLMs and reveal different compositional patterns in how various LLM families encode and integrate subword information.

Abstract: Large language models (LLMs) take sequences of subwords as input, requiring
them to effective compose subword representations into meaningful word-level
representations. In this paper, we present a comprehensive set of experiments
to probe how LLMs compose subword information, focusing on three key aspects:
structural similarity, semantic decomposability, and form retention. Our
analysis of the experiments suggests that these five LLM families can be
classified into three distinct groups, likely reflecting difference in their
underlying composition strategies. Specifically, we observe (i) three distinct
patterns in the evolution of structural similarity between subword compositions
and whole-word representations across layers; (ii) great performance when
probing layer by layer their sensitivity to semantic decompositionality; and
(iii) three distinct patterns when probing sensitivity to formal features,
e.g., character sequence length. These findings provide valuable insights into
the compositional dynamics of LLMs and highlight different compositional
pattens in how LLMs encode and integrate subword information.

</details>


### [93] [German4All - A Dataset and Model for Readability-Controlled Paraphrasing in German](https://arxiv.org/abs/2508.17973)
*Miriam AnschÃ¼tz,Thanh Mai Pham,Eslam Nasrallah,Maximilian MÃ¼ller,Cristian-George Craciun,Georg Groh*

Main category: cs.CL

TL;DR: German4All is the first large-scale German dataset with aligned readability-controlled paraphrases across 5 levels, containing over 25,000 samples synthesized using GPT-4, used to train a state-of-the-art German text simplification model.


<details>
  <summary>Details</summary>
Motivation: To create accessible texts tailored for diverse reader groups by enabling paraphrasing across different complexity levels in German.

Method: Automatically synthesized dataset using GPT-4 with rigorous human and LLM-based evaluation, then used to train an open-source readability-controlled paraphrasing model.

Result: Created a dataset spanning 5 readability levels with over 25,000 samples and achieved state-of-the-art performance in German text simplification.

Conclusion: The German4All dataset and model enable more nuanced and reader-specific text adaptations, with both resources being open-sourced to encourage further research on multi-level paraphrasing.

Abstract: The ability to paraphrase texts across different complexity levels is
essential for creating accessible texts that can be tailored toward diverse
reader groups. Thus, we introduce German4All, the first large-scale German
dataset of aligned readability-controlled, paragraph-level paraphrases. It
spans five readability levels and comprises over 25,000 samples. The dataset is
automatically synthesized using GPT-4 and rigorously evaluated through both
human and LLM-based judgments. Using German4All, we train an open-source,
readability-controlled paraphrasing model that achieves state-of-the-art
performance in German text simplification, enabling more nuanced and
reader-specific adaptations. We opensource both the dataset and the model to
encourage further research on multi-level paraphrasing

</details>


### [94] [A Retail-Corpus for Aspect-Based Sentiment Analysis with Large Language Models](https://arxiv.org/abs/2508.17994)
*Oleg Silcenco,Marcos R. Machad,Wallace C. Ugulino,Daniel Braun*

Main category: cs.CL

TL;DR: This paper introduces a new multilingual dataset for aspect-based sentiment analysis and benchmarks GPT-4 and LLaMA-3, showing both achieve over 85% accuracy with GPT-4 performing better.


<details>
  <summary>Details</summary>
Motivation: To enhance sentiment analysis by focusing on specific aspects and provide deeper insights than traditional sentiment analysis methods.

Method: Created a manually annotated dataset of 10,814 multilingual customer reviews for retail stores with 8 aspect categories, then evaluated GPT-4 and LLaMA-3 performance on aspect-based sentiment analysis.

Result: Both models achieved over 85% accuracy in aspect-based sentiment analysis, with GPT-4 outperforming LLaMA-3 across all relevant metrics.

Conclusion: The study establishes a baseline for the new dataset and demonstrates strong performance of large language models in aspect-based sentiment analysis, with GPT-4 showing superior results.

Abstract: Aspect-based sentiment analysis enhances sentiment detection by associating
it with specific aspects, offering deeper insights than traditional sentiment
analysis. This study introduces a manually annotated dataset of 10,814
multilingual customer reviews covering brick-and-mortar retail stores, labeled
with eight aspect categories and their sentiment. Using this dataset, the
performance of GPT-4 and LLaMA-3 in aspect based sentiment analysis is
evaluated to establish a baseline for the newly introduced data. The results
show both models achieving over 85% accuracy, while GPT-4 outperforms LLaMA-3
overall with regard to all relevant metrics.

</details>


### [95] [Neither Valid nor Reliable? Investigating the Use of LLMs as Judges](https://arxiv.org/abs/2508.18076)
*Khaoula Chehbouni,Mohammed Haddou,Jackie Chi Kit Cheung,Golnoosh Farnadi*

Main category: cs.CL

TL;DR: This position paper critically examines the premature adoption of large language models as judges (LLJs) for NLG evaluation, questioning their reliability and validity despite current enthusiasm.


<details>
  <summary>Details</summary>
Motivation: The rise of LLMs as general-purpose systems has complicated NLG evaluation, and while LLJs have emerged as promising alternatives to traditional metrics, their validity remains underexplored with adoption outpacing rigorous scrutiny.

Method: Drawing on measurement theory from social sciences, the paper critically assesses four core assumptions underlying LLJ use: proxy for human judgment, evaluator capabilities, scalability, and cost-effectiveness, examining limitations through three application areas.

Result: The analysis reveals that current LLJ practices may be undermined by inherent limitations of LLMs and evaluation methods, challenging the assumptions that support their widespread adoption.

Conclusion: The paper calls for more responsible evaluation practices to ensure LLJs support rather than undermine progress in NLG, emphasizing the need for rigorous validation before widespread implementation.

Abstract: Evaluating natural language generation (NLG) systems remains a core challenge
of natural language processing (NLP), further complicated by the rise of large
language models (LLMs) that aims to be general-purpose. Recently, large
language models as judges (LLJs) have emerged as a promising alternative to
traditional metrics, but their validity remains underexplored. This position
paper argues that the current enthusiasm around LLJs may be premature, as their
adoption has outpaced rigorous scrutiny of their reliability and validity as
evaluators. Drawing on measurement theory from the social sciences, we identify
and critically assess four core assumptions underlying the use of LLJs: their
ability to act as proxies for human judgment, their capabilities as evaluators,
their scalability, and their cost-effectiveness. We examine how each of these
assumptions may be challenged by the inherent limitations of LLMs, LLJs, or
current practices in NLG evaluation. To ground our analysis, we explore three
applications of LLJs: text summarization, data annotation, and safety
alignment. Finally, we highlight the need for more responsible evaluation
practices in LLJs evaluation, to ensure that their growing role in the field
supports, rather than undermines, progress in NLG.

</details>


### [96] [How Quantization Shapes Bias in Large Language Models](https://arxiv.org/abs/2508.18088)
*Federico Marcuzzi,Xuefei Ning,Roy Schwartz,Iryna Gurevych*

Main category: cs.CL

TL;DR: Quantization has nuanced effects on model bias - reduces toxicity but slightly increases stereotypes and unfairness, especially with aggressive compression.


<details>
  <summary>Details</summary>
Motivation: To comprehensively evaluate how quantization affects model bias across different demographic subgroups and bias types.

Method: Evaluated weight and activation quantization strategies across nine benchmarks using probabilistic and generated text-based metrics on models with varying architectures and reasoning abilities.

Result: Quantization reduces model toxicity and doesn't significantly impact sentiment, but tends to slightly increase stereotypes and unfairness in generative tasks, particularly under aggressive compression.

Conclusion: Careful balancing of efficiency and ethical considerations is crucial when applying quantization in practice, as it has complex and nuanced impacts on model bias.

Abstract: This work presents a comprehensive evaluation of how quantization affects
model bias, with particular attention to its impact on individual demographic
subgroups. We focus on weight and activation quantization strategies and
examine their effects across a broad range of bias types, including
stereotypes, toxicity, sentiment, and fairness. We employ both probabilistic
and generated text-based metrics across nine benchmarks and evaluate models
varying in architecture family and reasoning ability. Our findings show that
quantization has a nuanced impact on bias: while it can reduce model toxicity
and does not significantly impact sentiment, it tends to slightly increase
stereotypes and unfairness in generative tasks, especially under aggressive
compression. These trends are generally consistent across demographic
categories and model types, although their magnitude depends on the specific
setting. Overall, our results highlight the importance of carefully balancing
efficiency and ethical considerations when applying quantization in practice.

</details>


### [97] [Speech-Based Depressive Mood Detection in the Presence of Multiple Sclerosis: A Cross-Corpus and Cross-Lingual Study](https://arxiv.org/abs/2508.18092)
*Monica Gonzalez-Machorro,Uwe Reichel,Pascal Hecker,Helly Hammer,Hesam Sagha,Florian Eyben,Robert Hoepner,BjÃ¶rn W. Schuller*

Main category: cs.CL

TL;DR: Speech-based AI can detect depression in Multiple Sclerosis patients with 66-74% accuracy using emotional and speech features, showing cross-lingual transferability between English general population and German MS patient data.


<details>
  <summary>Details</summary>
Motivation: Depression commonly co-occurs with neurodegenerative disorders like Multiple Sclerosis, but speech-based AI for depression detection in such contexts remains unexplored. The study aims to examine transferability of speech-based depression detection methods to people with MS.

Method: Used supervised machine learning with: 1) conventional speech and language features, 2) emotional dimensions from Speech Emotion Recognition model, 3) exploratory speech feature analysis. Cross-corpus and cross-lingual analysis using English general population data and German MS patient data.

Result: Models detected depressive mood in MS patients with moderate generalizability (66% UAR on binary task). Feature selection improved performance to 74% UAR. Emotional changes were identified as relevant indicators of depressive mood in both general population and MS patients.

Conclusion: Speech-based depression detection can generalize to neurodegenerative disease contexts. The study provides initial exploration into generalizing speech-based depression detection even with co-occurring conditions like neurodegenerative diseases.

Abstract: Depression commonly co-occurs with neurodegenerative disorders like Multiple
Sclerosis (MS), yet the potential of speech-based Artificial Intelligence for
detecting depression in such contexts remains unexplored. This study examines
the transferability of speech-based depression detection methods to people with
MS (pwMS) through cross-corpus and cross-lingual analysis using English data
from the general population and German data from pwMS. Our approach implements
supervised machine learning models using: 1) conventional speech and language
features commonly used in the field, 2) emotional dimensions derived from a
Speech Emotion Recognition (SER) model, and 3) exploratory speech feature
analysis. Despite limited data, our models detect depressive mood in pwMS with
moderate generalisability, achieving a 66% Unweighted Average Recall (UAR) on a
binary task. Feature selection further improved performance, boosting UAR to
74%. Our findings also highlight the relevant role emotional changes have as an
indicator of depressive mood in both the general population and within PwMS.
This study provides an initial exploration into generalising speech-based
depression detection, even in the presence of co-occurring conditions, such as
neurodegenerative diseases.

</details>


### [98] [Agri-Query: A Case Study on RAG vs. Long-Context LLMs for Cross-Lingual Technical Question Answering](https://arxiv.org/abs/2508.18093)
*Julius Gun,Timo Oksanen*

Main category: cs.CL

TL;DR: Evaluation of 9 LLMs with 128K-token context on cross-lingual technical QA using agricultural machine manuals. Hybrid RAG outperforms direct prompting, with Gemini 2.5 Flash and Qwen 2.5 7B achieving >85% accuracy across English, French, and German.


<details>
  <summary>Details</summary>
Motivation: To assess LLM performance on realistic technical question answering with long context windows and cross-lingual information retrieval challenges, including testing for hallucinations with unanswerable questions.

Method: Built benchmark using agricultural machine manual in three languages, compared 9 long-context LLMs using direct prompting vs three RAG strategies (keyword, semantic, hybrid) with LLM-as-judge evaluation.

Result: Hybrid RAG consistently outperformed direct long-context prompting. Gemini 2.5 Flash and smaller Qwen 2.5 7B achieved over 85% accuracy across all languages with RAG approaches.

Conclusion: Provides detailed analysis of LLM performance in industrial domains and open framework for evaluations, highlighting practical trade-offs between RAG and direct prompting approaches for technical QA.

Abstract: We present a case study evaluating large language models (LLMs) with
128K-token context windows on a technical question answering (QA) task. Our
benchmark is built on a user manual for an agricultural machine, available in
English, French, and German. It simulates a cross-lingual information retrieval
scenario where questions are posed in English against all three language
versions of the manual. The evaluation focuses on realistic
"needle-in-a-haystack" challenges and includes unanswerable questions to test
for hallucinations. We compare nine long-context LLMs using direct prompting
against three Retrieval-Augmented Generation (RAG) strategies (keyword,
semantic, hybrid), with an LLM-as-a-judge for evaluation. Our findings for this
specific manual show that Hybrid RAG consistently outperforms direct
long-context prompting. Models like Gemini 2.5 Flash and the smaller Qwen 2.5
7B achieve high accuracy (over 85%) across all languages with RAG. This paper
contributes a detailed analysis of LLM performance in a specialized industrial
domain and an open framework for similar evaluations, highlighting practical
trade-offs and challenges.

</details>


### [99] [Detecting and Characterizing Planning in Language Models](https://arxiv.org/abs/2508.18098)
*Jatin Nainani,Sankaran Vaidyanathan,Connor Watts,Andre N. Assis,Alice Rigg*

Main category: cs.CL

TL;DR: The paper presents a method to distinguish planning from improvisation in LLMs, finding that planning is not universal and varies across models and tasks.


<details>
  <summary>Details</summary>
Motivation: To understand whether LLMs perform planning (selecting future targets in advance) or just improvise token-by-token, and to develop a systematic way to detect planning behaviors.

Method: Developed formal and causally grounded criteria for detecting planning, operationalized as a semi-automated annotation pipeline, applied to Gemma-2-2B models on MBPP code generation and poem generation tasks.

Result: Planning is not universal - Gemma-2-2B uses improvisation for poem generation (unlike Claude 3.5 Haiku) and switches between planning and improvisation on MBPP tasks. Instruction tuning refines existing planning rather than creating new planning behaviors.

Conclusion: Provides a reproducible foundation for mechanistic studies of planning in LLMs, showing planning behaviors vary across models and tasks rather than being a universal capability.

Abstract: Modern large language models (LLMs) have demonstrated impressive performance
across a wide range of multi-step reasoning tasks. Recent work suggests that
LLMs may perform planning - selecting a future target token in advance and
generating intermediate tokens that lead towards it - rather than merely
improvising one token at a time. However, existing studies assume fixed
planning horizons and often focus on single prompts or narrow domains. To
distinguish planning from improvisation across models and tasks, we present
formal and causally grounded criteria for detecting planning and operationalize
them as a semi-automated annotation pipeline. We apply this pipeline to both
base and instruction-tuned Gemma-2-2B models on the MBPP code generation
benchmark and a poem generation task where Claude 3.5 Haiku was previously
shown to plan. Our findings show that planning is not universal: unlike Haiku,
Gemma-2-2B solves the same poem generation task through improvisation, and on
MBPP it switches between planning and improvisation across similar tasks and
even successive token predictions. We further show that instruction tuning
refines existing planning behaviors in the base model rather than creating them
from scratch. Together, these studies provide a reproducible and scalable
foundation for mechanistic studies of planning in LLMs.

</details>


### [100] [SentiMM: A Multimodal Multi-Agent Framework for Sentiment Analysis in Social Media](https://arxiv.org/abs/2508.18108)
*Xilai Xu,Zilin Zhao,Chengye Song,Zining Wang,Jinhe Qiang,Jiongrui Yan,Yuhuai Lin*

Main category: cs.CL

TL;DR: SentiMM is a multi-agent framework for multimodal sentiment analysis that processes text and visual inputs through specialized agents, fuses features, integrates external knowledge, and achieves state-of-the-art performance on a new large-scale dataset.


<details>
  <summary>Details</summary>
Motivation: Address challenges in multimodal sentiment analysis including processing heterogeneous data, recognizing multi-label emotions, and overcoming limitations in cross-modal fusion and external knowledge integration in existing methods.

Method: Proposes SentiMM - a multi-agent framework with specialized agents for text and visual processing, multimodal feature fusion, knowledge retrieval for context enrichment, and result aggregation for final sentiment classification. Also introduces SentiMMD dataset with 7 fine-grained sentiment categories.

Result: Extensive experiments show SentiMM achieves superior performance compared to state-of-the-art baselines, demonstrating the effectiveness of the structured multi-agent approach.

Conclusion: The proposed SentiMM framework successfully addresses multimodal sentiment analysis challenges through systematic multi-agent processing, effective cross-modal fusion, and knowledge integration, validated by strong experimental results on the new SentiMMD dataset.

Abstract: With the increasing prevalence of multimodal content on social media,
sentiment analysis faces significant challenges in effectively processing
heterogeneous data and recognizing multi-label emotions. Existing methods often
lack effective cross-modal fusion and external knowledge integration. We
propose SentiMM, a novel multi-agent framework designed to systematically
address these challenges. SentiMM processes text and visual inputs through
specialized agents, fuses multimodal features, enriches context via knowledge
retrieval, and aggregates results for final sentiment classification. We also
introduce SentiMMD, a large-scale multimodal dataset with seven fine-grained
sentiment categories. Extensive experiments demonstrate that SentiMM achieves
superior performance compared to state-of-the-art baselines, validating the
effectiveness of our structured approach.

</details>


### [101] [Toward a Better Localization of Princeton WordNet](https://arxiv.org/abs/2508.18134)
*Abed Alhakim Freihat*

Main category: cs.CL

TL;DR: A framework for high-quality Arabic localization of Princeton WordNet that preserves cultural authenticity, with successful application to 10,000 synsets.


<details>
  <summary>Details</summary>
Motivation: The growing importance of Princeton WordNet in NLP requires proper localization, but existing efforts lack scale, rigor, and cultural alignment for Arabic.

Method: Proposes a structured framework detailing stages and procedures for localization while maintaining cultural authenticity.

Result: Successfully applied the framework to localize 10,000 synsets, demonstrating practical implementation.

Conclusion: The proposed framework enables high-quality Arabic WordNet localization that respects cultural context, addressing previous limitations in scale and rigor.

Abstract: As Princeton WordNet continues to gain significance as a semantic lexicon in
Natural Language Processing, the need for its localization and for ensuring the
quality of this process has become increasingly critical. Existing efforts
remain limited in both scale and rigor, and there is a notable absence of
studies addressing the accuracy of localization or its alignment with the
cultural context of Arabic. This paper proposes a structured framework for the
localization of Princeton WordNet, detailing the stages and procedures required
to achieve high-quality results without compromising cultural authenticity. We
further present our experience in applying this framework, reporting outcomes
from the localization of 10,000 synsets.

</details>


### [102] [S2Sent: Nested Selectivity Aware Sentence Representation Learning](https://arxiv.org/abs/2508.18164)
*Jianxiang Zang,Nijia Mo,Yonda Wei,Meiling Ning,Hui Liu*

Main category: cs.CL

TL;DR: SÂ²Sent is a novel sentence representation selection mechanism that performs spatial and frequency selection across different Transformer blocks to optimize cross-block representation fusion with minimal redundancy and semantic loss.


<details>
  <summary>Details</summary>
Motivation: Current Transformer-based contrastive learning approaches rely solely on the last block's hidden states, but different blocks have varying semantic perception abilities. Knowledge neurons' semantic potential is modulated by stimuli, making rational cross-block fusion a worthwhile optimization direction.

Method: Proposes SÂ²Sent - a parameterized nested selector downstream of Transformer encoders. It performs spatial selection (SS) using spatial squeeze self-gating for adaptive weights, and nested frequency selection (FS) using DCT basis functions instead of GAP for spatial squeeze with low semantic loss.

Result: Extensive experiments show SÂ²Sent achieves significant improvements over baseline methods with negligible additional parameters and inference latency, while demonstrating high integrability and scalability.

Conclusion: The proposed SÂ²Sent mechanism effectively balances semantic redundancy and loss in cross-block representation fusion, providing an efficient and scalable solution for optimizing sentence representation learning in Transformer-based encoders.

Abstract: The combination of Transformer-based encoders with contrastive learning
represents the current mainstream paradigm for sentence representation
learning. This paradigm is typically based on the hidden states of the last
Transformer block of the encoder. However, within Transformer-based encoders,
different blocks exhibit varying degrees of semantic perception ability. From
the perspective of interpretability, the semantic perception potential of
knowledge neurons is modulated by stimuli, thus rational cross-block
representation fusion is a direction worth optimizing. To balance the semantic
redundancy and loss across block fusion, we propose a sentence representation
selection mechanism S\textsuperscript{2}Sent, which integrates a parameterized
nested selector downstream of the Transformer-based encoder. This selector
performs spatial selection (SS) and nested frequency selection (FS) from a
modular perspective. The SS innovatively employs a spatial squeeze based
self-gating mechanism to obtain adaptive weights, which not only achieves
fusion with low information redundancy but also captures the dependencies
between embedding features. The nested FS replaces GAP with different DCT basis
functions to achieve spatial squeeze with low semantic loss. Extensive
experiments have demonstrated that S\textsuperscript{2}Sent achieves
significant improvements over baseline methods with negligible additional
parameters and inference latency, while highlighting high integrability and
scalability.

</details>


### [103] [DiscussLLM: Teaching Large Language Models When to Speak](https://arxiv.org/abs/2508.18167)
*Deep Anil Patel,Iain Melvin,Christopher Malon,Martin Renqiang Min*

Main category: cs.CL

TL;DR: DiscussLLM framework enables LLMs to proactively decide when to speak in human discussions, bridging the awareness gap through a two-stage data generation pipeline and silent token training.


<details>
  <summary>Details</summary>
Motivation: Current LLMs operate as reactive agents only responding when prompted, creating an awareness gap that limits their potential as collaborative partners in dynamic human discussions.

Method: Two-stage data generation pipeline synthesizing realistic multi-turn discussions annotated with intervention types, training models to predict silent tokens when no intervention is needed. Two architectural baselines: integrated end-to-end model and decoupled classifier-generator system.

Result: Models learn to remain quiet until helpful contributions can be made, enabling accurate timing of interventions and generation of helpful responses.

Conclusion: The framework paves the way for more situationally aware and proactive conversational AI that can better collaborate in human discussions.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
understanding and generating human-like text, yet they largely operate as
reactive agents, responding only when directly prompted. This passivity creates
an "awareness gap," limiting their potential as truly collaborative partners in
dynamic human discussions. We introduce $\textit{DiscussLLM}$, a framework
designed to bridge this gap by training models to proactively decide not just
$\textit{what}$ to say, but critically, $\textit{when}$ to speak. Our primary
contribution is a scalable two-stage data generation pipeline that synthesizes
a large-scale dataset of realistic multi-turn human discussions. Each
discussion is annotated with one of five intervention types (e.g., Factual
Correction, Concept Definition) and contains an explicit conversational trigger
where an AI intervention adds value. By training models to predict a special
silent token when no intervention is needed, they learn to remain quiet until a
helpful contribution can be made. We explore two architectural baselines: an
integrated end-to-end model and a decoupled classifier-generator system
optimized for low-latency inference. We evaluate these models on their ability
to accurately time interventions and generate helpful responses, paving the way
for more situationally aware and proactive conversational AI.

</details>


### [104] [Improving End-to-End Training of Retrieval-Augmented Generation Models via Joint Stochastic Approximation](https://arxiv.org/abs/2508.18168)
*Hongyu Cao,Yuxuan Wu,Yucheng Cai,Xianyu Zhao,Zhijian Ou*

Main category: cs.CL

TL;DR: Proposes JSA-RAG, a joint stochastic approximation method for end-to-end training of retrieval-augmented generation models, overcoming biased/high-variance gradient issues in traditional approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG models face challenges in end-to-end optimization due to the need for marginalization over discrete latent variables (relevant passages), with existing methods like top-K marginalization and VRAG suffering from biased or high-variance gradient estimates.

Method: Develops JSA-RAG using joint stochastic approximation algorithm, which is a stochastic extension of the EM algorithm, specifically designed for estimating discrete latent variable models in RAG frameworks.

Result: Extensive experiments on five datasets for open-domain question answering and knowledge-grounded dialogs show JSA-RAG significantly outperforms both vanilla RAG and VRAG.

Conclusion: JSA-RAG demonstrates efficacy in generation, retrieval, and provides low-variance gradient estimates, making it a superior approach for end-to-end RAG optimization.

Abstract: Retrieval-augmented generation (RAG) has become a widely recognized paradigm
to combine parametric memory with non-parametric memories. An RAG model
consists of two serial connecting components (retriever and generator). A major
challenge in end-to-end optimization of the RAG model is that marginalization
over relevant passages (modeled as discrete latent variables) from a knowledge
base is required. Traditional top-K marginalization and variational RAG (VRAG)
suffer from biased or high-variance gradient estimates. In this paper, we
propose and develop joint stochastic approximation (JSA) based end-to-end
training of RAG, which is referred to as JSA-RAG. The JSA algorithm is a
stochastic extension of the EM (expectation-maximization) algorithm and is
particularly powerful in estimating discrete latent variable models. Extensive
experiments are conducted on five datasets for two tasks (open-domain question
answering, knowledge-grounded dialogs) and show that JSA-RAG significantly
outperforms both vanilla RAG and VRAG. Further analysis shows the efficacy of
JSA-RAG from the perspectives of generation, retrieval, and low-variance
gradient estimate.

</details>


### [105] [Leveraging Large Language Models for Accurate Sign Language Translation in Low-Resource Scenarios](https://arxiv.org/abs/2508.18183)
*Luana Bulla,Gabriele Tuccio,Misael MongiovÃ¬,Aldo Gangemi*

Main category: cs.CL

TL;DR: AulSign leverages LLMs with dynamic prompting and in-context learning for sign language translation, achieving superior performance in low-data scenarios by associating signs with natural language descriptions.


<details>
  <summary>Details</summary>
Motivation: Address the limited availability of parallel corpora for sign language translation and overcome the challenge of data scarcity in this underexplored field to enhance accessibility for underrepresented linguistic communities.

Method: Proposes AulSign method that uses Large Language Models with dynamic prompting and in-context learning, incorporating sample selection and sign association through compact natural language descriptions of signs.

Result: Demonstrates superior performance compared to state-of-the-art models on both English (SignBank+) and Italian (LaCAM CNR-ISTC) datasets in low-data scenarios.

Conclusion: AulSign effectively bridges the gap in sign language translation using LLMs, showing potential to significantly improve accessibility and inclusivity in communication technologies for sign language communities.

Abstract: Translating natural languages into sign languages is a highly complex and
underexplored task. Despite growing interest in accessibility and inclusivity,
the development of robust translation systems remains hindered by the limited
availability of parallel corpora which align natural language with sign
language data. Existing methods often struggle to generalize in these
data-scarce environments, as the few datasets available are typically
domain-specific, lack standardization, or fail to capture the full linguistic
richness of sign languages. To address this limitation, we propose Advanced Use
of LLMs for Sign Language Translation (AulSign), a novel method that leverages
Large Language Models via dynamic prompting and in-context learning with sample
selection and subsequent sign association. Despite their impressive abilities
in processing text, LLMs lack intrinsic knowledge of sign languages; therefore,
they are unable to natively perform this kind of translation. To overcome this
limitation, we associate the signs with compact descriptions in natural
language and instruct the model to use them. We evaluate our method on both
English and Italian languages using SignBank+, a recognized benchmark in the
field, as well as the Italian LaCAM CNR-ISTC dataset. We demonstrate superior
performance compared to state-of-the-art models in low-data scenario. Our
findings demonstrate the effectiveness of AulSign, with the potential to
enhance accessibility and inclusivity in communication technologies for
underrepresented linguistic communities.

</details>


### [106] [Exploring the Interplay between Musical Preferences and Personality through the Lens of Language](https://arxiv.org/abs/2508.18208)
*Eliran Shem-Tov,Ella Rabinovich*

Main category: cs.CL

TL;DR: This study investigates whether musical preferences can be recognized through spontaneous language analysis using Big Five personality traits as a bridge between music psychology and computational linguistics.


<details>
  <summary>Details</summary>
Motivation: To bridge two research domains: established correlations between musical preferences and personality traits, and the detection of personality through linguistic analysis, exploring whether musical preferences are reflected in spontaneous language.

Method: Used a curated dataset of over 500,000 text samples from nearly 5,000 authors with reliably identified musical preferences, building advanced models to assess personality characteristics through computational linguistics approaches.

Result: Revealed significant personality differences across fans of five musical genres, demonstrating that musical preferences are recognizable through language analysis.

Conclusion: The study successfully bridges music psychology and computational linguistics, providing resources for future research at this intersection and showing that musical preferences can be detected through spontaneous language via personality trait analysis.

Abstract: Music serves as a powerful reflection of individual identity, often aligning
with deeper psychological traits. Prior research has established correlations
between musical preferences and personality traits, while separate studies have
demonstrated that personality is detectable through linguistic analysis. Our
study bridges these two research domains by investigating whether individuals'
musical preferences are recognizable in their spontaneous language through the
lens of the Big Five personality traits (Openness, Conscientiousness,
Extroversion, Agreeableness, and Neuroticism). Using a carefully curated
dataset of over 500,000 text samples from nearly 5,000 authors with reliably
identified musical preferences, we build advanced models to assess personality
characteristics. Our results reveal significant personality differences across
fans of five musical genres. We release resources for future research at the
intersection of computational linguistics, music psychology and personality
analysis.

</details>


### [107] [Why Synthetic Isn't Real Yet: A Diagnostic Framework for Contact Center Dialogue Generation](https://arxiv.org/abs/2508.18210)
*Rishikesh Devanathan,Varun Nathan,Ayush Kumar*

Main category: cs.CL

TL;DR: Synthetic transcript generation for contact center conversations using derived call attributes as supervision, with a diagnostic framework of 18 metrics to evaluate quality across multiple generation strategies.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns and data scarcity in contact center domains limit model training and evaluation, requiring synthetic dialogue generation that captures the unique characteristics of goal-oriented, role-asymmetric conversations with disfluencies and ASR noise.

Method: Leverage derived call attributes (Intent Summaries, Topic Flow, QA Evaluation Forms) as supervision signals. Benchmark four language-agnostic generation strategies from simple prompting to multi-stage approaches, and introduce a diagnostic framework with 18 linguistically and behaviorally grounded metrics.

Result: No method excels across all traits, with notable deficits in disfluency, sentiment, and behavioral realism. The diagnostic tool effectively exposes these gaps for fine-grained evaluation.

Conclusion: Synthetic dialogue generation for contact centers remains challenging, but the proposed diagnostic framework enables comprehensive evaluation and stress testing across languages, highlighting areas needing improvement in behavioral realism and linguistic accuracy.

Abstract: Synthetic transcript generation is critical in contact center domains, where
privacy and data scarcity limit model training and evaluation. Unlike prior
synthetic dialogue generation work on open-domain or medical dialogues, contact
center conversations are goal-oriented, role-asymmetric, and behaviorally
complex, featuring disfluencies, ASR noise, and compliance-driven agent
actions. In deployments where transcripts are unavailable, standard pipelines
still yield derived call attributes such as Intent Summaries, Topic Flow, and
QA Evaluation Forms. We leverage these as supervision signals to guide
generation. To assess the quality of such outputs, we introduce a diagnostic
framework of 18 linguistically and behaviorally grounded metrics for comparing
real and synthetic transcripts. We benchmark four language-agnostic generation
strategies, from simple prompting to characteristic-aware multi-stage
approaches, alongside reference-free baselines. Results reveal persistent
challenges: no method excels across all traits, with notable deficits in
disfluency, sentiment, and behavioral realism. Our diagnostic tool exposes
these gaps, enabling fine-grained evaluation and stress testing of synthetic
dialogue across languages.

</details>


### [108] [Better Language Model-Based Judging Reward Modeling through Scaling Comprehension Boundaries](https://arxiv.org/abs/2508.18212)
*Meiling Ning,Zhongbao Zhang,Junda Ye,Jiabao Guo,Qingyuan Guan*

Main category: cs.CL

TL;DR: The paper proposes ESFP-RM, a two-stage reward model that reframes reward modeling as natural language inference and uses masked language models with contextual explanations to provide more stable and generalizable reward signals for RLHF and OOD scenarios.


<details>
  <summary>Details</summary>
Motivation: To advance LM-based judging reward modeling by recognizing its formal consistency with natural language inference (NLI) and scaling model comprehension boundaries for superior reward models.

Method: Proposes ESFP-RM, a two-stage reward model using explanation-based slot framework with masked language models (MLMs) that incorporate contextual explanations, outperforming autoregressive models on NLI tasks.

Result: Extensive experiments show ESFP-RM delivers more stable and generalizable reward signals compared to generative reward models in both RLHF and out-of-distribution scenarios.

Conclusion: Reframing reward modeling as NLI and leveraging MLMs with contextual explanations through the ESFP-RM framework provides superior performance and generalization for AI feedback reinforcement learning.

Abstract: The emergence of LM-based judging reward modeling, represented by generative
reward models, has successfully made reinforcement learning from AI feedback
(RLAIF) efficient and scalable. To further advance this paradigm, we propose a
core insight: this form of reward modeling shares fundamental formal
consistency with natural language inference (NLI), a core task in natural
language understanding. This reframed perspective points to a key path for
building superior reward models: scaling the model's comprehension boundaries.
Pursuing this path, exploratory experiments on NLI tasks demonstrate that the
slot prediction masked language models (MLMs) incorporating contextual
explanations achieve significantly better performance compared to mainstream
autoregressive models. Based on this key finding, we propose ESFP-RM, a
two-stage LM-based judging reward model that utilizes an explanation based slot
framework for prediction to fully leverage the advantages of MLMs. Extensive
experiments demonstrate that in both reinforcement learning from human feedback
(RLHF) and out-of-distribution (OOD) scenarios, the ESFP-RM framework delivers
more stable and generalizable reward signals compared to generative reward
models.

</details>


### [109] [MTalk-Bench: Evaluating Speech-to-Speech Models in Multi-Turn Dialogues via Arena-style and Rubrics Protocols](https://arxiv.org/abs/2508.18240)
*Yuhao Du,Qianwei Huang,Guo Zhu,Zhanchen Dai,Sunian Chen,Qiming Zhu,Yuhao Zhang,Li Zhou,Benyou Wang*

Main category: cs.CL

TL;DR: MTalk-Bench is a multi-turn speech-to-speech benchmark covering semantic, paralinguistic, and ambient sound dimensions with dual evaluation methods (Arena-style and Rubrics-based) that reveals S2S LLMs excel at semantics but struggle with paralinguistics and ambient sounds.


<details>
  <summary>Details</summary>
Motivation: Current evaluation frameworks are inadequate for assessing speech-to-speech large language models in complex, multi-turn dialogues, necessitating a more comprehensive benchmark.

Method: Introduced MTalk-Bench with three core dimensions (Semantic, Paralinguistic, Ambient Sound) across nine realistic scenarios, using dual evaluation framework combining Arena-style pairwise comparison and Rubrics-based absolute scoring with both human and LLM evaluators.

Result: S2S LLMs excel at semantic processing but underperform on paralinguistic information and ambient sounds; models regain coherence by increasing response length at efficiency cost; modality-aware designs outperform brute scaling. Evaluation methods show consistent rankings but require large performance gaps for reliable distinctions.

Conclusion: The study highlights current limitations in S2S evaluation and demonstrates the need for more robust, speech-aware assessment frameworks that address both semantic and non-semantic dimensions of spoken interaction.

Abstract: The rapid advancement of speech-to-speech (S2S) large language models (LLMs)
has significantly improved real-time spoken interaction. However, current
evaluation frameworks remain inadequate for assessing performance in complex,
multi-turn dialogues. To address this, we introduce MTalk-Bench, a multi-turn
S2S benchmark covering three core dimensions: Semantic Information,
Paralinguistic Information, and Ambient Sound. Each dimension includes nine
realistic scenarios, along with targeted tasks to assess specific capabilities
such as reasoning. Our dual-method evaluation framework combines Arena-style
evaluation (pairwise comparison) and Rubrics-based evaluation (absolute
scoring) for relative and absolute assessment. The benchmark includes both
model and human outputs, evaluated by human evaluators and LLMs. Experimental
results reveal two sets of findings. Overall performance of S2S LLMs: (1)
models excel at semantic information processing yet underperform on
paralinguistic information and ambient sounds perception; (2) models typically
regain coherence by increasing response length, sacrificing efficiency in
multi-turn dialogues; (3) modality-aware, task-specific designs outperform
brute scaling. Evaluation framework and reliability: (1) Arena and Rubrics
yield consistent, complementary rankings, but reliable distinctions emerge only
when performance gaps are large; (2) LLM-as-a-judge aligns with humans when
gaps are clear or criteria explicit, but exhibits position and length biases
and is reliable on nonverbal evaluation only with text annotations. These
results highlight current limitations in S2S evaluation and the need for more
robust, speech-aware assessment frameworks.

</details>


### [110] [Demographic Biases and Gaps in the Perception of Sexism in Large Language Models](https://arxiv.org/abs/2508.18245)
*Judith Tavarez-RodrÃ­guez,Fernando SÃ¡nchez-Vega,A. Pastor LÃ³pez-Monroy*

Main category: cs.CL

TL;DR: LLMs show some capability in sexism detection but fail to accurately replicate diverse demographic perceptions, highlighting biases and the need for better-calibrated models.


<details>
  <summary>Details</summary>
Motivation: Previous studies show LLMs contain biases that don't accurately reflect reality, especially for minority groups, making sexism detection challenging despite various improvement efforts.

Method: Evaluate different LLMs using EXIST 2024 tweet dataset with annotations from six distinct profiles, analyze demographic biases, and conduct statistical analysis to identify which demographic characteristics contribute most to sexism detection.

Result: LLMs can detect sexism to some extent when considering overall population opinion, but they do not accurately replicate the diversity of perceptions among different demographic groups.

Conclusion: There is a need for better-calibrated models that account for the diversity of perspectives across different populations to improve sexism detection accuracy.

Abstract: The use of Large Language Models (LLMs) has proven to be a tool that could
help in the automatic detection of sexism. Previous studies have shown that
these models contain biases that do not accurately reflect reality, especially
for minority groups. Despite various efforts to improve the detection of sexist
content, this task remains a significant challenge due to its subjective nature
and the biases present in automated models. We explore the capabilities of
different LLMs to detect sexism in social media text using the EXIST 2024 tweet
dataset. It includes annotations from six distinct profiles for each tweet,
allowing us to evaluate to what extent LLMs can mimic these groups' perceptions
in sexism detection. Additionally, we analyze the demographic biases present in
the models and conduct a statistical analysis to identify which demographic
characteristics (age, gender) contribute most effectively to this task. Our
results show that, while LLMs can to some extent detect sexism when considering
the overall opinion of populations, they do not accurately replicate the
diversity of perceptions among different demographic groups. This highlights
the need for better-calibrated models that account for the diversity of
perspectives across different populations.

</details>


### [111] [From BERT to LLMs: Comparing and Understanding Chinese Classifier Prediction in Language Models](https://arxiv.org/abs/2508.18253)
*ZiqiZhang,Jianfei Ma,Emmanuele Chersoni,Jieshun You,Zhaoxin Feng*

Main category: cs.CL

TL;DR: LLMs perform worse than BERT at predicting Chinese classifiers, with bidirectional attention models like BERT showing advantages due to better utilization of noun information.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether popular Large Language Models possess proper knowledge of Chinese classifiers, which are important for educational applications but remain unexplored in NLP literature.

Method: Employed various masking strategies to evaluate LLMs' intrinsic ability, contribution of different sentence elements, attention mechanisms, and explored fine-tuning to enhance classifier performance.

Result: LLMs performed worse than BERT even with fine-tuning. Prediction greatly benefits from information about the following noun, explaining the advantage of bidirectional attention models like BERT.

Conclusion: Current LLMs lack proper knowledge of Chinese classifiers compared to BERT, with bidirectional attention mechanisms proving more effective for this specific linguistic task.

Abstract: Classifiers are an important and defining feature of the Chinese language,
and their correct prediction is key to numerous educational applications. Yet,
whether the most popular Large Language Models (LLMs) possess proper knowledge
the Chinese classifiers is an issue that has largely remain unexplored in the
Natural Language Processing (NLP) literature.
  To address such a question, we employ various masking strategies to evaluate
the LLMs' intrinsic ability, the contribution of different sentence elements,
and the working of the attention mechanisms during prediction. Besides, we
explore fine-tuning for LLMs to enhance the classifier performance.
  Our findings reveal that LLMs perform worse than BERT, even with fine-tuning.
The prediction, as expected, greatly benefits from the information about the
following noun, which also explains the advantage of models with a
bidirectional attention mechanism such as BERT.

</details>


### [112] [MIRAGE: Scaling Test-Time Inference with Parallel Graph-Retrieval-Augmented Reasoning Chains](https://arxiv.org/abs/2508.18260)
*Kaiwen Wei,Rui Shan,Dongsheng Zou,Jianzhong Yang,Bi Zhao,Junnan Zhu,Jiang Zhong*

Main category: cs.CL

TL;DR: MIRAGE is a novel test-time scalable reasoning framework that uses multi-chain inference over medical knowledge graphs to improve accuracy and traceability in medical QA tasks, outperforming GPT-4o and other baselines.


<details>
  <summary>Details</summary>
Motivation: Current approaches like search-o1 use single linear reasoning chains with flat RAG integration, leading to error accumulation and limited effectiveness in medical QA where accuracy and traceability are critical.

Method: MIRAGE decomposes queries into entity-grounded sub-questions, executes parallel inference chains, retrieves evidence via neighbor expansion and multi-hop traversal, and integrates answers using cross-chain verification.

Result: Experiments on three medical QA benchmarks show MIRAGE consistently outperforms GPT-4o, Tree-of-Thought variants, and other retrieval-augmented baselines in both automatic and human evaluations.

Conclusion: MIRAGE improves interpretability by generating explicit reasoning chains traceable to knowledge graph sources, making it well-suited for complex medical reasoning scenarios.

Abstract: Large reasoning models (LRMs) have shown significant progress in test-time
scaling through chain-of-thought prompting. Current approaches like search-o1
integrate retrieval augmented generation (RAG) into multi-step reasoning
processes but rely on a single, linear reasoning chain while incorporating
unstructured textual information in a flat, context-agnostic manner. As a
result, these approaches can lead to error accumulation throughout the
reasoning chain, which significantly limits its effectiveness in medical
question-answering (QA) tasks where both accuracy and traceability are critical
requirements. To address these challenges, we propose MIRAGE (Multi-chain
Inference with Retrieval-Augmented Graph Exploration), a novel test-time
scalable reasoning framework that performs dynamic multi-chain inference over
structured medical knowledge graphs. Specifically, MIRAGE 1) decomposes complex
queries into entity-grounded sub-questions, 2) executes parallel inference
chains, 3) retrieves evidence adaptively via neighbor expansion and multi-hop
traversal, and 4) integrates answers using cross-chain verification to resolve
contradictions. Experiments on three medical QA benchmarks (GenMedGPT-5k,
CMCQA, and ExplainCPE) show that MIRAGE consistently outperforms GPT-4o,
Tree-of-Thought variants, and other retrieval-augmented baselines in both
automatic and human evaluations. Additionally, MIRAGE improves interpretability
by generating explicit reasoning chains that trace each factual claim to
concrete chains within the knowledge graph, making it well-suited for complex
medical reasoning scenarios. The code will be available for further research.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [113] [Towards High-Precision Depth Sensing via Monocular-Aided iToF and RGB Integration](https://arxiv.org/abs/2508.16579)
*Yansong Du,Yutong Deng,Yuting Zhou,Feiyu Jiao,Jian Song,Xun Guan*

Main category: cs.CV

TL;DR: A novel iToF-RGB fusion framework that enhances depth sensing by combining narrow-FoV iToF depth with wide-FoV RGB images through geometric alignment and dual-encoder fusion, achieving improved resolution, accuracy, and expanded field-of-view.


<details>
  <summary>Details</summary>
Motivation: Address limitations of indirect Time-of-Flight (iToF) depth sensing including low spatial resolution, limited field-of-view, and structural distortion in complex scenes by leveraging complementary RGB information.

Method: Reprojects narrow-FoV iToF depth onto wide-FoV RGB coordinates via geometric calibration, then uses dual-encoder fusion network with monocular depth priors to extract complementary features and perform depth super-resolution with cross-modal structural cues.

Result: Significantly outperforms state-of-the-art methods in accuracy, structural consistency, and visual quality on both synthetic and real-world datasets, achieving enhanced depth accuracy, improved edge sharpness, and seamless FoV expansion.

Conclusion: The proposed iToF-RGB fusion framework effectively overcomes iToF limitations by integrating cross-modal information, demonstrating superior performance in depth sensing quality and field-of-view expansion.

Abstract: This paper presents a novel iToF-RGB fusion framework designed to address the
inherent limitations of indirect Time-of-Flight (iToF) depth sensing, such as
low spatial resolution, limited field-of-view (FoV), and structural distortion
in complex scenes. The proposed method first reprojects the narrow-FoV iToF
depth map onto the wide-FoV RGB coordinate system through a precise geometric
calibration and alignment module, ensuring pixel-level correspondence between
modalities. A dual-encoder fusion network is then employed to jointly extract
complementary features from the reprojected iToF depth and RGB image, guided by
monocular depth priors to recover fine-grained structural details and perform
depth super-resolution. By integrating cross-modal structural cues and depth
consistency constraints, our approach achieves enhanced depth accuracy,
improved edge sharpness, and seamless FoV expansion. Extensive experiments on
both synthetic and real-world datasets demonstrate that the proposed framework
significantly outperforms state-of-the-art methods in terms of accuracy,
structural consistency, and visual quality.

</details>


### [114] [CountLoop: Training-Free High-Instance Image Generation via Iterative Agent Guidance](https://arxiv.org/abs/2508.16644)
*Anindya Mondal,Ayan Banerjee,Sauradip Nag,Josep LladÃ³s,Xiatian Zhu,Anjan Dutta*

Main category: cs.CV

TL;DR: CountLoop is a training-free framework that enables diffusion models to generate images with precise object instance counts through iterative multimodal feedback and attention masking techniques.


<details>
  <summary>Details</summary>
Motivation: Diffusion models struggle with generating scenes containing exact numbers of object instances, especially in complex, high-density settings where precise counting and spatial arrangement are challenging.

Method: Uses iterative structured feedback alternating between image generation and multimodal agent evaluation. Includes language-guided planner/critic for count assessment, instance-driven attention masking, and compositional generation techniques to improve object separation.

Result: Achieves up to 98% counting accuracy on COCO Count, T2I CompBench, and new high-instance benchmarks while maintaining spatial fidelity and visual quality, outperforming baselines with a score of 0.97.

Conclusion: CountLoop successfully provides diffusion models with accurate instance control without requiring retraining, demonstrating significant improvements in object counting precision for complex scene generation.

Abstract: Diffusion models have shown remarkable progress in photorealistic image
synthesis, yet they remain unreliable for generating scenes with a precise
number of object instances, particularly in complex and high-density settings.
We present CountLoop, a training-free framework that provides diffusion models
with accurate instance control through iterative structured feedback. The
approach alternates between image generation and multimodal agent evaluation,
where a language-guided planner and critic assess object counts, spatial
arrangements, and attribute consistency. This feedback is then used to refine
layouts and guide subsequent generations. To further improve separation between
objects, especially in occluded scenes, we introduce instance-driven attention
masking and compositional generation techniques. Experiments on COCO Count, T2I
CompBench, and two new high-instance benchmarks show that CountLoop achieves
counting accuracy of up to 98% while maintaining spatial fidelity and visual
quality, outperforming layout-based and gradient-guided baselines with a score
of 0.97.

</details>


### [115] [Do VLMs Have Bad Eyes? Diagnosing Compositional Failures via Mechanistic Interpretability](https://arxiv.org/abs/2508.16652)
*Ashwath Vaithinathan Aravindan,Abha Jha,Mihir Kulkarni*

Main category: cs.CV

TL;DR: VLMs struggle with compositional generalization and object binding due to superposition in MLP neurons, where individual neurons represent multiple features, hindering compositional reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models (VLMs) perform well on tasks like image captioning and visual question answering but fail at compositional generalization and object binding, limiting their ability to handle novel object-attribute combinations.

Method: The study uses mechanistic interpretability techniques to analyze CLIP's vision encoder, specifically examining how individual neurons in MLP layers represent features and contribute to compositional failures.

Result: Evidence shows that superposition in MLP neurons (where single neurons represent multiple features) directly hinders compositional feature representation, affecting both compositional reasoning and object binding capabilities.

Conclusion: This work identifies superposition as a root cause of compositional failures in VLMs and serves as an initial step toward understanding the mechanistic underpinnings of these limitations in vision-language models.

Abstract: Vision-Language Models (VLMs) have shown remarkable performance in
integrating visual and textual information for tasks such as image captioning
and visual question answering. However, these models struggle with
compositional generalization and object binding, which limit their ability to
handle novel combinations of objects and their attributes. Our work explores
the root causes of these failures using mechanistic interpretability
techniques. We show evidence that individual neurons in the MLP layers of
CLIP's vision encoder represent multiple features, and this "superposition"
directly hinders its compositional feature representation which consequently
affects compositional reasoning and object binding capabilities. We hope this
study will serve as an initial step toward uncovering the mechanistic roots of
compositional failures in VLMs. The code and supporting results can be found
https://github.com/Mystic-Slice/Do-VLMs-Have-Bad-Eyes .

</details>


### [116] [MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and LLM Spatial Reasoning](https://arxiv.org/abs/2508.16654)
*Chenghao Liu,Zhimu Zhou,Jiachen Zhang,Minghao Zhang,Songfang Huang,Huiling Duan*

Main category: cs.CV

TL;DR: MSNav is a novel framework that addresses VLN challenges by integrating memory, spatial reasoning, and decision modules, achieving state-of-the-art performance on navigation benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current VLN approaches using single LLMs suffer from poor spatial reasoning, weak cross-modal grounding, and memory overload in long-horizon tasks, requiring a more systematic solution.

Method: MSNav integrates three modules: Memory Module for dynamic map memory with selective node pruning, Spatial Module for spatial reasoning and object relationship inference, and Decision Module for LLM-based path planning. Also introduces I-O-S dataset and fine-tunes Qwen3-4B into Qwen-Spatial model.

Result: Achieves higher F1 and NDCG scores on I-O-S test set for object list extraction. Demonstrates state-of-the-art performance on R2R and REVERIE datasets with significant improvements in Success Rate (SR) and Success weighted by Path Length (SPL).

Conclusion: MSNav successfully addresses VLN vulnerabilities through its synergistic three-module architecture, transforming fragile inference into robust, integrated intelligence for vision-and-language navigation tasks.

Abstract: Vision-and-Language Navigation (VLN) requires an agent to interpret natural
language instructions and navigate complex environments. Current approaches
often adopt a "black-box" paradigm, where a single Large Language Model (LLM)
makes end-to-end decisions. However, it is plagued by critical vulnerabilities,
including poor spatial reasoning, weak cross-modal grounding, and memory
overload in long-horizon tasks. To systematically address these issues, we
propose Memory Spatial Navigation(MSNav), a framework that fuses three modules
into a synergistic architecture, which transforms fragile inference into a
robust, integrated intelligence. MSNav integrates three modules: Memory Module,
a dynamic map memory module that tackles memory overload through selective node
pruning, enhancing long-range exploration; Spatial Module, a module for spatial
reasoning and object relationship inference that improves endpoint recognition;
and Decision Module, a module using LLM-based path planning to execute robust
actions. Powering Spatial Module, we also introduce an Instruction-Object-Space
(I-O-S) dataset and fine-tune the Qwen3-4B model into Qwen-Spatial (Qwen-Sp),
which outperforms leading commercial LLMs in object list extraction, achieving
higher F1 and NDCG scores on the I-O-S test set. Extensive experiments on the
Room-to-Room (R2R) and REVERIE datasets demonstrate MSNav's state-of-the-art
performance with significant improvements in Success Rate (SR) and Success
weighted by Path Length (SPL).

</details>


### [117] [Optimizing Hyper parameters in CNN for Soil Classification using PSO and Whale Optimization Algorithm](https://arxiv.org/abs/2508.16660)
*Yasir Nooruldeen Ibrahim,Fawziya Mahmood Ramo,Mahmood Siddeeq Qadir,Muna Jaffer Al-Shamdeen*

Main category: cs.CV

TL;DR: This paper presents an intelligent soil classification system using Convolutional Neural Networks optimized with swarm algorithms (Whale Optimization and Particle Swarm Optimization) to achieve better performance in classifying soil types from images.


<details>
  <summary>Details</summary>
Motivation: Soil classification contributes to better land management, increased agricultural output, and environmental solutions. Understanding soil quality aids agriculture, civil engineering, and natural resource management by enabling risk reduction, performance improvement, and informed decision-making.

Method: Used Convolutional Neural Networks for soil image classification, enhanced with machine learning algorithms. Employed Whale Optimization Algorithm and Particle Swarm Optimization algorithm to select optimal hyperparameters for the CNN network, comparing both swarm optimization approaches.

Result: The proposed system achieved efficient results in multiple soil type classification, with performance measured using Accuracy and F1 metrics. The swarm optimization algorithms successfully improved CNN performance.

Conclusion: The integration of swarm optimization algorithms with Convolutional Neural Networks provides an effective approach for intelligent soil classification, offering valuable results for practical applications in agriculture and environmental management.

Abstract: Classifying soil images contributes to better land management, increased
agricultural output, and practical solutions for environmental issues. The
development of various disciplines, particularly agriculture, civil
engineering, and natural resource management, is aided by understanding of soil
quality since it helps with risk reduction, performance improvement, and sound
decision-making . Artificial intelligence has recently been used in a number of
different fields. In this study, an intelligent model was constructed using
Convolutional Neural Networks to classify soil kinds, and machine learning
algorithms were used to enhance the performance of soil classification . To
achieve better implementation and performance of the Convolutional Neural
Networks algorithm and obtain valuable results for the process of classifying
soil type images, swarm algorithms were employed to obtain the best performance
by choosing Hyper parameters for the Convolutional Neural Networks network
using the Whale optimization algorithm and the Particle swarm optimization
algorithm, and comparing the results of using the two algorithms in the process
of multiple classification of soil types. The Accuracy and F1 measures were
adopted to test the system, and the results of the proposed work were efficient
result

</details>


### [118] [QA-VLM: Providing human-interpretable quality assessment for wire-feed laser additive manufacturing parts with Vision Language Models](https://arxiv.org/abs/2508.16661)
*Qiaojie Zheng,Jiucai Zhang,Joy Gockel,Michael B. Wakin,Craig Brice,Xiaoli Zhang*

Main category: cs.CV

TL;DR: QA-VLM framework uses vision-language models with domain knowledge to provide interpretable quality assessment in additive manufacturing, outperforming standard VLMs in validity and consistency.


<details>
  <summary>Details</summary>
Motivation: Current machine learning methods for image-based quality assessment in additive manufacturing are black-box systems that lack interpretable justifications, limiting trust and adoption in real-world industrial settings.

Method: A novel QA-VLM framework that leverages vision-language models' attention mechanisms and reasoning capabilities, enriched with application-specific knowledge from peer-reviewed journal articles to generate human-interpretable quality assessments.

Result: Evaluated on 24 single-bead samples from laser wire direct energy deposition, the framework demonstrated higher validity and consistency in explanation quality compared to off-the-shelf VLMs.

Conclusion: The approach shows potential for enabling trustworthy, interpretable quality assessment in additive manufacturing applications by providing human-understandable justifications for quality decisions.

Abstract: Image-based quality assessment (QA) in additive manufacturing (AM) often
relies heavily on the expertise and constant attention of skilled human
operators. While machine learning and deep learning methods have been
introduced to assist in this task, they typically provide black-box outputs
without interpretable justifications, limiting their trust and adoption in
real-world settings. In this work, we introduce a novel QA-VLM framework that
leverages the attention mechanisms and reasoning capabilities of
vision-language models (VLMs), enriched with application-specific knowledge
distilled from peer-reviewed journal articles, to generate human-interpretable
quality assessments. Evaluated on 24 single-bead samples produced by laser wire
direct energy deposition (DED-LW), our framework demonstrates higher validity
and consistency in explanation quality than off-the-shelf VLMs. These results
highlight the potential of our approach to enable trustworthy, interpretable
quality assessment in AM applications.

</details>


### [119] [The Loupe: A Plug-and-Play Attention Module for Amplifying Discriminative Features in Vision Transformers](https://arxiv.org/abs/2508.16663)
*Naren Sengodan*

Main category: cs.CV

TL;DR: The Loupe is a lightweight attention module that improves fine-grained visual classification accuracy by 2.66% on CUB-200-2011 while providing interpretable visual explanations without needing part annotations.


<details>
  <summary>Details</summary>
Motivation: FGVC requires identifying subtle visual cues for critical applications like biodiversity monitoring and medical diagnostics, but current Vision Transformers lack interpretability needed for trust and verification.

Method: A plug-and-play attention module inserted into pre-trained backbones like Swin Transformer, trained end-to-end with composite loss to focus on discriminative object parts without explicit part-level annotations.

Result: Improved Swin-Base model accuracy from 85.40% to 88.06% on CUB-200-2011 dataset, with attention maps effectively localizing semantically meaningful features.

Conclusion: A simple intrinsic attention mechanism can serve as a powerful regularizer, significantly boosting performance while providing clear visual explanations for trustworthy decision-making.

Abstract: Fine-Grained Visual Classification (FGVC) is a critical and challenging area
within computer vision, demanding the identification of highly subtle,
localized visual cues. The importance of FGVC extends to critical applications
such as biodiversity monitoring and medical diagnostics, where precision is
paramount. While large-scale Vision Transformers have achieved state-of-the-art
performance, their decision-making processes often lack the interpretability
required for trust and verification in such domains. In this paper, we
introduce The Loupe, a novel, lightweight, and plug-and-play attention module
designed to be inserted into pre-trained backbones like the Swin Transformer.
The Loupe is trained end-to-end with a composite loss function that implicitly
guides the model to focus on the most discriminative object parts without
requiring explicit part-level annotations. Our unique contribution lies in
demonstrating that a simple, intrinsic attention mechanism can act as a
powerful regularizer, significantly boosting performance while simultaneously
providing clear visual explanations. Our experimental evaluation on the
challenging CUB-200-2011 dataset shows that The Loupe improves the accuracy of
a Swin-Base model from 85.40% to 88.06%, a significant gain of 2.66%.
Crucially, our qualitative analysis of the learned attention maps reveals that
The Loupe effectively localizes semantically meaningful features, providing a
valuable tool for understanding and trusting the model's decision-making
process.

</details>


### [120] [COVID19 Prediction Based On CT Scans Of Lungs Using DenseNet Architecture](https://arxiv.org/abs/2508.16670)
*Deborup Sanyal*

Main category: cs.CV

TL;DR: Using CNN to analyze lung CT scans for predicting COVID-19 severity within one month of positive test, helping doctors make treatment decisions.


<details>
  <summary>Details</summary>
Motivation: COVID-19 pandemic caused respiratory failure deaths due to overwhelmed healthcare systems and shortage of resources. Need for automated tools to assess infection severity from CT scans to assist medical decision-making.

Method: Convolutional Neural Network model trained on lung CT scans to analyze and predict COVID-19 infection severity (promising vs unfavorable outcomes including intubation or death).

Result: Model aims to provide accurate severity assessment within one month of positive COVID-19 test based on CT scan analysis, reducing human error in diagnosis.

Conclusion: Machine learning approach using CNN can help doctors better predict COVID-19 severity and outcomes from CT scans, potentially improving patient care and resource allocation during pandemics.

Abstract: COVID19 took the world by storm since December 2019. A highly infectious
communicable disease, COVID19 is caused by the SARSCoV2 virus. By March 2020,
the World Health Organization (WHO) declared COVID19 as a global pandemic. A
pandemic in the 21st century after almost 100 years was something the world was
not prepared for, which resulted in the deaths of around 1.6 million people
worldwide. The most common symptoms of COVID19 were associated with the
respiratory system and resembled a cold, flu, or pneumonia. After extensive
research, doctors and scientists concluded that the main reason for lives being
lost due to COVID19 was failure of the respiratory system. Patients were dying
gasping for breath. Top healthcare systems of the world were failing badly as
there was an acute shortage of hospital beds, oxygen cylinders, and
ventilators. Many were dying without receiving any treatment at all. The aim of
this project is to help doctors decide the severity of COVID19 by reading the
patient's Computed Tomography (CT) scans of the lungs. Computer models are less
prone to human error, and Machine Learning or Neural Network models tend to
give better accuracy as training improves over time. We have decided to use a
Convolutional Neural Network model. Given that a patient tests positive, our
model will analyze the severity of COVID19 infection within one month of the
positive test result. The severity of the infection may be promising or
unfavorable (if it leads to intubation or death), based entirely on the CT
scans in the dataset.

</details>


### [121] [MedRepBench: A Comprehensive Benchmark for Medical Report Interpretation](https://arxiv.org/abs/2508.16674)
*Fangxin Shang,Yuan Xia,Dalu Yang,Yahui Wang,Binglin Yang*

Main category: cs.CV

TL;DR: MedRepBench is a comprehensive benchmark for evaluating structured medical report interpretation using 1,900 real-world Chinese medical reports, featuring both objective field-level recall metrics and automated subjective LLM-based evaluation.


<details>
  <summary>Details</summary>
Motivation: There is a lack of standardized benchmarks to assess structured interpretation quality in medical reports despite recent advances in vision-language models and large language models for document understanding.

Method: Built benchmark from 1,900 de-identified Chinese medical reports across diverse departments and formats. Includes both end-to-end VLM evaluation and text-only OCR+LLM pipeline for comparison. Uses objective field-level recall metrics and automated subjective evaluation with LLM scoring agent.

Result: The OCR+LLM pipeline shows strong performance but suffers from layout-blindness and latency issues. Using GRPO optimization on a mid-scale VLM achieved up to 6% recall gain based on objective metrics.

Conclusion: The benchmark enables controlled comparisons and reveals limitations of current approaches, motivating further progress toward robust, fully vision-based medical report understanding systems.

Abstract: Medical report interpretation plays a crucial role in healthcare, enabling
both patient-facing explanations and effective information flow across clinical
systems. While recent vision-language models (VLMs) and large language models
(LLMs) have demonstrated general document understanding capabilities, there
remains a lack of standardized benchmarks to assess structured interpretation
quality in medical reports. We introduce MedRepBench, a comprehensive benchmark
built from 1,900 de-identified real-world Chinese medical reports spanning
diverse departments, patient demographics, and acquisition formats. The
benchmark is designed primarily to evaluate end-to-end VLMs for structured
medical report understanding. To enable controlled comparisons, we also include
a text-only evaluation setting using high-quality OCR outputs combined with
LLMs, allowing us to estimate the upper-bound performance when character
recognition errors are minimized. Our evaluation framework supports two
complementary protocols: (1) an objective evaluation measuring field-level
recall of structured clinical items, and (2) an automated subjective evaluation
using a powerful LLM as a scoring agent to assess factuality, interpretability,
and reasoning quality. Based on the objective metric, we further design a
reward function and apply Group Relative Policy Optimization (GRPO) to improve
a mid-scale VLM, achieving up to 6% recall gain. We also observe that the
OCR+LLM pipeline, despite strong performance, suffers from layout-blindness and
latency issues, motivating further progress toward robust, fully vision-based
report understanding.

</details>


### [122] [Two-Stage Framework for Efficient UAV-Based Wildfire Video Analysis with Adaptive Compression and Fire Source Detection](https://arxiv.org/abs/2508.16739)
*Yanbing Bai,Rui-Yang Ju,Lemeng Zhao,Junjie Hu,Jianchao Bi,Erick Mas,Shunichi Koshimura*

Main category: cs.CV

TL;DR: A lightweight two-stage framework for real-time wildfire monitoring on UAVs using frame compression and improved YOLOv8 for fire detection.


<details>
  <summary>Details</summary>
Motivation: UAVs have limited computational resources, making real-time analysis with large models challenging for disaster emergency response.

Method: Two-stage approach: Stage 1 uses policy network with frame compression to filter redundant clips and station point mechanism for accuracy. Stage 2 employs improved YOLOv8 for fire source localization.

Result: Significantly reduces computational costs while maintaining classification accuracy in Stage 1, and achieves higher detection accuracy with similar inference time in Stage 2 compared to baselines.

Conclusion: The proposed framework enables efficient real-time wildfire monitoring on resource-constrained UAV platforms.

Abstract: Unmanned Aerial Vehicles (UAVs) have become increasingly important in
disaster emergency response by enabling real-time aerial video analysis. Due to
the limited computational resources available on UAVs, large models cannot be
run independently for real-time analysis. To overcome this challenge, we
propose a lightweight and efficient two-stage framework for real-time wildfire
monitoring and fire source detection on UAV platforms. Specifically, in Stage
1, we utilize a policy network to identify and discard redundant video clips
using frame compression techniques, thereby reducing computational costs. In
addition, we introduce a station point mechanism that leverages future frame
information within the sequential policy network to improve prediction
accuracy. In Stage 2, once the frame is classified as "fire", we employ the
improved YOLOv8 model to localize the fire source. We evaluate the Stage 1
method using the FLAME and HMDB51 datasets, and the Stage 2 method using the
Fire & Smoke dataset. Experimental results show that our method significantly
reduces computational costs while maintaining classification accuracy in Stage
1, and achieves higher detection accuracy with similar inference time in Stage
2 compared to baseline methods.

</details>


### [123] [CellEcoNet: Decoding the Cellular Language of Pathology with Deep Learning for Invasive Lung Adenocarcinoma Recurrence Prediction](https://arxiv.org/abs/2508.16742)
*Abdul Rehman Akbar,Usama Sajjad,Ziyu Su,Wencheng Li,Fei Xing,Jimmy Ruiz,Wei Chen,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: CellEcoNet is a spatially aware deep learning framework that models lung cancer pathology images as a language, achieving superior recurrence prediction compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: 70% of invasive lung adenocarcinoma patients recur within five years after surgery, and current tools fail to identify those needing adjuvant therapy, creating an unmet clinical need.

Method: CellEcoNet treats whole slide images through natural language analogy: cells as words, cellular neighborhoods as phrases, and tissue architecture as sentences. It learns context-dependent meanings automatically to capture subtle variations and spatial interactions that predict recurrence risk.

Result: On 456 H&E-stained WSIs, CellEcoNet achieved AUC:77.8% and HR:9.54, outperforming IASLC grading (AUC:71.4%), AJCC Stage (AUC:64.0%), and state-of-the-art computational methods (AUCs:62.2-67.4%). It demonstrated fairness and consistent performance across diverse subgroups.

Conclusion: CellEcoNet represents a paradigm shift by decoding the tumor microenvironment's cellular 'language' to reveal how subtle cell variations encode recurrence risk, providing superior prognostic capabilities for lung adenocarcinoma patients.

Abstract: Despite surgical resection, ~70% of invasive lung adenocarcinoma (ILA)
patients recur within five years, and current tools fail to identify those
needing adjuvant therapy. To address this unmet clinical need, we introduce
CellEcoNet, a novel spatially aware deep learning framework that models whole
slide images (WSIs) through natural language analogy, defining a "language of
pathology," where cells act as words, cellular neighborhoods become phrases,
and tissue architecture forms sentences. CellEcoNet learns these
context-dependent meanings automatically, capturing how subtle variations and
spatial interactions derive recurrence risk. On a dataset of 456 H&E-stained
WSIs, CellEcoNet achieved superior predictive performance (AUC:77.8% HR:9.54),
outperforming IASLC grading system (AUC:71.4% HR:2.36), AJCC Stage (AUC:64.0%
HR:1.17) and state-of-the-art computational methods (AUCs:62.2-67.4%).
CellEcoNet demonstrated fairness and consistent performance across diverse
demographic and clinical subgroups. Beyond prognosis, CellEcoNet marks a
paradigm shift by decoding the tumor microenvironment's cellular "language" to
reveal how subtle cell variations encode recurrence risk.

</details>


### [124] [A Framework for Benchmarking Fairness-Utility Trade-offs in Text-to-Image Models via Pareto Frontiers](https://arxiv.org/abs/2508.16752)
*Marco N. Bochernitsan,Rodrigo C. Barros,Lucas S. KupssinskÃ¼*

Main category: cs.CV

TL;DR: Proposes a method for evaluating fairness and utility in text-to-image models using Pareto-optimal frontiers across hyperparameter configurations, showing most default settings are suboptimal and better parameters can be easily found.


<details>
  <summary>Details</summary>
Motivation: Current fairness evaluation methods rely on qualitative judgment and narrow comparisons that limit reproducible assessment of debiasing methods and prevent comprehensive evaluation of both fairness and utility.

Method: Uses Pareto-optimal frontiers across hyperparameterization of debiasing methods, with Normalized Shannon Entropy for fairness evaluation and ClipScore for utility evaluation.

Result: Evaluation of Stable Diffusion, Fair Diffusion, SDXL, DeCoDi, and FLUX models shows most default hyperparameterizations are dominated solutions in fairness-utility space, and better hyperparameters can be readily identified.

Conclusion: The proposed method enables systematic comparison between text-to-image models and provides a reproducible framework for optimizing both fairness and utility through hyperparameter tuning.

Abstract: Achieving fairness in text-to-image generation demands mitigating social
biases without compromising visual fidelity, a challenge critical to
responsible AI. Current fairness evaluation procedures for text-to-image models
rely on qualitative judgment or narrow comparisons, which limit the capacity to
assess both fairness and utility in these models and prevent reproducible
assessment of debiasing methods. Existing approaches typically employ ad-hoc,
human-centered visual inspections that are both error-prone and difficult to
replicate. We propose a method for evaluating fairness and utility in
text-to-image models using Pareto-optimal frontiers across hyperparametrization
of debiasing methods. Our method allows for comparison between distinct
text-to-image models, outlining all configurations that optimize fairness for a
given utility and vice-versa. To illustrate our evaluation method, we use
Normalized Shannon Entropy and ClipScore for fairness and utility evaluation,
respectively. We assess fairness and utility in Stable Diffusion, Fair
Diffusion, SDXL, DeCoDi, and FLUX text-to-image models. Our method shows that
most default hyperparameterizations of the text-to-image model are dominated
solutions in the fairness-utility space, and it is straightforward to find
better hyperparameters.

</details>


### [125] [WebMMU: A Benchmark for Multimodal Multilingual Website Understanding and Code Generation](https://arxiv.org/abs/2508.16763)
*Rabiul Awal,Mahsa Massoud,Aarash Feizi,Zichao Li,Suyuchen Wang,Christopher Pal,Aishwarya Agrawal,David Vazquez,Siva Reddy,Juan A. Rodriguez,Perouz Taslakian,Spandana Gella,Sai Rajeswar*

Main category: cs.CV

TL;DR: WebMMU is a multilingual benchmark that evaluates multimodal LLMs on three unified web tasks: visual QA, code editing, and mockup-to-code generation, revealing significant limitations in reasoning and functional coding abilities.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive benchmarks that evaluate multimodal large language models on real-world web development tasks involving complex reasoning, precise element grounding, and functional UI comprehension across multiple languages.

Method: Created a unified benchmark with expert-annotated real-world web data that combines three core web tasks: website visual question answering, HTML/CSS/JavaScript code editing, and mockup-to-code generation to assess multimodal reasoning capabilities.

Result: Multimodal LLMs perform well on basic information extraction but struggle with reasoning and grounding, editing code while preserving functionality, and generating design-to-code that maintains hierarchy and supports multilingual content.

Conclusion: Current MLLMs have significant limitations in multimodal and cross-lingual reasoning, highlighting the need for improved capabilities to develop future web agents capable of automating diverse web development tasks.

Abstract: We present WebMMU, a multilingual benchmark that evaluates three core web
tasks: (1) website visual question answering, (2) code editing involving
HTML/CSS/JavaScript, and (3) mockup-to-code generation. Unlike prior benchmarks
that treat these tasks separately, WebMMU unifies them using expert-annotated,
real-world web data to assess models' abilities in complex multi-step
reasoning, precise element grounding, and functional UI comprehension and
coding. Our evaluation shows that while multimodal large language models
(MLLMs) perform well on basic information extraction, they struggle with
reasoning and grounding, editing code to preserve functionality, and generating
design-to-code that maintains hierarchy and supports multilingual content.
These findings reveal key limitations in current MLLMs and underscore the need
for improved multimodal and cross-lingual reasoning to build future web agents
capable of automating diverse web development tasks.

</details>


### [126] [Improving Performance, Robustness, and Fairness of Radiographic AI Models with Finely-Controllable Synthetic Data](https://arxiv.org/abs/2508.16783)
*Stefania L. Moroianu,Christian Bluethgen,Pierre Chambon,Mehdi Cherti,Jean-Benoit Delbrouck,Magdalini Paschali,Brandon Price,Judy Gichoya,Jenia Jitsev,Curtis P. Langlotz,Akshay S. Chaudhari*

Main category: cs.CV

TL;DR: RoentGen-v2 is a text-to-image diffusion model that generates clinically plausible chest radiographs with demographic control, used to create a large synthetic dataset that improves disease classification model performance, generalization, and fairness when used for supervised pretraining.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in achieving robust performance and fairness across diverse patient populations in medical imaging AI, particularly due to limitations in dataset scale and diversity.

Method: Developed RoentGen-v2 diffusion model for fine-grained control over radiographic findings and demographic attributes, created 565K+ synthetic images, and proposed synthetic pretraining followed by real data fine-tuning strategy.

Result: Synthetic pretraining improved downstream classification accuracy by 6.5% (vs 2.7% with naive combination), reduced underdiagnosis fairness gap by 19.3%, and enhanced generalization across 137K+ chest radiographs from five institutions.

Conclusion: Synthetic imaging with demographic conditioning can advance equitable and generalizable medical deep learning, with synthetic pretraining being more effective than naive data combination approaches.

Abstract: Achieving robust performance and fairness across diverse patient populations
remains a challenge in developing clinically deployable deep learning models
for diagnostic imaging. Synthetic data generation has emerged as a promising
strategy to address limitations in dataset scale and diversity. We introduce
RoentGen-v2, a text-to-image diffusion model for chest radiographs that enables
fine-grained control over both radiographic findings and patient demographic
attributes, including sex, age, and race/ethnicity. RoentGen-v2 is the first
model to generate clinically plausible images with demographic conditioning,
facilitating the creation of a large, demographically balanced synthetic
dataset comprising over 565,000 images. We use this large synthetic dataset to
evaluate optimal training pipelines for downstream disease classification
models. In contrast to prior work that combines real and synthetic data
naively, we propose an improved training strategy that leverages synthetic data
for supervised pretraining, followed by fine-tuning on real data. Through
extensive evaluation on over 137,000 chest radiographs from five institutions,
we demonstrate that synthetic pretraining consistently improves model
performance, generalization to out-of-distribution settings, and fairness
across demographic subgroups. Across datasets, synthetic pretraining led to a
6.5% accuracy increase in the performance of downstream classification models,
compared to a modest 2.7% increase when naively combining real and synthetic
data. We observe this performance improvement simultaneously with the reduction
of the underdiagnosis fairness gap by 19.3%. These results highlight the
potential of synthetic imaging to advance equitable and generalizable medical
deep learning under real-world data constraints. We open source our code,
trained models, and synthetic dataset at
https://github.com/StanfordMIMI/RoentGen-v2 .

</details>


### [127] [Towards Open-Vocabulary Multimodal 3D Object Detection with Attributes](https://arxiv.org/abs/2508.16812)
*Xinhao Xiang,Kuan-Chuan Peng,Suhas Lohit,Michael J. Jones,Jiawei Zhang*

Main category: cs.CV

TL;DR: OVODA is an open-vocabulary 3D object and attribute detection framework that uses foundation models to bridge 3D features with text, eliminating the need for novel class anchor sizes and enabling attribute recognition.


<details>
  <summary>Details</summary>
Motivation: Existing 3D object detection methods are limited by closed-set assumptions and struggle with novel objects and their attributes in real-world autonomous systems.

Method: Uses foundation model feature concatenation, prompt tuning strategies, perspective-specified prompts, and horizontal flip augmentation to jointly detect objects and attributes without requiring novel class anchor sizes.

Result: Outperforms state-of-the-art methods on nuScenes and Argoverse 2 datasets in open-vocabulary 3D object detection while successfully recognizing object attributes.

Conclusion: OVODA enables effective open-vocabulary 3D object and attribute detection, with the OVAD dataset providing comprehensive attribute annotations to support this research direction.

Abstract: 3D object detection plays a crucial role in autonomous systems, yet existing
methods are limited by closed-set assumptions and struggle to recognize novel
objects and their attributes in real-world scenarios. We propose OVODA, a novel
framework enabling both open-vocabulary 3D object and attribute detection with
no need to know the novel class anchor size. OVODA uses foundation models to
bridge the semantic gap between 3D features and texts while jointly detecting
attributes, e.g., spatial relationships, motion states, etc. To facilitate such
research direction, we propose OVAD, a new dataset that supplements existing 3D
object detection benchmarks with comprehensive attribute annotations. OVODA
incorporates several key innovations, including foundation model feature
concatenation, prompt tuning strategies, and specialized techniques for
attribute detection, including perspective-specified prompts and horizontal
flip augmentation. Our results on both the nuScenes and Argoverse 2 datasets
show that under the condition of no given anchor sizes of novel classes, OVODA
outperforms the state-of-the-art methods in open-vocabulary 3D object detection
while successfully recognizing object attributes. Our OVAD dataset is released
here: https://doi.org/10.5281/zenodo.16904069 .

</details>


### [128] [AIM 2025 Low-light RAW Video Denoising Challenge: Dataset, Methods and Results](https://arxiv.org/abs/2508.16830)
*Alexander Yakovenko,George Chakvetadze,Ilya Khrapov,Maksim Zhelezov,Dmitry Vatolin,Radu Timofte,Youngjin Oh,Junhyeong Kwon,Junyoung Park,Nam Ik Cho,Senyan Xu,Ruixuan Jiang,Long Peng,Xueyang Fu,Zheng-Jun Zha,Xiaoping Peng,Hansen Feng,Zhanyi Tie,Ziming Xia,Lizhi Wang*

Main category: cs.CV

TL;DR: The AIM 2025 Low-Light RAW Video Denoising Challenge focused on developing methods to denoise low-light RAW video by leveraging temporal redundancy while working within exposure-time constraints and adapting to sensor-specific noise.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of denoising low-light RAW video sequences captured under various illumination and exposure conditions, while preserving temporal information and handling sensor-specific noise characteristics.

Method: A new benchmark dataset of 756 ten-frame sequences captured with 14 smartphone camera sensors across nine different conditions (varying illumination and exposure times), with high-SNR references obtained through burst averaging. Participants processed linear RAW sequences and output denoised 10th frames while maintaining the Bayer pattern.

Result: The challenge established a comprehensive evaluation framework with submissions assessed on a private test set using full-reference PSNR and SSIM metrics, with final ranking determined by the mean of per-metric ranks.

Conclusion: The paper presents the AIM 2025 challenge protocol, dataset, and submitted approaches, providing a standardized benchmark for evaluating low-light RAW video denoising methods that must handle temporal redundancy, exposure constraints, and sensor-specific noise characteristics.

Abstract: This paper reviews the AIM 2025 (Advances in Image Manipulation) Low-Light
RAW Video Denoising Challenge. The task is to develop methods that denoise
low-light RAW video by exploiting temporal redundancy while operating under
exposure-time limits imposed by frame rate and adapting to sensor-specific,
signal-dependent noise. We introduce a new benchmark of 756 ten-frame sequences
captured with 14 smartphone camera sensors across nine conditions
(illumination: 1/5/10 lx; exposure: 1/24, 1/60, 1/120 s), with high-SNR
references obtained via burst averaging. Participants process linear RAW
sequences and output the denoised 10th frame while preserving the Bayer
pattern. Submissions are evaluated on a private test set using full-reference
PSNR and SSIM, with final ranking given by the mean of per-metric ranks. This
report describes the dataset, challenge protocol, and submitted approaches.

</details>


### [129] [Transformer-Based Neural Network for Transient Detection without Image Subtraction](https://arxiv.org/abs/2508.16844)
*Adi Inada,Masao Sako,Tatiana Acero-Cuellar,Federica Bianco*

Main category: cs.CV

TL;DR: Transformer-based neural network for classifying real vs bogus transient detections in astronomy, achieving 97.4% accuracy without expensive difference imaging.


<details>
  <summary>Details</summary>
Motivation: To improve supernova detection in astronomical surveys by moving beyond conventional CNN methods and eliminating computationally-expensive difference imaging while maintaining high accuracy.

Method: Transformer-based architecture designed for detailed pixel-by-pixel comparison of search and template images only, enabling efficient analysis without difference imaging.

Result: Achieved 97.4% classification accuracy on Dark Energy Survey data, with performance maintained even when input images are not centered on supernova candidates.

Conclusion: The transformer network effectively enhances both accuracy and efficiency of supernova detection in large-scale astronomical surveys by eliminating the need for difference imaging while maintaining high performance.

Abstract: We introduce a transformer-based neural network for the accurate
classification of real and bogus transient detections in astronomical images.
This network advances beyond the conventional convolutional neural network
(CNN) methods, widely used in image processing tasks, by adopting an
architecture better suited for detailed pixel-by-pixel comparison. The
architecture enables efficient analysis of search and template images only,
thus removing the necessity for computationally-expensive difference imaging,
while maintaining high performance. Our primary evaluation was conducted using
the autoScan dataset from the Dark Energy Survey (DES), where the network
achieved a classification accuracy of 97.4% and diminishing performance utility
for difference image as the size of the training set grew. Further experiments
with DES data confirmed that the network can operate at a similar level even
when the input images are not centered on the supernova candidate. These
findings highlight the network's effectiveness in enhancing both accuracy and
efficiency of supernova detection in large-scale astronomical surveys.

</details>


### [130] [NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flows](https://arxiv.org/abs/2508.16845)
*Denis Tarasov,Alexander Nikulin,Ilya Zisman,Albina Klepach,Nikita Lyubaykin,Andrei Polubarov,Alexander Derevyagin,Vladislav Kurenkov*

Main category: cs.CV

TL;DR: NinA replaces diffusion action decoders with Normalizing Flows for faster one-shot sampling in Vision-Language-Action models, matching performance while significantly reducing inference time.


<details>
  <summary>Details</summary>
Motivation: Diffusion-based action decoders require multiple iterative denoising steps, limiting practicality in real-world settings where high-frequency control is crucial.

Method: Replace diffusion action decoder with Normalizing Flow (NF) that enables one-shot sampling through invertible transformation. Integrated into FLOWER VLA architecture and fine-tuned on LIBERO benchmark.

Result: NinA matches performance of diffusion-based counterpart under same training regime while achieving substantially faster inference.

Conclusion: NinA offers a promising path toward efficient, high-frequency VLA control without compromising performance.

Abstract: Recent advances in Vision-Language-Action (VLA) models have established a
two-component architecture, where a pre-trained Vision-Language Model (VLM)
encodes visual observations and task descriptions, and an action decoder maps
these representations to continuous actions. Diffusion models have been widely
adopted as action decoders due to their ability to model complex, multimodal
action distributions. However, they require multiple iterative denoising steps
at inference time or downstream techniques to speed up sampling, limiting their
practicality in real-world settings where high-frequency control is crucial. In
this work, we present NinA (Normalizing Flows in Action), a fast and expressive
alter- native to diffusion-based decoders for VLAs. NinA replaces the diffusion
action decoder with a Normalizing Flow (NF) that enables one-shot sampling
through an invertible transformation, significantly reducing inference time. We
integrate NinA into the FLOWER VLA architecture and fine-tune on the LIBERO
benchmark. Our experiments show that NinA matches the performance of its
diffusion-based counterpart under the same training regime, while achieving
substantially faster inference. These results suggest that NinA offers a
promising path toward efficient, high-frequency VLA control without
compromising performance.

</details>


### [131] [RF-PGS: Fully-structured Spatial Wireless Channel Representation with Planar Gaussian Splatting](https://arxiv.org/abs/2508.16849)
*Lihao Zhang,Zongtan Li,Haijian Sun*

Main category: cs.CV

TL;DR: RF-PGS is a novel framework that reconstructs high-fidelity radio propagation paths from sparse path loss spectra using Planar Gaussians and RF-specific optimizations, achieving accurate and efficient 6G spatial channel modeling.


<details>
  <summary>Details</summary>
Motivation: 6G requires large-scale antenna arrays and accurate spatial channel state information, but traditional channel modeling methods face challenges in spatial resolution, efficiency, and scalability. Radiance field methods show promise but suffer from geometric inaccuracy and costly supervision.

Method: Two-stage framework: 1) Geometry training stage uses Planar Gaussians as geometry primitives with RF-specific optimizations for dense, surface-aligned scene reconstruction. 2) RF training stage employs fully-structured radio radiance with tailored multi-view loss to model radio propagation behavior from sparse path loss spectra.

Result: RF-PGS significantly improves reconstruction accuracy, reduces training costs, and enables efficient representation of wireless channels compared to prior radiance field methods.

Conclusion: RF-PGS offers a practical solution for scalable 6G Spatial-CSI modeling by addressing geometric inaccuracy and supervision costs while maintaining high fidelity in radio propagation path reconstruction.

Abstract: In the 6G era, the demand for higher system throughput and the implementation
of emerging 6G technologies require large-scale antenna arrays and accurate
spatial channel state information (Spatial-CSI). Traditional channel modeling
approaches, such as empirical models, ray tracing, and measurement-based
methods, face challenges in spatial resolution, efficiency, and scalability.
Radiance field-based methods have emerged as promising alternatives but still
suffer from geometric inaccuracy and costly supervision. This paper proposes
RF-PGS, a novel framework that reconstructs high-fidelity radio propagation
paths from only sparse path loss spectra. By introducing Planar Gaussians as
geometry primitives with certain RF-specific optimizations, RF-PGS achieves
dense, surface-aligned scene reconstruction in the first geometry training
stage. In the subsequent Radio Frequency (RF) training stage, the proposed
fully-structured radio radiance, combined with a tailored multi-view loss,
accurately models radio propagation behavior. Compared to prior radiance field
methods, RF-PGS significantly improves reconstruction accuracy, reduces
training costs, and enables efficient representation of wireless channels,
offering a practical solution for scalable 6G Spatial-CSI modeling.

</details>


### [132] [Gaussian Primitive Optimized Deformable Retinal Image Registration](https://arxiv.org/abs/2508.16852)
*Xin Tian,Jiazheng Wang,Yuxi Zhang,Xiang Chen,Renjiu Hu,Gaolei Li,Min Liu,Hang Zhang*

Main category: cs.CV

TL;DR: GPO is a novel deformable retinal image registration framework that uses Gaussian primitives at key vascular features to overcome vanishing gradient issues in homogeneous regions, achieving state-of-the-art performance on the FIRE dataset.


<details>
  <summary>Details</summary>
Motivation: Deformable retinal image registration is challenging due to large homogeneous regions and sparse vascular features that cause limited gradient signals in standard learning-based methods.

Method: Extracts keypoints at major vessels as descriptor-based control nodes modeled as Gaussian primitives with trainable parameters. Uses KNN Gaussian interpolation to propagate displacement signals from information-rich nodes to create a globally coherent displacement field.

Result: Reduces target registration error from 6.2px to ~2.4px and increases AUC at 25px from 0.770 to 0.938 on the FIRE dataset, substantially outperforming existing methods.

Conclusion: GPO effectively addresses vanishing gradient issues in retinal image registration by strategically anchoring nodes in high-gradient regions and using structured message passing, demonstrating superior performance compared to current approaches.

Abstract: Deformable retinal image registration is notoriously difficult due to large
homogeneous regions and sparse but critical vascular features, which cause
limited gradient signals in standard learning-based frameworks. In this paper,
we introduce Gaussian Primitive Optimization (GPO), a novel iterative framework
that performs structured message passing to overcome these challenges. After an
initial coarse alignment, we extract keypoints at salient anatomical structures
(e.g., major vessels) to serve as a minimal set of descriptor-based control
nodes (DCN). Each node is modelled as a Gaussian primitive with trainable
position, displacement, and radius, thus adapting its spatial influence to
local deformation scales. A K-Nearest Neighbors (KNN) Gaussian interpolation
then blends and propagates displacement signals from these information-rich
nodes to construct a globally coherent displacement field; focusing
interpolation on the top (K) neighbors reduces computational overhead while
preserving local detail. By strategically anchoring nodes in high-gradient
regions, GPO ensures robust gradient flow, mitigating vanishing gradient signal
in textureless areas. The framework is optimized end-to-end via a multi-term
loss that enforces both keypoint consistency and intensity alignment.
Experiments on the FIRE dataset show that GPO reduces the target registration
error from 6.2\,px to ~2.4\,px and increases the AUC at 25\,px from 0.770 to
0.938, substantially outperforming existing methods. The source code can be
accessed via https://github.com/xintian-99/GPOreg.

</details>


### [133] [Beyond Emotion Recognition: A Multi-Turn Multimodal Emotion Understanding and Reasoning Benchmark](https://arxiv.org/abs/2508.16859)
*Jinpeng Hu,Hongchang Shi,Chongyuan Dai,Zhuo Li,Peipei Song,Meng Wang*

Main category: cs.CV

TL;DR: A new benchmark MTMEUR with 1,451 real-life videos and 5,101 progressive questions for multimodal emotion understanding and reasoning, plus a multi-agent framework to improve reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Current MLLM research focuses too much on emotion recognition while neglecting emotion reasoning, which is crucial for natural human-machine interactions.

Method: Created MTMEUR benchmark with real-life scenario videos and progressive questions covering emotion recognition, causes, and future actions. Proposed multi-agent framework with specialized agents for background context, character dynamics, and event details.

Result: Experiments show most existing MLLMs face significant challenges with emotion reasoning tasks on the proposed benchmark.

Conclusion: The MTMEUR benchmark reveals limitations in current MLLMs' emotion reasoning capabilities, and the multi-agent framework shows promise for improving these abilities in human-machine interaction applications.

Abstract: Multimodal large language models (MLLMs) have been widely applied across
various fields due to their powerful perceptual and reasoning capabilities. In
the realm of psychology, these models hold promise for a deeper understanding
of human emotions and behaviors. However, recent research primarily focuses on
enhancing their emotion recognition abilities, leaving the substantial
potential in emotion reasoning, which is crucial for improving the naturalness
and effectiveness of human-machine interactions. Therefore, in this paper, we
introduce a multi-turn multimodal emotion understanding and reasoning (MTMEUR)
benchmark, which encompasses 1,451 video data from real-life scenarios, along
with 5,101 progressive questions. These questions cover various aspects,
including emotion recognition, potential causes of emotions, future action
prediction, etc. Besides, we propose a multi-agent framework, where each agent
specializes in a specific aspect, such as background context, character
dynamics, and event details, to improve the system's reasoning capabilities.
Furthermore, we conduct experiments with existing MLLMs and our agent-based
method on the proposed benchmark, revealing that most models face significant
challenges with this task.

</details>


### [134] [Delta-SVD: Efficient Compression for Personalized Text-to-Image Models](https://arxiv.org/abs/2508.16863)
*Tangyuan Zhang,Shangyu Chen,Qixiang Chen,Jianfei Cai*

Main category: cs.CV

TL;DR: Delta-SVD is a training-free compression method that uses SVD to compress DreamBooth fine-tuned models by exploiting the low-rank structure of weight deltas, achieving significant storage reduction with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: Personalized text-to-image models like DreamBooth require storing many subject-specific models, creating substantial storage overhead that limits scalability and practical deployment.

Method: Applies Singular Value Decomposition (SVD) to factorize weight deltas from fine-tuning, followed by energy-based rank truncation to balance compression efficiency and reconstruction fidelity without additional training.

Result: Achieves substantial compression with negligible loss in generation quality (measured by CLIP score, SSIM, and FID) while maintaining plug-and-play compatibility and original model architecture.

Conclusion: Delta-SVD enables scalable and efficient deployment of personalized diffusion models, providing a practical solution for storing and deploying large-scale subject customizations in real-world applications.

Abstract: Personalized text-to-image models such as DreamBooth require fine-tuning
large-scale diffusion backbones, resulting in significant storage overhead when
maintaining many subject-specific models. We present Delta-SVD, a post-hoc,
training-free compression method that targets the parameter weights update
induced by DreamBooth fine-tuning. Our key observation is that these delta
weights exhibit strong low-rank structure due to the sparse and localized
nature of personalization. Delta-SVD first applies Singular Value Decomposition
(SVD) to factorize the weight deltas, followed by an energy-based rank
truncation strategy to balance compression efficiency and reconstruction
fidelity. The resulting compressed models are fully plug-and-play and can be
re-constructed on-the-fly during inference. Notably, the proposed approach is
simple, efficient, and preserves the original model architecture. Experiments
on a multiple subject dataset demonstrate that Delta-SVD achieves substantial
compression with negligible loss in generation quality measured by CLIP score,
SSIM and FID. Our method enables scalable and efficient deployment of
personalized diffusion models, making it a practical solution for real-world
applications that require storing and deploying large-scale subject
customizations.

</details>


### [135] [Do Multimodal LLMs See Sentiment?](https://arxiv.org/abs/2508.16873)
*Neemias B. da Silva,John Harrison,Rodrigo Minetto,Myriam R. Delgado,Bogdan T. Nassu,Thiago H. Silva*

Main category: cs.CV

TL;DR: MLLMsent framework uses Multimodal Large Language Models for visual sentiment analysis through three approaches, achieving state-of-the-art results and demonstrating strong cross-dataset generalization.


<details>
  <summary>Details</summary>
Motivation: Understanding visual sentiment is crucial for online social platforms, but remains challenging due to complex scene-level semantics that current methods struggle to capture effectively.

Method: Proposes MLLMsent framework with three perspectives: (1) direct sentiment classification from images using MLLMs, (2) sentiment analysis on generated image descriptions using pre-trained LLMs, and (3) fine-tuning LLMs on sentiment-labeled image descriptions.

Result: Achieves state-of-the-art results, outperforming Lexicon-, CNN-, and Transformer-based baselines by up to 30.9%, 64.8%, and 42.4% respectively. Cross-dataset testing shows 8.26% improvement over best runner-up without training on new data.

Conclusion: The framework demonstrates significant potential for advancing affective computing and establishes new benchmarks for visual sentiment analysis research.

Abstract: Understanding how visual content communicates sentiment is critical in an era
where online interaction is increasingly dominated by this kind of media on
social platforms. However, this remains a challenging problem, as sentiment
perception is closely tied to complex, scene-level semantics. In this paper, we
propose an original framework, MLLMsent, to investigate the sentiment reasoning
capabilities of Multimodal Large Language Models (MLLMs) through three
perspectives: (1) using those MLLMs for direct sentiment classification from
images; (2) associating them with pre-trained LLMs for sentiment analysis on
automatically generated image descriptions; and (3) fine-tuning the LLMs on
sentiment-labeled image descriptions. Experiments on a recent and established
benchmark demonstrate that our proposal, particularly the fine-tuned approach,
achieves state-of-the-art results outperforming Lexicon-, CNN-, and
Transformer-based baselines by up to 30.9%, 64.8%, and 42.4%, respectively,
across different levels of evaluators' agreement and sentiment polarity
categories. Remarkably, in a cross-dataset test, without any training on these
new data, our model still outperforms, by up to 8.26%, the best runner-up,
which has been trained directly on them. These results highlight the potential
of the proposed visual reasoning scheme for advancing affective computing,
while also establishing new benchmarks for future research.

</details>


### [136] [AWM-Fuse: Multi-Modality Image Fusion for Adverse Weather via Global and Local Text Perception](https://arxiv.org/abs/2508.16881)
*Xilai Li,Huichun Liu,Xiaosong Li,Tao Ye,Zhenyu Kuang,Huafeng Li*

Main category: cs.CV

TL;DR: AWM-Fuse is a novel multi-modality image fusion method that uses global and local text perception to handle adverse weather degradations, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Address the loss of visual information in adverse weather conditions and improve semantic perception by effectively incorporating textual information, which previous methods lacked in categorization and analysis.

Method: Uses a unified shared weight architecture with global feature perception (BLIP captions for overall scene features and degradation types) and local module (ChatGPT detailed descriptions for specific degradation effects). Textual descriptions constrain fusion image generation to align with real semantic labels.

Result: Extensive experiments show AWM-Fuse outperforms current state-of-the-art methods in complex weather conditions and downstream tasks.

Conclusion: The proposed method effectively handles multiple degradations through text perception and promotes learning of meaningful visual features, demonstrating superior performance in adverse weather conditions.

Abstract: Multi-modality image fusion (MMIF) in adverse weather aims to address the
loss of visual information caused by weather-related degradations, providing
clearer scene representations. Although less studies have attempted to
incorporate textual information to improve semantic perception, they often lack
effective categorization and thorough analysis of textual content. In response,
we propose AWM-Fuse, a novel fusion method for adverse weather conditions,
designed to handle multiple degradations through global and local text
perception within a unified, shared weight architecture. In particular, a
global feature perception module leverages BLIP-produced captions to extract
overall scene features and identify primary degradation types, thus promoting
generalization across various adverse weather conditions. Complementing this,
the local module employs detailed scene descriptions produced by ChatGPT to
concentrate on specific degradation effects through concrete textual cues,
thereby capturing finer details. Furthermore, textual descriptions are used to
constrain the generation of fusion images, effectively steering the network
learning process toward better alignment with real semantic labels, thereby
promoting the learning of more meaningful visual features. Extensive
experiments demonstrate that AWM-Fuse outperforms current state-of-the-art
methods in complex weather conditions and downstream tasks. Our code is
available at https://github.com/Feecuin/AWM-Fuse.

</details>


### [137] [A Lightweight Convolution and Vision Transformer integrated model with Multi-scale Self-attention Mechanism](https://arxiv.org/abs/2508.16884)
*Yi Zhang,Lingxiao Wei,Bowei Zhang,Ziwei Liu,Kai Yi,Shu Hu*

Main category: cs.CV

TL;DR: SAEViT is a lightweight Vision Transformer that combines sparse attention and convolution blocks to reduce computational costs while maintaining performance on vision tasks.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers have strong performance but suffer from large model size, high computational cost, and weak local feature modeling, limiting real-world applications.

Method: Introduces Sparsely Aggregated Attention (SAA) for adaptive sparse sampling, Channel-Interactive Feed-Forward Network (CIFFN) for inter-channel information exchange, and hierarchical pyramid structure with depth-wise separable convolutions.

Result: Achieves 76.3% and 79.6% Top-1 accuracy on ImageNet-1K with only 0.8 GFLOPs and 1.3 GFLOPs respectively, demonstrating efficient performance.

Conclusion: SAEViT provides a lightweight solution that balances computation efficiency and performance for various vision tasks, addressing ViT's limitations in real-world scenarios.

Abstract: Vision Transformer (ViT) has prevailed in computer vision tasks due to its
strong long-range dependency modelling ability. However, its large model size
with high computational cost and weak local feature modeling ability hinder its
application in real scenarios. To balance computation efficiency and
performance, we propose SAEViT (Sparse-Attention-Efficient-ViT), a lightweight
ViT based model with convolution blocks, in this paper to achieve efficient
downstream vision tasks. Specifically, SAEViT introduces a Sparsely Aggregated
Attention (SAA) module that performs adaptive sparse sampling based on image
redundancy and recovers the feature map via deconvolution operation, which
significantly reduces the computational complexity of attention operations. In
addition, a Channel-Interactive Feed-Forward Network (CIFFN) layer is developed
to enhance inter-channel information exchange through feature decomposition and
redistribution, mitigating redundancy in traditional feed-forward networks
(FNN). Finally, a hierarchical pyramid structure with embedded depth-wise
separable convolutional blocks (DWSConv) is devised to further strengthen
convolutional features. Extensive experiments on mainstream datasets show that
SAEViT achieves Top-1 accuracies of 76.3\% and 79.6\% on the ImageNet-1K
classification task with only 0.8 GFLOPs and 1.3 GFLOPs, respectively,
demonstrating a lightweight solution for various fundamental vision tasks.

</details>


### [138] [MDIQA: Unified Image Quality Assessment for Multi-dimensional Evaluation and Restoration](https://arxiv.org/abs/2508.16887)
*Shunyu Yao,Ming Liu,Zhilu Zhang,Zhaolin Wan,Zhilong Ji,Jinfeng Bai,Wangmeng Zuo*

Main category: cs.CV

TL;DR: Proposes a multi-dimensional image quality assessment framework that models quality across technical and aesthetic dimensions, outperforming single-score methods and enabling flexible image restoration based on user preferences.


<details>
  <summary>Details</summary>
Motivation: Existing IQA methods focus only on overall scores, neglecting that humans evaluate image quality from multiple perceptual dimensions before arriving at a final assessment.

Method: MDIQA framework with separate branches for five technical and four aesthetic dimensions, each trained individually then combined for final score. Also enables flexible training of image restoration models by adjusting dimension weights.

Result: Extensive experiments show superior performance compared to existing methods and effective application to image restoration tasks.

Conclusion: Multi-dimensional approach better captures human visual perception and provides flexible framework for quality assessment and image restoration aligned with user preferences.

Abstract: Recent advancements in image quality assessment (IQA), driven by
sophisticated deep neural network designs, have significantly improved the
ability to approach human perceptions. However, most existing methods are
obsessed with fitting the overall score, neglecting the fact that humans
typically evaluate image quality from different dimensions before arriving at
an overall quality assessment. To overcome this problem, we propose a
multi-dimensional image quality assessment (MDIQA) framework. Specifically, we
model image quality across various perceptual dimensions, including five
technical and four aesthetic dimensions, to capture the multifaceted nature of
human visual perception within distinct branches. Each branch of our MDIQA is
initially trained under the guidance of a separate dimension, and the
respective features are then amalgamated to generate the final IQA score.
Additionally, when the MDIQA model is ready, we can deploy it for a flexible
training of image restoration (IR) models, enabling the restoration results to
better align with varying user preferences through the adjustment of perceptual
dimension weights. Extensive experiments demonstrate that our MDIQA achieves
superior performance and can be effectively and flexibly applied to image
restoration tasks. The code is available: https://github.com/YaoShunyu19/MDIQA.

</details>


### [139] [Structural Energy-Guided Sampling for View-Consistent Text-to-3D](https://arxiv.org/abs/2508.16917)
*Qing Zhang,Jinguang Tong,Jie Hong,Jing Zhang,Xuesong Li*

Main category: cs.CV

TL;DR: SEGS is a training-free framework that addresses the Janus problem in text-to-3D generation by enforcing multi-view consistency through structural energy guidance in diffusion sampling.


<details>
  <summary>Details</summary>
Motivation: Text-to-3D generation suffers from viewpoint bias in 2D diffusion priors, causing objects to appear correct from front views but collapse into duplicated/distorted geometry from other angles (Janus problem).

Method: Structural Energy-Guided Sampling (SEGS) defines structural energy in PCA subspace of intermediate U-Net features and injects its gradients into denoising trajectory to steer geometry toward intended viewpoints while preserving appearance.

Result: SEGS significantly reduces Janus artifacts, achieves improved geometric alignment and viewpoint consistency without requiring retraining or weight modifications.

Conclusion: The proposed plug-and-play framework successfully addresses viewpoint bias in 3D generation by enforcing multi-view consistency entirely at sampling time, seamlessly integrating into existing SDS/VSD pipelines.

Abstract: Text-to-3D generation often suffers from the Janus problem, where objects
look correct from the front but collapse into duplicated or distorted geometry
from other angles. We attribute this failure to viewpoint bias in 2D diffusion
priors, which propagates into 3D optimization. To address this, we propose
Structural Energy-Guided Sampling (SEGS), a training-free, plug-and-play
framework that enforces multi-view consistency entirely at sampling time. SEGS
defines a structural energy in a PCA subspace of intermediate U-Net features
and injects its gradients into the denoising trajectory, steering geometry
toward the intended viewpoint while preserving appearance fidelity. Integrated
seamlessly into SDS/VSD pipelines, SEGS significantly reduces Janus artifacts,
achieving improved geometric alignment and viewpoint consistency without
retraining or weight modification.

</details>


### [140] [MSPCaps: A Multi-Scale Patchify Capsule Network with Cross-Agreement Routing for Visual Recognition](https://arxiv.org/abs/2508.16922)
*Yudong Hu,Yueju Han,Rui Sun,Jinke Ren*

Main category: cs.CV

TL;DR: MSPCaps is a novel capsule network architecture that integrates multi-scale feature learning with efficient capsule routing through three key components: Multi-Scale ResNet Backbone, Patchify Capsule Layer, and Cross-Agreement Routing blocks, achieving superior scalability and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing CapsNet variants rely on single high-level feature maps and struggle with multi-scale feature fusion, leading to suboptimal performance. The paper aims to capture rich complementary information from multi-scale features and reconcile feature discrepancies.

Method: Proposes MSPCaps with three components: 1) Multi-Scale ResNet Backbone extracts diverse multi-scale features, 2) Patchify Capsule Layer partitions features into primary capsules with uniform patch size, 3) Cross-Agreement Routing blocks adaptively route multi-scale capsules by identifying cross-scale prediction pairs with maximum agreement.

Result: Achieves remarkable scalability and superior robustness, consistently surpassing baseline methods in classification accuracy. Models range from Tiny (344.3K parameters) to Large (10.9M parameters), demonstrating excellent performance across different scales.

Conclusion: MSPCaps effectively advances feature representation learning by successfully integrating multi-scale feature learning with efficient capsule routing, providing a scalable and robust solution that outperforms existing methods.

Abstract: Capsule Network (CapsNet) has demonstrated significant potential in visual
recognition by capturing spatial relationships and part-whole hierarchies for
learning equivariant feature representations. However, existing CapsNet and
variants often rely on a single high-level feature map, overlooking the rich
complementary information from multi-scale features. Furthermore, conventional
feature fusion strategies (e.g., addition and concatenation) struggle to
reconcile multi-scale feature discrepancies, leading to suboptimal
classification performance. To address these limitations, we propose the
Multi-Scale Patchify Capsule Network (MSPCaps), a novel architecture that
integrates multi-scale feature learning and efficient capsule routing.
Specifically, MSPCaps consists of three key components: a Multi-Scale ResNet
Backbone (MSRB), a Patchify Capsule Layer (PatchifyCaps), and Cross-Agreement
Routing (CAR) blocks. First, the MSRB extracts diverse multi-scale feature
representations from input images, preserving both fine-grained details and
global contextual information. Second, the PatchifyCaps partitions these
multi-scale features into primary capsules using a uniform patch size,
equipping the model with the ability to learn from diverse receptive fields.
Finally, the CAR block adaptively routes the multi-scale capsules by
identifying cross-scale prediction pairs with maximum agreement. Unlike the
simple concatenation of multiple self-routing blocks, CAR ensures that only the
most coherent capsules contribute to the final voting. Our proposed MSPCaps
achieves remarkable scalability and superior robustness, consistently
surpassing multiple baseline methods in terms of classification accuracy, with
configurations ranging from a highly efficient Tiny model (344.3K parameters)
to a powerful Large model (10.9M parameters), highlighting its potential in
advancing feature representation learning.

</details>


### [141] [LGE-Guided Cross-Modality Contrastive Learning for Gadolinium-Free Cardiomyopathy Screening in Cine CMR](https://arxiv.org/abs/2508.16927)
*Siqing Yuan,Yulin Wang,Zirui Cao,Yueyan Wang,Zehao Weng,Hui Wang,Lei Xu,Zixian Chen,Lei Chen,Zhong Xue,Dinggang Shen*

Main category: cs.CV

TL;DR: CC-CMR is a contrastive learning framework that enables gadolinium-free cardiomyopathy screening by aligning cine CMR with LGE sequences, achieving 94.3% accuracy without contrast agent.


<details>
  <summary>Details</summary>
Motivation: Current CMR screening relies on gadolinium contrast and labor-intensive interpretation, limiting population-scale deployment. There's a need for accurate, contrast-free screening methods.

Method: Contrastive learning and cross-modal alignment framework that encodes fibrosis-specific pathology into cine CMR embeddings. Uses Feature Interaction Module and uncertainty-guided adaptive training for optimization.

Result: Achieved 0.943 accuracy (95% CI: 0.886-0.986) on multi-center data from 231 subjects, outperforming state-of-the-art cine-CMR-only models by 4.3%.

Conclusion: CC-CMR demonstrates clinical viability for wide population screening by eliminating gadolinium dependency while maintaining high diagnostic accuracy.

Abstract: Cardiomyopathy, a principal contributor to heart failure and sudden cardiac
mortality, demands precise early screening. Cardiac Magnetic Resonance (CMR),
recognized as the diagnostic 'gold standard' through multiparametric protocols,
holds the potential to serve as an accurate screening tool. However, its
reliance on gadolinium contrast and labor-intensive interpretation hinders
population-scale deployment. We propose CC-CMR, a Contrastive Learning and
Cross-Modal alignment framework for gadolinium-free cardiomyopathy screening
using cine CMR sequences. By aligning the latent spaces of cine CMR and Late
Gadolinium Enhancement (LGE) sequences, our model encodes fibrosis-specific
pathology into cine CMR embeddings. A Feature Interaction Module concurrently
optimizes diagnostic precision and cross-modal feature congruence, augmented by
an uncertainty-guided adaptive training mechanism that dynamically calibrates
task-specific objectives to ensure model generalizability. Evaluated on
multi-center data from 231 subjects, CC-CMR achieves accuracy of 0.943 (95% CI:
0.886-0.986), outperforming state-of-the-art cine-CMR-only models by 4.3% while
eliminating gadolinium dependency, demonstrating its clinical viability for
wide range of populations and healthcare environments.

</details>


### [142] [Align 3D Representation and Text Embedding for 3D Content Personalization](https://arxiv.org/abs/2508.16932)
*Qi Song,Ziyuan Luo,Ka Chun Cheung,Simon See,Renjie Wan*

Main category: cs.CV

TL;DR: Invert3D enables efficient 3D content personalization through natural language prompts by aligning 3D representations with text embeddings, eliminating the need for computationally expensive retraining procedures.


<details>
  <summary>Details</summary>
Motivation: Current 3D personalization methods rely on knowledge distillation-based approaches that require computationally expensive retraining, creating a need for more efficient personalization techniques.

Method: Develops a camera-conditioned 3D-to-text inverse mechanism that projects 3D contents into a 3D embedding aligned with text embeddings, bridging the gap between 3D representations and text embedding spaces.

Result: Extensive experiments demonstrate that Invert3D achieves effective personalization of 3D content through natural language prompts.

Conclusion: The proposed framework provides convenient and efficient 3D content personalization without requiring computationally expensive retraining procedures, enabling direct manipulation through natural language.

Abstract: Recent advances in NeRF and 3DGS have significantly enhanced the efficiency
and quality of 3D content synthesis. However, efficient personalization of
generated 3D content remains a critical challenge. Current 3D personalization
approaches predominantly rely on knowledge distillation-based methods, which
require computationally expensive retraining procedures. To address this
challenge, we propose \textbf{Invert3D}, a novel framework for convenient 3D
content personalization. Nowadays, vision-language models such as CLIP enable
direct image personalization through aligned vision-text embedding spaces.
However, the inherent structural differences between 3D content and 2D images
preclude direct application of these techniques to 3D personalization. Our
approach bridges this gap by establishing alignment between 3D representations
and text embedding spaces. Specifically, we develop a camera-conditioned
3D-to-text inverse mechanism that projects 3D contents into a 3D embedding
aligned with text embeddings. This alignment enables efficient manipulation and
personalization of 3D content through natural language prompts, eliminating the
need for computationally retraining procedures. Extensive experiments
demonstrate that Invert3D achieves effective personalization of 3D content. Our
work is available at: https://github.com/qsong2001/Invert3D.

</details>


### [143] [Addressing Annotation Scarcity in Hyperspectral Brain Image Segmentation with Unsupervised Domain Adaptation](https://arxiv.org/abs/2508.16934)
*Tim Mach,Daniel Rueckert,Alex Berger,Laurin Lux,Ivan Ezhov*

Main category: cs.CV

TL;DR: Novel deep learning framework for cerebral vasculature segmentation in hyperspectral brain images using unsupervised domain adaptation to overcome severe label scarcity.


<details>
  <summary>Details</summary>
Motivation: Address the critical challenge of severe label scarcity that impedes conventional supervised training in cerebral vasculature segmentation.

Method: Utilizes unsupervised domain adaptation methodology, combining a small expert-annotated ground truth dataset with unlabeled data for training.

Result: Quantitative and qualitative evaluations confirm the method significantly outperforms existing state-of-the-art approaches.

Conclusion: Demonstrates the efficacy of domain adaptation for label-scarce biomedical imaging tasks, particularly in cerebral vasculature segmentation.

Abstract: This work presents a novel deep learning framework for segmenting cerebral
vasculature in hyperspectral brain images. We address the critical challenge of
severe label scarcity, which impedes conventional supervised training. Our
approach utilizes a novel unsupervised domain adaptation methodology, using a
small, expert-annotated ground truth alongside unlabeled data. Quantitative and
qualitative evaluations confirm that our method significantly outperforms
existing state-of-the-art approaches, demonstrating the efficacy of domain
adaptation for label-scarce biomedical imaging tasks.

</details>


### [144] [NAT: Learning to Attack Neurons for Enhanced Adversarial Transferability](https://arxiv.org/abs/2508.16937)
*Krishna Kanth Nakka,Alexandre Alahi*

Main category: cs.CV

TL;DR: NAT introduces neuron-level adversarial attacks that target specific neurons rather than entire layers, achieving superior transferability across models and domains compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Previous layer-level adversarial attack methods disproportionately focus on a few neurons representing similar concepts, leaving other neurons minimally affected, which limits transferability across different models.

Method: Neuron Attack for Transferability (NAT) shifts from embedding-level separation to neuron-specific targeting, disrupting core neural network units to create a common basis for transferability across models.

Result: NAT achieves fooling rates surpassing existing baselines by over 14% in cross-model and 4% in cross-domain settings on 41 ImageNet models and 9 fine-grained models, with impressive performance within just 10 queries.

Conclusion: Targeting individual neurons provides a more fundamental approach to adversarial attacks that significantly improves transferability across diverse models and domains compared to layer-level optimization methods.

Abstract: The generation of transferable adversarial perturbations typically involves
training a generator to maximize embedding separation between clean and
adversarial images at a single mid-layer of a source model. In this work, we
build on this approach and introduce Neuron Attack for Transferability (NAT), a
method designed to target specific neuron within the embedding. Our approach is
motivated by the observation that previous layer-level optimizations often
disproportionately focus on a few neurons representing similar concepts,
leaving other neurons within the attacked layer minimally affected. NAT shifts
the focus from embedding-level separation to a more fundamental,
neuron-specific approach. We find that targeting individual neurons effectively
disrupts the core units of the neural network, providing a common basis for
transferability across different models. Through extensive experiments on 41
diverse ImageNet models and 9 fine-grained models, NAT achieves fooling rates
that surpass existing baselines by over 14\% in cross-model and 4\% in
cross-domain settings. Furthermore, by leveraging the complementary attacking
capabilities of the trained generators, we achieve impressive fooling rates
within just 10 queries. Our code is available at:
https://krishnakanthnakka.github.io/NAT/

</details>


### [145] [HieroAction: Hierarchically Guided VLM for Fine-Grained Action Analysis](https://arxiv.org/abs/2508.16942)
*Junhao Wu,Xiuer Gu,Zhiying Li,Yeying Jin,Yunfeng Diao,Zhiyu Li,Zhenbo Song,Xiaomei Zhang,Zhaoxin Fan*

Main category: cs.CV

TL;DR: HieroAction is a vision-language model that provides structured, interpretable assessments of human actions using stepwise reasoning and hierarchical policy learning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing action evaluation methods only provide final scores without explanations, limiting their practical applicability in domains like sports, healthcare, and robotics where interpretable reasoning is crucial.

Method: Combines Stepwise Action Reasoning (chain of thought process for structured evaluation from recognition to scoring) with Hierarchical Policy Learning (reinforcement learning to align sub-action dynamics with overall action quality).

Result: Superior performance across multiple benchmark datasets, demonstrating accurate and interpretable action assessments.

Conclusion: HieroAction effectively addresses the limitation of score-only evaluations by providing structured, explainable assessments through integrated reasoning and policy learning approaches.

Abstract: Evaluating human actions with clear and detailed feedback is important in
areas such as sports, healthcare, and robotics, where decisions rely not only
on final outcomes but also on interpretable reasoning. However, most existing
methods provide only a final score without explanation or detailed analysis,
limiting their practical applicability. To address this, we introduce
HieroAction, a vision-language model that delivers accurate and structured
assessments of human actions. HieroAction builds on two key ideas: (1) Stepwise
Action Reasoning, a tailored chain of thought process designed specifically for
action assessment, which guides the model to evaluate actions step by step,
from overall recognition through sub action analysis to final scoring, thus
enhancing interpretability and structured understanding; and (2) Hierarchical
Policy Learning, a reinforcement learning strategy that enables the model to
learn fine grained sub action dynamics and align them with high level action
quality, thereby improving scoring precision. The reasoning pathway structures
the evaluation process, while policy learning refines each stage through reward
based optimization. Their integration ensures accurate and interpretable
assessments, as demonstrated by superior performance across multiple benchmark
datasets. Code will be released upon acceptance.

</details>


### [146] [RPD-Diff: Region-Adaptive Physics-Guided Diffusion Model for Visibility Enhancement under Dense and Non-Uniform Haze](https://arxiv.org/abs/2508.16956)
*Ruicheng Zhang,Puxin Yan,Zeyu Zhang,Yicheng Chang,Hongyi Chen,Zhi Jin*

Main category: cs.CV

TL;DR: RPD-Diff is a novel diffusion model for single-image dehazing that handles dense and non-uniform haze through physics-guided intermediate targeting and adaptive patch-specific denoising.


<details>
  <summary>Details</summary>
Motivation: Traditional diffusion-based dehazing methods struggle with insufficient generation conditioning and lack adaptability to spatially varying haze distributions, leading to suboptimal restoration in complex haze scenarios.

Method: Proposes RPD-Diff with Physics-guided Intermediate State Targeting (PIST) strategy using physical priors to reformulate diffusion Markov chain, and Haze-Aware Denoising Timestep Predictor (HADTP) that dynamically adjusts patch-specific denoising timesteps using transmission map cross-attention.

Result: Extensive experiments across four real-world datasets demonstrate state-of-the-art performance in challenging dense and non-uniform haze scenarios, delivering high-quality haze-free images with superior detail clarity and color fidelity.

Conclusion: RPD-Diff effectively addresses the limitations of traditional methods by combining physics guidance with adaptive patch processing, achieving robust visibility enhancement in complex haze conditions.

Abstract: Single-image dehazing under dense and non-uniform haze conditions remains
challenging due to severe information degradation and spatial heterogeneity.
Traditional diffusion-based dehazing methods struggle with insufficient
generation conditioning and lack of adaptability to spatially varying haze
distributions, which leads to suboptimal restoration. To address these
limitations, we propose RPD-Diff, a Region-adaptive Physics-guided Dehazing
Diffusion Model for robust visibility enhancement in complex haze scenarios.
RPD-Diff introduces a Physics-guided Intermediate State Targeting (PIST)
strategy, which leverages physical priors to reformulate the diffusion Markov
chain by generation target transitions, mitigating the issue of insufficient
conditioning in dense haze scenarios. Additionally, the Haze-Aware Denoising
Timestep Predictor (HADTP) dynamically adjusts patch-specific denoising
timesteps employing a transmission map cross-attention mechanism, adeptly
managing non-uniform haze distributions. Extensive experiments across four
real-world datasets demonstrate that RPD-Diff achieves state-of-the-art
performance in challenging dense and non-uniform haze scenarios, delivering
high-quality, haze-free images with superior detail clarity and color fidelity.

</details>


### [147] [Local Information Matters: A Rethink of Crowd Counting](https://arxiv.org/abs/2508.16970)
*Tianhang Pan,Xiuyi Jia*

Main category: cs.CV

TL;DR: LIMM proposes a crowd counting model that emphasizes local modeling capability through window partitioning and contrastive learning, achieving state-of-the-art performance by focusing on small-sized individuals that dominate crowd images.


<details>
  <summary>Details</summary>
Motivation: Existing crowd counting models use the same backbones as other visual tasks and pursue large receptive fields, but fail to address that individuals (heads) in crowd counting typically occupy very small portions of images. This paper rethinks this essential characteristic and proposes emphasizing local modeling capability.

Method: Proposes Local Information Matters Model (LIMM) with two main strategies: 1) window partitioning design that applies grid windows to model input, 2) window-wise contrastive learning to enhance model's ability to distinguish between local density levels. Also includes a global attention module at the end to handle occasionally large-sized individuals.

Result: Extensive experiments show significant improvement in local modeling capability (8.7% MAE improvement on JHU-Crowd++ high-density subset) without compromising ability to count large-sized individuals. Achieves state-of-the-art performance on multiple public datasets.

Conclusion: The paper demonstrates that emphasizing local modeling capability is crucial for crowd counting due to the small size of individuals in crowd images. The proposed LIMM model successfully addresses this characteristic and sets new state-of-the-art performance.

Abstract: The motivation of this paper originates from rethinking an essential
characteristic of crowd counting: individuals (heads of humans) in the crowd
counting task typically occupy a very small portion of the image. This
characteristic has never been the focus of existing works: they typically use
the same backbone as other visual tasks and pursue a large receptive field.
This drives us to propose a new model design principle of crowd counting:
emphasizing local modeling capability of the model. We follow the principle and
design a crowd counting model named Local Information Matters Model (LIMM). The
main innovation lies in two strategies: a window partitioning design that
applies grid windows to the model input, and a window-wise contrastive learning
design to enhance the model's ability to distinguish between local density
levels. Moreover, a global attention module is applied to the end of the model
to handle the occasionally occurring large-sized individuals. Extensive
experiments on multiple public datasets illustrate that the proposed model
shows a significant improvement in local modeling capability (8.7\% in MAE on
the JHU-Crowd++ high-density subset for example), without compromising its
ability to count large-sized ones, which achieves state-of-the-art performance.
Code is available at: https://github.com/tianhangpan/LIMM.

</details>


### [148] [Robust Diagram Reasoning: A Framework for Enhancing LVLM Performance on Visually Perturbed Scientific Diagrams](https://arxiv.org/abs/2508.16972)
*Minghao Zhou,Rafael Souza,Yaqian Hu,Luming Che*

Main category: cs.CV

TL;DR: The paper introduces RDR framework to enhance LVLMs' robustness against visual perturbations in scientific diagrams, proposing new metrics and a benchmark dataset showing significant performance degradation in current models.


<details>
  <summary>Details</summary>
Motivation: LVLMs lack robustness to common visual perturbations (noise, blur, occlusions) in real-world scientific documents, and existing benchmarks overlook this critical challenge for practical deployment.

Method: Proposes Robust Diagram Reasoning (RDR) framework with Adaptive Multi-View & Consistency Verification (AMCV) mechanism - generating multiple perturbed diagram versions, parallel inference, and consistency-based self-correction loop.

Result: Experiments show state-of-the-art LVLMs like GPT-4V suffer significant performance degradation with perturbed inputs (Clean Accuracy 85.2% vs. PRS 72.1%).

Conclusion: The work addresses a critical gap in LVLM evaluation and provides a framework to enhance robustness for scientific diagram reasoning under real-world visual degradation conditions.

Abstract: Large Language Models (LLMs) and their multimodal variants (LVLMs) hold
immense promise for scientific and engineering applications, particularly in
processing visual information like scientific diagrams. However, their
practical deployment is hindered by a critical lack of robustness to common
visual perturbations such as noise, blur, and occlusions, which are prevalent
in real-world scientific documents. Existing evaluation benchmarks largely
overlook this challenge, leaving the robust reasoning capabilities of LVLMs on
visually degraded scientific diagrams underexplored. To address this, we
introduce the Robust Diagram Reasoning (RDR) framework, a novel approach
designed to enhance and rigorously evaluate LVLMs' performance under such
conditions. At its core, RDR employs an Adaptive Multi-View & Consistency
Verification (AMCV) mechanism, which involves generating multiple perturbed
versions of a diagram, performing parallel inference, and then applying a
consistency-based self-correction loop. We also propose two new metrics,
Perturbation Robustness Score (PRS) and Visual Degradation Consistency (VDC),
to quantify robustness. Furthermore, we construct SciDiagram-Robust, the first
large-scale scientific diagram question-answering dataset specifically
augmented with diverse, programmatically generated visual perturbations. Our
extensive experiments demonstrate that even state-of-the-art closed-source
LVLMs like GPT-4V exhibit significant performance degradation when faced with
perturbed inputs (Clean Accuracy 85.2% vs. PRS 72.1%).

</details>


### [149] [Balanced Sharpness-Aware Minimization for Imbalanced Regression](https://arxiv.org/abs/2508.16973)
*Yahao Liu,Qin Wang,Lixin Duan,Wen Li*

Main category: cs.CV

TL;DR: BSAM addresses imbalanced regression by reframing it as an imbalanced generalization problem, using a targeted reweighting strategy with sharpness-aware minimization to ensure uniform generalization across all observations.


<details>
  <summary>Details</summary>
Motivation: Real-world regression data often has imbalanced distributions, causing poor performance for rare target values. Traditional regression models struggle with this imbalanced regression problem.

Method: Proposes Balanced Sharpness-Aware Minimization (BSAM) that combines traditional sharpness-aware minimization with a novel targeted reweighting strategy to homogenize generalization ability across the entire observation space.

Result: Extensive experiments on multiple vision regression tasks (age and depth estimation) show BSAM consistently outperforms existing approaches.

Conclusion: BSAM effectively addresses imbalanced regression by ensuring uniform generalization ability across all target values, providing theoretical generalization guarantees and superior performance on real-world vision tasks.

Abstract: Regression is fundamental in computer vision and is widely used in various
tasks including age estimation, depth estimation, target localization, \etc
However, real-world data often exhibits imbalanced distribution, making
regression models perform poorly especially for target values with rare
observations~(known as the imbalanced regression problem). In this paper, we
reframe imbalanced regression as an imbalanced generalization problem. To
tackle that, we look into the loss sharpness property for measuring the
generalization ability of regression models in the observation space. Namely,
given a certain perturbation on the model parameters, we check how model
performance changes according to the loss values of different target
observations. We propose a simple yet effective approach called Balanced
Sharpness-Aware Minimization~(BSAM) to enforce the uniform generalization
ability of regression models for the entire observation space. In particular,
we start from the traditional sharpness-aware minimization and then introduce a
novel targeted reweighting strategy to homogenize the generalization ability
across the observation space, which guarantees a theoretical generalization
bound. Extensive experiments on multiple vision regression tasks, including age
and depth estimation, demonstrate that our BSAM method consistently outperforms
existing approaches. The code is available
\href{https://github.com/manmanjun/BSAM_for_Imbalanced_Regression}{here}.

</details>


### [150] [Hierarchical Contextual Grounding LVLM: Enhancing Fine-Grained Visual-Language Understanding with Robust Grounding](https://arxiv.org/abs/2508.16974)
*Leilei Guo,Antonio Carlos Rivera,Peiyu Tang,Haoxuan Ren,Zheyu Song*

Main category: cs.CV

TL;DR: HCG-LVLM is a hierarchical vision-language model that uses coarse-to-fine processing with global contextual perception and fine-grained local grounding to improve accuracy and reduce hallucination in visual-language tasks.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs show insufficient robustness, proneness to hallucination, and reasoning errors in complex real-world scenarios requiring precise image region localization and fine-grained visual reasoning.

Method: Two-layered architecture: Global Contextual Perception layer for broad understanding and Fine-grained Local Grounding layer with Local Detail Enhancement Module and Semantic Consistency Validator, using adaptive fusion mechanism.

Result: Outperforms state-of-the-art models (Flamingo, BLIP-2, MiniGPT-4) on GQA, A-OKVQA, and RefCOCO/+/g datasets with superior accuracy and significantly reduced hallucination.

Conclusion: The hierarchical design effectively enhances fine-grained visual-language understanding and precise grounding capabilities, validating the coarse-to-fine cognitive processing approach.

Abstract: Large Language Models (LLMs) and Vision-Language Large Models (LVLMs) have
achieved remarkable progress in natural language processing and multimodal
understanding. Despite their impressive generalization capabilities, current
LVLMs often exhibit insufficient robustness, proneness to hallucination, and
reasoning errors in complex real-world scenarios, particularly when precise
image region localization and fine-grained visual reasoning are required. To
address these limitations, we propose the Hierarchical Contextual Grounding
LVLM (HCG-LVLM), a novel architecture that mimics human coarse-to-fine
cognitive processing. HCG-LVLM employs a two-layered approach: a Global
Contextual Perception layer for initial broad understanding and a Fine-grained
Local Grounding layer. The latter incorporates a Local Detail Enhancement
Module to extract high-resolution features and a Semantic Consistency Validator
to ensure accurate, hallucination-free visual-language alignment. Through an
adaptive fusion mechanism, information from both layers is integrated for
robust and precise outputs. Extensive experiments on challenging datasets,
including GQA, A-OKVQA for fine-grained VQA, and RefCOCO/+/g for Referring
Expression Comprehension, demonstrate that HCG-LVLM consistently outperforms
state-of-the-art models such as Flamingo, BLIP-2, and MiniGPT-4. Our model
achieves superior accuracy and significantly reduces hallucination, validating
the effectiveness of its hierarchical design in enhancing fine-grained
visual-language understanding and precise grounding capabilities.

</details>


### [151] [Combating Digitally Altered Images: Deepfake Detection](https://arxiv.org/abs/2508.16975)
*Saksham Kumar,Rhythm Narang*

Main category: cs.CV

TL;DR: A modified Vision Transformer model achieves state-of-the-art Deepfake detection using the OpenForensics Dataset with augmentation and class imbalance handling.


<details>
  <summary>Details</summary>
Motivation: Deepfake technology creates hyper-realistic manipulated images/videos that pose significant challenges to public trust and authorities, requiring robust detection methods.

Method: Modified Vision Transformer (ViT) model trained on OpenForensics Dataset subset with multiple augmentation techniques, oversampling for class imbalance, and stratified train-validation split.

Result: The model demonstrates state-of-the-art performance on test dataset, effectively detecting Deepfake images with high accuracy.

Conclusion: The proposed modified Vision Transformer approach provides a robust solution for Deepfake detection, capable of handling diverse image manipulations and class imbalance issues.

Abstract: The rise of Deepfake technology to generate hyper-realistic manipulated
images and videos poses a significant challenge to the public and relevant
authorities. This study presents a robust Deepfake detection based on a
modified Vision Transformer(ViT) model, trained to distinguish between real and
Deepfake images. The model has been trained on a subset of the OpenForensics
Dataset with multiple augmentation techniques to increase robustness for
diverse image manipulations. The class imbalance issues are handled by
oversampling and a train-validation split of the dataset in a stratified
manner. Performance is evaluated using the accuracy metric on the training and
testing datasets, followed by a prediction score on a random image of people,
irrespective of their realness. The model demonstrates state-of-the-art results
on the test dataset to meticulously detect Deepfake images.

</details>


### [152] [Preserving Domain Generalization in Fine-Tuning via Joint Parameter Selection](https://arxiv.org/abs/2508.16976)
*Bin Pan,Shiyu Shen,Zongbin Wang,Zhenwei Shi,Xia Xu*

Main category: cs.CV

TL;DR: JPS is a parameter-efficient domain generalization method that selectively fine-tunes a sparse subset of parameters to preserve pre-trained model generalization while adapting to new tasks.


<details>
  <summary>Details</summary>
Motivation: Full fine-tuning of large pre-trained vision models can compromise their intrinsic generalization capabilities. Parameter-efficient adaptation strategies are needed to balance task adaptation with preservation of generalization.

Method: Joint Parameter Selection (JPS) restricts updates to a small, sparse subset of parameters using dual operators to identify parameters with consistent and significant gradients across all source domains.

Result: Extensive benchmark experiments show JPS achieves superior performance compared to state-of-the-art domain generalization methods.

Conclusion: JPS provides an efficient and effective approach for domain generalization by selectively fine-tuning parameters while preserving the generalization strength of pre-trained models, with theoretical justification through generalization error bounds.

Abstract: Domain generalization seeks to develop models trained on a limited set of
source domains that are capable of generalizing effectively to unseen target
domains. While the predominant approach leverages large-scale pre-trained
vision models as initialization, recent studies have highlighted that full
fine-tuning can compromise the intrinsic generalization capabilities of these
models. To address this limitation, parameter-efficient adaptation strategies
have emerged, wherein only a subset of model parameters is selectively
fine-tuned, thereby balancing task adaptation with the preservation of
generalization. Motivated by this paradigm, we introduce Joint Parameter
Selection (JPS), a novel method that restricts updates to a small, sparse
subset of parameters, thereby retaining and harnessing the generalization
strength of pre-trained models. Theoretically, we establish a generalization
error bound that explicitly accounts for the sparsity of parameter updates,
thereby providing a principled justification for selective fine-tuning.
Practically, we design a selection mechanism employing dual operators to
identify and update parameters exhibiting consistent and significant gradients
across all source domains. Extensive benchmark experiments demonstrate that JPS
achieves superior performance compared to state-of-the-art domain
generalization methods, substantiating both the efficiency and efficacy of the
proposed approach.

</details>


### [153] [HiCache: Training-free Acceleration of Diffusion Models via Hermite Polynomial-based Feature Caching](https://arxiv.org/abs/2508.16984)
*Liang Feng,Shikang Zheng,Jiacheng Liu,Yuqi Lin,Qinming Zhou,Peiliang Cai,Xinyu Wang,Junjie Chen,Chang Zou,Yue Ma,Linfeng Zhang*

Main category: cs.CV

TL;DR: HiCache is a training-free acceleration framework for diffusion models that uses Hermite polynomials for feature prediction, achieving 6.24x speedup while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion models suffer from high computational costs due to iterative sampling, and existing feature caching methods fail to model complex feature evolution dynamics, leading to quality loss.

Method: Uses Hermite polynomials as theoretically optimal basis for Gaussian-correlated feature derivative approximations, with dual-scaling mechanism for numerical stability and predictive accuracy.

Result: Achieves 6.24x speedup on FLUX.1-dev while exceeding baseline quality, with strong performance across text-to-image, video generation, and super-resolution tasks.

Conclusion: HiCache provides an effective training-free acceleration solution that fundamentally improves feature prediction in diffusion models through mathematical alignment with empirical properties.

Abstract: Diffusion models have achieved remarkable success in content generation but
suffer from prohibitive computational costs due to iterative sampling. While
recent feature caching methods tend to accelerate inference through temporal
extrapolation, these methods still suffer from server quality loss due to the
failure in modeling the complex dynamics of feature evolution. To solve this
problem, this paper presents HiCache, a training-free acceleration framework
that fundamentally improves feature prediction by aligning mathematical tools
with empirical properties. Our key insight is that feature derivative
approximations in Diffusion Transformers exhibit multivariate Gaussian
characteristics, motivating the use of Hermite polynomials-the potentially
theoretically optimal basis for Gaussian-correlated processes. Besides, We
further introduce a dual-scaling mechanism that ensures numerical stability
while preserving predictive accuracy. Extensive experiments demonstrate
HiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding
baseline quality, maintaining strong performance across text-to-image, video
generation, and super-resolution tasks. Core implementation is provided in the
appendix, with complete code to be released upon acceptance.

</details>


### [154] [An Efficient Dual-Line Decoder Network with Multi-Scale Convolutional Attention for Multi-organ Segmentation](https://arxiv.org/abs/2508.17007)
*Riad Hassan,M. Rubaiyat Hossain Mondal,Sheikh Iqbal Ahamed,Fahad Mostafa,Md Mostafijur Rahman*

Main category: cs.CV

TL;DR: EDLDNet introduces an efficient dual-line decoder segmentation network that balances accuracy and computational efficiency for medical image segmentation, achieving state-of-the-art performance with 84.00% Dice score on Synapse dataset while reducing computational operations by 89.7%.


<details>
  <summary>Details</summary>
Motivation: Current deep learning segmentation methods fail to balance accuracy with computational efficiency, either prioritizing performance at high computational cost or compromising accuracy for efficiency.

Method: Proposes EDLDNet with noisy decoder for training robustness, multi-scale convolutional attention modules, attention gates, up-convolution blocks, and mutation-based loss function using multi-scale segmentation masks from dual decoders.

Result: Outperforms SOTA methods on four medical imaging datasets, achieves 84.00% Dice score on Synapse dataset (13.89% improvement over UNet) with 89.7% reduction in MACs, and maintains comparable efficiency to recent approaches like EMCAD.

Conclusion: EDLDNet demonstrates strong generalization, computational efficiency, and robustness across diverse datasets, establishing it as an effective solution for organ-at-risk segmentation in medical imaging.

Abstract: Proper segmentation of organs-at-risk is important for radiation therapy,
surgical planning, and diagnostic decision-making in medical image analysis.
While deep learning-based segmentation architectures have made significant
progress, they often fail to balance segmentation accuracy with computational
efficiency. Most of the current state-of-the-art methods either prioritize
performance at the cost of high computational complexity or compromise accuracy
for efficiency. This paper addresses this gap by introducing an efficient
dual-line decoder segmentation network (EDLDNet). The proposed method features
a noisy decoder, which learns to incorporate structured perturbation at
training time for better model robustness, yet at inference time only the
noise-free decoder is executed, leading to lower computational cost.
Multi-Scale convolutional Attention Modules (MSCAMs), Attention Gates (AGs),
and Up-Convolution Blocks (UCBs) are further utilized to optimize feature
representation and boost segmentation performance. By leveraging multi-scale
segmentation masks from both decoders, we also utilize a mutation-based loss
function to enhance the model's generalization. Our approach outperforms SOTA
segmentation architectures on four publicly available medical imaging datasets.
EDLDNet achieves SOTA performance with an 84.00% Dice score on the Synapse
dataset, surpassing baseline model like UNet by 13.89% in Dice score while
significantly reducing Multiply-Accumulate Operations (MACs) by 89.7%. Compared
to recent approaches like EMCAD, our EDLDNet not only achieves higher Dice
score but also maintains comparable computational efficiency. The outstanding
performance across diverse datasets establishes EDLDNet's strong
generalization, computational efficiency, and robustness. The source code,
pre-processed data, and pre-trained weights will be available at
https://github.com/riadhassan/EDLDNet .

</details>


### [155] [Contrastive Prompt Clustering for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2508.17009)
*Wangyu Wu,Zhenhong Chen,Xiaowen Ma,Wenqiao Zhang,Xianglin Qiu,Siqi Song,Xiaowei Huang,Fei Ma,Jimin Xiao*

Main category: cs.CV

TL;DR: CPC is a novel weakly supervised semantic segmentation framework that uses LLMs to create category clusters capturing inter-class relationships and employs contrastive learning for better intra-class consistency and inter-class separation.


<details>
  <summary>Details</summary>
Motivation: Existing WSSS methods focus too much on inter-class separation while neglecting shared semantics among related categories and lack fine-grained discrimination, leading to confusion among visually similar categories.

Method: Uses Large Language Models to derive category clusters encoding intrinsic inter-class relationships, and introduces class-aware patch-level contrastive loss to enforce intra-class consistency and inter-class separation in a hierarchical design.

Result: Experiments on PASCAL VOC 2012 and MS COCO 2014 demonstrate that CPC surpasses existing state-of-the-art methods in weakly supervised semantic segmentation.

Conclusion: CPC effectively addresses the limitations of existing WSSS methods by leveraging hierarchical semantic priors from LLMs and contrastive learning, achieving superior performance while reducing confusion among visually similar categories.

Abstract: Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has
gained attention for its cost-effectiveness. Most existing methods emphasize
inter-class separation, often neglecting the shared semantics among related
categories and lacking fine-grained discrimination. To address this, we propose
Contrastive Prompt Clustering (CPC), a novel WSSS framework. CPC exploits Large
Language Models (LLMs) to derive category clusters that encode intrinsic
inter-class relationships, and further introduces a class-aware patch-level
contrastive loss to enforce intra-class consistency and inter-class separation.
This hierarchical design leverages clusters as coarse-grained semantic priors
while preserving fine-grained boundaries, thereby reducing confusion among
visually similar categories. Experiments on PASCAL VOC 2012 and MS COCO 2014
demonstrate that CPC surpasses existing state-of-the-art methods in WSSS.

</details>


### [156] [Fiducial Marker Splatting for High-Fidelity Robotics Simulations](https://arxiv.org/abs/2508.17012)
*Diram Tabaa,Gianni Di Caro*

Main category: cs.CV

TL;DR: A hybrid framework combining Gaussian Splatting's photorealism with structured marker generation for improved robotic simulation in complex environments like greenhouses.


<details>
  <summary>Details</summary>
Motivation: Traditional mesh-based 3D simulations struggle in complex environments with occlusions and repetitive structures, while neural rendering methods lack support for fiducial markers essential for robotic localization.

Method: Proposes a hybrid framework that combines Gaussian Splatting's visual realism with a novel algorithm for efficiently generating GS-based fiducial markers (e.g., AprilTags) within cluttered scenes.

Result: Outperforms traditional image-fitting techniques in both efficiency and pose-estimation accuracy, demonstrated in challenging greenhouse environments with dense foliage and occlusions.

Conclusion: The framework shows strong potential for real-world robotic applications, particularly in agricultural settings where complex visual environments push perception limits.

Abstract: High-fidelity 3D simulation is critical for training mobile robots, but its
traditional reliance on mesh-based representations often struggle in complex
environments, such as densely packed greenhouses featuring occlusions and
repetitive structures. Recent neural rendering methods, like Gaussian Splatting
(GS), achieve remarkable visual realism but lack flexibility to incorporate
fiducial markers, which are essential for robotic localization and control. We
propose a hybrid framework that combines the photorealism of GS with structured
marker representations. Our core contribution is a novel algorithm for
efficiently generating GS-based fiducial markers (e.g., AprilTags) within
cluttered scenes. Experiments show that our approach outperforms traditional
image-fitting techniques in both efficiency and pose-estimation accuracy. We
further demonstrate the framework's potential in a greenhouse simulation. This
agricultural setting serves as a challenging testbed, as its combination of
dense foliage, similar-looking elements, and occlusions pushes the limits of
perception, thereby highlighting the framework's value for real-world
applications.

</details>


### [157] [Dual Orthogonal Guidance for Robust Diffusion-based Handwritten Text Generation](https://arxiv.org/abs/2508.17017)
*Konstantina Nikolaidou,George Retsinas,Giorgos Sfikas,Silvia Cascianelli,Rita Cucchiara,Marcus Liwicki*

Main category: cs.CV

TL;DR: Proposes Dual Orthogonal Guidance (DOG) to improve diffusion-based handwritten text generation by reducing artifacts and enhancing readability while maintaining style diversity.


<details>
  <summary>Details</summary>
Motivation: Standard diffusion models for handwritten text generation suffer from memorization, style variability issues, and produce artifacts that reduce readability, especially for challenging styles and out-of-vocabulary words.

Method: Introduces DOG - a novel sampling guidance strategy using orthogonal projection of negatively perturbed prompts onto positive prompts, combined with a triangular schedule to control guidance strength throughout denoising.

Result: Experimental results on DiffusionPen and One-DM show DOG improves content clarity and style variability, even for out-of-vocabulary words and challenging writing styles.

Conclusion: DOG provides more stable and disentangled guidance than standard CFG, effectively addressing artifacts and readability issues in diffusion-based handwritten text generation.

Abstract: Diffusion-based Handwritten Text Generation (HTG) approaches achieve
impressive results on frequent, in-vocabulary words observed at training time
and on regular styles. However, they are prone to memorizing training samples
and often struggle with style variability and generation clarity. In
particular, standard diffusion models tend to produce artifacts or distortions
that negatively affect the readability of the generated text, especially when
the style is hard to produce. To tackle these issues, we propose a novel
sampling guidance strategy, Dual Orthogonal Guidance (DOG), that leverages an
orthogonal projection of a negatively perturbed prompt onto the original
positive prompt. This approach helps steer the generation away from artifacts
while maintaining the intended content, and encourages more diverse, yet
plausible, outputs. Unlike standard Classifier-Free Guidance (CFG), which
relies on unconditional predictions and produces noise at high guidance scales,
DOG introduces a more stable, disentangled direction in the latent space. To
control the strength of the guidance across the denoising process, we apply a
triangular schedule: weak at the start and end of denoising, when the process
is most sensitive, and strongest in the middle steps. Experimental results on
the state-of-the-art DiffusionPen and One-DM demonstrate that DOG improves both
content clarity and style variability, even for out-of-vocabulary words and
challenging writing styles.

</details>


### [158] [Probabilistic Temporal Masked Attention for Cross-view Online Action Detection](https://arxiv.org/abs/2508.17025)
*Liping Xie,Yang Tan,Shicheng Jing,Huimin Lu,Kanjian Zhang*

Main category: cs.CV

TL;DR: PTMA model uses probabilistic modeling and temporal masked attention for cross-view online action detection, achieving state-of-the-art results on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Mainstream OAD models struggle with generalization across different video viewpoints, limiting their performance on unseen data sources.

Method: Probabilistic Temporal Masked Attention (PTMA) with GRU-based temporal masked attention cell that leverages latent compressed representations for cross-view video analysis and view-invariant feature extraction.

Result: Achieves state-of-the-art performance on DAHLIA, IKEA ASM, and Breakfast datasets under cross-subject, cross-view, and cross-subject-view evaluation protocols.

Conclusion: PTMA effectively addresses viewpoint sensitivity in online action detection through probabilistic modeling and cross-view feature learning, demonstrating superior generalization capabilities.

Abstract: As a critical task in video sequence classification within computer vision,
Online Action Detection (OAD) has garnered significant attention. The
sensitivity of mainstream OAD models to varying video viewpoints often hampers
their generalization when confronted with unseen sources. To address this
limitation, we propose a novel Probabilistic Temporal Masked Attention (PTMA)
model, which leverages probabilistic modeling to derive latent compressed
representations of video frames in a cross-view setting. The PTMA model
incorporates a GRU-based temporal masked attention (TMA) cell, which leverages
these representations to effectively query the input video sequence, thereby
enhancing information interaction and facilitating autoregressive frame-level
video analysis. Additionally, multi-view information can be integrated into the
probabilistic modeling to facilitate the extraction of view-invariant features.
Experiments conducted under three evaluation protocols: cross-subject (cs),
cross-view (cv), and cross-subject-view (csv) show that PTMA achieves
state-of-the-art performance on the DAHLIA, IKEA ASM, and Breakfast datasets.

</details>


### [159] [A Novel Local Focusing Mechanism for Deepfake Detection Generalization](https://arxiv.org/abs/2508.17029)
*Mingliang Li,Lin Yuanbo Wu,Changhong Liu,Hanxi Li*

Main category: cs.CV

TL;DR: Proposes Local Focus Mechanism (LFM) for deepfake detection that addresses CNN limitations by focusing on discriminative local features instead of global pooling, achieving state-of-the-art performance with 3.7% accuracy improvement and high efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection methods based on reconstruction learning with deep CNNs show poor generalization across object categories and generation domains due to overfitting to semantic features and loss of local forgery cues through Global Average Pooling.

Method: Local Focus Mechanism (LFM) integrates a Salience Network with Top-K Pooling to select the most informative local patterns, plus regularization techniques (Rank-Based Linear Dropout and Random-K Sampling) to prevent overfitting from Top-K pooling.

Result: LFM achieves 3.7% improvement in accuracy and 2.8% increase in average precision over state-of-the-art NPR method, while maintaining exceptional efficiency at 1789 FPS on a single NVIDIA A6000 GPU.

Conclusion: The approach sets a new benchmark for cross-domain deepfake detection by effectively addressing CNN limitations and preserving critical local forgery cues that are vital for real-fake classification.

Abstract: The rapid advancement of deepfake generation techniques has intensified the
need for robust and generalizable detection methods. Existing approaches based
on reconstruction learning typically leverage deep convolutional networks to
extract differential features. However, these methods show poor generalization
across object categories (e.g., from faces to cars) and generation domains
(e.g., from GANs to Stable Diffusion), due to intrinsic limitations of deep
CNNs. First, models trained on a specific category tend to overfit to semantic
feature distributions, making them less transferable to other categories,
especially as network depth increases. Second, Global Average Pooling (GAP)
compresses critical local forgery cues into a single vector, thus discarding
discriminative patterns vital for real-fake classification. To address these
issues, we propose a novel Local Focus Mechanism (LFM) that explicitly attends
to discriminative local features for differentiating fake from real images. LFM
integrates a Salience Network (SNet) with a task-specific Top-K Pooling (TKP)
module to select the K most informative local patterns. To mitigate potential
overfitting introduced by Top-K pooling, we introduce two regularization
techniques: Rank-Based Linear Dropout (RBLD) and Random-K Sampling (RKS), which
enhance the model's robustness. LFM achieves a 3.7 improvement in accuracy and
a 2.8 increase in average precision over the state-of-the-art Neighboring Pixel
Relationships (NPR) method, while maintaining exceptional efficiency at 1789
FPS on a single NVIDIA A6000 GPU. Our approach sets a new benchmark for
cross-domain deepfake detection. The source code are available in
https://github.com/lmlpy/LFM.git

</details>


### [160] [F4-ITS: Fine-grained Feature Fusion for Food Image-Text Search](https://arxiv.org/abs/2508.17037)
*Raghul Asokan*

Main category: cs.CV

TL;DR: F4-ITS is a training-free vision-language framework that improves food image-text matching through multi-modal feature fusion and ingredient-based re-ranking, achieving significant performance gains over baselines.


<details>
  <summary>Details</summary>
Motivation: The proliferation of digital food content requires robust systems for fine-grained visual understanding and retrieval, particularly for applications like dietary monitoring, smart kitchens, and restaurant automation.

Method: Proposes a training-free VLM-guided framework with uni/bi-directional multi-modal fusion (combining image embeddings with VLM-generated text descriptions) and a feature-based re-ranking mechanism using predicted food ingredients for top-k retrieval.

Result: Achieves ~10% and ~7.7% improvements in top-1 retrieval under dense and sparse caption scenarios, and ~28.6% gain in top-k ingredient-level retrieval. Smaller models can match larger counterparts when augmented with textual fusion.

Conclusion: The framework demonstrates significant improvements in food image-text matching performance and shows effectiveness in resource-constrained settings, with code and datasets to be made publicly available.

Abstract: The proliferation of digital food content has intensified the need for robust
and accurate systems capable of fine-grained visual understanding and
retrieval. In this work, we address the challenging task of food image-to-text
matching, a critical component in applications such as dietary monitoring,
smart kitchens, and restaurant automation. We propose F4-ITS: Fine-grained
Feature Fusion for Food Image-Text Search, a training-free, vision-language
model (VLM)-guided framework that significantly improves retrieval performance
through enhanced multi-modal feature representations. Our approach introduces
two key contributions: (1) a uni-directional(and bi-directional) multi-modal
fusion strategy that combines image embeddings with VLM-generated textual
descriptions to improve query expressiveness, and (2) a novel feature-based
re-ranking mechanism for top-k retrieval, leveraging predicted food ingredients
to refine results and boost precision. Leveraging open-source image-text
encoders, we demonstrate substantial gains over standard baselines - achieving
~10% and ~7.7% improvements in top-1 retrieval under dense and sparse caption
scenarios, and a ~28.6% gain in top-k ingredient-level retrieval. Additionally,
we show that smaller models (e.g., ViT-B/32) can match or outperform larger
counterparts (e.g., ViT-H, ViT-G, ViT-bigG) when augmented with textual fusion,
highlighting the effectiveness of our method in resource-constrained settings.
Code and test datasets will be made publicly available at:
https://github.com/mailcorahul/f4-its

</details>


### [161] [M3DMap: Object-aware Multimodal 3D Mapping for Dynamic Environments](https://arxiv.org/abs/2508.17044)
*Dmitry Yudin*

Main category: cs.CV

TL;DR: A comprehensive analysis of multimodal 3D mapping methods with a proposed taxonomy and M3DMap system for dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of universal representations for dynamic 3D scenes that incorporate multimodal data (images, point clouds, text) in robotics and autonomous transportation.

Method: Proposes a taxonomy classifying methods by scene types, representations, learning methods, and applications. Introduces M3DMap - a modular system with neural multimodal object segmentation/tracking, odometry estimation, 3D map construction/updating, and multimodal data retrieval modules.

Result: Provides structured analysis of recent methods and presents original implementations demonstrating advantages in practical tasks like 3D object grounding and mobile manipulation.

Conclusion: Theoretical propositions show positive effects of using multimodal data and modern foundational models in 3D mapping, with M3DMap offering a flexible framework for both static and dynamic scene mapping.

Abstract: 3D mapping in dynamic environments poses a challenge for modern researchers
in robotics and autonomous transportation. There are no universal
representations for dynamic 3D scenes that incorporate multimodal data such as
images, point clouds, and text. This article takes a step toward solving this
problem. It proposes a taxonomy of methods for constructing multimodal 3D maps,
classifying contemporary approaches based on scene types and representations,
learning methods, and practical applications. Using this taxonomy, a brief
structured analysis of recent methods is provided. The article also describes
an original modular method called M3DMap, designed for object-aware
construction of multimodal 3D maps for both static and dynamic scenes. It
consists of several interconnected components: a neural multimodal object
segmentation and tracking module; an odometry estimation module, including
trainable algorithms; a module for 3D map construction and updating with
various implementations depending on the desired scene representation; and a
multimodal data retrieval module. The article highlights original
implementations of these modules and their advantages in solving various
practical tasks, from 3D object grounding to mobile manipulation. Additionally,
it presents theoretical propositions demonstrating the positive effect of using
multimodal data and modern foundational models in 3D mapping methods. Details
of the taxonomy and method implementation are available at
https://yuddim.github.io/M3DMap.

</details>


### [162] [Styleclone: Face Stylization with Diffusion Based Data Augmentation](https://arxiv.org/abs/2508.17045)
*Neeraj Matiyali,Siddharth Srivastava,Gaurav Sharma*

Main category: cs.CV

TL;DR: StyleClone uses textual inversion and diffusion guidance to augment small style datasets, then trains fast image-to-image networks that outperform diffusion methods in speed and quality for face stylization.


<details>
  <summary>Details</summary>
Motivation: To enable high-quality face stylization in specific styles with limited style images, overcoming the challenge of small datasets for training effective image-to-image translation networks.

Method: Leverages textual inversion and diffusion-based guided image generation to systematically augment small style datasets with diverse samples, then trains fast image-to-image translation networks on the augmented dataset.

Result: Outperforms diffusion-based methods in both speed and quality, improves stylization quality, better preserves source image content, and significantly accelerates inference across multiple styles.

Conclusion: The approach successfully enables effective face stylization with limited style images through systematic dataset augmentation and demonstrates superior performance compared to diffusion methods in terms of both quality and efficiency.

Abstract: We present StyleClone, a method for training image-to-image translation
networks to stylize faces in a specific style, even with limited style images.
Our approach leverages textual inversion and diffusion-based guided image
generation to augment small style datasets. By systematically generating
diverse style samples guided by both the original style images and real face
images, we significantly enhance the diversity of the style dataset. Using this
augmented dataset, we train fast image-to-image translation networks that
outperform diffusion-based methods in speed and quality. Experiments on
multiple styles demonstrate that our method improves stylization quality,
better preserves source image content, and significantly accelerates inference.
Additionally, we provide a systematic evaluation of the augmentation techniques
and their impact on stylization performance.

</details>


### [163] [PVNet: Point-Voxel Interaction LiDAR Scene Upsampling Via Diffusion Models](https://arxiv.org/abs/2508.17050)
*Xianjing Cheng,Lintai Wu,Zuowen Wang,Junhui Hou,Jie Wen,Yong Xu*

Main category: cs.CV

TL;DR: PVNet is a diffusion model-based point-voxel interaction framework for LiDAR point cloud upsampling in outdoor scenes without dense supervision, achieving state-of-the-art performance with arbitrary upsampling rates.


<details>
  <summary>Details</summary>
Motivation: LiDAR-scanned data often suffer from extreme sparsity that hinders 3D perception tasks, and existing point cloud upsampling methods focus on individual objects with limited generalization to complex outdoor scenes.

Method: Uses classifier-free guidance-based DDPMs with sparse point cloud as guiding condition and synthesized nearby frames as input. Includes voxel completion module to refine features and point-voxel interaction module to integrate point and voxel features.

Result: Extensive experiments on various benchmarks demonstrate state-of-the-art performance. The method is the first scene-level point cloud upsampling approach supporting arbitrary upsampling rates.

Conclusion: PVNet effectively addresses LiDAR point cloud sparsity in outdoor environments through a novel diffusion-based framework with point-voxel interaction, achieving superior performance for scene-level upsampling.

Abstract: Accurate 3D scene understanding in outdoor environments heavily relies on
high-quality point clouds. However, LiDAR-scanned data often suffer from
extreme sparsity, severely hindering downstream 3D perception tasks. Existing
point cloud upsampling methods primarily focus on individual objects, thus
demonstrating limited generalization capability for complex outdoor scenes. To
address this issue, we propose PVNet, a diffusion model-based point-voxel
interaction framework to perform LiDAR point cloud upsampling without dense
supervision. Specifically, we adopt the classifier-free guidance-based DDPMs to
guide the generation, in which we employ a sparse point cloud as the guiding
condition and the synthesized point clouds derived from its nearby frames as
the input. Moreover, we design a voxel completion module to refine and complete
the coarse voxel features for enriching the feature representation. In
addition, we propose a point-voxel interaction module to integrate features
from both points and voxels, which efficiently improves the environmental
perception capability of each upsampled point. To the best of our knowledge,
our approach is the first scene-level point cloud upsampling method supporting
arbitrary upsampling rates. Extensive experiments on various benchmarks
demonstrate that our method achieves state-of-the-art performance. The source
code will be available at https://github.com/chengxianjing/PVNet.

</details>


### [164] [DeltaFlow: An Efficient Multi-frame Scene Flow Estimation Method](https://arxiv.org/abs/2508.17054)
*Qingwen Zhang,Xiaomeng Zhu,Yushan Zhang,Yixi Cai,Olov Andersson,Patric Jensfelt*

Main category: cs.CV

TL;DR: DeltaFlow is a lightweight 3D scene flow estimation framework that efficiently captures temporal information across multiple frames with minimal computational cost, achieving state-of-the-art performance with 22% lower error and 2x faster inference.


<details>
  <summary>Details</summary>
Motivation: Previous scene flow methods focus on two consecutive frames, neglecting valuable temporal information. Multi-frame approaches suffer from rapidly escalating computational costs as frame count increases.

Method: Proposes DeltaFlow framework with a Î scheme for efficient temporal feature extraction, Category-Balanced Loss for underrepresented classes, and Instance Consistency Loss for coherent object motion.

Result: Achieves state-of-the-art performance on Argoverse 2 and Waymo datasets with 22% lower error and 2x faster inference compared to next-best multi-frame method, plus strong cross-domain generalization.

Conclusion: DeltaFlow successfully addresses computational efficiency and accuracy challenges in multi-frame scene flow estimation through innovative temporal feature extraction and specialized loss functions.

Abstract: Previous dominant methods for scene flow estimation focus mainly on input
from two consecutive frames, neglecting valuable information in the temporal
domain. While recent trends shift towards multi-frame reasoning, they suffer
from rapidly escalating computational costs as the number of frames grows. To
leverage temporal information more efficiently, we propose DeltaFlow
($\Delta$Flow), a lightweight 3D framework that captures motion cues via a
$\Delta$ scheme, extracting temporal features with minimal computational cost,
regardless of the number of frames. Additionally, scene flow estimation faces
challenges such as imbalanced object class distributions and motion
inconsistency. To tackle these issues, we introduce a Category-Balanced Loss to
enhance learning across underrepresented classes and an Instance Consistency
Loss to enforce coherent object motion, improving flow accuracy. Extensive
evaluations on the Argoverse 2 and Waymo datasets show that $\Delta$Flow
achieves state-of-the-art performance with up to 22% lower error and $2\times$
faster inference compared to the next-best multi-frame supervised method, while
also demonstrating a strong cross-domain generalization ability. The code is
open-sourced at https://github.com/Kin-Zhang/DeltaFlow along with trained model
weights.

</details>


### [165] [REGEN: Real-Time Photorealism Enhancement in Games via a Dual-Stage Generative Network Framework](https://arxiv.org/abs/2508.17061)
*Stefanos Pasios,Nikos Nikolaidis*

Main category: cs.CV

TL;DR: REGEN framework uses dual-stage generative adversarial networks to enhance video game photorealism in real-time, achieving 32x faster inference while maintaining visual quality comparable to robust unpaired methods.


<details>
  <summary>Details</summary>
Motivation: Achieving true photorealism in dynamic game environments at real-time frame rates remains challenging due to the tradeoff between visual quality and performance, despite recent hardware and rendering advancements.

Method: Proposes REGEN framework with dual-stage generative network that transforms unpaired image-to-image translation into a simpler paired task, enabling lightweight training and real-time inference without compromising quality.

Result: Demonstrated on GTA V, achieves visual results comparable to robust unpaired methods while improving inference speed by 32.14 times, outperforming directly trained lightweight unpaired methods.

Conclusion: The REGEN framework successfully addresses the photorealism-performance tradeoff in games, providing real-time photorealistic enhancement with significantly improved inference speed while maintaining high visual quality.

Abstract: Photorealism is an important aspect of modern video games since it can shape
the player experience and simultaneously impact the immersion, narrative
engagement, and visual fidelity. Although recent hardware technological
breakthroughs, along with state-of-the-art rendering technologies, have
significantly improved the visual realism of video games, achieving true
photorealism in dynamic environments at real-time frame rates still remains a
major challenge due to the tradeoff between visual quality and performance. In
this short paper, we present a novel approach for enhancing the photorealism of
rendered game frames using generative adversarial networks. To this end, we
propose Real-time photorealism Enhancement in Games via a dual-stage gEnerative
Network framework (REGEN), which employs a robust unpaired image-to-image
translation model to produce semantically consistent photorealistic frames that
transform the problem into a simpler paired image-to-image translation task.
This enables training with a lightweight method that can achieve real-time
inference time without compromising visual quality. We demonstrate the
effectiveness of our framework on Grand Theft Auto V, showing that the approach
achieves visual results comparable to the ones produced by the robust unpaired
Im2Im method while improving inference speed by 32.14 times. Our findings also
indicate that the results outperform the photorealism-enhanced frames produced
by directly training a lightweight unpaired Im2Im translation method to
translate the video game frames towards the visual characteristics of
real-world images. Code, pre-trained models, and demos for this work are
available at: https://github.com/stefanos50/REGEN.

</details>


### [166] [SSG-Dit: A Spatial Signal Guided Framework for Controllable Video Generation](https://arxiv.org/abs/2508.17062)
*Peng Hu,Yu Gu,Liang Luo,Fuji Ren*

Main category: cs.CV

TL;DR: SSG-DiT is a novel framework that uses spatial signal prompting and a dual-branch attention mechanism to improve semantic consistency in controllable video generation, achieving state-of-the-art performance on VBench metrics.


<details>
  <summary>Details</summary>
Motivation: Existing video generation models struggle with maintaining semantic consistency and often deviate from nuanced prompt details, creating a need for better alignment between user conditions and generated content.

Method: A decoupled two-stage process: 1) Spatial Signal Prompting generates visual prompts using pre-trained multi-modal models, 2) SSG-Adapter with dual-branch attention injects joint conditions into a frozen video DiT backbone.

Result: SSG-DiT achieves state-of-the-art performance on VBench benchmark, particularly excelling in spatial relationship control and overall consistency metrics.

Conclusion: The proposed SSG-DiT framework effectively addresses semantic consistency challenges in controllable video generation through spatial signal guidance and parameter-efficient adaptation.

Abstract: Controllable video generation aims to synthesize video content that aligns
precisely with user-provided conditions, such as text descriptions and initial
images. However, a significant challenge persists in this domain: existing
models often struggle to maintain strong semantic consistency, frequently
generating videos that deviate from the nuanced details specified in the
prompts. To address this issue, we propose SSG-DiT (Spatial Signal Guided
Diffusion Transformer), a novel and efficient framework for high-fidelity
controllable video generation. Our approach introduces a decoupled two-stage
process. The first stage, Spatial Signal Prompting, generates a spatially aware
visual prompt by leveraging the rich internal representations of a pre-trained
multi-modal model. This prompt, combined with the original text, forms a joint
condition that is then injected into a frozen video DiT backbone via our
lightweight and parameter-efficient SSG-Adapter. This unique design, featuring
a dual-branch attention mechanism, allows the model to simultaneously harness
its powerful generative priors while being precisely steered by external
spatial signals. Extensive experiments demonstrate that SSG-DiT achieves
state-of-the-art performance, outperforming existing models on multiple key
metrics in the VBench benchmark, particularly in spatial relationship control
and overall consistency.

</details>


### [167] [Proximal Vision Transformer: Enhancing Feature Representation through Two-Stage Manifold Geometry](https://arxiv.org/abs/2508.17081)
*Haoyu Yun,Hamid Krim*

Main category: cs.CV

TL;DR: Proposes integrating Vision Transformer with proximal tools to enable global geometric optimization, overcoming ViT's limitation of only modeling local relationships within individual images.


<details>
  <summary>Details</summary>
Motivation: ViT's optimization is confined to modeling local relationships within images, limiting its ability to capture global geometric relationships between data points.

Method: Integrates ViT with proximal tools where ViT constructs tangent bundle through self-attention (each head = tangent space), then uses proximal iterations to define sections and project data from tangent spaces to base space for global feature alignment.

Result: Experimental results show the proposed method outperforms traditional ViT in classification accuracy and data distribution.

Conclusion: The framework successfully enhances ViT's feature representation and classification performance by enabling unified geometric optimization through proximal tools integration.

Abstract: The Vision Transformer (ViT) architecture has become widely recognized in
computer vision, leveraging its self-attention mechanism to achieve remarkable
success across various tasks. Despite its strengths, ViT's optimization remains
confined to modeling local relationships within individual images, limiting its
ability to capture the global geometric relationships between data points. To
address this limitation, this paper proposes a novel framework that integrates
ViT with the proximal tools, enabling a unified geometric optimization approach
to enhance feature representation and classification performance. In this
framework, ViT constructs the tangent bundle of the manifold through its
self-attention mechanism, where each attention head corresponds to a tangent
space, offering geometric representations from diverse local perspectives.
Proximal iterations are then introduced to define sections within the tangent
bundle and project data from tangent spaces onto the base space, achieving
global feature alignment and optimization. Experimental results confirm that
the proposed method outperforms traditional ViT in terms of classification
accuracy and data distribution.

</details>


### [168] [PD-Loss: Proxy-Decidability for Efficient Metric Learning](https://arxiv.org/abs/2508.17082)
*Pedro Silva,Guilherme A. L. Silva,Pablo Coelho,Vander Freitas,Gladston Moreira,David Menotii,Eduardo Luz*

Main category: cs.CV

TL;DR: PD-Loss combines proxy-based efficiency with distribution-aware optimization using the decidability index, achieving state-of-the-art performance without large mini-batch requirements.


<details>
  <summary>Details</summary>
Motivation: Existing DML methods face challenges: pairwise losses have complex sampling and slow convergence, proxy-based methods lack global distribution optimization, and D-Loss requires computationally expensive large mini-batches.

Method: Integrates learnable proxies with the statistical framework of decidability index (d') to estimate genuine and impostor distributions, combining proxy efficiency with principled separability optimization.

Result: Achieves performance comparable to state-of-the-art methods across fine-grained classification and face verification tasks, while being computationally efficient.

Conclusion: PD-Loss offers a scalable, distribution-aware approach to deep metric learning with potential for broader applications in embedding optimization.

Abstract: Deep Metric Learning (DML) aims to learn embedding functions that map
semantically similar inputs to proximate points in a metric space while
separating dissimilar ones. Existing methods, such as pairwise losses, are
hindered by complex sampling requirements and slow convergence. In contrast,
proxy-based losses, despite their improved scalability, often fail to optimize
global distribution properties. The Decidability-based Loss (D-Loss) addresses
this by targeting the decidability index (d') to enhance distribution
separability, but its reliance on large mini-batches imposes significant
computational constraints. We introduce Proxy-Decidability Loss (PD-Loss), a
novel objective that integrates learnable proxies with the statistical
framework of d' to optimize embedding spaces efficiently. By estimating genuine
and impostor distributions through proxies, PD-Loss combines the computational
efficiency of proxy-based methods with the principled separability of D-Loss,
offering a scalable approach to distribution-aware DML. Experiments across
various tasks, including fine-grained classification and face verification,
demonstrate that PD-Loss achieves performance comparable to that of
state-of-the-art methods while introducing a new perspective on embedding
optimization, with potential for broader applications.

</details>


### [169] [GRASP: Geospatial pixel Reasoning viA Structured Policy learning](https://arxiv.org/abs/2508.17102)
*Chengjie Jiang,Yunqi Zhou,Jiafeng Yan,Jing Li*

Main category: cs.CV

TL;DR: GRASP is a reinforcement learning framework for geospatial pixel reasoning that uses MLLM-generated bounding boxes and points as prompts for segmentation, achieving state-of-the-art results without mask supervision.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM-based systems require expensive dense pixel supervision and perform poorly on out-of-domain data. The authors aim to create a more efficient and generalizable approach that leverages foundation model priors.

Method: A multimodal LLM generates task-relevant bounding boxes and positive points from vision-language instructions. These outputs are used as prompts for a pre-trained segmentation model to generate final masks. The system is optimized purely with reinforcement learning (GRPO) using format and accuracy rewards on boxes/points, eliminating the need for mask supervision.

Result: Achieves about 4% improvement in-domain and up to 54% improvement on out-of-domain benchmarks compared to state-of-the-art methods. Demonstrates robust generalization and shows complex segmentation behaviors can be learned via RL from weak spatial cues.

Conclusion: GRASP provides an effective framework for geospatial pixel reasoning that minimizes trainable parameters, enables learning from inexpensive annotations, and achieves superior generalization performance through structured policy learning with reinforcement learning.

Abstract: Geospatial pixel reasoning is a nascent remote-sensing task that aims to
generate segmentation masks directly from natural-language instructions.
Prevailing MLLM-based systems co-train a language model and a mask decoder with
dense pixel supervision, which is expensive and often weak on out-of-domain
(OOD) data. We introduce GRASP, a structured policy-learning framework. In our
design, a multimodal large language model first emits task-relevant bounding
boxes and positive points from a vision-language instruction. These outputs are
then passed to a pre-trained segmentation model, which consumes them as prompts
to generate the final mask. Instead of supervised fine-tuning, we optimize the
system purely with reinforcement learning: the model is trained solely with
GRPO, guided by format rewards and accuracy rewards computed on boxes and
points (no mask supervision). This leverages strong priors in foundation
models, minimizes trainable parameters, and enables learning from inexpensive
annotations. We additionally curate GRASP-1k, which contains
reasoning-intensive queries, detailed reasoning traces, and fine-grained
segmentation annotations. Evaluations on both in-domain and out-of-domain test
sets show state-of-the-art results: about 4% improvement in-domain and up to
54% on OOD benchmarks. The experiment results evidence our model's robust
generalization and demonstrate that complex geospatial segmentation behaviors
can be learned via RL from weak spatial cues. Code and the dataset will be
released open-source.

</details>


### [170] [SugarcaneShuffleNet: A Very Fast, Lightweight Convolutional Neural Network for Diagnosis of 15 Sugarcane Leaf Diseases](https://arxiv.org/abs/2508.17107)
*Shifat E. Arman,Hasan Muhammad Abdullah,Syed Nazmus Sakib,RM Saiem,Shamima Nasrin Asha,Md Mehedi Hasan,Shahrear Bin Amin,S M Mahin Abrar*

Main category: cs.CV

TL;DR: SugarcaneLD-BD dataset and SugarcaneShuffleNet model provide lightweight, efficient disease diagnosis for sugarcane farmers in low-resource regions, achieving 98% accuracy with fast inference times.


<details>
  <summary>Details</summary>
Motivation: Address the lack of scalable, efficient, and interpretable AI tools for sugarcane leaf disease diagnosis in low-resource regions where deep learning models often fail to generalize and require substantial computational resources.

Method: Created SugarcaneLD-BD dataset with 638 curated images across 5 classes, combined with additional datasets. Developed SugarcaneShuffleNet lightweight model optimized for on-device use, compared against other CNNs (MnasNet, EdgeNeXt, EfficientNet-Lite, MobileNet, SqueezeNet) using transfer learning and Bayesian optimization.

Result: SugarcaneShuffleNet achieved 98.02% accuracy, 0.98 F1-score, with only 9.26 MB size and 4.14 ms average inference time per image. Outperformed other models in efficiency while maintaining comparable accuracy.

Conclusion: The integrated solution (dataset + lightweight model + web app) provides a practical, efficient tool for real-time sugarcane disease classification in resource-constrained environments, with Grad-CAM explanations for interpretability.

Abstract: Despite progress in AI-based plant diagnostics, sugarcane farmers in
low-resource regions remain vulnerable to leaf diseases due to the lack of
scalable, efficient, and interpretable tools. Many deep learning models fail to
generalize under real-world conditions and require substantial computational
resources, limiting their use in resource-constrained regions. In this paper,
we present SugarcaneLD-BD, a curated dataset for sugarcane leaf-disease
classification; SugarcaneShuffleNet, an optimized lightweight model for rapid
on-device diagnosis; and SugarcaneAI, a Progressive Web Application for field
deployment. SugarcaneLD-BD contains 638 curated images across five classes,
including four major sugarcane diseases, collected in Bangladesh under diverse
field conditions and verified by expert pathologists. To enhance diversity, we
combined SugarcaneLD-BD with two additional datasets, yielding a larger and
more representative corpus. Our optimized model, SugarcaneShuffleNet, offers
the best trade-off between speed and accuracy for real-time, on-device
diagnosis. This 9.26 MB model achieved 98.02% accuracy, an F1-score of 0.98,
and an average inference time of 4.14 ms per image. For comparison, we
fine-tuned five other lightweight convolutional neural networks: MnasNet,
EdgeNeXt, EfficientNet-Lite, MobileNet, and SqueezeNet via transfer learning
and Bayesian optimization. MnasNet and EdgeNeXt achieved comparable accuracy to
SugarcaneShuffleNet, but required significantly more parameters, memory, and
computation, limiting their suitability for low-resource deployment. We
integrate SugarcaneShuffleNet into SugarcaneAI, delivering Grad-CAM-based
explanations in the field. Together, these contributions offer a diverse
benchmark, efficient models for low-resource environments, and a practical tool
for sugarcane disease classification. It spans varied lighting, backgrounds and
devices used on-farm

</details>


### [171] [PlantVillageVQA: A Visual Question Answering Dataset for Benchmarking Vision-Language Models in Plant Science](https://arxiv.org/abs/2508.17117)
*Syed Nazmus Sakib,Nafiul Haque,Mohammad Zabed Hossain,Shifat E. Arman*

Main category: cs.CV

TL;DR: PlantVillageVQA is a large-scale visual question answering dataset for agricultural applications, containing 193,609 QA pairs across 55,448 images covering 14 crop species and 38 diseases, with expert-verified content and structured complexity levels.


<details>
  <summary>Details</summary>
Motivation: To advance vision-language models for agricultural decision-making by providing a standardized, expert-verified dataset for plant disease identification and analysis.

Method: Created through a two-stage pipeline: (1) template-based QA synthesis from image metadata and (2) multi-stage linguistic re-engineering, followed by iterative expert review for scientific accuracy.

Result: A comprehensive dataset with 193,609 high-quality QA pairs organized into 3 cognitive complexity levels and 9 categories, evaluated using three state-of-the-art models.

Conclusion: Provides a publicly available, standardized database to enhance diagnostic accuracy for plant disease identification and advance agricultural research.

Abstract: PlantVillageVQA is a large-scale visual question answering (VQA) dataset
derived from the widely used PlantVillage image corpus. It was designed to
advance the development and evaluation of vision-language models for
agricultural decision-making and analysis. The PlantVillageVQA dataset
comprises 193,609 high-quality question-answer (QA) pairs grounded over 55,448
images spanning 14 crop species and 38 disease conditions. Questions are
organised into 3 levels of cognitive complexity and 9 distinct categories. Each
question category was phrased manually following expert guidance and generated
via an automated two-stage pipeline: (1) template-based QA synthesis from image
metadata and (2) multi-stage linguistic re-engineering. The dataset was
iteratively reviewed by domain experts for scientific accuracy and relevancy.
The final dataset was evaluated using three state-of-the-art models for quality
assessment. Our objective remains to provide a publicly available, standardised
and expert-verified database to enhance diagnostic accuracy for plant disease
identifications and advance scientific research in the agricultural domain. Our
dataset will be open-sourced at
https://huggingface.co/datasets/SyedNazmusSakib/PlantVillageVQA.

</details>


### [172] [CE-RS-SBCIT A Novel Channel Enhanced Hybrid CNN Transformer with Residual, Spatial, and Boundary-Aware Learning for Brain Tumor MRI Analysis](https://arxiv.org/abs/2508.17128)
*Mirza Mumtaz Zahoor,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: A novel hybrid framework CE-RS-SBCIT combines CNNs and Transformers for brain tumor classification from MRI, achieving state-of-the-art performance with 98.30% accuracy.


<details>
  <summary>Details</summary>
Motivation: Brain tumors are lethal diseases requiring early detection. Existing deep learning methods (CNNs and Transformers) face challenges with computational cost, sensitivity to minor contrast variations, and structural/texture inconsistencies in MRI data.

Method: Hybrid framework integrating residual/spatial learning CNNs with transformer modules. Four innovations: 1) Smoothing and boundary-based CNN-integrated Transformer (SBCIT), 2) Tailored residual and spatial learning CNNs, 3) Channel enhancement strategy, 4) Novel spatial attention mechanism. Uses stem convolution, contextual interaction transformer blocks, and auxiliary transfer-learned feature maps.

Result: Achieved 98.30% accuracy, 98.08% sensitivity, 98.25% F1-score, and 98.43% precision on challenging MRI datasets (Kaggle and Figshare) covering glioma, meningioma, pituitary tumors, and healthy controls.

Conclusion: The CE-RS-SBCIT framework effectively addresses limitations of conventional methods by combining local fine-grained and global contextual features, demonstrating superior performance in brain tumor classification from MRI data.

Abstract: Brain tumors remain among the most lethal human diseases, where early
detection and accurate classification are critical for effective diagnosis and
treatment planning. Although deep learning-based computer-aided diagnostic
(CADx) systems have shown remarkable progress. However, conventional
convolutional neural networks (CNNs) and Transformers face persistent
challenges, including high computational cost, sensitivity to minor contrast
variations, structural heterogeneity, and texture inconsistencies in MRI data.
Therefore, a novel hybrid framework, CE-RS-SBCIT, is introduced, integrating
residual and spatial learning-based CNNs with transformer-driven modules. The
proposed framework exploits local fine-grained and global contextual cues
through four core innovations: (i) a smoothing and boundary-based
CNN-integrated Transformer (SBCIT), (ii) tailored residual and spatial learning
CNNs, (iii) a channel enhancement (CE) strategy, and (iv) a novel spatial
attention mechanism. The developed SBCIT employs stem convolution and
contextual interaction transformer blocks with systematic smoothing and
boundary operations, enabling efficient global feature modeling. Moreover,
Residual and spatial CNNs, enhanced by auxiliary transfer-learned feature maps,
enrich the representation space, while the CE module amplifies discriminative
channels and mitigates redundancy. Furthermore, the spatial attention mechanism
selectively emphasizes subtle contrast and textural variations across tumor
classes. Extensive evaluation on challenging MRI datasets from Kaggle and
Figshare, encompassing glioma, meningioma, pituitary tumors, and healthy
controls, demonstrates superior performance, achieving 98.30% accuracy, 98.08%
sensitivity, 98.25% F1-score, and 98.43% precision.

</details>


### [173] [Structural Damage Detection Using AI Super Resolution and Visual Language Model](https://arxiv.org/abs/2508.17130)
*Catherine Hoier,Khandaker Mamun Ahmed*

Main category: cs.CV

TL;DR: A novel AI framework using drone footage, video super-resolution (VRT), and visual language model (Gemma3:27b) for automated disaster damage assessment with 84.5% accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional damage assessment methods are labor-intensive, costly, and hazardous, making them impractical for rapid response in resource-limited disaster settings.

Method: Integrated system combining aerial drone footage, Video Restoration Transformer (VRT) for super-resolution, and Gemma3:27b VLM to improve low-resolution footage and classify building damage into four categories with risk levels.

Result: Achieved 84.5% classification accuracy using data from 2023 Turkey earthquakes and 2013 Moore Tornado, demonstrating highly accurate automated damage assessment.

Conclusion: The framework provides cost-effective, accessible disaster damage assessment that enables non-technical users to perform preliminary analyses, improving responsiveness and efficiency of disaster management.

Abstract: Natural disasters pose significant challenges to timely and accurate damage
assessment due to their sudden onset and the extensive areas they affect.
Traditional assessment methods are often labor-intensive, costly, and hazardous
to personnel, making them impractical for rapid response, especially in
resource-limited settings. This study proposes a novel, cost-effective
framework that leverages aerial drone footage, an advanced AI-based video
super-resolution model, Video Restoration Transformer (VRT), and Gemma3:27b, a
27 billion parameter Visual Language Model (VLM). This integrated system is
designed to improve low-resolution disaster footage, identify structural
damage, and classify buildings into four damage categories, ranging from
no/slight damage to total destruction, along with associated risk levels. The
methodology was validated using pre- and post-event drone imagery from the 2023
Turkey earthquakes (courtesy of The Guardian) and satellite data from the 2013
Moore Tornado (xBD dataset). The framework achieved a classification accuracy
of 84.5%, demonstrating its ability to provide highly accurate results.
Furthermore, the system's accessibility allows non-technical users to perform
preliminary analyses, thereby improving the responsiveness and efficiency of
disaster management efforts.

</details>


### [174] [Beyond Play and Pause: Turning GPT-4o Spatial Weakness into a Strength for In-Depth Interactive Video Learning](https://arxiv.org/abs/2508.17160)
*Sajad Goudarzi,Samaneh Zamanifard*

Main category: cs.CV

TL;DR: Untwist is an AI system that enables interactive video learning by allowing users to ask questions about specific video regions using bounding boxes, providing context-aware multimodal responses through GPT and computer vision integration.


<details>
  <summary>Details</summary>
Motivation: Traditional video learning is passive, and current AI tools lack real-time, region-specific interaction capabilities, limiting user engagement and comprehension.

Method: Integrates GPT APIs with computer vision techniques, using annotated frames instead of raw coordinates to overcome GPT-4o's spatial limitations, with video pre-processing and real-time interaction architecture.

Result: Significantly improves accuracy in localizing and interpreting video content compared to using raw coordinate data with GPT-4o.

Conclusion: Untwist transforms passive video consumption into interactive AI-driven learning, enhancing engagement and comprehension through region-specific multimodal interactions.

Abstract: Traditional video-based learning remains passive, offering limited
opportunities for users to engage dynamically with content. While current
AI-powered tools offer transcription and summarization, they lack real-time,
region-specific interaction capabilities. This paper introduces Untwist, an
AI-driven system that enables interactive video learning by allowing users to
ask questions about the entire video or specific regions using a bounding box,
receiving context-aware, multimodal responses. By integrating GPT APIs with
Computer Vision techniques, Untwist extracts, processes, and structures video
content to enhance comprehension. Our approach addresses GPT-4o spatial
weakness by leveraging annotated frames instead of raw coordinate data,
significantly improving accuracy in localizing and interpreting video content.
This paper describes the system architecture, including video pre-processing
and real-time interaction, and outlines how Untwist can transform passive video
consumption into an interactive, AI-driven learning experience with the
potential to enhance engagement and comprehension.

</details>


### [175] [Development of an isotropic segmentation model for medial temporal lobe subregions on anisotropic MRI atlas using implicit neural representation](https://arxiv.org/abs/2508.17171)
*Yue Li,Pulkit Khandelwal,Rohit Jena,Long Xie,Michael Duong,Amanda E. Denning,Christopher A. Brown,Laura E. M. Wisse,Sandhitsu R. Das,David A. Wolk,Paul A. Yushkevich*

Main category: cs.CV

TL;DR: Used implicit neural representation to combine T1w and T2w MRI advantages for isotropic MTL subregion segmentation, improving Alzheimer's biomarker accuracy without extra annotation work.


<details>
  <summary>Details</summary>
Motivation: MRI imaging biomarkers are crucial for Alzheimer's diagnosis, but anisotropic resolution in T2w MRI makes accurate cortical thickness extraction difficult in medial temporal lobe subregions where AD first appears.

Method: Combined T1w and T2w MRI resolution advantages using implicit neural representation to upsample MTL subregion atlas from anisotropic to isotropic space, then developed isotropic segmentation model.

Result: Isotropic model showed higher significance in distinguishing mild cognitive impairment from cognitively unimpaired participants, and greater biomarker stability in longitudinal analysis of CU participants.

Conclusion: Improved AD imaging biomarker accuracy without increasing annotation workload, enabling more precise quantification of AD-brain atrophy relationship and better disease tracking measures.

Abstract: Imaging biomarkers in magnetic resonance imaging (MRI) are important tools
for diagnosing and tracking Alzheimer's disease (AD). As medial temporal lobe
(MTL) is the earliest region to show AD-related hallmarks, brain atrophy caused
by AD can first be observed in the MTL. Accurate segmentation of MTL subregions
and extraction of imaging biomarkers from them are important. However, due to
imaging limitations, the resolution of T2-weighted (T2w) MRI is anisotropic,
which makes it difficult to accurately extract the thickness of cortical
subregions in the MTL. In this study, we used an implicit neural representation
method to combine the resolution advantages of T1-weighted and T2w MRI to
accurately upsample an MTL subregion atlas set from anisotropic space to
isotropic space, establishing a multi-modality, high-resolution atlas set.
Based on this atlas, we developed an isotropic MTL subregion segmentation
model. In an independent test set, the cortical subregion thickness extracted
using this isotropic model showed higher significance than an anisotropic
method in distinguishing between participants with mild cognitive impairment
and cognitively unimpaired (CU) participants. In longitudinal analysis, the
biomarkers extracted using isotropic method showed greater stability in CU
participants. This study improved the accuracy of AD imaging biomarkers without
increasing the amount of atlas annotation work, which may help to more
accurately quantify the relationship between AD and brain atrophy and provide
more accurate measures for disease tracking.

</details>


### [176] [VROOM - Visual Reconstruction over Onboard Multiview](https://arxiv.org/abs/2508.17172)
*Yajat Yadav,Varun Bharadwaj,Jathin Korrapati,Tanish Baranwal*

Main category: cs.CV

TL;DR: VROOM is a system that reconstructs 3D models of Formula 1 circuits using only onboard camera footage from racecars, addressing challenges like high-speed motion and camera cuts through a pipeline combining SLAM methods and preprocessing techniques.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the feasibility of using onboard video footage from high-speed racecars for scalable 4D reconstruction in real-world settings, particularly for complex environments like Formula 1 circuits.

Method: The pipeline analyzes DROID-SLAM, AnyCam, and Monst3r methods, combined with preprocessing techniques including different masking approaches, temporal chunking, and resolution scaling to handle dynamic motion and computational constraints.

Result: VROOM successfully partially recovers track and vehicle trajectories in complex environments using footage from the 2023 Monaco Grand Prix.

Conclusion: The findings indicate that onboard video footage can be effectively used for scalable 4D reconstruction, opening possibilities for real-world applications in motorsports and similar high-speed environments.

Abstract: We introduce VROOM, a system for reconstructing 3D models of Formula 1
circuits using only onboard camera footage from racecars. Leveraging video data
from the 2023 Monaco Grand Prix, we address video challenges such as high-speed
motion and sharp cuts in camera frames. Our pipeline analyzes different methods
such as DROID-SLAM, AnyCam, and Monst3r and combines preprocessing techniques
such as different methods of masking, temporal chunking, and resolution scaling
to account for dynamic motion and computational constraints. We show that Vroom
is able to partially recover track and vehicle trajectories in complex
environments. These findings indicate the feasibility of using onboard video
for scalable 4D reconstruction in real-world settings. The project page can be
found at https://varun-bharadwaj.github.io/vroom, and our code is available at
https://github.com/yajatyadav/vroom.

</details>


### [177] [Advancing Weakly-Supervised Change Detection in Satellite Images via Adversarial Class Prompting](https://arxiv.org/abs/2508.17186)
*Zhenghui Zhao,Chen Wu,Di Wang,Hongruixuan Chen,Cuiqun Chen,Zhuo Zheng,Bo Du,Liangpei Zhang*

Main category: cs.CV

TL;DR: AdvCP method uses adversarial prompting to address background noise misclassification in weakly-supervised change detection, improving performance without extra inference cost.


<details>
  <summary>Details</summary>
Motivation: Weakly-supervised change detection methods often misclassify background variations as object changes due to limited image-level supervision, especially in complex remote-sensing scenarios.

Method: Two-phase approach: 1) Adversarial Prompt Mining - uses incorrect labels to activate erroneous feature mappings and identify misclassified background variations; 2) Adversarial Sample Rectification - integrates these samples via online global prototype built from current and historical data.

Result: Significant performance enhancements demonstrated on ConvNet, Transformer, and SAM-based baselines. Method shows generalizability to other multi-class weakly-supervised dense prediction scenarios.

Conclusion: AdvCP effectively addresses co-occurring noise problem in WSCD, can be seamlessly integrated into existing methods without additional inference overhead, and shows broad applicability across different architectures and scenarios.

Abstract: Weakly-Supervised Change Detection (WSCD) aims to distinguish specific object
changes (e.g., objects appearing or disappearing) from background variations
(e.g., environmental changes due to light, weather, or seasonal shifts) in
paired satellite images, relying only on paired image (i.e., image-level)
classification labels. This technique significantly reduces the need for dense
annotations required in fully-supervised change detection. However, as
image-level supervision only indicates whether objects have changed in a scene,
WSCD methods often misclassify background variations as object changes,
especially in complex remote-sensing scenarios. In this work, we propose an
Adversarial Class Prompting (AdvCP) method to address this co-occurring noise
problem, including two phases: a) Adversarial Prompt Mining: After each
training iteration, we introduce adversarial prompting perturbations, using
incorrect one-hot image-level labels to activate erroneous feature mappings.
This process reveals co-occurring adversarial samples under weak supervision,
namely background variation features that are likely to be misclassified as
object changes. b) Adversarial Sample Rectification: We integrate these
adversarially prompt-activated pixel samples into training by constructing an
online global prototype. This prototype is built from an exponentially weighted
moving average of the current batch and all historical training data. Our AdvCP
can be seamlessly integrated into current WSCD methods without adding
additional inference cost. Experiments on ConvNet, Transformer, and Segment
Anything Model (SAM)-based baselines demonstrate significant performance
enhancements. Furthermore, we demonstrate the generalizability of AdvCP to
other multi-class weakly-supervised dense prediction scenarios. Code is
available at https://github.com/zhenghuizhao/AdvCP

</details>


### [178] [MMCIG: Multimodal Cover Image Generation for Text-only Documents and Its Dataset Construction via Pseudo-labeling](https://arxiv.org/abs/2508.17199)
*Hyeyeon Kim,Sungwoo Han,Jingun Kwon,Hidetaka Kamigaito,Manabu Okumura*

Main category: cs.CV

TL;DR: Novel multimodal pseudo-labeling method for cover image generation that produces both summary and corresponding image from text documents, outperforming text-only and image-only approaches.


<details>
  <summary>Details</summary>
Motivation: No existing datasets available for the cover image generation task that requires producing both a summary and visually corresponding image from text-only documents.

Method: Multimodal pseudo-labeling approach that collects documents with multiple images and captions, excludes factually inconsistent instances, ranks images and captions independently using gold summaries, and selects images only when both image and caption rank first. Also removes documents with direct image references.

Result: The proposed multimodal pseudo-labeling method constructs more precise datasets and generates higher quality images than text-only and image-only pseudo-labeling methods.

Conclusion: Multimodal approach considering both images and captions together is more effective for cover image generation task than separate consideration of text or image components alone.

Abstract: In this study, we introduce a novel cover image generation task that produces
both a concise summary and a visually corresponding image from a given
text-only document. Because no existing datasets are available for this task,
we propose a multimodal pseudo-labeling method to construct high-quality
datasets at low cost. We first collect documents that contain multiple images
with their captions, and their summaries by excluding factually inconsistent
instances. Our approach selects one image from the multiple images accompanying
the documents. Using the gold summary, we independently rank both the images
and their captions. Then, we annotate a pseudo-label for an image when both the
image and its corresponding caption are ranked first in their respective
rankings. Finally, we remove documents that contain direct image references
within texts. Experimental results demonstrate that the proposed multimodal
pseudo-labeling method constructs more precise datasets and generates higher
quality images than text- and image-only pseudo-labeling methods, which
consider captions and images separately. We release our code at:
https://github.com/HyeyeeonKim/MMCIG

</details>


### [179] [Multi-Agent Visual-Language Reasoning for Comprehensive Highway Scene Understanding](https://arxiv.org/abs/2508.17205)
*Yunxiang Yang,Ningning Xu,Jidong J. Yang*

Main category: cs.CV

TL;DR: A multi-agent framework using mixture-of-experts strategy with large VLM for prompt generation and smaller VLM for reasoning, achieving robust highway scene understanding across weather classification, pavement wetness assessment, and traffic congestion detection.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive highway scene understanding system that balances accuracy and computational efficiency while addressing multiple critical perception tasks simultaneously in diverse traffic and environmental conditions.

Method: Uses a mixture-of-experts strategy where a large VLM (e.g., GPT-4o) generates task-specific chain-of-thought prompts, which guide a smaller efficient VLM (e.g., Qwen2.5-VL-7B) for reasoning over short videos with complementary multimodal data.

Result: Achieves consistently strong performance across diverse conditions, validated on three specialized datasets including a multimodal pavement wetness dataset combining video streams with road weather sensor data.

Conclusion: The framework provides robust multi-task reasoning, can integrate with existing traffic camera systems, and enables continuous monitoring of high-risk locations for enhanced situational awareness and timely alerts in resource-constrained environments.

Abstract: This paper introduces a multi-agent framework for comprehensive highway scene
understanding, designed around a mixture-of-experts strategy. In this
framework, a large generic vision-language model (VLM), such as GPT-4o, is
contextualized with domain knowledge to generates task-specific
chain-of-thought (CoT) prompts. These fine-grained prompts are then used to
guide a smaller, efficient VLM (e.g., Qwen2.5-VL-7B) in reasoning over short
videos, along with complementary modalities as applicable. The framework
simultaneously addresses multiple critical perception tasks, including weather
classification, pavement wetness assessment, and traffic congestion detection,
achieving robust multi-task reasoning while balancing accuracy and
computational efficiency. To support empirical validation, we curated three
specialized datasets aligned with these tasks. Notably, the pavement wetness
dataset is multimodal, combining video streams with road weather sensor data,
highlighting the benefits of multimodal reasoning. Experimental results
demonstrate consistently strong performance across diverse traffic and
environmental conditions. From a deployment perspective, the framework can be
readily integrated with existing traffic camera systems and strategically
applied to high-risk rural locations, such as sharp curves, flood-prone
lowlands, or icy bridges. By continuously monitoring the targeted sites, the
system enhances situational awareness and delivers timely alerts, even in
resource-constrained environments.

</details>


### [180] [Multi-modal Knowledge Decomposition based Online Distillation for Biomarker Prediction in Breast Cancer Histopathology](https://arxiv.org/abs/2508.17213)
*Qibin Zhang,Xinyu Hao,Qiao Chen,Rui Xu,Fengyu Cong,Cheng Lu,Hongming Xu*

Main category: cs.CV

TL;DR: Online distillation approach using Multi-modal Knowledge Decomposition (MKD) to improve IHC biomarker prediction from H&E histopathology images, enabling inference with single or multi-modal data.


<details>
  <summary>Details</summary>
Motivation: Simultaneous acquisition of multi-modal data (genomic and pathological) is challenging due to cost/technical limitations, but IHC biomarker prediction benefits from multi-modal fusion analysis.

Method: Proposes MKD with two teacher models and one student model to extract modality-specific and modality-general features. Uses Similarity-preserving Knowledge Distillation (SKD) and Collaborative Learning for Online Distillation (CLOD) for mutual learning between models.

Result: Superior performance in IHC biomarker prediction using uni-modal data on TCGA-BRCA and QHSU datasets.

Conclusion: The approach effectively leverages multi-modal training data while enabling flexible inference with single or multi-modal inputs, addressing practical challenges in clinical settings.

Abstract: Immunohistochemical (IHC) biomarker prediction benefits from multi-modal data
fusion analysis. However, the simultaneous acquisition of multi-modal data,
such as genomic and pathological information, is often challenging due to cost
or technical limitations. To address this challenge, we propose an online
distillation approach based on Multi-modal Knowledge Decomposition (MKD) to
enhance IHC biomarker prediction in haematoxylin and eosin (H\&E) stained
histopathology images. This method leverages paired genomic-pathology data
during training while enabling inference using either pathology slides alone or
both modalities. Two teacher and one student models are developed to extract
modality-specific and modality-general features by minimizing the MKD loss. To
maintain the internal structural relationships between samples,
Similarity-preserving Knowledge Distillation (SKD) is applied. Additionally,
Collaborative Learning for Online Distillation (CLOD) facilitates mutual
learning between teacher and student models, encouraging diverse and
complementary learning dynamics. Experiments on the TCGA-BRCA and in-house QHSU
datasets demonstrate that our approach achieves superior performance in IHC
biomarker prediction using uni-modal data. Our code is available at
https://github.com/qiyuanzz/MICCAI2025_MKD.

</details>


### [181] [Deep Learning with Self-Attention and Enhanced Preprocessing for Precise Diagnosis of Acute Lymphoblastic Leukemia from Bone Marrow Smears in Hemato-Oncology](https://arxiv.org/abs/2508.17216)
*Md. Maruf,Md. Mahbubul Haque,Bishowjit Paul*

Main category: cs.CV

TL;DR: Deep learning framework using VGG19 with multi-head self-attention and Focal Loss achieves 99.25% accuracy for automated acute lymphoblastic leukemia diagnosis from bone marrow images.


<details>
  <summary>Details</summary>
Motivation: Conventional ALL diagnosis workflows are complex, time-consuming, and error-prone, requiring automated solutions for early and accurate detection with precise subtyping.

Method: Combines robust preprocessing with CNN architecture (VGG19 backbone) enhanced with multi-head self-attention block to model long-range dependencies, trained with Focal Loss to address class imbalance.

Result: Enhanced VGG19+MHSA with Focal Loss achieves 99.25% accuracy, outperforming ResNet101 baseline (98.62%), demonstrating superior discriminative representations of leukemic cell morphology.

Conclusion: Attention-augmented CNNs with targeted loss optimization and preprocessing provide highly accurate and computationally efficient tool for automated ALL recognition, potentially accelerating diagnostic workflows in clinical settings.

Abstract: Acute lymphoblastic leukemia (ALL) is a prevalent hematological malignancy in
both pediatric and adult populations. Early and accurate detection with precise
subtyping is essential for guiding therapy. Conventional workflows are complex,
time-consuming, and prone to human error. We present a deep learning framework
for automated ALL diagnosis from bone marrow smear images. The method combines
a robust preprocessing pipeline with convolutional neural networks (CNNs) to
standardize image quality and improve inference efficiency. As a key design, we
insert a multi-head self-attention (MHSA) block into a VGG19 backbone to model
long-range dependencies and contextual relationships among cellular features.
To mitigate class imbalance, we train with Focal Loss. Across evaluated
architectures, the enhanced VGG19+MHSA trained with Focal Loss achieves 99.25%
accuracy, surpassing a strong ResNet101 baseline (98.62%). These results
indicate that attention-augmented CNNs, coupled with targeted loss optimization
and preprocessing, yield more discriminative representations of leukemic cell
morphology. Our approach offers a highly accurate and computationally efficient
tool for automated ALL recognition and subtyping, with potential to accelerate
diagnostic workflows and support reliable decision-making in clinical settings.

</details>


### [182] [4D Visual Pre-training for Robot Learning](https://arxiv.org/abs/2508.17230)
*Chengkai Hou,Yanjie Ze,Yankai Fu,Zeyu Gao,Songbo Hu,Yue Yu,Shanghang Zhang,Huazhe Xu*

Main category: cs.CV

TL;DR: FVP is a 4D visual pre-training framework that uses next-point-cloud-prediction with diffusion models to improve 3D representations for robotics, achieving 28% performance boost on manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Current visual representations for robotics are mostly 2D-based, neglecting the 3D nature of the world, and there's a scarcity of large-scale 3D data for pre-training universal 3D representations.

Method: FVP frames visual pre-training as a next-point-cloud-prediction problem using diffusion models, pre-trained on larger public datasets to enhance 3D representations.

Result: FVP boosts 3D Diffusion Policy performance by 28% across twelve real-world manipulation tasks, achieves state-of-the-art performance, and works across various point cloud encoders and datasets.

Conclusion: FVP provides an effective alternative to direct 3D representation learning, significantly improving robot manipulation performance and demonstrating broad applicability across different models and tasks.

Abstract: General visual representations learned from web-scale datasets for robotics
have achieved great success in recent years, enabling data-efficient robot
learning on manipulation tasks; yet these pre-trained representations are
mostly on 2D images, neglecting the inherent 3D nature of the world. However,
due to the scarcity of large-scale 3D data, it is still hard to extract a
universal 3D representation from web datasets. Instead, we are seeking a
general visual pre-training framework that could improve all 3D representations
as an alternative. Our framework, called FVP, is a novel 4D Visual Pre-training
framework for real-world robot learning. FVP frames the visual pre-training
objective as a next-point-cloud-prediction problem, models the prediction model
as a diffusion model, and pre-trains the model on the larger public datasets
directly. Across twelve real-world manipulation tasks, FVP boosts the average
success rate of 3D Diffusion Policy (DP3) for these tasks by 28%. The FVP
pre-trained DP3 achieves state-of-the-art performance across imitation learning
methods. Moreover, the efficacy of FVP adapts across various point cloud
encoders and datasets. Finally, we apply FVP to the RDT-1B, a larger
Vision-Language-Action robotic model, enhancing its performance on various
robot tasks. Our project page is available at: https://4d-
visual-pretraining.github.io/.

</details>


### [183] [PersPose: 3D Human Pose Estimation with Perspective Encoding and Perspective Rotation](https://arxiv.org/abs/2508.17239)
*Xiaoyang Hao,Han Li*

Main category: cs.CV

TL;DR: PersPose introduces Perspective Encoding and Perspective Rotation to address camera intrinsics and perspective distortion issues in monocular 3D human pose estimation, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing 3D HPE methods use cropped images without camera intrinsics, making relative depth estimation inaccurate. Human subjects appearing away from image center cause perspective distortions that complicate model fitting.

Method: Proposes Perspective Encoding (PE) to encode camera intrinsics of cropped images, and Perspective Rotation (PR) to center human subjects and reduce perspective distortions. Combines both in PersPose framework.

Result: Achieves SOTA performance: MPJPE of 60.1 mm on 3DPW (7.54% improvement over previous SOTA), and strong results on MPI-INF-3DHP and Human3.6M datasets.

Conclusion: Incorporating camera intrinsics through PE and reducing perspective distortions through PR significantly improves monocular 3D human pose estimation accuracy across multiple datasets.

Abstract: Monocular 3D human pose estimation (HPE) methods estimate the 3D positions of
joints from individual images. Existing 3D HPE approaches often use the cropped
image alone as input for their models. However, the relative depths of joints
cannot be accurately estimated from cropped images without the corresponding
camera intrinsics, which determine the perspective relationship between 3D
objects and the cropped images. In this work, we introduce Perspective Encoding
(PE) to encode the camera intrinsics of the cropped images. Moreover, since the
human subject can appear anywhere within the original image, the perspective
relationship between the 3D scene and the cropped image differs significantly,
which complicates model fitting. Additionally, the further the human subject
deviates from the image center, the greater the perspective distortions in the
cropped image. To address these issues, we propose Perspective Rotation (PR), a
transformation applied to the original image that centers the human subject,
thereby reducing perspective distortions and alleviating the difficulty of
model fitting. By incorporating PE and PR, we propose a novel 3D HPE framework,
PersPose. Experimental results demonstrate that PersPose achieves
state-of-the-art (SOTA) performance on the 3DPW, MPIINF-3DHP, and Human3.6M
datasets. For example, on the in-the-wild dataset 3DPW, PersPose achieves an
MPJPE of 60.1 mm, 7.54% lower than the previous SOTA approach. Code is
available at: https://github.com/ KenAdamsJoseph/PersPose.

</details>


### [184] [CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Models](https://arxiv.org/abs/2508.17243)
*Zicong Tang,Ziyang Ma,Suqing Wang,Zuchao Li,Lefei Zhang,Hai Zhao,Yun Li,Qianren Wang*

Main category: cs.CV

TL;DR: CoViPAL is a layer-wise contextualized visual token pruning method that uses a lightweight Plug-and-Play Pruning Module to remove redundant vision tokens in LVLMs, improving inference efficiency without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Large Vision-Language Models generate thousands of vision tokens from images, leading to high computational costs and memory overhead during prefilling and decoding stages. Existing pruning methods struggle in shallow layers due to insufficient contextual information.

Method: Proposes CoViPAL with a Plug-and-Play Pruning Module (PPM) that predicts and removes redundant vision tokens before processing by LVLM. The PPM is lightweight, model-agnostic, and operates independently of LVLM architecture.

Result: Extensive experiments show CoViPAL outperforms training-free pruning methods under equal token budgets and surpasses training-based methods with comparable supervision. It improves inference efficiency without compromising accuracy.

Conclusion: CoViPAL provides a scalable and efficient solution for improving LVLM inference efficiency by effectively pruning redundant visual tokens in shallow layers using contextual signals.

Abstract: Large Vision-Language Models (LVLMs) process multimodal inputs consisting of
text tokens and vision tokens extracted from images or videos. Due to the rich
visual information, a single image can generate thousands of vision tokens,
leading to high computational costs during the prefilling stage and significant
memory overhead during decoding. Existing methods attempt to prune redundant
vision tokens, revealing substantial redundancy in visual representations.
However, these methods often struggle in shallow layers due to the lack of
sufficient contextual information. We argue that many visual tokens are
inherently redundant even in shallow layers and can be safely and effectively
pruned with appropriate contextual signals. In this work, we propose CoViPAL, a
layer-wise contextualized visual token pruning method that employs a
Plug-and-Play Pruning Module (PPM) to predict and remove redundant vision
tokens before they are processed by the LVLM. The PPM is lightweight,
model-agnostic, and operates independently of the LVLM architecture, ensuring
seamless integration with various models. Extensive experiments on multiple
benchmarks demonstrate that CoViPAL outperforms training-free pruning methods
under equal token budgets and surpasses training-based methods with comparable
supervision. CoViPAL offers a scalable and efficient solution to improve
inference efficiency in LVLMs without compromising accuracy.

</details>


### [185] [Uncovering and Mitigating Destructive Multi-Embedding Attacks in Deepfake Proactive Forensics](https://arxiv.org/abs/2508.17247)
*Lixin Jia,Haiyang Sun,Zhiqing Guo,Yunfeng Diao,Dan Ma,Gaobo Yang*

Main category: cs.CV

TL;DR: Proposes Adversarial Interference Simulation (AIS) to protect deepfake proactive forensics against Multi-Embedding Attacks where multiple watermark embeddings destroy original forensic watermarks.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake forensic methods assume single watermark embedding, but real-world scenarios involve multiple embeddings that can destroy original watermarks, rendering forensic mechanisms ineffective.

Method: Adversarial Interference Simulation (AIS) training paradigm that simulates multi-embedding attack scenarios during fine-tuning with resilience-driven loss function to learn sparse and stable watermark representations.

Result: Extensive experiments show AIS significantly enhances robustness of various existing methods against multi-embedding attacks, maintaining original watermark extraction ability even after second embedding.

Conclusion: AIS provides a plug-and-play training solution to address the critical vulnerability of multi-embedding attacks in deepfake proactive forensics without requiring architectural changes.

Abstract: With the rapid evolution of deepfake technologies and the wide dissemination
of digital media, personal privacy is facing increasingly serious security
threats. Deepfake proactive forensics, which involves embedding imperceptible
watermarks to enable reliable source tracking, serves as a crucial defense
against these threats. Although existing methods show strong forensic ability,
they rely on an idealized assumption of single watermark embedding, which
proves impractical in real-world scenarios. In this paper, we formally define
and demonstrate the existence of Multi-Embedding Attacks (MEA) for the first
time. When a previously protected image undergoes additional rounds of
watermark embedding, the original forensic watermark can be destroyed or
removed, rendering the entire proactive forensic mechanism ineffective. To
address this vulnerability, we propose a general training paradigm named
Adversarial Interference Simulation (AIS). Rather than modifying the network
architecture, AIS explicitly simulates MEA scenarios during fine-tuning and
introduces a resilience-driven loss function to enforce the learning of sparse
and stable watermark representations. Our method enables the model to maintain
the ability to extract the original watermark correctly even after a second
embedding. Extensive experiments demonstrate that our plug-and-play AIS
training paradigm significantly enhances the robustness of various existing
methods against MEA.

</details>


### [186] [A biological vision inspired framework for machine perception of abutting grating illusory contours](https://arxiv.org/abs/2508.17254)
*Xiao Zhang,Kai-Fu Yang,Xian-Shi Zhang,Hong-Zhi You,Hong-Mei Yan,Yong-Jie Li*

Main category: cs.CV

TL;DR: ICPNet is a novel deep network that improves machine perception of illusory contours to better align with human vision, achieving state-of-the-art performance on abutting grating tasks.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks fail to perceive illusory contours like humans do, creating a misalignment between machine intelligence and human perception patterns that needs to be addressed.

Method: Proposes ICPNet with three key modules: multi-scale feature projection (MFP) for multi-scale representations, feature interaction attention module (FIAM) for feedforward-feedback interaction, and edge fusion module (EFM) with shape constraints inspired by human shape bias.

Result: ICPNet shows significantly higher sensitivity to abutting grating illusory contours than state-of-the-art models, with notable top-1 accuracy improvements across AG-MNIST and AG-Fashion-MNIST test sets.

Conclusion: This work represents a step toward human-level intelligence for DNN-based models by better aligning machine perception with human visual cognition of illusory contours.

Abstract: Higher levels of machine intelligence demand alignment with human perception
and cognition. Deep neural networks (DNN) dominated machine intelligence have
demonstrated exceptional performance across various real-world tasks.
Nevertheless, recent evidence suggests that DNNs fail to perceive illusory
contours like the abutting grating, a discrepancy that misaligns with human
perception patterns. Departing from previous works, we propose a novel deep
network called illusory contour perception network (ICPNet) inspired by the
circuits of the visual cortex. In ICPNet, a multi-scale feature projection
(MFP) module is designed to extract multi-scale representations. To boost the
interaction between feedforward and feedback features, a feature interaction
attention module (FIAM) is introduced. Moreover, drawing inspiration from the
shape bias observed in human perception, an edge detection task conducted via
the edge fusion module (EFM) injects shape constraints that guide the network
to concentrate on the foreground. We assess our method on the existing AG-MNIST
test set and the AG-Fashion-MNIST test sets constructed by this work.
Comprehensive experimental results reveal that ICPNet is significantly more
sensitive to abutting grating illusory contours than state-of-the-art models,
with notable improvements in top-1 accuracy across various subsets. This work
is expected to make a step towards human-level intelligence for DNN-based
models.

</details>


### [187] [SEER-VAR: Semantic Egocentric Environment Reasoner for Vehicle Augmented Reality](https://arxiv.org/abs/2508.17255)
*Yuzhi Lai,Shenghai Yuan,Peizheng Li,Jun Lou,Andreas Zell*

Main category: cs.CV

TL;DR: SEER-VAR is a novel egocentric vehicle AR framework that uses semantic scene decomposition, dual SLAM tracking, and LLM-driven recommendations for context-aware AR overlays in driving scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing AR systems for vehicles assume static or single-view settings, lacking dynamic context-aware capabilities for real-world driving scenarios with both cabin and road environments.

Method: Uses depth-guided vision-language grounding to separate cabin/road scenes, dual Context-Aware SLAM Branches for tracking, and GPT-based module for generating context-aware AR overlays like dashboard cues and hazard alerts.

Result: Achieves robust spatial alignment and perceptually coherent AR rendering across varied environments, enhances scene understanding, overlay relevance, and driver ease in user studies.

Conclusion: Provides an effective foundation for LLM-based AR recommendation in egocentric driving, with open-source code and dataset (EgoSLAM-Drive) to support future research.

Abstract: We present SEER-VAR, a novel framework for egocentric vehicle-based augmented
reality (AR) that unifies semantic decomposition, Context-Aware SLAM Branches
(CASB), and LLM-driven recommendation. Unlike existing systems that assume
static or single-view settings, SEER-VAR dynamically separates cabin and road
scenes via depth-guided vision-language grounding. Two SLAM branches track
egocentric motion in each context, while a GPT-based module generates
context-aware overlays such as dashboard cues and hazard alerts. To support
evaluation, we introduce EgoSLAM-Drive, a real-world dataset featuring
synchronized egocentric views, 6DoF ground-truth poses, and AR annotations
across diverse driving scenarios. Experiments demonstrate that SEER-VAR
achieves robust spatial alignment and perceptually coherent AR rendering across
varied environments. As one of the first to explore LLM-based AR recommendation
in egocentric driving, we address the lack of comparable systems through
structured prompting and detailed user studies. Results show that SEER-VAR
enhances perceived scene understanding, overlay relevance, and driver ease,
providing an effective foundation for future research in this direction. Code
and dataset will be made open source.

</details>


### [188] [ResLink: A Novel Deep Learning Architecture for Brain Tumor Classification with Area Attention and Residual Connections](https://arxiv.org/abs/2508.17259)
*Sumedha Arya,Nirmal Gaud*

Main category: cs.CV

TL;DR: ResLink: Novel deep learning architecture with area attention mechanisms and residual connections for brain tumor classification from CT scans, achieving 95% accuracy.


<details>
  <summary>Details</summary>
Motivation: Brain tumors pose significant health challenges and require early, accurate diagnosis for effective treatment. There is a need for improved classification techniques in medical imaging.

Method: ResLink integrates novel area attention mechanisms with residual connections in a multi-stage convolutional pipeline. It incorporates dropout, regularization, downsampling, and attention-based refinement for classification.

Result: The model achieves 95% accuracy on a balanced dataset and demonstrates strong generalizability in brain tumor classification tasks.

Conclusion: ResLink shows significant potential for improving brain tumor classification and offers a robust, efficient technique for medical imaging applications.

Abstract: Brain tumors show significant health challenges due to their potential to
cause critical neurological functions. Early and accurate diagnosis is crucial
for effective treatment. In this research, we propose ResLink, a novel deep
learning architecture for brain tumor classification using CT scan images.
ResLink integrates novel area attention mechanisms with residual connections to
enhance feature learning and spatial understanding for spatially rich image
classification tasks. The model employs a multi-stage convolutional pipeline,
incorporating dropout, regularization, and downsampling, followed by a final
attention-based refinement for classification. Trained on a balanced dataset,
ResLink achieves a high accuracy of 95% and demonstrates strong
generalizability. This research demonstrates the potential of ResLink in
improving brain tumor classification, offering a robust and efficient technique
for medical imaging applications.

</details>


### [189] [CLIFF: Continual Learning for Incremental Flake Features in 2D Material Identification](https://arxiv.org/abs/2508.17261)
*Sankalp Pandey,Xuan Bac Nguyen,Nicholas Borys,Hugh Churchill,Khoa Luu*

Main category: cs.CV

TL;DR: CLIFF is a continual learning framework for automated classification of 2D material flakes that addresses appearance shifts across different materials by freezing a backbone model and learning material-specific components.


<details>
  <summary>Details</summary>
Motivation: Automated layer classification from optical microscopy is challenging due to substantial appearance shifts across different 2D materials, making quantum flake identification difficult for scalable quantum hardware.

Method: Freeze backbone and base head trained on reference material, learn material-specific prompts, embeddings, and delta heads for new materials. Use prompt pool and cosine-similarity gate to modulate features, with memory replay and knowledge distillation.

Result: CLIFF achieves competitive accuracy with significantly lower forgetting compared to naive fine-tuning and prompt-based baselines.

Conclusion: This is the first systematic study of continual learning for 2D materials, providing an effective framework for scalable quantum flake identification across diverse materials.

Abstract: Identifying quantum flakes is crucial for scalable quantum hardware; however,
automated layer classification from optical microscopy remains challenging due
to substantial appearance shifts across different materials. In this paper, we
propose a new Continual-Learning Framework for Flake Layer Classification
(CLIFF). To our knowledge, this is the first systematic study of continual
learning in the domain of two-dimensional (2D) materials. Our method enables
the model to differentiate between materials and their physical and optical
properties by freezing a backbone and base head trained on a reference
material. For each new material, it learns a material-specific prompt,
embedding, and a delta head. A prompt pool and a cosine-similarity gate
modulate features and compute material-specific corrections. Additionally, we
incorporate memory replay with knowledge distillation. CLIFF achieves
competitive accuracy with significantly lower forgetting than naive fine-tuning
and a prompt-based baseline.

</details>


### [190] [AdaGAT: Adaptive Guidance Adversarial Training for the Robustness of Deep Neural Networks](https://arxiv.org/abs/2508.17265)
*Zhenyu Liu,Huizhi Liang,Xinrun Li,Vaclav Snasel,Varun Ojha*

Main category: cs.CV

TL;DR: AdaGAT is a novel adversarial distillation method that dynamically adjusts a learnable guide model's training state to enhance student model robustness through two specialized loss functions.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial distillation methods struggle to maintain optimal guide model states during co-training, limiting effective robustness transfer to student models.

Method: Proposes Adaptive Guidance Adversarial Training (AdaGAT) with two separate loss functions that allow the guide model to actively participate in backpropagation to achieve optimal state for robustness transfer.

Result: Extensive experiments on CIFAR-10, CIFAR-100, and TinyImageNet show enhanced target model robustness across various adversarial attacks compared to baseline models when guide model accuracy is appropriately adjusted.

Conclusion: Dynamic adjustment of guide model training state within optimal accuracy ranges significantly improves student model robustness in adversarial distillation scenarios.

Abstract: Adversarial distillation (AD) is a knowledge distillation technique that
facilitates the transfer of robustness from teacher deep neural network (DNN)
models to lightweight target (student) DNN models, enabling the target models
to perform better than only training the student model independently. Some
previous works focus on using a small, learnable teacher (guide) model to
improve the robustness of a student model. Since a learnable guide model starts
learning from scratch, maintaining its optimal state for effective knowledge
transfer during co-training is challenging. Therefore, we propose a novel
Adaptive Guidance Adversarial Training (AdaGAT) method. Our method, AdaGAT,
dynamically adjusts the training state of the guide model to install robustness
to the target model. Specifically, we develop two separate loss functions as
part of the AdaGAT method, allowing the guide model to participate more
actively in backpropagation to achieve its optimal state. We evaluated our
approach via extensive experiments on three datasets: CIFAR-10, CIFAR-100, and
TinyImageNet, using the WideResNet-34-10 model as the target model. Our
observations reveal that appropriately adjusting the guide model within a
certain accuracy range enhances the target model's robustness across various
adversarial attacks compared to a variety of baseline models.

</details>


### [191] [Spatial-Temporal Human-Object Interaction Detection](https://arxiv.org/abs/2508.17270)
*Xu Sun,Yunqing He,Tongwei Ren,Gangshan Wu*

Main category: cs.CV

TL;DR: Proposes ST-HOID for spatiotemporal human-object interaction detection in videos, introduces a novel method with trajectory detection and interaction reasoning modules, and creates VidOR-HOID dataset with 10,831 instances.


<details>
  <summary>Details</summary>
Motivation: Human-object interaction is crucial for human-centric video content understanding, requiring fine-grained detection of interactions and object trajectories over time.

Method: Novel approach with object trajectory detection module and interaction reasoning module to handle spatiotemporal HOI detection in videos.

Result: Method outperforms baselines from state-of-the-art image HOI detection, video visual relation detection, and video HOI recognition methods.

Conclusion: Proposed ST-HOID task and method effectively address spatiotemporal human-object interaction detection, demonstrated through comprehensive experiments on the new VidOR-HOID dataset.

Abstract: In this paper, we propose a new instance-level human-object interaction
detection task on videos called ST-HOID, which aims to distinguish fine-grained
human-object interactions (HOIs) and the trajectories of subjects and objects.
It is motivated by the fact that HOI is crucial for human-centric video content
understanding. To solve ST-HOID, we propose a novel method consisting of an
object trajectory detection module and an interaction reasoning module.
Furthermore, we construct the first dataset named VidOR-HOID for ST-HOID
evaluation, which contains 10,831 spatial-temporal HOI instances. We conduct
extensive experiments to evaluate the effectiveness of our method. The
experimental results demonstrate that our method outperforms the baselines
generated by the state-of-the-art methods of image human-object interaction
detection, video visual relation detection and video human-object interaction
recognition.

</details>


### [192] [Deep Learning-Assisted Detection of Sarcopenia in Cross-Sectional Computed Tomography Imaging](https://arxiv.org/abs/2508.17275)
*Manish Bhardwaj,Huizhi Liang,Ashwin Sivaharan,Sandip Nandhra,Vaclav Snasel,Tamer El-Sayed,Varun Ojha*

Main category: cs.CV

TL;DR: Deep learning models for automated sarcopenia assessment using CT scans, achieving high accuracy in skeletal muscle area measurement with 93% dice similarity coefficient.


<details>
  <summary>Details</summary>
Motivation: Sarcopenia assessment through manual SMA measurement is time-consuming and adds to clinical workload, limiting timely detection and management. AI automation could make this process more efficient and scalable.

Method: Used transfer learning and self-supervised learning approaches with labeled and unlabeled CT scan datasets. Developed deep-learning models to measure SMA in CT images, focusing on quantitative assessment rather than qualitative detection.

Result: Model predicted SMA with average error of Â±3 percentage points against manual measurements. Achieved 93% dice similarity coefficient for predicted segmentation masks.

Conclusion: The approach shows a pathway to full automation of sarcopenia assessment and detection, mitigating issues of class imbalance and limited data availability while providing precise quantitative measurements.

Abstract: Sarcopenia is a progressive loss of muscle mass and function linked to poor
surgical outcomes such as prolonged hospital stays, impaired mobility, and
increased mortality. Although it can be assessed through cross-sectional
imaging by measuring skeletal muscle area (SMA), the process is time-consuming
and adds to clinical workloads, limiting timely detection and management;
however, this process could become more efficient and scalable with the
assistance of artificial intelligence applications. This paper presents
high-quality three-dimensional cross-sectional computed tomography (CT) images
of patients with sarcopenia collected at the Freeman Hospital, Newcastle upon
Tyne Hospitals NHS Foundation Trust. Expert clinicians manually annotated the
SMA at the third lumbar vertebra, generating precise segmentation masks. We
develop deep-learning models to measure SMA in CT images and automate this
task. Our methodology employed transfer learning and self-supervised learning
approaches using labelled and unlabeled CT scan datasets. While we developed
qualitative assessment models for detecting sarcopenia, we observed that the
quantitative assessment of SMA is more precise and informative. This approach
also mitigates the issue of class imbalance and limited data availability. Our
model predicted the SMA, on average, with an error of +-3 percentage points
against the manually measured SMA. The average dice similarity coefficient of
the predicted masks was 93%. Our results, therefore, show a pathway to full
automation of sarcopenia assessment and detection.

</details>


### [193] [MTNet: Learning modality-aware representation with transformer for RGBT tracking](https://arxiv.org/abs/2508.17280)
*Ruichao Hou,Boyue Xu,Tongwei Ren,Gangshan Wu*

Main category: cs.CV

TL;DR: MTNet is a transformer-based RGBT tracker with modality-aware modules for feature interaction, transformer fusion for global dependencies, and dynamic template updates for robust multi-modality representation learning.


<details>
  <summary>Details</summary>
Motivation: Regular fusion paradigms and invariable tracking templates limit feature interaction in RGBT tracking, requiring more robust multi-modality representation learning.

Method: Proposes MTNet with modality-aware network (CADM and SSPM modules), transformer fusion network for global dependencies, trident prediction head, and dynamic update strategy for template maintenance.

Result: Achieves satisfactory results compared to state-of-the-art methods on three RGBT benchmarks while maintaining real-time speed.

Conclusion: The proposed MTNet framework effectively addresses RGBT tracking challenges through modality-aware feature interaction and dynamic template management, demonstrating superior performance across multiple benchmarks.

Abstract: The ability to learn robust multi-modality representation has played a
critical role in the development of RGBT tracking. However, the regular fusion
paradigm and the invariable tracking template remain restrictive to the feature
interaction. In this paper, we propose a modality-aware tracker based on
transformer, termed MTNet. Specifically, a modality-aware network is presented
to explore modality-specific cues, which contains both channel aggregation and
distribution module(CADM) and spatial similarity perception module (SSPM). A
transformer fusion network is then applied to capture global dependencies to
reinforce instance representations. To estimate the precise location and tackle
the challenges, such as scale variation and deformation, we design a trident
prediction head and a dynamic update strategy which jointly maintain a reliable
template for facilitating inter-frame communication. Extensive experiments
validate that the proposed method achieves satisfactory results compared with
the state-of-the-art competitors on three RGBT benchmarks while reaching
real-time speed.

</details>


### [194] [Quickly Tuning Foundation Models for Image Segmentation](https://arxiv.org/abs/2508.17283)
*Breenda Das,Lennart Purucker,Timur Carstensen,Frank Hutter*

Main category: cs.CV

TL;DR: QTT-SEG is a meta-learning approach that automates fine-tuning of SAM for image segmentation, achieving better performance than zero-shot SAM and AutoGluon Multimodal within minutes.


<details>
  <summary>Details</summary>
Motivation: Foundation models like SAM have strong zero-shot segmentation but perform poorly on domain-specific tasks, and manual fine-tuning requires significant effort and expertise.

Method: Built on Quick-Tune hyperparameter optimization framework, uses meta-learned cost and performance models to predict optimal configurations from over 200 million possibilities.

Result: Consistently improves SAM's zero-shot performance, surpasses AutoGluon Multimodal on most binary tasks within 3 minutes, and delivers gains on multiclass datasets.

Conclusion: Meta-learning shows promise for automating model adaptation for specialized segmentation tasks, making fine-tuning efficient and accessible.

Abstract: Foundation models like SAM (Segment Anything Model) exhibit strong zero-shot
image segmentation performance, but often fall short on domain-specific tasks.
Fine-tuning these models typically requires significant manual effort and
domain expertise. In this work, we introduce QTT-SEG, a meta-learning-driven
approach for automating and accelerating the fine-tuning of SAM for image
segmentation. Built on the Quick-Tune hyperparameter optimization framework,
QTT-SEG predicts high-performing configurations using meta-learned cost and
performance models, efficiently navigating a search space of over 200 million
possibilities. We evaluate QTT-SEG on eight binary and five multiclass
segmentation datasets under tight time constraints. Our results show that
QTT-SEG consistently improves upon SAM's zero-shot performance and surpasses
AutoGluon Multimodal, a strong AutoML baseline, on most binary tasks within
three minutes. On multiclass datasets, QTT-SEG delivers consistent gains as
well. These findings highlight the promise of meta-learning in automating model
adaptation for specialized segmentation tasks. Code available at:
https://github.com/ds-brx/QTT-SEG/

</details>


### [195] [Explain Before You Answer: A Survey on Compositional Visual Reasoning](https://arxiv.org/abs/2508.17298)
*Fucai Ke,Joy Hsu,Zhixi Cai,Zixian Ma,Xin Zheng,Xindi Wu,Sukai Huang,Weiqing Wang,Pari Delir Haghighi,Gholamreza Haffari,Ranjay Krishna,Jiajun Wu,Hamid Rezatofighi*

Main category: cs.CV

TL;DR: A comprehensive survey (2023-2025) of 260+ papers on compositional visual reasoning, covering paradigm shifts, benchmarks, challenges, and future directions in multimodal AI.


<details>
  <summary>Details</summary>
Motivation: To provide a dedicated synthesis of the rapidly expanding compositional visual reasoning literature, which was missing despite early surveys on monolithic vision-language models or general multimodal reasoning.

Method: Systematic review of 260+ papers from top venues, formalizing core definitions, tracing five-stage paradigm shifts, cataloging 60+ benchmarks and metrics, and analyzing architectural designs and limitations.

Result: A unified taxonomy and historical roadmap covering prompt-enhanced pipelines, tool-enhanced LLMs/VLMs, chain-of-thought reasoning, and unified agentic VLMs, with insights into cognitive alignment advantages and current limitations.

Conclusion: The survey serves as a foundational reference that identifies key challenges (hallucination, reasoning biases, benchmark limitations) and outlines future directions including world-model integration and human-AI collaborative reasoning.

Abstract: Compositional visual reasoning has emerged as a key research frontier in
multimodal AI, aiming to endow machines with the human-like ability to
decompose visual scenes, ground intermediate concepts, and perform multi-step
logical inference. While early surveys focus on monolithic vision-language
models or general multimodal reasoning, a dedicated synthesis of the rapidly
expanding compositional visual reasoning literature is still missing. We fill
this gap with a comprehensive survey spanning 2023 to 2025 that systematically
reviews 260+ papers from top venues (CVPR, ICCV, NeurIPS, ICML, ACL, etc.). We
first formalize core definitions and describe why compositional approaches
offer advantages in cognitive alignment, semantic fidelity, robustness,
interpretability, and data efficiency. Next, we trace a five-stage paradigm
shift: from prompt-enhanced language-centric pipelines, through tool-enhanced
LLMs and tool-enhanced VLMs, to recently minted chain-of-thought reasoning and
unified agentic VLMs, highlighting their architectural designs, strengths, and
limitations. We then catalog 60+ benchmarks and corresponding metrics that
probe compositional visual reasoning along dimensions such as grounding
accuracy, chain-of-thought faithfulness, and high-resolution perception.
Drawing on these analyses, we distill key insights, identify open challenges
(e.g., limitations of LLM-based reasoning, hallucination, a bias toward
deductive reasoning, scalable supervision, tool integration, and benchmark
limitations), and outline future directions, including world-model integration,
human-AI collaborative reasoning, and richer evaluation protocols. By offering
a unified taxonomy, historical roadmap, and critical outlook, this survey aims
to serve as a foundational reference and inspire the next generation of
compositional visual reasoning research.

</details>


### [196] [FoundDiff: Foundational Diffusion Model for Generalizable Low-Dose CT Denoising](https://arxiv.org/abs/2508.17299)
*Zhihao Chen,Qi Gao,Zilong Li,Junping Zhang,Yi Zhang,Jun Zhao,Hongming Shan*

Main category: cs.CV

TL;DR: FoundDiff is a foundational diffusion model that provides unified LDCT denoising across various dose levels and anatomical regions using a two-stage approach with dose-anatomy perception and adaptive denoising.


<details>
  <summary>Details</summary>
Motivation: Existing DL-based LDCT denoising methods struggle with diverse noise characteristics and anatomical heterogeneity across different scanning conditions, limiting their clinical generalizability and robustness.

Method: Two-stage strategy: (1) Dose-anatomy perception using DA-CLIP with contrastive learning for continuous dose representations and anatomical identification; (2) Adaptive denoising using DA-Diff with novel dose and anatomy conditional blocks based on Mamba architecture.

Result: Superior denoising performance over state-of-the-art methods on two public LDCT datasets with eight dose levels and three anatomical regions, with remarkable generalization to unseen dose levels.

Conclusion: FoundDiff demonstrates strong generalizability and robustness for clinical LDCT denoising applications across diverse dose levels and anatomical regions, addressing limitations of current specialized approaches.

Abstract: Low-dose computed tomography (CT) denoising is crucial for reduced radiation
exposure while ensuring diagnostically acceptable image quality. Despite
significant advancements driven by deep learning (DL) in recent years, existing
DL-based methods, typically trained on a specific dose level and anatomical
region, struggle to handle diverse noise characteristics and anatomical
heterogeneity during varied scanning conditions, limiting their
generalizability and robustness in clinical scenarios. In this paper, we
propose FoundDiff, a foundational diffusion model for unified and generalizable
LDCT denoising across various dose levels and anatomical regions. FoundDiff
employs a two-stage strategy: (i) dose-anatomy perception and (ii) adaptive
denoising. First, we develop a dose- and anatomy-aware contrastive language
image pre-training model (DA-CLIP) to achieve robust dose and anatomy
perception by leveraging specialized contrastive learning strategies to learn
continuous representations that quantify ordinal dose variations and identify
salient anatomical regions. Second, we design a dose- and anatomy-aware
diffusion model (DA-Diff) to perform adaptive and generalizable denoising by
synergistically integrating the learned dose and anatomy embeddings from DACLIP
into diffusion process via a novel dose and anatomy conditional block (DACB)
based on Mamba. Extensive experiments on two public LDCT datasets encompassing
eight dose levels and three anatomical regions demonstrate superior denoising
performance of FoundDiff over existing state-of-the-art methods and the
remarkable generalization to unseen dose levels. The codes and models are
available at https://github.com/hao1635/FoundDiff.

</details>


### [197] [PosBridge: Multi-View Positional Embedding Transplant for Identity-Aware Image Editing](https://arxiv.org/abs/2508.17302)
*Peilin Xiong,Junwen Chen,Honghui Yuan,Keiji Yanai*

Main category: cs.CV

TL;DR: PosBridge is a training-free framework for localized subject-driven image editing that uses positional embedding transplant and Corner Centered Layout to insert custom objects into target scenes with high fidelity and efficiency.


<details>
  <summary>Details</summary>
Motivation: As generative models scale, training becomes increasingly costly in terms of memory and computation, highlighting the need for training-free and scalable editing frameworks for localized subject-driven image editing.

Method: Uses positional embedding transplant to guide diffusion models to replicate structural characteristics of reference objects, and Corner Centered Layout that concatenates reference and background images as input to FLUX.1-Fill model during progressive denoising.

Result: Extensive experiments show PosBridge outperforms mainstream baselines in structural consistency, appearance fidelity, and computational efficiency.

Conclusion: PosBridge demonstrates practical value and potential for broad adoption as an efficient and flexible framework for inserting custom objects into images without requiring training.

Abstract: Localized subject-driven image editing aims to seamlessly integrate
user-specified objects into target scenes. As generative models continue to
scale, training becomes increasingly costly in terms of memory and computation,
highlighting the need for training-free and scalable editing frameworks.To this
end, we propose PosBridge an efficient and flexible framework for inserting
custom objects. A key component of our method is positional embedding
transplant, which guides the diffusion model to faithfully replicate the
structural characteristics of reference objects.Meanwhile, we introduce the
Corner Centered Layout, which concatenates reference images and the background
image as input to the FLUX.1-Fill model. During progressive denoising,
positional embedding transplant is applied to guide the noise distribution in
the target region toward that of the reference object. In this way, Corner
Centered Layout effectively directs the FLUX.1-Fill model to synthesize
identity-consistent content at the desired location. Extensive experiments
demonstrate that PosBridge outperforms mainstream baselines in structural
consistency, appearance fidelity, and computational efficiency, showcasing its
practical value and potential for broad adoption.

</details>


### [198] [First Place Solution to the MLCAS 2025 GWFSS Challenge: The Devil is in the Detail and Minority](https://arxiv.org/abs/2508.17305)
*Songliang Cao,Tianqi Hu,Hao Lu*

Main category: cs.CV

TL;DR: Winning solution for MLCAS 2025 wheat segmentation challenge using ViT-Adapter baseline with three stem-focused improvements: dynamic upsampler, semi-supervised distillation, and test-time scaling.


<details>
  <summary>Details</summary>
Motivation: Most segmentation tricks are now integrated into standard codebases, so the key differentiator is focusing on the specific problem nature of wheat plants, particularly the challenging stem segmentation due to fine structure and class imbalance.

Method: 1) SAPA dynamic upsampler for detail delineation; 2) Semi-supervised guided distillation with stem-aware sample selection; 3) Test-time scaling strategy to zoom and segment images twice.

Result: Achieved first place in the competition, outperforming second place by clear margins.

Conclusion: Simple but targeted improvements focused on the specific challenges of stem segmentation in wheat plants can lead to significant performance gains in specialized segmentation tasks.

Abstract: In this report, we present our solution during the participation of the MLCAS
2025 GWFSS Challenge. This challenge hosts a semantic segmentation competition
specific to wheat plants, which requires to segment three wheat organs
including the head, leaf, and stem, and another background class. In 2025,
participating a segmentation competition is significantly different from that
in previous years where many tricks can play important roles. Nowadays most
segmentation tricks have been well integrated into existing codebases such that
our naive ViT-Adapter baseline has already achieved sufficiently good
performance. Hence, we believe the key to stand out among other competitors is
to focus on the problem nature of wheat per se. By probing visualizations, we
identify the key -- the stem matters. In contrast to heads and leaves, stems
exhibit fine structure and occupy only few pixels, which suffers from fragile
predictions and class imbalance. Building on our baseline, we present three
technical improvements tailored to stems: i) incorporating a dynamic upsampler
SAPA used to enhance detail delineation; ii) leveraging semi-supervised guided
distillation with stem-aware sample selection to mine the treasure beneath
unlabeled data; and iii) applying a test-time scaling strategy to zoom in and
segment twice the image. Despite being simple, the three improvements bring us
to the first place of the competition, outperforming the second place by clear
margins. Code and models will be released at
https://github.com/tiny-smart/gwfss25.

</details>


### [199] [Defending Deepfake via Texture Feature Perturbation](https://arxiv.org/abs/2508.17315)
*Xiao Zhang,Changfang Chen,Tianyi Wang*

Main category: cs.CV

TL;DR: Proactive Deepfake detection using texture-guided invisible perturbations that target facial texture regions to disrupt Deepfake generation while minimizing visual impact on humans.


<details>
  <summary>Details</summary>
Motivation: Existing Deepfake detection methods are mostly passive and struggle with high-quality forgeries. Proactive defense by inserting invisible signals before image editing offers a more effective approach to combat Deepfake threats to social trust and information security.

Method: Texture-guided perturbation framework that uses Local Binary Patterns (LBP) to extract facial texture features, applies localized perturbations to key texture regions with low perceptual saliency, and employs a dual-model attention strategy to generate and optimize texture perturbations.

Result: Experiments on CelebA-HQ and LFW datasets show promising performance in distorting Deepfake generation and producing obvious visual defects under multiple attack models, demonstrating effective proactive detection capabilities.

Conclusion: The proposed method provides an efficient and scalable solution for proactive Deepfake detection by leveraging texture features to insert invisible perturbations that disrupt Deepfake generation while maintaining visual quality for human perception.

Abstract: The rapid development of Deepfake technology poses severe challenges to
social trust and information security. While most existing detection methods
primarily rely on passive analyses, due to unresolvable high-quality Deepfake
contents, proactive defense has recently emerged by inserting invisible signals
in advance of image editing. In this paper, we introduce a proactive Deepfake
detection approach based on facial texture features. Since human eyes are more
sensitive to perturbations in smooth regions, we invisibly insert perturbations
within texture regions that have low perceptual saliency, applying localized
perturbations to key texture regions while minimizing unwanted noise in
non-textured areas. Our texture-guided perturbation framework first extracts
preliminary texture features via Local Binary Patterns (LBP), and then
introduces a dual-model attention strategy to generate and optimize texture
perturbations. Experiments on CelebA-HQ and LFW datasets demonstrate the
promising performance of our method in distorting Deepfake generation and
producing obvious visual defects under multiple attack models, providing an
efficient and scalable solution for proactive Deepfake detection.

</details>


### [200] [SpecGen: Neural Spectral BRDF Generation via Spectral-Spatial Tri-plane Aggregation](https://arxiv.org/abs/2508.17316)
*Zhenyu Jin,Wenjie Li,Zhanyu Ma,Heng Guo*

Main category: cs.CV

TL;DR: SpecGen generates spectral BRDFs from single RGB sphere images, enabling spectral rendering under arbitrary lighting and shapes using a novel Spectral-Spatial Tri-plane Aggregation network that leverages abundant RGB BRDF data.


<details>
  <summary>Details</summary>
Motivation: Synthesizing spectral images across wavelengths is crucial for photorealistic rendering, but existing methods only convert RGB to spectral images. There's a need to generate spectral BRDFs from limited data to enable material-based spectral rendering.

Method: Proposed SpecGen method with Spectral-Spatial Tri-plane Aggregation (SSTA) network that models reflectance responses across wavelengths and incident-outgoing directions, leveraging abundant RGB BRDF data to enhance spectral BRDF generation from single RGB sphere images.

Result: Accurately reconstructs spectral BRDFs from limited spectral data and surpasses state-of-the-art methods in hyperspectral image reconstruction, achieving 8 dB improvement in PSNR.

Conclusion: The method successfully generates spectral BRDFs from single RGB images, enabling spectral rendering under arbitrary illuminations and shapes, addressing the challenge of scarce measured spectral BRDF data through innovative network architecture and training strategy.

Abstract: Synthesizing spectral images across different wavelengths is essential for
photorealistic rendering. Unlike conventional spectral uplifting methods that
convert RGB images into spectral ones, we introduce SpecGen, a novel method
that generates spectral bidirectional reflectance distribution functions
(BRDFs) from a single RGB image of a sphere. This enables spectral image
rendering under arbitrary illuminations and shapes covered by the corresponding
material. A key challenge in spectral BRDF generation is the scarcity of
measured spectral BRDF data. To address this, we propose the Spectral-Spatial
Tri-plane Aggregation (SSTA) network, which models reflectance responses across
wavelengths and incident-outgoing directions, allowing the training strategy to
leverage abundant RGB BRDF data to enhance spectral BRDF generation.
Experiments show that our method accurately reconstructs spectral BRDFs from
limited spectral data and surpasses state-of-the-art methods in hyperspectral
image reconstruction, achieving an improvement of 8 dB in PSNR. Codes and data
will be released upon acceptance.

</details>


### [201] [Mind the (Language) Gap: Towards Probing Numerical and Cross-Lingual Limits of LVLMs](https://arxiv.org/abs/2508.17334)
*Somraj Gautam,Abhirama Subramanyam Penamakuri,Abhishek Bhandari,Gaurav Harit*

Main category: cs.CV

TL;DR: MMCRICBENCH-3K is a benchmark for evaluating large vision-language models on VQA tasks involving cricket scorecards, testing numerical reasoning and cross-lingual capabilities across English and Hindi scorecards.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current LVLMs in handling complex numerical reasoning, structured data interpretation, and cross-lingual generalization in visual question answering tasks, particularly for domain-specific content like cricket scorecards.

Method: Created a benchmark with 1,463 synthetically generated cricket scorecard images (ODI, T20, Test formats) and 1,500 English QA pairs, divided into English and Hindi subsets with identical English questions to enable controlled cross-script evaluation.

Result: State-of-the-art LVLMs (GPT-4o, Qwen2.5VL) struggle significantly on both English and Hindi subsets, with performance dropping further on Hindi scorecards despite questions being in English, revealing deficiencies in structure-aware text understanding and cross-lingual generalization.

Conclusion: The benchmark exposes critical limitations in LVLMs' numerical reasoning, structured data processing, and cross-lingual capabilities, providing a valuable resource for advancing research in these areas through publicly available dataset.

Abstract: We introduce MMCRICBENCH-3K, a benchmark for Visual Question Answering (VQA)
on cricket scorecards, designed to evaluate large vision-language models
(LVLMs) on complex numerical and cross-lingual reasoning over semi-structured
tabular images. MMCRICBENCH-3K comprises 1,463 synthetically generated
scorecard images from ODI, T20, and Test formats, accompanied by 1,500 English
QA pairs. It includes two subsets: MMCRICBENCH-E-1.5K, featuring English
scorecards, and MMCRICBENCH-H-1.5K, containing visually similar Hindi
scorecards, with all questions and answers kept in English to enable controlled
cross-script evaluation. The task demands reasoning over structured numerical
data, multi-image context, and implicit domain knowledge. Empirical results
show that even state-of-the-art LVLMs, such as GPT-4o and Qwen2.5VL, struggle
on the English subset despite it being their primary training language and
exhibit a further drop in performance on the Hindi subset. This reveals key
limitations in structure-aware visual text understanding, numerical reasoning,
and cross-lingual generalization. The dataset is publicly available via Hugging
Face at https://huggingface.co/datasets/DIALab/MMCricBench, to promote LVLM
research in this direction.

</details>


### [202] [No Pixel Left Behind: A Detail-Preserving Architecture for Robust High-Resolution AI-Generated Image Detection](https://arxiv.org/abs/2508.17346)
*Lianrui Mu,Zou Xingze,Jianhong Bai,Jiaqi Hu,Wenjie Zheng,Jiangnan Ye,Jiedong Zhuang,Mudassar Ali,Jing Wang,Haoji Hu*

Main category: cs.CV

TL;DR: HiDA-Net is a novel framework for detecting high-resolution AI-generated images that preserves native-resolution details through feature aggregation from multiple tiles and global views, achieving significant accuracy improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing AI-generated image detection methods struggle with high-resolution images because they typically resize or crop images, losing subtle artifacts and information. Current datasets are low-resolution and don't represent real-world high-resolution scenarios.

Method: Proposes HiDA-Net with Feature Aggregation Module (FAM) that fuses features from multiple full-resolution local tiles with down-sampled global view. Includes Token-wise Forgery Localization (TFL) for spatial sensitivity and JPEG Quality Factor Estimation (QFE) to separate generative artifacts from compression noise. Also introduces HiRes-50K benchmark with 50,568 high-resolution images.

Result: Achieves state-of-the-art performance with over 13% accuracy improvement on Chameleon dataset and 10% improvement on the new HiRes-50K benchmark. Demonstrates superior handling of high-resolution AI-generated images.

Conclusion: HiDA-Net effectively addresses the limitations of existing detection methods for high-resolution AI-generated images by preserving native-resolution details and providing robust detection capabilities, setting a new standard for high-resolution image forensics.

Abstract: The rapid growth of high-resolution, meticulously crafted AI-generated images
poses a significant challenge to existing detection methods, which are often
trained and evaluated on low-resolution, automatically generated datasets that
do not align with the complexities of high-resolution scenarios. A common
practice is to resize or center-crop high-resolution images to fit standard
network inputs. However, without full coverage of all pixels, such strategies
risk either obscuring subtle, high-frequency artifacts or discarding
information from uncovered regions, leading to input information loss. In this
paper, we introduce the High-Resolution Detail-Aggregation Network (HiDA-Net),
a novel framework that ensures no pixel is left behind. We use the Feature
Aggregation Module (FAM), which fuses features from multiple full-resolution
local tiles with a down-sampled global view of the image. These local features
are aggregated and fused with global representations for final prediction,
ensuring that native-resolution details are preserved and utilized for
detection. To enhance robustness against challenges such as localized AI
manipulations and compression, we introduce Token-wise Forgery Localization
(TFL) module for fine-grained spatial sensitivity and JPEG Quality Factor
Estimation (QFE) module to disentangle generative artifacts from compression
noise explicitly. Furthermore, to facilitate future research, we introduce
HiRes-50K, a new challenging benchmark consisting of 50,568 images with up to
64 megapixels. Extensive experiments show that HiDA-Net achieves
state-of-the-art, increasing accuracy by over 13% on the challenging Chameleon
dataset and 10% on our HiRes-50K.

</details>


### [203] [DiCache: Let Diffusion Model Determine Its Own Cache](https://arxiv.org/abs/2508.17356)
*Jiazi Bu,Pengyang Ling,Yujie Zhou,Yibin Wang,Yuhang Zang,Tong Wu,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: DiCache is a training-free adaptive caching strategy that uses shallow-layer feature analysis to dynamically determine when and how to cache in diffusion models, achieving better efficiency and visual quality than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing caching-based acceleration methods for diffusion models rely on predefined empirical laws and handcrafted rules, which lack generalizability and fail on outlier samples due to the dynamic nature of diffusion processes.

Method: DiCache uses two main components: (1) Online Probe Profiling Scheme that leverages shallow-layer features to obtain real-time caching error priors for autonomous caching schedule determination, and (2) Dynamic Cache Trajectory Alignment that combines multi-step caches based on feature trajectory similarity to better approximate current features.

Result: Extensive experiments show DiCache achieves higher efficiency and improved visual fidelity over state-of-the-art methods on various diffusion models including WAN 2.1, HunyuanVideo, and Flux.

Conclusion: The proposed DiCache framework successfully addresses both when and how to cache in diffusion models through adaptive, training-free strategies based on feature correlation analysis, demonstrating superior performance across different model architectures.

Abstract: Recent years have witnessed the rapid development of acceleration techniques
for diffusion models, especially caching-based acceleration methods. These
studies seek to answer two fundamental questions: "When to cache" and "How to
use cache", typically relying on predefined empirical laws or dataset-level
priors to determine the timing of caching and utilizing handcrafted rules for
leveraging multi-step caches. However, given the highly dynamic nature of the
diffusion process, they often exhibit limited generalizability and fail on
outlier samples. In this paper, a strong correlation is revealed between the
variation patterns of the shallow-layer feature differences in the diffusion
model and those of final model outputs. Moreover, we have observed that the
features from different model layers form similar trajectories. Based on these
observations, we present DiCache, a novel training-free adaptive caching
strategy for accelerating diffusion models at runtime, answering both when and
how to cache within a unified framework. Specifically, DiCache is composed of
two principal components: (1) Online Probe Profiling Scheme leverages a
shallow-layer online probe to obtain a stable prior for the caching error in
real time, enabling the model to autonomously determine caching schedules. (2)
Dynamic Cache Trajectory Alignment combines multi-step caches based on
shallow-layer probe feature trajectory to better approximate the current
feature, facilitating higher visual quality. Extensive experiments validate
DiCache's capability in achieving higher efficiency and improved visual
fidelity over state-of-the-art methods on various leading diffusion models
including WAN 2.1, HunyuanVideo for video generation, and Flux for image
generation.

</details>


### [204] [Condition Weaving Meets Expert Modulation: Towards Universal and Controllable Image Generation](https://arxiv.org/abs/2508.17364)
*Guoqing Zhang,Xingtong Ge,Lu Shi,Xin Zhang,Muqing Xue,Wanru Xu,Yigang Cen*

Main category: cs.CV

TL;DR: UniGen is a unified framework for image-to-image generation that handles diverse conditional inputs efficiently using CoMoE modules to reduce redundancy and WeaveNet connections to bridge information gaps between backbone and control branches.


<details>
  <summary>Details</summary>
Motivation: Existing methods train separate control branches for each condition type, leading to redundant model structures and inefficient computational resource usage.

Method: Proposes Condition Modulated Expert (CoMoE) module to aggregate similar patch features and assign to expert modules, and WeaveNet dynamic connection mechanism to enable interaction between global text-level control and fine-grained conditional control.

Result: Extensive experiments on Subjects-200K and MultiGen-20M datasets show state-of-the-art performance across various conditional image generation tasks.

Conclusion: The method achieves superior performance in both versatility and effectiveness for unified image-to-image generation with diverse conditional inputs.

Abstract: The image-to-image generation task aims to produce controllable images by
leveraging conditional inputs and prompt instructions. However, existing
methods often train separate control branches for each type of condition,
leading to redundant model structures and inefficient use of computational
resources. To address this, we propose a Unified image-to-image Generation
(UniGen) framework that supports diverse conditional inputs while enhancing
generation efficiency and expressiveness. Specifically, to tackle the widely
existing parameter redundancy and computational inefficiency in controllable
conditional generation architectures, we propose the Condition Modulated Expert
(CoMoE) module. This module aggregates semantically similar patch features and
assigns them to dedicated expert modules for visual representation and
conditional modeling. By enabling independent modeling of foreground features
under different conditions, CoMoE effectively mitigates feature entanglement
and redundant computation in multi-condition scenarios. Furthermore, to bridge
the information gap between the backbone and control branches, we propose
WeaveNet, a dynamic, snake-like connection mechanism that enables effective
interaction between global text-level control from the backbone and
fine-grained control from conditional branches. Extensive experiments on the
Subjects-200K and MultiGen-20M datasets across various conditional image
generation tasks demonstrate that our method consistently achieves
state-of-the-art performance, validating its advantages in both versatility and
effectiveness. The code has been uploaded to
https://github.com/gavin-gqzhang/UniGen.

</details>


### [205] [Lightweight Joint Optimization of General-Purpose Vision-Language Models and Retrievers for Medical Diagnosis](https://arxiv.org/abs/2508.17394)
*Nir Mazor,Tom Hope*

Main category: cs.CV

TL;DR: A joint optimization model combining multimodal retriever with LVLM for medical diagnosis, outperforming standard RAG and achieving competitive results with medically-pretrained models using only general-purpose backbones and lightweight fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To enhance diagnostic accuracy in clinical decision-making by retrieving relevant visual information from medical literature and hospital records, addressing limitations of standard RAG where LVLM error signal doesn't propagate to retriever.

Method: Developed a model where multimodal retriever is jointly optimized with LVLM for medical diagnosis, using only general-purpose backbones with lightweight fine-tuning.

Result: Achieved competitive results with medically-pretrained models across clinical multi-label classification and visual question answering tasks. Joint optimization significantly improved challenging cases over standard RAG.

Conclusion: While correct diagnosis is frequently achievable using top retrieved images, there's a large performance gap from oracle, and frontier LVLM rerankers don't close this gap, leaving room for future improvement.

Abstract: Clinical decision-making often involves interpreting images (e.g., radiology)
for making diagnoses. Retrieving relevant visual information from medical
literature and hospital records could enhance diagnostic accuracy. In this
paper, we develop a model in which a multimodal retriever is jointly optimized
with an LVLM for medical diagnosis, unlike standard RAG where LVLM error signal
is not propagated down to the retriever. We show that using only
general-purpose backbones, with only lightweight fine-tuning, our model is able
to achieve competitive results with medically-pretrained models across clinical
multi-label classification and visual question answering tasks. In a novel
analysis, we additionally find that in many cases different top retrieved
images each lead to different predictions for a given target, and that these
cases are empirically challenging for all models, even for non-retrieval
models. Our joint retrieval optimization significantly improves these
challenging cases over standard RAG. However, oracle analysis reveals that
while the correct diagnosis is frequently achievable using one of the top
retrieved images, in practice there is a large performance gap from the oracle,
and rerankers using frontier LVLMs do not close this gap -- leaving ample room
for improvement by future methods. Code will be made publicly available.

</details>


### [206] [Enhancing Underwater Images via Deep Learning: A Comparative Study of VGG19 and ResNet50-Based Approaches](https://arxiv.org/abs/2508.17397)
*Aoqi Li,Yanghui Song,Jichao Dao,Chengfu Yang*

Main category: cs.CV

TL;DR: Deep learning-based underwater image enhancement using VGG19 and ResNet50 fusion for multi-scale feature analysis, evaluated with PSNR/UCIQE/UIQM metrics.


<details>
  <summary>Details</summary>
Motivation: Address challenging underwater image enhancement in complex scenes where traditional methods struggle with visibility and color distortion issues.

Method: Integrates VGG19 and ResNet50 CNN models to leverage their complementary feature extraction capabilities for multi-scale and multi-level analysis of underwater images.

Result: Achieves comprehensive and accurate image enhancement effects through unified model integration, with quantitative evaluation using quality assessment metrics.

Conclusion: Provides practical suggestions for model optimization, multi-model fusion, and hardware selection to improve practicality and stability of underwater visual enhancement systems.

Abstract: This paper addresses the challenging problem of image enhancement in complex
underwater scenes by proposing a solution based on deep learning. The proposed
method skillfully integrates two deep convolutional neural network models,
VGG19 and ResNet50, leveraging their powerful feature extraction capabilities
to perform multi-scale and multi-level deep feature analysis of underwater
images. By constructing a unified model, the complementary advantages of the
two models are effectively integrated, achieving a more comprehensive and
accurate image enhancement effect.To objectively evaluate the enhancement
effect, this paper introduces image quality assessment metrics such as PSNR,
UCIQE, and UIQM to quantitatively compare images before and after enhancement
and deeply analyzes the performance of different models in different
scenarios.Furthermore, to improve the practicality and stability of the
underwater visual enhancement system, this paper also provides practical
suggestions from aspects such as model optimization, multi-model fusion, and
hardware selection, aiming to provide strong technical support for visual
enhancement tasks in complex underwater environments.

</details>


### [207] [MoCo: Motion-Consistent Human Video Generation via Structure-Appearance Decoupling](https://arxiv.org/abs/2508.17404)
*Haoyu Wang,Hao Tang,Donglin Di,Zhilu Zhang,Wangmeng Zuo,Feng Gao,Siwei Ma,Shiliang Zhang*

Main category: cs.CV

TL;DR: MoCo is a novel human video generation framework that decouples structure and appearance generation to produce realistic whole-body human motions from text prompts, addressing limitations in existing methods that prioritize appearance over motion coherence.


<details>
  <summary>Details</summary>
Motivation: Existing video generation models focus too much on appearance fidelity, resulting in unrealistic human movements with poor structural coherence. Most datasets only contain facial/upper-body motions or dance videos, limiting generation capabilities to simple movements.

Method: Decouples human video generation into: 1) 3D structure generator for motion sequences from text, 2) appearance synthesis guided by structural sequence. Introduces Human-Aware Dynamic Control modules and dense tracking constraints for fine-grained control. Constructs large-scale whole-body human video dataset.

Result: Extensive experiments show MoCo outperforms existing approaches in generating realistic and structurally coherent human videos with complex whole-body motions.

Conclusion: MoCo successfully addresses the challenge of generating consistent human motion from text by separating structure and appearance generation, enabling more realistic and physically plausible whole-body human video generation.

Abstract: Generating human videos with consistent motion from text prompts remains a
significant challenge, particularly for whole-body or long-range motion.
Existing video generation models prioritize appearance fidelity, resulting in
unrealistic or physically implausible human movements with poor structural
coherence. Additionally, most existing human video datasets primarily focus on
facial or upper-body motions, or consist of vertically oriented dance videos,
limiting the scope of corresponding generation methods to simple movements. To
overcome these challenges, we propose MoCo, which decouples the process of
human video generation into two components: structure generation and appearance
generation. Specifically, our method first employs an efficient 3D structure
generator to produce a human motion sequence from a text prompt. The remaining
video appearance is then synthesized under the guidance of the generated
structural sequence. To improve fine-grained control over sparse human
structures, we introduce Human-Aware Dynamic Control modules and integrate
dense tracking constraints during training. Furthermore, recognizing the
limitations of existing datasets, we construct a large-scale whole-body human
video dataset featuring complex and diverse motions. Extensive experiments
demonstrate that MoCo outperforms existing approaches in generating realistic
and structurally coherent human videos.

</details>


### [208] [E-BayesSAM: Efficient Bayesian Adaptation of SAM with Self-Optimizing KAN-Based Interpretation for Uncertainty-Aware Ultrasonic Segmentation](https://arxiv.org/abs/2508.17408)
*Bin Huang,Zhong Liu,Huiying Wen,Bingsheng Huang,Xin Chen,Shuo Li*

Main category: cs.CV

TL;DR: E-BayesSAM is an efficient Bayesian adaptation of SAM that enables uncertainty-aware medical image segmentation with real-time inference, improved accuracy, and interpretability through token-wise variational inference and self-optimizing networks.


<details>
  <summary>Details</summary>
Motivation: To address three key limitations in Bayesian adaptation of SAM for medical segmentation: instability in fine-tuning, high computational cost due to massive parameters, and lack of interpretability in SAM's black-box design.

Method: Proposes E-BayesSAM framework with Token-wise Variational Bayesian Inference (T-VBI) for efficient Bayesian adaptation without training, and Self-Optimizing Kolmogorov-Arnold Network (SO-KAN) with learnable spline activations for improved interpretability and token pruning.

Result: Achieves real-time inference (0.03s/image), superior segmentation accuracy (89.0% DSC vs 88.3% for MedSAM), identifies 4 critical decision-making tokens, and demonstrates effectiveness across 5 ultrasound datasets.

Conclusion: E-BayesSAM successfully unifies efficiency, reliability, and interpretability, bridging SAM's versatility with clinical needs for safety-critical medical applications.

Abstract: Although the Segment Anything Model (SAM) has advanced medical image
segmentation, its Bayesian adaptation for uncertainty-aware segmentation
remains hindered by three key issues: (1) instability in Bayesian fine-tuning
of large pre-trained SAMs; (2) high computation cost due to SAM's massive
parameters; (3) SAM's black-box design limits interpretability. To overcome
these, we propose E-BayesSAM, an efficient framework combining Token-wise
Variational Bayesian Inference (T-VBI) for efficienty Bayesian adaptation and
Self-Optimizing Kolmogorov-Arnold Network (SO-KAN) for improving
interpretability. T-VBI innovatively reinterprets SAM's output tokens as
dynamic probabilistic weights and reparameterizes them as latent variables
without auxiliary training, enabling training-free VBI for uncertainty
estimation. SO-KAN improves token prediction with learnable spline activations
via self-supervised learning, providing insight to prune redundant tokens to
boost efficiency and accuracy. Experiments on five ultrasound datasets
demonstrated that E-BayesSAM achieves: (i) real-time inference (0.03s/image),
(ii) superior segmentation accuracy (average DSC: Pruned E-BayesSAM's 89.0\%
vs. E-BayesSAM's 88.0% vs. MedSAM's 88.3%), and (iii) identification of four
critical tokens governing SAM's decisions. By unifying efficiency, reliability,
and interpretability, E-BayesSAM bridges SAM's versatility with clinical needs,
advancing deployment in safety-critical medical applications. The source code
is available at https://github.com/mp31192/E-BayesSAM.

</details>


### [209] [Data Leakage in Visual Datasets](https://arxiv.org/abs/2508.17416)
*Patrick Ramos,Ryan Ramos,Noa Garcia*

Main category: cs.CV

TL;DR: Analysis reveals data leakage exists in all visual datasets studied, compromising fair model evaluation through various types of leakage from severe to subtle cases.


<details>
  <summary>Details</summary>
Motivation: To investigate data leakage in visual datasets where training images appear in evaluation benchmarks, which undermines fair model assessment, especially since large datasets are often sourced from the internet where benchmarks are publicly available.

Method: Applied image retrieval techniques to identify and characterize visual data leakage, categorizing it by modality, coverage, and degree of leakage.

Result: Found that all analyzed datasets exhibit some form of data leakage, and all types of leakage (from severe to subtle) compromise the reliability of model evaluation in downstream tasks.

Conclusion: Data leakage is a widespread problem in visual datasets that significantly undermines the validity and fairness of model evaluation, requiring attention to ensure proper benchmarking.

Abstract: We analyze data leakage in visual datasets. Data leakage refers to images in
evaluation benchmarks that have been seen during training, compromising fair
model evaluation. Given that large-scale datasets are often sourced from the
internet, where many computer vision benchmarks are publicly available, our
efforts are focused into identifying and studying this phenomenon. We
characterize visual leakage into different types according to its modality,
coverage, and degree. By applying image retrieval techniques, we unequivocally
show that all the analyzed datasets present some form of leakage, and that all
types of leakage, from severe instances to more subtle cases, compromise the
reliability of model evaluation in downstream tasks.

</details>


### [210] [Constrained Prompt Enhancement for Improving Zero-Shot Generalization of Vision-Language Models](https://arxiv.org/abs/2508.17417)
*Xiaojie Yin,Qilong Wang,Qinghua Hu*

Main category: cs.CV

TL;DR: A novel constrained prompt enhancement method that improves visual-textual alignment in VLMs through comprehensive textual prompts and compact visual prompts using topology-guided semantic generation and discriminative region selection.


<details>
  <summary>Details</summary>
Motivation: Vision-language models suffer from semantic misalignment due to domain gaps between pre-training and downstream tasks, with existing approaches facing issues of incomplete textual prompts and noisy visual prompts from random cropping.

Method: Proposes CPE with two components: TGSSG for generating comprehensive textual prompts using LLMs with semantic ambiguity entropy and persistent homology analysis, and CADRS for identifying discriminative visual regions using activation maps to filter noise. Uses set-to-set matching with TTA and OT for alignment.

Result: The method achieves improved visual-textual alignment by addressing incomplete semantic expression in text prompts and irrelevant visual noise, leading to better zero-shot generalization.

Conclusion: CPE effectively enhances prompt quality from both textual and visual perspectives, providing a robust solution for improving zero-shot performance of vision-language models through semantic-driven prompt construction and alignment strategies.

Abstract: Vision-language models (VLMs) pre-trained on web-scale data exhibit promising
zero-shot generalization but often suffer from semantic misalignment due to
domain gaps between pre-training and downstream tasks. Existing approaches
primarily focus on text prompting with class-specific descriptions and
visual-text adaptation via aligning cropped image regions with textual
descriptions. However, they still face the issues of incomplete textual prompts
and noisy visual prompts. In this paper, we propose a novel constrained prompt
enhancement (CPE) method to improve visual-textual alignment by constructing
comprehensive textual prompts and compact visual prompts from the semantic
perspective. Specifically, our approach consists of two key components:
Topology-Guided Synonymous Semantic Generation (TGSSG) and Category-Agnostic
Discriminative Region Selection (CADRS). Textually, to address the issue of
incomplete semantic expression in textual prompts, our TGSSG first generates
synonymous semantic set for each category via large language models, and
constructs comprehensive textual prompts based on semantic ambiguity entropy
and persistent homology analysis. Visually, to mitigate the irrelevant visual
noise introduced by random cropping, our CADRS identifies discriminative
regions with activation maps outputted by a pre-trained vision model,
effectively filtering out noisy regions and generating compact visual prompts.
Given the comprehensive set of textual prompts and compact set of visual
prompts, we introduce two set-to-set matching strategies based on test-time
adaptation (TTA) and optimal transport (OT) to achieve effective visual-textual
alignment, and so improve zero-shot generalization of VLMs.

</details>


### [211] [Robust Point Cloud Registration via Geometric Overlapping Guided Rotation Search](https://arxiv.org/abs/2508.17427)
*Zhao Zheng,Jingfan Fan,Long Shao,Hong Song,Danni Ai,Tianyu Fu,Deqiang Xiao,Yongtian Wang,Jian Yang*

Main category: cs.CV

TL;DR: A novel point cloud registration method using rotation-only branch-and-bound search with Chasles' theorem decomposition and range maximum query optimization for improved accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current state-of-the-art methods for point cloud registration either require quadratic space/time complexity for graph construction or suffer from inaccuracy due to local optima in multi-stage branch-and-bound search, especially under high outlier ratios.

Method: Decomposes rigid transformation using Chasles' theorem into translation along rotation axis and 2D rigid transformation. Uses BnB search for optimal rotation axis/angle, with residual parameters as range maximum query problems. Employs cube mapping for axis parameterization, interval stabbing for translation estimation, and sweep line algorithm with segment tree for 2D RMQ.

Result: Superior accuracy and efficiency demonstrated on 3DMatch, 3DLoMatch, and KITTI datasets compared to state-of-the-art methods, with polynomial time complexity and linear space complexity even in worst-case scenarios.

Conclusion: The proposed geometric maximum overlapping registration framework provides an effective solution that overcomes limitations of existing methods, offering both high accuracy and computational efficiency for point cloud registration tasks.

Abstract: Point cloud registration based on correspondences computes the rigid
transformation that maximizes the number of inliers constrained within the
noise threshold. Current state-of-the-art (SOTA) methods employing spatial
compatibility graphs or branch-and-bound (BnB) search mainly focus on
registration under high outlier ratios. However, graph-based methods require at
least quadratic space and time complexity for graph construction, while
multi-stage BnB search methods often suffer from inaccuracy due to local optima
between decomposed stages. This paper proposes a geometric maximum overlapping
registration framework via rotation-only BnB search. The rigid transformation
is decomposed using Chasles' theorem into a translation along rotation axis and
a 2D rigid transformation. The optimal rotation axis and angle are searched via
BnB, with residual parameters formulated as range maximum query (RMQ) problems.
Firstly, the top-k candidate rotation axes are searched within a hemisphere
parameterized by cube mapping, and the translation along each axis is estimated
through interval stabbing of the correspondences projected onto that axis.
Secondly, the 2D registration is relaxed to 1D rotation angle search with 2D
RMQ of geometric overlapping for axis-aligned rectangles, which is solved
deterministically in polynomial time using sweep line algorithm with segment
tree. Experimental results on 3DMatch, 3DLoMatch, and KITTI datasets
demonstrate superior accuracy and efficiency over SOTA methods, while the time
complexity is polynomial and the space complexity increases linearly with the
number of points, even in the worst case.

</details>


### [212] [FedKLPR: Personalized Federated Learning for Person Re-Identification with Adaptive Pruning](https://arxiv.org/abs/2508.17431)
*Po-Hsien Yu,Yu-Syuan Tseng,Shao-Yi Chien*

Main category: cs.CV

TL;DR: FedKLPR is a lightweight federated learning framework for person re-identification that addresses statistical heterogeneity and communication overhead through KL-divergence regularization, pruning-based aggregation, and dynamic compression techniques.


<details>
  <summary>Details</summary>
Motivation: Person re-identification requires privacy-preserving collaborative training, but faces challenges with non-IID data distributions across clients and high communication costs from frequent large model transmissions in federated learning settings.

Method: Proposes FedKLPR with four components: 1) KL-Divergence Regularization Loss to minimize divergence from global distribution, 2) KL-Divergence-Prune Weighted Aggregation for robust aggregation with reduced communication, 3) sparse Activation Skipping to preserve critical parameters, and 4) Cross-Round Recovery for dynamic pruning control.

Result: Achieves 33%-38% communication reduction on ResNet-50 and 20%-40% on ResNet-34 while maintaining model accuracy within 1% degradation across eight benchmark datasets.

Conclusion: FedKLPR effectively addresses both statistical heterogeneity and communication overhead challenges in federated person re-identification, providing a practical solution for privacy-preserving collaborative learning with significant efficiency improvements.

Abstract: Person re-identification (Re-ID) is a fundamental task in intelligent
surveillance and public safety. Federated learning (FL) offers a
privacy-preserving solution by enabling collaborative model training without
centralized data collection. However, applying FL to real-world re-ID systems
faces two major challenges: statistical heterogeneity across clients due to
non-IID data distributions, and substantial communication overhead caused by
frequent transmission of large-scale models. To address these issues, we
propose FedKLPR, a lightweight and communication-efficient federated learning
framework for person re-identification. FedKLPR introduces four key components.
First, the KL-Divergence Regularization Loss (KLL) constrains local models by
minimizing the divergence from the global feature distribution, effectively
mitigating the effects of statistical heterogeneity and improving convergence
stability under non-IID conditions. Secondly, KL-Divergence-Prune Weighted
Aggregation (KLPWA) integrates pruning ratio and distributional similarity into
the aggregation process, thereby improving the robustness of the global model
while significantly reducing communication overhead. Furthermore, sparse
Activation Skipping (SAS) mitigates the dilution of critical parameters during
the aggregation of pruned client models by excluding zero-valued weights from
the update process. Finally, Cross-Round Recovery (CRR) introduces a dynamic
pruning control mechanism that halts pruning when necessary, enabling deeper
compression while maintaining model accuracy. Experimental results on eight
benchmark datasets demonstrate that FedKLPR achieves significant communication
reduction. Compared with the state-of-the-art, FedKLPR reduces 33\%-38\%
communication cost on ResNet-50 and 20\%-40\% communication cost on ResNet-34,
while maintaining model accuracy within 1\% degradation.

</details>


### [213] [TinySR: Pruning Diffusion for Real-World Image Super-Resolution](https://arxiv.org/abs/2508.17434)
*Linwei Dong,Qingnan Fan,Yuhang Yu,Qi Zhang,Jinwei Chen,Yawei Luo,Changqing Zou*

Main category: cs.CV

TL;DR: TinySR is a compact diffusion model for real-time image super-resolution that achieves 5.68x speedup and 83% parameter reduction while maintaining quality through architectural optimizations.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models for Real-ISR suffer from high computational overhead due to iterative denoising, and even one-step methods have large, over-parameterized architectures that hinder real-time applications.

Method: Introduces Dynamic Inter-block Activation and Expansion-Corrosion Strategy for depth pruning, VAE compression through channel pruning and attention removal, eliminates time/prompt modules, and uses pre-caching techniques for acceleration.

Result: Achieves up to 5.68x speedup and 83% parameter reduction compared to teacher model TSD-SR while maintaining high perceptual quality in real-time image super-resolution.

Conclusion: TinySR demonstrates that compact diffusion models can achieve real-time performance in image super-resolution through careful architectural design and optimization strategies without sacrificing quality.

Abstract: Real-world image super-resolution (Real-ISR) focuses on recovering
high-quality images from low-resolution inputs that suffer from complex
degradations like noise, blur, and compression. Recently, diffusion models
(DMs) have shown great potential in this area by leveraging strong generative
priors to restore fine details. However, their iterative denoising process
incurs high computational overhead, posing challenges for real-time
applications. Although one-step distillation methods, such as OSEDiff and
TSD-SR, offer faster inference, they remain fundamentally constrained by their
large, over-parameterized model architectures. In this work, we present TinySR,
a compact yet effective diffusion model specifically designed for Real-ISR that
achieves real-time performance while maintaining perceptual quality. We
introduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy
to facilitate more effective decision-making in depth pruning. We achieve VAE
compression through channel pruning, attention removal and lightweight SepConv.
We eliminate time- and prompt-related modules and perform pre-caching
techniques to further speed up the model. TinySR significantly reduces
computational cost and model size, achieving up to 5.68x speedup and 83%
parameter reduction compared to its teacher TSD-SR, while still providing high
quality results.

</details>


### [214] [An LLM-LVLM Driven Agent for Iterative and Fine-Grained Image Editing](https://arxiv.org/abs/2508.17435)
*Zihan Liang,Jiahao Sun,Haoran Ma*

Main category: cs.CV

TL;DR: RefineEdit-Agent is a training-free intelligent agent framework that uses LLMs and LVLMs for complex, iterative image editing with better granular instruction understanding and context preservation than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image generation models struggle with fine-grained iterative editing, lacking granular instruction understanding, robust context preservation, and intelligent feedback mechanisms for refinement.

Method: A closed-loop system combining LLM planning capabilities and LVLM visual understanding. Includes LVLM instruction parser, multi-level LLM editing planner, iterative editing module, and LVLM feedback/evaluation loop.

Result: Achieved average score of 3.67 on LongBench-T2I-Edit benchmark, significantly outperforming baselines: Direct Re-Prompting (2.29), InstructPix2Pix (2.91), GLIGEN-based Edit (3.16), and ControlNet-XL (3.39).

Conclusion: RefineEdit-Agent demonstrates superior edit fidelity and context preservation through its agentic design, validated by ablation studies and human evaluations across various instruction complexities.

Abstract: Despite the remarkable capabilities of text-to-image (T2I) generation models,
real-world applications often demand fine-grained, iterative image editing that
existing methods struggle to provide. Key challenges include granular
instruction understanding, robust context preservation during modifications,
and the lack of intelligent feedback mechanisms for iterative refinement. This
paper introduces RefineEdit-Agent, a novel, training-free intelligent agent
framework designed to address these limitations by enabling complex, iterative,
and context-aware image editing. RefineEdit-Agent leverages the powerful
planning capabilities of Large Language Models (LLMs) and the advanced visual
understanding and evaluation prowess of Vision-Language Large Models (LVLMs)
within a closed-loop system. Our framework comprises an LVLM-driven instruction
parser and scene understanding module, a multi-level LLM-driven editing planner
for goal decomposition, tool selection, and sequence generation, an iterative
image editing module, and a crucial LVLM-driven feedback and evaluation loop.
To rigorously evaluate RefineEdit-Agent, we propose LongBench-T2I-Edit, a new
benchmark featuring 500 initial images with complex, multi-turn editing
instructions across nine visual dimensions. Extensive experiments demonstrate
that RefineEdit-Agent significantly outperforms state-of-the-art baselines,
achieving an average score of 3.67 on LongBench-T2I-Edit, compared to 2.29 for
Direct Re-Prompting, 2.91 for InstructPix2Pix, 3.16 for GLIGEN-based Edit, and
3.39 for ControlNet-XL. Ablation studies, human evaluations, and analyses of
iterative refinement, backbone choices, tool usage, and robustness to
instruction complexity further validate the efficacy of our agentic design in
delivering superior edit fidelity and context preservation.

</details>


### [215] [Disentangled Geometry and Appearance for Efficient Multi-View Surface Reconstruction and Rendering](https://arxiv.org/abs/2508.17436)
*Qitong Zhang,Jieqing Feng*

Main category: cs.CV

TL;DR: Efficient mesh-based neural rendering method that eliminates separate mesh extraction, achieves fast training/rendering speeds while maintaining competitive reconstruction quality and enabling practical editing applications.


<details>
  <summary>Details</summary>
Motivation: Address limitations of neural rendering methods that require additional mesh extraction steps, which are inconvenient and produce poor-quality surfaces with mesh aliasing, restricting downstream applications.

Method: Uses explicit mesh representation with differentiable rasterization. Introduces disentangled geometry/appearance model without deep networks, neural deformation field for global geometric context, novel regularization for geometric features, and view-invariant diffuse term baked into vertices.

Result: Achieves state-of-the-art training (4.84 minutes) and rendering (0.023 seconds) speeds with competitive reconstruction quality. Enables practical mesh and texture editing applications.

Conclusion: The method combines efficiency, competitive quality, and broad applicability, making it a valuable contribution to multi-view surface reconstruction and rendering.

Abstract: This paper addresses the limitations of neural rendering-based multi-view
surface reconstruction methods, which require an additional mesh extraction
step that is inconvenient and would produce poor-quality surfaces with mesh
aliasing, restricting downstream applications. Building on the explicit mesh
representation and differentiable rasterization framework, this work proposes
an efficient solution that preserves the high efficiency of this framework
while significantly improving reconstruction quality and versatility.
Specifically, we introduce a disentangled geometry and appearance model that
does not rely on deep networks, enhancing learning and broadening
applicability. A neural deformation field is constructed to incorporate global
geometric context, enhancing geometry learning, while a novel regularization
constrains geometric features passed to a neural shader to ensure its accuracy
and boost shading. For appearance, a view-invariant diffuse term is separated
and baked into mesh vertices, further improving rendering efficiency.
Experimental results demonstrate that the proposed method achieves
state-of-the-art training (4.84 minutes) and rendering (0.023 seconds) speeds,
with reconstruction quality that is competitive with top-performing methods.
Moreover, the method enables practical applications such as mesh and texture
editing, showcasing its versatility and application potential. This combination
of efficiency, competitive quality, and broad applicability makes our approach
a valuable contribution to multi-view surface reconstruction and rendering.

</details>


### [216] [Pixie: Fast and Generalizable Supervised Learning of 3D Physics from Pixels](https://arxiv.org/abs/2508.17437)
*Long Le,Ryan Lucas,Chen Wang,Chuhao Chen,Dinesh Jayaraman,Eric Eaton,Lingjie Liu*

Main category: cs.CV

TL;DR: PIXIE is a fast, generalizable neural network that predicts physical material properties from 3D visual features using supervised learning, outperforming optimization methods by 1.46-4.39x and enabling realistic physics simulations.


<details>
  <summary>Details</summary>
Motivation: Existing methods for inferring physical properties from 3D scenes rely on slow per-scene optimization, limiting generalizability and practical application in creating interactive virtual worlds.

Method: Trains a generalizable neural network using supervised losses to predict physical properties from 3D visual features. Uses feed-forward inference and couples with learned static scene representations like Gaussian Splatting for physics simulation.

Result: Achieves 1.46-4.39x better performance than test-time optimization methods with orders of magnitude faster inference. Can zero-shot generalize to real-world scenes using pretrained visual features like CLIP, despite training only on synthetic data.

Conclusion: PIXIE provides an efficient and generalizable solution for physical property inference from 3D scenes, enabling realistic physics simulations and demonstrating strong zero-shot generalization capabilities to real-world applications.

Abstract: Inferring the physical properties of 3D scenes from visual information is a
critical yet challenging task for creating interactive and realistic virtual
worlds. While humans intuitively grasp material characteristics such as
elasticity or stiffness, existing methods often rely on slow, per-scene
optimization, limiting their generalizability and application. To address this
problem, we introduce PIXIE, a novel method that trains a generalizable neural
network to predict physical properties across multiple scenes from 3D visual
features purely using supervised losses. Once trained, our feed-forward network
can perform fast inference of plausible material fields, which coupled with a
learned static scene representation like Gaussian Splatting enables realistic
physics simulation under external forces. To facilitate this research, we also
collected PIXIEVERSE, one of the largest known datasets of paired 3D assets and
physic material annotations. Extensive evaluations demonstrate that PIXIE is
about 1.46-4.39x better and orders of magnitude faster than test-time
optimization methods. By leveraging pretrained visual features like CLIP, our
method can also zero-shot generalize to real-world scenes despite only ever
been trained on synthetic data. https://pixie-3d.github.io/

</details>


### [217] [Investigating Domain Gaps for Indoor 3D Object Detection](https://arxiv.org/abs/2508.17439)
*Zijing Zhao,Zhu Xu,Qingchao Chen,Yuxin Peng,Yang Liu*

Main category: cs.CV

TL;DR: This paper introduces a comprehensive benchmark for domain adaptive indoor 3D object detection across multiple datasets, analyzing different domain gaps and providing baseline approaches to improve cross-dataset generalization.


<details>
  <summary>Details</summary>
Motivation: Existing 3D object detection research has been limited to datasets with identical training and testing distributions, but real-world applications require detectors that can generalize across different datasets with varying collection methods and characteristics.

Method: The authors create a benchmark using ScanNet, SUN RGB-D, 3D Front datasets plus newly generated synthetic datasets ProcTHOR-OD and ProcFront. They conduct experiments across different adaptation scenarios including synthetic-to-real, point cloud quality, layout, and instance feature adaptation.

Result: The study analyzes the impact of various domain gaps on 3D object detectors and introduces several approaches to improve adaptation performance, establishing baselines for domain adaptive indoor 3D object detection.

Conclusion: The work provides a foundation for developing 3D object detectors with stronger generalization ability across different domains and datasets, addressing the challenge of domain shift in indoor scene understanding.

Abstract: As a fundamental task for indoor scene understanding, 3D object detection has
been extensively studied, and the accuracy on indoor point cloud data has been
substantially improved. However, existing researches have been conducted on
limited datasets, where the training and testing sets share the same
distribution. In this paper, we consider the task of adapting indoor 3D object
detectors from one dataset to another, presenting a comprehensive benchmark
with ScanNet, SUN RGB-D and 3D Front datasets, as well as our newly proposed
large-scale datasets ProcTHOR-OD and ProcFront generated by a 3D simulator.
Since indoor point cloud datasets are collected and constructed in different
ways, the object detectors are likely to overfit to specific factors within
each dataset, such as point cloud quality, bounding box layout and instance
features. We conduct experiments across datasets on different adaptation
scenarios including synthetic-to-real adaptation, point cloud quality
adaptation, layout adaptation and instance feature adaptation, analyzing the
impact of different domain gaps on 3D object detectors. We also introduce
several approaches to improve adaptation performances, providing baselines for
domain adaptive indoor 3D object detection, hoping that future works may
propose detectors with stronger generalization ability across domains. Our
project homepage can be found in
https://jeremyzhao1998.github.io/DAVoteNet-release/.

</details>


### [218] [Multi-Level LVLM Guidance for Untrimmed Video Action Recognition](https://arxiv.org/abs/2508.17442)
*Liyang Peng,Sihan Zhu,Yunjie Guo*

Main category: cs.CV

TL;DR: ECVT is a novel video transformer architecture that uses Large Vision-Language Models to generate multi-granularity semantic descriptions for improved action recognition and localization in untrimmed videos, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with capturing fine-grained actions, long-term temporal dependencies, and high-level semantic information from low-level visual features in complex, untrimmed videos.

Method: Dual-branch design with Video Encoding Branch for spatio-temporal features and Cross-Modal Guidance Branch using LVLM for multi-granularity semantic descriptions. Includes adaptive gating, cross-modal attention, and event graph module for integration.

Result: Achieves state-of-the-art performance with average mAP of 40.5% on ActivityNet v1.3 and mAP@0.5 of 67.1% on THUMOS14, outperforming leading baselines.

Conclusion: ECVT effectively bridges the semantic gap in video understanding by leveraging LVLMs for contextual guidance, significantly enhancing temporal structure and event logic comprehension.

Abstract: Action recognition and localization in complex, untrimmed videos remain a
formidable challenge in computer vision, largely due to the limitations of
existing methods in capturing fine-grained actions, long-term temporal
dependencies, and high-level semantic information from low-level visual
features. This paper introduces the Event-Contextualized Video Transformer
(ECVT), a novel architecture that leverages the advanced semantic understanding
capabilities of Large Vision-Language Models (LVLMs) to bridge this gap. ECVT
employs a dual-branch design, comprising a Video Encoding Branch for
spatio-temporal feature extraction and a Cross-Modal Guidance Branch. The
latter utilizes an LVLM to generate multi-granularity semantic descriptions,
including Global Event Prompting for macro-level narrative and Temporal
Sub-event Prompting for fine-grained action details. These multi-level textual
cues are integrated into the video encoder's learning process through
sophisticated mechanisms such as adaptive gating for high-level semantic
fusion, cross-modal attention for fine-grained feature refinement, and an event
graph module for temporal context calibration. Trained end-to-end with a
comprehensive loss function incorporating semantic consistency and temporal
calibration terms, ECVT significantly enhances the model's ability to
understand video temporal structures and event logic. Extensive experiments on
ActivityNet v1.3 and THUMOS14 datasets demonstrate that ECVT achieves
state-of-the-art performance, with an average mAP of 40.5% on ActivityNet v1.3
and mAP@0.5 of 67.1% on THUMOS14, outperforming leading baselines.

</details>


### [219] [A Synthetic Dataset for Manometry Recognition in Robotic Applications](https://arxiv.org/abs/2508.17468)
*Pedro Antonio Rabelo Saraiva,Enzo Ferreira de Souza,Joao Manoel Herrera Pinheiro,Thiago H. Segreto,Ricardo V. Godoy,Marcelo Becker*

Main category: cs.CV

TL;DR: Hybrid data synthesis pipeline combining procedural rendering and AI video generation to overcome data scarcity in industrial object detection, achieving superior performance with 1:1 real-synthetic data mixture.


<details>
  <summary>Details</summary>
Motivation: Address data scarcity and high acquisition costs for training object detection models in hazardous industrial environments like offshore oil platforms, where collecting real-world data is economically and practically challenging.

Method: Proposes hybrid data synthesis using BlenderProc for photorealistic images with precise annotations and domain randomization, integrated with NVIDIA's Cosmos-Predict2 for physically plausible video sequences with temporal diversity and rare viewpoints.

Result: YOLO-based detection network trained on composite dataset (real + synthetic data) achieves superior performance compared to real-only training, with 1:1 mixture yielding highest accuracy.

Conclusion: Synthetic-first approach is viable, efficient, cost-effective, and safe alternative for developing reliable perception systems in safety-critical industrial applications with resource constraints.

Abstract: This work addresses the challenges of data scarcity and high acquisition
costs for training robust object detection models in complex industrial
environments, such as offshore oil platforms. The practical and economic
barriers to collecting real-world data in these hazardous settings often hamper
the development of autonomous inspection systems. To overcome this, in this
work we propose and validate a hybrid data synthesis pipeline that combines
procedural rendering with AI-driven video generation. Our methodology leverages
BlenderProc to create photorealistic images with precise annotations and
controlled domain randomization, and integrates NVIDIA's Cosmos-Predict2
world-foundation model to synthesize physically plausible video sequences with
temporal diversity, capturing rare viewpoints and adverse conditions. We
demonstrate that a YOLO-based detection network trained on a composite dataset,
blending real images with our synthetic data, achieves superior performance
compared to models trained exclusively on real-world data. Notably, a 1:1
mixture of real and synthetic data yielded the highest accuracy, surpassing the
real-only baseline. These findings highlight the viability of a synthetic-first
approach as an efficient, cost-effective, and safe alternative for developing
reliable perception systems in safety-critical and resource-constrained
industrial applications.

</details>


### [220] [T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image Generation](https://arxiv.org/abs/2508.17472)
*Kaiyue Sun,Rongyao Fang,Chengqi Duan,Xian Liu,Xihui Liu*

Main category: cs.CV

TL;DR: T2I-ReasonBench is a new benchmark for evaluating text-to-image models' reasoning capabilities across four dimensions, with a two-stage evaluation protocol for accuracy and quality assessment.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate and benchmark the reasoning capabilities of text-to-image generation models, which is crucial for understanding their ability to handle complex prompts requiring reasoning.

Method: Proposed a benchmark with four reasoning dimensions (Idiom Interpretation, Textual Image Design, Entity-Reasoning, Scientific-Reasoning) and a two-stage evaluation protocol to assess both reasoning accuracy and image quality.

Result: The paper benchmarks various T2I generation models and provides comprehensive analysis of their performances across the proposed reasoning dimensions.

Conclusion: T2I-ReasonBench serves as an effective tool for evaluating and comparing the reasoning capabilities of different text-to-image models, revealing their strengths and weaknesses in handling complex reasoning tasks.

Abstract: We propose T2I-ReasonBench, a benchmark evaluating reasoning capabilities of
text-to-image (T2I) models. It consists of four dimensions: Idiom
Interpretation, Textual Image Design, Entity-Reasoning and
Scientific-Reasoning. We propose a two-stage evaluation protocol to assess the
reasoning accuracy and image quality. We benchmark various T2I generation
models, and provide comprehensive analysis on their performances.

</details>


### [221] [GraphMMP: A Graph Neural Network Model with Mutual Information and Global Fusion for Multimodal Medical Prognosis](https://arxiv.org/abs/2508.17478)
*Xuhao Shan,Ruiquan Ge,Jikui Liu,Linglong Wu,Chi Zhang,Siqi Liu,Wenjian Qin,Wenwen Min,Ahmed Elazab,Changmiao Wang*

Main category: cs.CV

TL;DR: GraphMMP: A two-stage graph neural network model for multimodal medical prognosis that uses mutual information to construct feature graphs and Mamba-based global fusion, achieving state-of-the-art performance on liver prognosis and METABRIC datasets.


<details>
  <summary>Details</summary>
Motivation: To address challenges in modeling complex interactions between heterogeneous medical data modalities while capturing both local and global dependencies across different data types in multimodal medical analysis.

Method: Proposes a two-stage multimodal prognosis model based on graph neural networks. Constructs feature graphs using mutual information and features a global fusion module built on Mamba architecture.

Result: Empirical results show GraphMMP surpasses existing methods on liver prognosis datasets and METABRIC study, demonstrating superior performance in multimodal medical prognosis tasks.

Conclusion: GraphMMP effectively addresses multimodal medical data analysis challenges and significantly boosts prognosis performance through its graph-based approach and Mamba fusion module.

Abstract: In the field of multimodal medical data analysis, leveraging diverse types of
data and understanding their hidden relationships continues to be a research
focus. The main challenges lie in effectively modeling the complex interactions
between heterogeneous data modalities with distinct characteristics while
capturing both local and global dependencies across modalities. To address
these challenges, this paper presents a two-stage multimodal prognosis model,
GraphMMP, which is based on graph neural networks. The proposed model
constructs feature graphs using mutual information and features a global fusion
module built on Mamba, which significantly boosts prognosis performance.
Empirical results show that GraphMMP surpasses existing methods on datasets
related to liver prognosis and the METABRIC study, demonstrating its
effectiveness in multimodal medical prognosis tasks.

</details>


### [222] [Optimizing Multi-Modal Trackers via Sensitivity-aware Regularized Tuning](https://arxiv.org/abs/2508.17488)
*Zhiwen Chen,Jinjian Wu,Zhiyu Zhu,Yifan Zhang,Guangming Shi,Junhui Hou*

Main category: cs.CV

TL;DR: A sensitivity-aware regularized tuning framework that optimizes multi-modal trackers by incorporating parameter sensitivities to improve plasticity-stability trade-off during fine-tuning of pre-trained RGB models.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuning approaches for multi-modal trackers suffer from either excessive freedom or over-restriction, leading to suboptimal plasticity-stability balance when adapting pre-trained RGB models.

Method: Proposes a sensitivity-aware regularized tuning framework that analyzes tangent space of pre-trained weights to measure prior sensitivities, then explores transfer sensitivities during tuning phase, incorporating both as regularization terms.

Result: The method significantly enhances transferability across modalities and surpasses state-of-the-art techniques in various multi-modal tracking tasks.

Conclusion: The proposed sensitivity-aware regularization framework effectively addresses the plasticity-stability dilemma in multi-modal tracker optimization, demonstrating superior performance across different tracking scenarios.

Abstract: This paper tackles the critical challenge of optimizing multi-modal trackers
by effectively adapting the pre-trained models for RGB data. Existing
fine-tuning paradigms oscillate between excessive freedom and over-restriction,
both leading to a suboptimal plasticity-stability trade-off. To mitigate this
dilemma, we propose a novel sensitivity-aware regularized tuning framework,
which delicately refines the learning process by incorporating intrinsic
parameter sensitivities. Through a comprehensive investigation from pre-trained
to multi-modal contexts, we identify that parameters sensitive to pivotal
foundational patterns and cross-domain shifts are primary drivers of this
issue. Specifically, we first analyze the tangent space of pre-trained weights
to measure and orient prior sensitivities, dedicated to preserving
generalization. Then, we further explore transfer sensitivities during the
tuning phase, emphasizing adaptability and stability. By incorporating these
sensitivities as regularization terms, our method significantly enhances the
transferability across modalities. Extensive experiments showcase the superior
performance of the proposed method, surpassing current state-of-the-art
techniques across various multi-modal tracking. The source code and models will
be publicly available at https://github.com/zhiwen-xdu/SRTrack.

</details>


### [223] [Social-MAE: A Transformer-Based Multimodal Autoencoder for Face and Voice](https://arxiv.org/abs/2508.17502)
*Hugo Bohy,Minh Tran,Kevin El Haddad,Thierry Dutoit,Mohammad Soleymani*

Main category: cs.CV

TL;DR: Social-MAE is a self-supervised audiovisual masked autoencoder pre-trained on social interaction data that achieves state-of-the-art results on emotion recognition and laughter detection tasks.


<details>
  <summary>Details</summary>
Motivation: Human social behaviors are inherently multimodal, requiring powerful audiovisual models for perception. Existing models need better pre-training on social interaction data to improve performance on social and affective computing tasks.

Method: Extended CAV-MAE (Contrastive Audio-Visual Masked Auto-Encoder) to handle more frames, pre-trained self-supervised on VoxCeleb2 human social interaction dataset, then fine-tuned on downstream tasks.

Result: Achieved state-of-the-art results on multimodal emotion recognition and laughter recognition, and competitive results for apparent personality estimation.

Conclusion: In-domain self-supervised pre-training on social data is effective for improving performance on social and affective computing tasks, demonstrating the value of domain-specific pre-training.

Abstract: Human social behaviors are inherently multimodal necessitating the
development of powerful audiovisual models for their perception. In this paper,
we present Social-MAE, our pre-trained audiovisual Masked Autoencoder based on
an extended version of Contrastive Audio-Visual Masked Auto-Encoder (CAV-MAE),
which is pre-trained on audiovisual social data. Specifically, we modify
CAV-MAE to receive a larger number of frames as input and pre-train it on a
large dataset of human social interaction (VoxCeleb2) in a self-supervised
manner. We demonstrate the effectiveness of this model by finetuning and
evaluating the model on different social and affective downstream tasks,
namely, emotion recognition, laughter detection and apparent personality
estimation. The model achieves state-of-the-art results on multimodal emotion
recognition and laughter recognition and competitive results for apparent
personality estimation, demonstrating the effectiveness of in-domain
self-supervised pre-training. Code and model weight are available here
https://github.com/HuBohy/SocialMAE.

</details>


### [224] [DinoTwins: Combining DINO and Barlow Twins for Robust, Label-Efficient Vision Transformers](https://arxiv.org/abs/2508.17509)
*Michael Podsiadly,Brendon K Lay*

Main category: cs.CV

TL;DR: Combining DINO and Barlow Twins techniques creates a hybrid self-supervised learning model that achieves comparable performance to DINO with better efficiency and smaller batch sizes.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing self-supervised learning methods - DINO's sensitivity to augmentations and Barlow Twins' requirement for large batch sizes - by leveraging their complementary strengths for more efficient training with fewer labels.

Method: Combined the redundancy-reduction objective from Barlow Twins with the self-distillation strategy from DINO, trained on MS COCO dataset using only 10% labeled data for linear probing, and compared against standalone implementations.

Result: The hybrid model achieved comparable loss and classification accuracy to DINO while maintaining strong feature representations, with attention visualizations showing improved semantic segmentation capability.

Conclusion: The combined approach provides a scalable, label-efficient alternative for training Vision Transformers in resource-constrained environments, offering the benefits of both methods without their individual limitations.

Abstract: Training AI models to understand images without costly labeled data remains a
challenge. We combine two techniques--DINO (teacher-student learning) and
Barlow Twins (redundancy reduction)--to create a model that learns better with
fewer labels and less compute. While both DINO and Barlow Twins have
independently demonstrated strong performance in self-supervised learning, each
comes with limitations--DINO may be sensitive to certain augmentations, and
Barlow Twins often requires batch sizes too large to fit on consumer hardware.
By combining the redundancy-reduction objective of Barlow Twins with the
self-distillation strategy of DINO, we aim to leverage their complementary
strengths. We train a hybrid model on the MS COCO dataset using only 10\% of
labeled data for linear probing, and evaluate its performance against
standalone DINO and Barlow Twins implementations. Preliminary results show that
the combined approach achieves comparable loss and classification accuracy to
DINO while maintaining strong feature representations. Attention visualizations
further suggest improved semantic segmentation capability in the hybrid model.
This combined method offers a scalable, label-efficient alternative for
training ViTs in resource-constrained environments.

</details>


### [225] [OmniMRI: A Unified Vision--Language Foundation Model for Generalist MRI Interpretation](https://arxiv.org/abs/2508.17524)
*Xingxin He,Aurora Rofena,Ruimin Feng,Haozhe Liao,Zhaoye Zhou,Albert Jang,Fang Liu*

Main category: cs.CV

TL;DR: OmniMRI is a unified vision-language foundation model that integrates the entire MRI workflow - from acquisition to diagnosis - into a single architecture, trained on large-scale multimodal data from 60+ datasets.


<details>
  <summary>Details</summary>
Motivation: Current MRI workflows are fragmented into separate stages (acquisition, reconstruction, segmentation, diagnosis) with limited generalizability across clinical settings and lack integration of language information that radiologists rely on.

Method: Multi-stage training on 220,000+ MRI volumes and 19M slices from 60 datasets, including self-supervised vision pretraining, vision-language alignment, multimodal pretraining, and multi-task instruction tuning.

Result: Qualitative results show OmniMRI can perform diverse tasks within one architecture: MRI reconstruction, anatomical/pathological segmentation, abnormality detection, diagnostic suggestion, and radiology report generation.

Conclusion: OmniMRI demonstrates potential to consolidate fragmented MRI pipelines into a scalable, generalist framework that unifies imaging and clinical language for comprehensive end-to-end MRI interpretation.

Abstract: Magnetic Resonance Imaging (MRI) is indispensable in clinical practice but
remains constrained by fragmented, multi-stage workflows encompassing
acquisition, reconstruction, segmentation, detection, diagnosis, and reporting.
While deep learning has achieved progress in individual tasks, existing
approaches are often anatomy- or application-specific and lack generalizability
across diverse clinical settings. Moreover, current pipelines rarely integrate
imaging data with complementary language information that radiologists rely on
in routine practice. Here, we introduce OmniMRI, a unified vision-language
foundation model designed to generalize across the entire MRI workflow. OmniMRI
is trained on a large-scale, heterogeneous corpus curated from 60 public
datasets, over 220,000 MRI volumes and 19 million MRI slices, incorporating
image-only data, paired vision-text data, and instruction-response data. Its
multi-stage training paradigm, comprising self-supervised vision pretraining,
vision-language alignment, multimodal pretraining, and multi-task instruction
tuning, progressively equips the model with transferable visual
representations, cross-modal reasoning, and robust instruction-following
capabilities. Qualitative results demonstrate OmniMRI's ability to perform
diverse tasks within a single architecture, including MRI reconstruction,
anatomical and pathological segmentation, abnormality detection, diagnostic
suggestion, and radiology report generation. These findings highlight OmniMRI's
potential to consolidate fragmented pipelines into a scalable, generalist
framework, paving the way toward foundation models that unify imaging and
clinical language for comprehensive, end-to-end MRI interpretation.

</details>


### [226] [Minimal Solvers for Full DoF Motion Estimation from Asynchronous Tracks](https://arxiv.org/abs/2508.17537)
*Petr Hruby,Marc Pollefeys*

Main category: cs.CV

TL;DR: Polynomial approximation method for estimating camera translational and angular velocity from asynchronous point tracks, with minimal solvers developed for low-degree problems.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of estimating camera velocity from asynchronous point tracks, which is particularly relevant for rolling shutter and event cameras where traditional synchronous methods fail.

Method: Proposed a polynomial approximation to handle the originally non-polynomial problem, classified resulting minimal problems, determined their algebraic degrees, and developed minimal solvers for low-degree cases.

Result: Developed working solvers that were evaluated on both synthetic and real datasets, demonstrating practical applicability of the approach.

Conclusion: The method provides an effective solution for camera velocity estimation from asynchronous data, with code availability ensuring reproducibility and further development.

Abstract: We address the problem of estimating both translational and angular velocity
of a camera from asynchronous point tracks, a formulation relevant to rolling
shutter and event cameras. Since the original problem is non-polynomial, we
propose a polynomial approximation, classify the resulting minimal problems,
and determine their algebraic degrees. Furthermore, we develop minimal solvers
for several problems with low degrees and evaluate them on synthetic and real
datasets. The code will be made publicly available.

</details>


### [227] [Towards Optimal Convolutional Transfer Learning Architectures for Breast Lesion Classification and ACL Tear Detection](https://arxiv.org/abs/2508.17567)
*Daniel Frees,Moritz Bolling,Aditri Bhagirath*

Main category: cs.CV

TL;DR: This paper investigates optimal CNN architectures for medical imaging tasks and compares RadImageNet vs ImageNet pre-training, finding no significant advantage for RadImageNet in ACL tear and breast lesion classification.


<details>
  <summary>Details</summary>
Motivation: Medical imaging data scarcity limits model performance, and while transfer learning helps, it's unclear which pre-training dataset (RadImageNet vs ImageNet) and architectures work best for specific medical tasks.

Method: Comprehensive investigation of CNN architectures for breast lesion malignancy and ACL tear detection, comparing RadImageNet and ImageNet pre-training with statistical analysis. Used 1D convolutional classifiers with skip connections, ResNet50 backbones, and partial unfreezing.

Result: Achieved AUCs of 0.9969 for ACL tear detection and 0.9641 for breast nodule malignancy detection, competitive with previous works. No evidence found that RadImageNet pre-training provides superior performance over ImageNet for these tasks.

Conclusion: Optimal medical classification performance comes from specific architectural choices (1D CNNs with skip connections, ResNet50, partial unfreezing), but RadImageNet pre-training doesn't show clear advantage over ImageNet for ACL tear and breast lesion classification.

Abstract: Modern computer vision models have proven to be highly useful for medical
imaging classification and segmentation tasks, but the scarcity of medical
imaging data often limits the efficacy of models trained from scratch. Transfer
learning has emerged as a pivotal solution to this, enabling the fine-tuning of
high-performance models on small data. Mei et al. (2022) found that
pre-training CNNs on a large dataset of radiologist-labeled images
(RadImageNet) enhanced model performance on downstream tasks compared to
ImageNet pretraining. The present work extends Mei et al. (2022) by conducting
a comprehensive investigation to determine optimal CNN architectures for breast
lesion malignancy detection and ACL tear detection, as well as performing
statistical analysis to compare the effect of RadImageNet and ImageNet
pre-training on downstream model performance. Our findings suggest that
1-dimensional convolutional classifiers with skip connections, ResNet50
pre-trained backbones, and partial backbone unfreezing yields optimal
downstream medical classification performance. Our best models achieve AUCs of
0.9969 for ACL tear detection and 0.9641 for breast nodule malignancy
detection, competitive with the results reported by Mei et al. (2022) and
surpassing other previous works. We do not find evidence confirming RadImageNet
pre-training to provide superior downstream performance for ACL tear and breast
lesion classification tasks.

</details>


### [228] [MetaGen: A DSL, Database, and Benchmark for VLM-Assisted Metamaterial Generation](https://arxiv.org/abs/2508.17568)
*Liane Makatura,Benjamin Jones,Siyuan Bian,Wojciech Matusik*

Main category: cs.CV

TL;DR: A framework for metamaterial design with three components: MetaDSL language for human/machine-readable designs, MetaDB repository of 150K+ programs with geometry and properties, and MetaBench benchmarks for testing vision-language models.


<details>
  <summary>Details</summary>
Motivation: Metamaterial design is challenging due to geometric complexity and non-trivial mapping from architecture to behavior, requiring better tools for design and understanding.

Method: Developed MetaDSL domain-specific language, created MetaDB repository with 150K+ parameterized programs, and established MetaBench benchmarks for testing vision-language models on structure reconstruction, inverse design, and performance prediction.

Result: Created a comprehensive framework with curated repository and benchmarks, established baselines by fine-tuning state-of-the-art vision-language models, and deployed an interactive CAD-like interface.

Conclusion: The framework provides a strong foundation for integrated design and understanding of structure-representation-property relationships in metamaterials.

Abstract: Metamaterials are micro-architected structures whose geometry imparts highly
tunable-often counter-intuitive-bulk properties. Yet their design is difficult
because of geometric complexity and a non-trivial mapping from architecture to
behaviour. We address these challenges with three complementary contributions.
(i) MetaDSL: a compact, semantically rich domain-specific language that
captures diverse metamaterial designs in a form that is both human-readable and
machine-parsable. (ii) MetaDB: a curated repository of more than 150,000
parameterized MetaDSL programs together with their
derivatives-three-dimensional geometry, multi-view renderings, and simulated
elastic properties. (iii) MetaBench: benchmark suites that test three core
capabilities of vision-language metamaterial assistants-structure
reconstruction, property-driven inverse design, and performance prediction. We
establish baselines by fine-tuning state-of-the-art vision-language models and
deploy an omni-model within an interactive, CAD-like interface. Case studies
show that our framework provides a strong first step toward integrated design
and understanding of structure-representation-property relationships.

</details>


### [229] [IDU: Incremental Dynamic Update of Existing 3D Virtual Environments with New Imagery Data](https://arxiv.org/abs/2508.17579)
*Meida Chen,Luis Leal,Yue Hu,Rong Liu,Butian Xiong,Andrew Feng,Jiuyi Xu,Yangming Shi*

Main category: cs.CV

TL;DR: IDU pipeline enables efficient incremental updates of 3D military training environments using minimal new imagery and AI-generated assets with human guidance.


<details>
  <summary>Details</summary>
Motivation: Military organizations need to maintain up-to-date 3D virtual environments for training, but full-scale updates are time-consuming and costly due to the dynamic nature of battlefield conditions where objects frequently appear or disappear.

Method: Proposes Incremental Dynamic Update (IDU) pipeline: camera pose estimation to align new images with existing 3D model, change detection to identify modifications, 3D generative AI to create new assets, and human-guided integration focusing on one object at a time.

Result: Experimental results show the IDU pipeline significantly reduces update time and labor compared to full-scale reconstruction methods.

Conclusion: The IDU pipeline provides a cost-effective and targeted solution for maintaining current 3D models in rapidly evolving military scenarios, offering substantial efficiency improvements over traditional update approaches.

Abstract: For simulation and training purposes, military organizations have made
substantial investments in developing high-resolution 3D virtual environments
through extensive imaging and 3D scanning. However, the dynamic nature of
battlefield conditions-where objects may appear or vanish over time-makes
frequent full-scale updates both time-consuming and costly. In response, we
introduce the Incremental Dynamic Update (IDU) pipeline, which efficiently
updates existing 3D reconstructions, such as 3D Gaussian Splatting (3DGS), with
only a small set of newly acquired images. Our approach starts with camera pose
estimation to align new images with the existing 3D model, followed by change
detection to pinpoint modifications in the scene. A 3D generative AI model is
then used to create high-quality 3D assets of the new elements, which are
seamlessly integrated into the existing 3D model. The IDU pipeline incorporates
human guidance to ensure high accuracy in object identification and placement,
with each update focusing on a single new object at a time. Experimental
results confirm that our proposed IDU pipeline significantly reduces update
time and labor, offering a cost-effective and targeted solution for maintaining
up-to-date 3D models in rapidly evolving military scenarios.

</details>


### [230] [HERO: Hierarchical Extrapolation and Refresh for Efficient World Models](https://arxiv.org/abs/2508.17588)
*Quanjian Song,Xinyu Wang,Donghao Zhou,Jingyu Lin,Cunjian Chen,Yue Ma,Xiu Li*

Main category: cs.CV

TL;DR: HERO is a training-free hierarchical acceleration framework that speeds up diffusion-based world models by 1.73Ã with minimal quality loss, using patch-wise refresh for shallow layers and linear extrapolation for deeper layers.


<details>
  <summary>Details</summary>
Motivation: Generation-driven world models suffer from slow inference due to iterative diffusion processes, and existing acceleration techniques cause quality degradation when applied to world models.

Method: HERO uses hierarchical strategies: patch-wise refresh mechanism with sampling and frequency-aware tracking for shallow layers (high temporal variability), and linear extrapolation to bypass attention and feed-forward computations in deeper layers (stable features).

Result: Achieves 1.73Ã speedup with minimal quality degradation, significantly outperforming existing diffusion acceleration methods.

Conclusion: HERO effectively accelerates world model inference by leveraging the multi-modal nature and hierarchical feature characteristics, providing efficient computation without training requirements.

Abstract: Generation-driven world models create immersive virtual environments but
suffer slow inference due to the iterative nature of diffusion models. While
recent advances have improved diffusion model efficiency, directly applying
these techniques to world models introduces limitations such as quality
degradation. In this paper, we present HERO, a training-free hierarchical
acceleration framework tailored for efficient world models. Owing to the
multi-modal nature of world models, we identify a feature coupling phenomenon,
wherein shallow layers exhibit high temporal variability, while deeper layers
yield more stable feature representations. Motivated by this, HERO adopts
hierarchical strategies to accelerate inference: (i) In shallow layers, a
patch-wise refresh mechanism efficiently selects tokens for recomputation. With
patch-wise sampling and frequency-aware tracking, it avoids extra metric
computation and remain compatible with FlashAttention. (ii) In deeper layers, a
linear extrapolation scheme directly estimates intermediate features. This
completely bypasses the computations in attention modules and feed-forward
networks. Our experiments show that HERO achieves a 1.73$\times$ speedup with
minimal quality degradation, significantly outperforming existing diffusion
acceleration methods.

</details>


### [231] [TinyGiantVLM: A Lightweight Vision-Language Architecture for Spatial Reasoning under Resource Constraints](https://arxiv.org/abs/2508.17595)
*Vinh-Thuan Ly,Hoang M. Truong,Xuan-Huong Nguyen*

Main category: cs.CV

TL;DR: TinyGiantVLM is a lightweight two-stage VLM framework that excels at fine-grained spatial reasoning in warehouse environments using RGB+depth inputs and MoE fusion, achieving strong performance on industrial spatial tasks.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs struggle with 3D spatial reasoning, object arrangements, and multimodal understanding in complex industrial logistics environments, requiring specialized solutions for warehouse-scale spatial analysis.

Method: Two-stage framework with global+region feature encoding from RGB/depth modalities, MoE fusion module for dynamic spatial representation combination, and two-phase training (free-form answer generation followed by normalized evaluation).

Result: 64M-parameter base model achieved 5th place (66.8861 score) on AI City Challenge 2025 Track 3; 80M-parameter variant with expanded MoE showed improved spatial reasoning performance.

Conclusion: TinyGiantVLM effectively bridges visual perception and spatial understanding in industrial environments through its modular design and multimodal fusion approach, demonstrating strong performance with lightweight architecture.

Abstract: Reasoning about fine-grained spatial relationships in warehouse-scale
environments poses a significant challenge for existing vision-language models
(VLMs), which often struggle to comprehend 3D layouts, object arrangements, and
multimodal cues in real-world industrial settings. In this paper, we present
TinyGiantVLM, a lightweight and modular two-stage framework designed for
physical spatial reasoning, distinguishing itself from traditional geographic
reasoning in complex logistics scenes. Our approach encodes both global and
region-level features from RGB and depth modalities using pretrained visual
backbones. To effectively handle the complexity of high-modality inputs and
diverse question types, we incorporate a Mixture-of-Experts (MoE) fusion
module, which dynamically combines spatial representations to support
downstream reasoning tasks and improve convergence. Training is conducted in a
two-phase strategy: the first phase focuses on generating free-form answers to
enhance spatial reasoning ability, while the second phase uses normalized
answers for evaluation. Evaluated on Track 3 of the AI City Challenge 2025, our
64M-parameter base model achieved 5th place on the leaderboard with a score of
66.8861, demonstrating strong performance in bridging visual perception and
spatial understanding in industrial environments. We further present an
80M-parameter variant with expanded MoE capacity, which demonstrates improved
performance on spatial reasoning tasks.

</details>


### [232] [HotSpotter - Patterned Species Instance Recognition](https://arxiv.org/abs/2508.17605)
*Jonathan P. Crall,Charles V. Stewart,Tanya Y. Berger-Wolf,Daniel I. Rubenstein,Siva R. Sundaresan*

Main category: cs.CV

TL;DR: HotSpotter is a fast, accurate algorithm for individual animal identification across multiple species using two keypoint-based matching approaches with competitive scoring.


<details>
  <summary>Details</summary>
Motivation: To develop a species-agnostic algorithm for identifying individual animals from images that is both fast and more accurate than existing methods, addressing the need for efficient wildlife monitoring and conservation efforts.

Method: Two approaches: 1) Sequential matching of query images against each database image using keypoint/hotspot extraction, 2) Fast nearest neighbor search with competitive scoring mechanism derived from Local Naive Bayes Nearest Neighbor algorithm.

Result: Successfully applied to multiple species (zebras, giraffes, leopards, lionfish), demonstrated on databases of 1000+ images, achieving higher accuracy than published methods with query matching in just a few seconds.

Conclusion: HotSpotter provides an effective and efficient solution for individual animal identification that works across multiple species, offering both speed and improved accuracy compared to existing approaches.

Abstract: We present HotSpotter, a fast, accurate algorithm for identifying individual
animals against a labeled database. It is not species specific and has been
applied to Grevy's and plains zebras, giraffes, leopards, and lionfish. We
describe two approaches, both based on extracting and matching keypoints or
"hotspots". The first tests each new query image sequentially against each
database image, generating a score for each database image in isolation, and
ranking the results. The second, building on recent techniques for instance
recognition, matches the query image against the database using a fast nearest
neighbor search. It uses a competitive scoring mechanism derived from the Local
Naive Bayes Nearest Neighbor algorithm recently proposed for category
recognition. We demonstrate results on databases of more than 1000 images,
producing more accurate matches than published methods and matching each query
image in just a few seconds.

</details>


### [233] [A Weighted Vision Transformer-Based Multi-Task Learning Framework for Predicting ADAS-Cog Scores](https://arxiv.org/abs/2508.17613)
*Nur Amirah Abd Hamid,Mohd Ibrahim Shapiai,Daphne Teck Ching Lai*

Main category: cs.CV

TL;DR: A weighted Vision Transformer multi-task learning framework that predicts both ADAS-Cog global scores and 13 sub-scores from baseline MRI scans, with sub-score-specific loss weighting to improve predictive accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing AD prognostic methods focus only on global ADAS-Cog scores and overlook the predictive value of 13 sub-scores that reflect distinct cognitive domains. Some sub-scores may have greater influence on determining global scores.

Method: Proposed a weighted ViT-based multi-task learning framework using baseline MRI scans to jointly predict ADAS-Cog global score and 13 sub-scores at Month 24. Integrated ViT as feature extractor and investigated sub-score-specific loss weighting strategies.

Result: Weighting strategies are group-dependent: strong weighting improves performance for MCI subjects with heterogeneous MRI patterns, while moderate weighting works better for CN subjects with lower variability. Uniform weighting underutilizes key sub-scores.

Conclusion: The framework offers a flexible, interpretable approach to AD prognosis using end-to-end MRI-based learning, demonstrating that tailored loss weighting enhances both predictive accuracy and model interpretability.

Abstract: Prognostic modeling is essential for forecasting future clinical scores and
enabling early detection of Alzheimers disease (AD). While most existing
methods focus on predicting the ADAS-Cog global score, they often overlook the
predictive value of its 13 sub-scores, which reflect distinct cognitive
domains. Some sub-scores may exert greater influence on determining global
scores. Assigning higher loss weights to these clinically meaningful sub-scores
can guide the model to focus on more relevant cognitive domains, enhancing both
predictive accuracy and interpretability. In this study, we propose a weighted
Vision Transformer (ViT)-based multi-task learning (MTL) framework to jointly
predict the ADAS-Cog global score using baseline MRI scans and its 13
sub-scores at Month 24. Our framework integrates ViT as a feature extractor and
systematically investigates the impact of sub-score-specific loss weighting on
model performance. Results show that our proposed weighting strategies are
group-dependent: strong weighting improves performance for MCI subjects with
more heterogeneous MRI patterns, while moderate weighting is more effective for
CN subjects with lower variability. Our findings suggest that uniform weighting
underutilizes key sub-scores and limits generalization. The proposed framework
offers a flexible, interpretable approach to AD prognosis using end-to-end
MRI-based learning. (Github repo link will be provided after review)

</details>


### [234] [JCo-MVTON: Jointly Controllable Multi-Modal Diffusion Transformer for Mask-Free Virtual Try-on](https://arxiv.org/abs/2508.17614)
*Aowen Wang,Wei Li,Hao Luo,Mengxing Ao,Chenyu Zhu,Xinyang Li,Fan Wang*

Main category: cs.CV

TL;DR: JCo-MVTON is a mask-free virtual try-on system using multi-modal diffusion transformers that integrates reference person and garment images directly into denoising, achieving state-of-the-art performance with strong real-world generalization.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of traditional virtual try-on systems that rely heavily on human body masks, have limited fine-grained control over garment attributes, and poor generalization to real-world scenarios.

Method: Built on Multi-Modal Diffusion Transformer (MM-DiT) backbone with dedicated conditional pathways that fuse features within self-attention layers. Uses bidirectional generation strategy for dataset construction: mask-based model for reference images and self-supervised "Try-Off" model for garment recovery. Includes refined positional encodings and attention masks for spatial alignment.

Result: Achieves state-of-the-art performance on public benchmarks including DressCode, significantly outperforming existing methods in both quantitative metrics and human evaluations. Shows strong generalization in real-world applications, surpassing commercial systems.

Conclusion: JCo-MVTON provides a robust mask-free virtual try-on solution with improved control, alignment, and real-world applicability through multi-modal diffusion and innovative dataset generation techniques.

Abstract: Virtual try-on systems have long been hindered by heavy reliance on human
body masks, limited fine-grained control over garment attributes, and poor
generalization to real-world, in-the-wild scenarios. In this paper, we propose
JCo-MVTON (Jointly Controllable Multi-Modal Diffusion Transformer for Mask-Free
Virtual Try-On), a novel framework that overcomes these limitations by
integrating diffusion-based image generation with multi-modal conditional
fusion. Built upon a Multi-Modal Diffusion Transformer (MM-DiT) backbone, our
approach directly incorporates diverse control signals -- such as the reference
person image and the target garment image -- into the denoising process through
dedicated conditional pathways that fuse features within the self-attention
layers. This fusion is further enhanced with refined positional encodings and
attention masks, enabling precise spatial alignment and improved garment-person
integration. To address data scarcity and quality, we introduce a bidirectional
generation strategy for dataset construction: one pipeline uses a mask-based
model to generate realistic reference images, while a symmetric ``Try-Off''
model, trained in a self-supervised manner, recovers the corresponding garment
images. The synthesized dataset undergoes rigorous manual curation, allowing
iterative improvement in visual fidelity and diversity. Experiments demonstrate
that JCo-MVTON achieves state-of-the-art performance on public benchmarks
including DressCode, significantly outperforming existing methods in both
quantitative metrics and human evaluations. Moreover, it shows strong
generalization in real-world applications, surpassing commercial systems.

</details>


### [235] [Improving Interpretability in Alzheimer's Prediction via Joint Learning of ADAS-Cog Scores](https://arxiv.org/abs/2508.17619)
*Nur Amirah Abd Hamid,Mohd Shahrizal Rusli,Muhammad Thaqif Iman Mohd Taufek,Mohd Ibrahim Shapiai,Daphne Teck Ching Lai*

Main category: cs.CV

TL;DR: Multi-task learning framework using Vision Transformer and Swin Transformer to jointly predict global ADAS-Cog score and its 13 sub-scores from baseline MRI and longitudinal clinical data, showing improved global prediction but revealing model instability due to clinical feature dominance.


<details>
  <summary>Details</summary>
Motivation: Existing approaches focus only on global ADAS-Cog score prediction and overlook the predictive value of domain-specific sub-scores, which could provide better insights into cognitive decline patterns in Alzheimer's disease.

Method: Proposed multi-task learning framework using Vision Transformer and Swin Transformer architectures to extract imaging features from baseline MRI, fused with longitudinal clinical scores from baseline and Month 6 to jointly predict global score and 13 sub-scores at Month 24.

Result: Incorporating sub-score learning improves global score prediction. Analysis reveals Q1 (Word Recall), Q4 (Delayed Recall), and Q8 (Word Recognition) dominate predicted global score, but some influential sub-scores show high prediction errors due to clinical feature dominance over MRI features.

Conclusion: Sub-score informed modeling provides value for AD prediction, but requires improved multimodal fusion and adaptive loss weighting to achieve balanced learning and build more interpretable, clinically robust frameworks.

Abstract: Accurate prediction of clinical scores is critical for early detection and
prognosis of Alzheimers disease (AD). While existing approaches primarily focus
on forecasting the ADAS-Cog global score, they often overlook the predictive
value of its sub-scores (13 items), which capture domain-specific cognitive
decline. In this study, we propose a multi task learning (MTL) framework that
jointly predicts the global ADAS-Cog score and its sub-scores (13 items) at
Month 24 using baseline MRI and longitudinal clinical scores from baseline and
Month 6. The main goal is to examine how each sub scores particularly those
associated with MRI features contribute to the prediction of the global score,
an aspect largely neglected in prior MTL studies. We employ Vision Transformer
(ViT) and Swin Transformer architectures to extract imaging features, which are
fused with longitudinal clinical inputs to model cognitive progression. Our
results show that incorporating sub-score learning improves global score
prediction. Subscore level analysis reveals that a small subset especially Q1
(Word Recall), Q4 (Delayed Recall), and Q8 (Word Recognition) consistently
dominates the predicted global score. However, some of these influential
sub-scores exhibit high prediction errors, pointing to model instability.
Further analysis suggests that this is caused by clinical feature dominance,
where the model prioritizes easily predictable clinical scores over more
complex MRI derived features. These findings emphasize the need for improved
multimodal fusion and adaptive loss weighting to achieve more balanced
learning. Our study demonstrates the value of sub score informed modeling and
provides insights into building more interpretable and clinically robust AD
prediction frameworks. (Github repo provided)

</details>


### [236] [Finding Outliers in a Haystack: Anomaly Detection for Large Pointcloud Scenes](https://arxiv.org/abs/2508.17634)
*Ryan Faulkner,Ian Reid,Simon Ratcliffe,Tat-Jun Chin*

Main category: cs.CV

TL;DR: Novel open-set segmentation approach for outdoor LiDAR point clouds combining object defect-detection principles with Mamba architecture for improved performance on outlier detection.


<details>
  <summary>Details</summary>
Motivation: Outdoor LiDAR scanning produces large-scale point clouds where outlier objects inevitably appear during applications like robotics and autonomous vehicles, requiring robust open-set segmentation methods.

Method: Combines learnings from object defect-detection research with Mamba architecture's capabilities for long-range dependencies and scalability to large data, creating a reconstruction-based approach for open-set segmentation.

Result: The approach improves performance when applied to both their own method and existing methods, and contributes a Mamba-based architecture competitive with voxel-convolution methods on large-scale point clouds.

Conclusion: The proposed reconstruction-based approach leveraging Mamba architecture effectively addresses open-set segmentation challenges in outdoor LiDAR data, demonstrating improved performance and scalability.

Abstract: LiDAR scanning in outdoor scenes acquires accurate distance measurements over
wide areas, producing large-scale point clouds. Application examples for this
data include robotics, automotive vehicles, and land surveillance. During such
applications, outlier objects from outside the training data will inevitably
appear. Our research contributes a novel approach to open-set segmentation,
leveraging the learnings of object defect-detection research. We also draw on
the Mamba architecture's strong performance in utilising long-range
dependencies and scalability to large data. Combining both, we create a
reconstruction based approach for the task of outdoor scene open-set
segmentation. We show that our approach improves performance not only when
applied to our our own open-set segmentation method, but also when applied to
existing methods. Furthermore we contribute a Mamba based architecture which is
competitive with existing voxel-convolution based methods on challenging,
large-scale pointclouds.

</details>


### [237] [Wound3DAssist: A Practical Framework for 3D Wound Assessment](https://arxiv.org/abs/2508.17635)
*Remi Chierchia,Rodrigo Santa Cruz,LÃ©o Lebrat,Yulia Arzhaeva,Mohammad Ali Armin,Jeremy Oorloff,Chuong Nguyen,Olivier Salvado,Clinton Fookes,David Ahmedt-Aristizabal*

Main category: cs.CV

TL;DR: Wound3DAssist is a 3D wound assessment framework using monocular smartphone videos that overcomes limitations of 2D methods by providing accurate 3D models, automatic measurements, and tissue analysis in under 20 minutes.


<details>
  <summary>Details</summary>
Motivation: Current chronic wound assessment relies on subjective manual methods and 2D digital approaches that struggle with perspective distortion, limited field of view, and inability to capture wound depth, especially in complex anatomical regions.

Method: A practical framework using monocular consumer-grade videos to generate accurate 3D models from handheld smartphone recordings. Integrates 3D reconstruction, wound segmentation, tissue classification, and periwound analysis into a modular workflow.

Result: The framework achieves high-quality wound bed visualization, millimeter-level accuracy, and reliable tissue composition analysis. Full assessments are completed in under 20 minutes, demonstrating clinical feasibility.

Conclusion: Wound3DAssist provides a practical, non-contact solution for 3D wound assessment that is view-independent, robust to camera motion, and suitable for real-world clinical use with consumer-grade devices.

Abstract: Managing chronic wounds remains a major healthcare challenge, with clinical
assessment often relying on subjective and time-consuming manual documentation
methods. Although 2D digital videometry frameworks aided the measurement
process, these approaches struggle with perspective distortion, a limited field
of view, and an inability to capture wound depth, especially in anatomically
complex or curved regions. To overcome these limitations, we present
Wound3DAssist, a practical framework for 3D wound assessment using monocular
consumer-grade videos. Our framework generates accurate 3D models from short
handheld smartphone video recordings, enabling non-contact, automatic
measurements that are view-independent and robust to camera motion. We
integrate 3D reconstruction, wound segmentation, tissue classification, and
periwound analysis into a modular workflow. We evaluate Wound3DAssist across
digital models with known geometry, silicone phantoms, and real patients.
Results show that the framework supports high-quality wound bed visualization,
millimeter-level accuracy, and reliable tissue composition analysis. Full
assessments are completed in under 20 minutes, demonstrating feasibility for
real-world clinical use.

</details>


### [238] [Few-Shot Pattern Detection via Template Matching and Regression](https://arxiv.org/abs/2508.17636)
*Eunchan Jo,Dahyun Kang,Sanghyun Kim,Yunseon Choi,Minsu Cho*

Main category: cs.CV

TL;DR: Proposes TMR, a simple yet effective few-shot pattern detector using template matching and regression that preserves spatial layout information, outperforms state-of-the-art methods on multiple benchmarks including new RPINE dataset.


<details>
  <summary>Details</summary>
Motivation: Previous few-shot object counting and detection methods fail to localize non-object patterns and lose structural information by collapsing exemplars into prototypes, limiting their applicability to broader pattern detection tasks.

Method: Template matching and regression approach with minimal learnable layers on frozen backbone to preserve spatial layout of exemplars, avoiding prototype collapse while maintaining efficiency.

Result: Outperforms state-of-the-art methods on RPINE, FSCD-147, and FSCD-LVIS benchmarks, demonstrating strong generalization in cross-dataset evaluation.

Conclusion: Revisiting classic template matching with modern backbones provides an effective solution for few-shot pattern detection that handles both object and non-object patterns better than previous prototype-based approaches.

Abstract: We address the problem of few-shot pattern detection, which aims to detect
all instances of a given pattern, typically represented by a few exemplars,
from an input image. Although similar problems have been studied in few-shot
object counting and detection (FSCD), previous methods and their benchmarks
have narrowed patterns of interest to object categories and often fail to
localize non-object patterns. In this work, we propose a simple yet effective
detector based on template matching and regression, dubbed TMR. While previous
FSCD methods typically represent target exemplars as spatially collapsed
prototypes and lose structural information, we revisit classic template
matching and regression. It effectively preserves and leverages the spatial
layout of exemplars through a minimalistic structure with a small number of
learnable convolutional or projection layers on top of a frozen backbone We
also introduce a new dataset, dubbed RPINE, which covers a wider range of
patterns than existing object-centric datasets. Our method outperforms the
state-of-the-art methods on the three benchmarks, RPINE, FSCD-147, and
FSCD-LVIS, and demonstrates strong generalization in cross-dataset evaluation.

</details>


### [239] [Dynamic Embedding of Hierarchical Visual Features for Efficient Vision-Language Fine-Tuning](https://arxiv.org/abs/2508.17638)
*Xinyu Wei,Guoli Yang,Jialu Zhou,Mingyue Yang,Leqian Li,Kedi Zhang,Chunping Qiu*

Main category: cs.CV

TL;DR: DEHVF is an efficient vision-language fine-tuning method that dynamically fuses hierarchical visual features into LLMs, avoiding sequence length expansion while achieving better performance than existing PEFT methods.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs suffer from increased input sequence length when concatenating visual features with text tokens, causing computational overhead. Existing fusion methods neglect hierarchical semantic representations and fine-grained visual information from shallower layers.

Method: Proposes DEHVF with lightweight hierarchical visual fuser that dynamically selects and fuses hierarchical visual features based on LLM layer semantics. Projects and aligns fused features into corresponding FFN layers, avoiding sequence expansion.

Result: Achieves higher accuracy than existing PEFT baselines on VL benchmarks including ScienceQA visual question answering and COCO Captions image captioning, while maintaining efficient training and inference.

Conclusion: DEHVF effectively addresses sequence length issues by leveraging hierarchical visual representations and dynamic feature fusion, enabling precise cross-modal alignment with minimal parameter tuning.

Abstract: Large Vision-Language Models (LVLMs) commonly follow a paradigm that projects
visual features and then concatenates them with text tokens to form a unified
sequence input for Large Language Models (LLMs). However, this paradigm leads
to a significant increase in the length of the input sequence, resulting in
substantial computational overhead. Existing methods attempt to fuse visual
information into the intermediate layers of LLMs, which alleviate the sequence
length issue but often neglect the hierarchical semantic representations within
the model and the fine-grained visual information available in the shallower
visual encoding layers. To address this limitation, we propose DEHVF, an
efficient vision-language fine-tuning method based on dynamic embedding and
fusion of hierarchical visual features. Its core lies in leveraging the
inherent hierarchical representation characteristics of visual encoders and
language models. Through a lightweight hierarchical visual fuser, it
dynamically selects and fuses hierarchical features corresponding to semantic
granularity based on the internal representations of each layer in LLMs. The
fused layer-related visual features are then projected and aligned before being
directly embedded into the Feed-Forward Network (FFN) of the corresponding
layer in LLMs. This approach not only avoids sequence expansion but also
dynamically fuses multi-layer visual information. By fine-tuning only a small
number of parameters, DEHVF achieves precise alignment and complementarity of
cross-modal information at the same semantic granularity. We conducted
experiments across various VL benchmarks, including visual question answering
on ScienceQA and image captioning on COCO Captions. The results demonstrate
that DEHVF achieves higher accuracy than existing parameter-efficient
fine-tuning (PEFT) baselines while maintaining efficient training and
inference.

</details>


### [240] [HyTver: A Novel Loss Function for Longitudinal Multiple Sclerosis Lesion Segmentation](https://arxiv.org/abs/2508.17639)
*Dayan Perera,Ting Fung Fung,Vishnu Monn*

Main category: cs.CV

TL;DR: Proposes HyTver, a novel hybrid loss function for longitudinal MS lesion segmentation that addresses data imbalance while maintaining good performance across multiple metrics including Dice score and distance-based measures.


<details>
  <summary>Details</summary>
Motivation: Longitudinal MS lesion segmentation faces severe input/output imbalance issues. Standard loss functions like Dice or Cross-Entropy are inadequate, and existing specialized loss functions have problems like computational complexity or poor performance on non-region-based metrics.

Method: Developed a hybrid loss function called HyTver that combines the benefits of different loss function types to handle data imbalance effectively while maintaining computational efficiency.

Result: Achieved a Dice score of 0.659 while ensuring distance-based metrics remain comparable to other popular loss functions. The method also shows good stability when used with pre-trained models.

Conclusion: HyTver loss function effectively addresses the imbalance problem in MS lesion segmentation, providing balanced performance across multiple evaluation metrics while being computationally practical.

Abstract: Longitudinal Multiple Sclerosis Lesion Segmentation is a particularly
challenging problem that involves both input and output imbalance in the data
and segmentation. Therefore in order to develop models that are practical, one
of the solutions is to develop better loss functions. Most models naively use
either Dice loss or Cross-Entropy loss or their combination without too much
consideration. However, one must select an appropriate loss function as the
imbalance can be mitigated by selecting a proper loss function. In order to
solve the imbalance problem, multiple loss functions were proposed that claimed
to solve it. They come with problems of their own which include being too
computationally complex due to hyperparameters as exponents or having
detrimental performance in metrics other than region-based ones. We propose a
novel hybrid loss called HyTver that achieves good segmentation performance
while maintaining performance in other metrics. We achieve a Dice score of
0.659 while also ensuring that the distance-based metrics are comparable to
other popular functions. In addition, we also evaluate the stability of the
loss functions when used on a pre- trained model and perform extensive
comparisons with other popular loss functions

</details>


### [241] [FloraSyntropy-Net: Scalable Deep Learning with Novel FloraSyntropy Archive for Large-Scale Plant Disease Diagnosis](https://arxiv.org/abs/2508.17653)
*Saif Ur Rehman Khan,Muhammad Nabeel Asim,Sebastian Vollmer,Andreas Dengel*

Main category: cs.CV

TL;DR: FloraSyntropy-Net is a novel federated learning framework that achieves state-of-the-art plant disease diagnosis with 96.38% accuracy on a new large-scale dataset and demonstrates exceptional generalization (99.84%) on unrelated datasets.


<details>
  <summary>Details</summary>
Motivation: Existing AI solutions for plant disease diagnosis lack generalization across diverse agricultural species and fail to perform accurately across the broad spectrum of cultivated plants, limiting real-world applicability.

Method: Proposed FloraSyntropy-Net framework integrates: 1) Memetic Algorithm for optimal base model selection (DenseNet201), 2) novel Deep Block for enhanced feature representation, and 3) client-cloning strategy for scalable, privacy-preserving federated learning training.

Result: Achieved 96.38% accuracy on the FloraSyntropy benchmark dataset (178,922 images, 35 species, 97 disease classes) and demonstrated exceptional generalization with 99.84% accuracy on the unrelated Pest dataset.

Conclusion: The work provides both a valuable new dataset resource and a robust, highly generalizable framework that advances practical, large-scale agricultural AI applications for plant disease diagnosis.

Abstract: Early diagnosis of plant diseases is critical for global food safety, yet
most AI solutions lack the generalization required for real-world agricultural
diversity. These models are typically constrained to specific species, failing
to perform accurately across the broad spectrum of cultivated plants. To
address this gap, we first introduce the FloraSyntropy Archive, a large-scale
dataset of 178,922 images across 35 plant species, annotated with 97 distinct
disease classes. We establish a benchmark by evaluating numerous existing
models on this archive, revealing a significant performance gap. We then
propose FloraSyntropy-Net, a novel federated learning framework (FL) that
integrates a Memetic Algorithm (MAO) for optimal base model selection
(DenseNet201), a novel Deep Block for enhanced feature representation, and a
client-cloning strategy for scalable, privacy-preserving training.
FloraSyntropy-Net achieves a state-of-the-art accuracy of 96.38% on the
FloraSyntropy benchmark. Crucially, to validate its generalization capability,
we test the model on the unrelated multiclass Pest dataset, where it
demonstrates exceptional adaptability, achieving 99.84% accuracy. This work
provides not only a valuable new resource but also a robust and highly
generalizable framework that advances the field towards practical, large-scale
agricultural AI applications.

</details>


### [242] [Rethinking the Detail-Preserved Completion of Complex Tubular Structures based on Point Cloud: a Dataset and a Benchmark](https://arxiv.org/abs/2508.17658)
*Yaolei Qi,Yikai Yang,Wenbo Peng,Shumei Miao,Yutao Hu,Guanyu Yang*

Main category: cs.CV

TL;DR: Proposes TSRNet for tubular structure completion using point clouds, creates PC-CAC dataset from clinical data, and achieves state-of-the-art performance on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing segmentation algorithms struggle with structural discontinuities in medical tubular structures like coronary arteries, leading to compromised diagnostic accuracy. There's a need to reconnect discontinuous structures to ensure completeness.

Method: TSRNet integrates a detail-preserved feature extractor, multiple dense refinement strategy, and global-to-local loss function. Uses point cloud-based approach for tubular structure completion with a novel dataset (PC-CAC) derived from real clinical data.

Result: Comprehensive experiments on PC-CAC and two additional public datasets (PC-ImageCAS and PC-PTR) show TSRNet consistently outperforms state-of-the-art approaches across multiple evaluation metrics.

Conclusion: The method sets a new benchmark for point cloud-based tubular structure reconstruction and provides a novel dataset for future research in medical imaging tubular structure completion.

Abstract: Complex tubular structures are essential in medical imaging and
computer-assisted diagnosis, where their integrity enhances anatomical
visualization and lesion detection. However, existing segmentation algorithms
struggle with structural discontinuities, particularly in severe clinical cases
such as coronary artery stenosis and vessel occlusions, which leads to
undesired discontinuity and compromising downstream diagnostic accuracy.
Therefore, it is imperative to reconnect discontinuous structures to ensure
their completeness. In this study, we explore the tubular structure completion
based on point cloud for the first time and establish a Point Cloud-based
Coronary Artery Completion (PC-CAC) dataset, which is derived from real
clinical data. This dataset provides a novel benchmark for tubular structure
completion. Additionally, we propose TSRNet, a Tubular Structure Reconnection
Network that integrates a detail-preservated feature extractor, a multiple
dense refinement strategy, and a global-to-local loss function to ensure
accurate reconnection while maintaining structural integrity. Comprehensive
experiments on our PC-CAC and two additional public datasets (PC-ImageCAS and
PC-PTR) demonstrate that our method consistently outperforms state-of-the-art
approaches across multiple evaluation metrics, setting a new benchmark for
point cloud-based tubular structure reconstruction. Our benchmark is available
at https://github.com/YaoleiQi/PCCAC.

</details>


### [243] [M^3-GloDets: Multi-Region and Multi-Scale Analysis of Fine-Grained Diseased Glomerular Detection](https://arxiv.org/abs/2508.17666)
*Tianyu Shi,Xinzi He,Kenji Ikemura,Mert R. Sabuncu,Yihe Yang,Ruining Deng*

Main category: cs.CV

TL;DR: M^3-GloDet framework systematically evaluates glomeruli detection models across regions, scales, and classes, finding optimal patch sizes and magnifications for diseased glomeruli detection in renal pathology.


<details>
  <summary>Details</summary>
Motivation: Current computer vision research focuses mainly on normal or globally sclerotic glomeruli, leaving diseased glomerular subtypes understudied. The nuanced morphological characteristics of disease variants challenge existing models, and there's ongoing debate about optimal imaging parameters for accurate classification.

Method: Developed M^3-GloDet framework to evaluate both benchmark and state-of-the-art detection models using diverse region-of-interest sizes and imaging resolutions. Tested on multi-class diseased glomerular dataset with systematic comparison of approaches.

Result: Intermediate patch sizes provided the best balance between context and efficiency. Moderate magnifications enhanced generalization by reducing overfitting. The framework revealed model strengths and limitations across different detection scenarios.

Conclusion: The study advances understanding of automated detection strategies for diseased glomeruli, offering actionable insights for refining clinical workflows in digital pathology through systematic evaluation of optimal imaging parameters and model performance.

Abstract: Accurate detection of diseased glomeruli is fundamental to progress in renal
pathology and underpins the delivery of reliable clinical diagnoses. Although
recent advances in computer vision have produced increasingly sophisticated
detection algorithms, the majority of research efforts have focused on normal
glomeruli or instances of global sclerosis, leaving the wider spectrum of
diseased glomerular subtypes comparatively understudied. This disparity is not
without consequence; the nuanced and highly variable morphological
characteristics that define these disease variants frequently elude even the
most advanced computational models. Moreover, ongoing debate surrounds the
choice of optimal imaging magnifications and region-of-view dimensions for
fine-grained glomerular analysis, adding further complexity to the pursuit of
accurate classification and robust segmentation.
  To bridge these gaps, we present M^3-GloDet, a systematic framework designed
to enable thorough evaluation of detection models across a broad continuum of
regions, scales, and classes. Within this framework, we evaluate both
long-standing benchmark architectures and recently introduced state-of-the-art
models that have achieved notable performance, using an experimental design
that reflects the diversity of region-of-interest sizes and imaging resolutions
encountered in routine digital renal pathology. As the results, we found that
intermediate patch sizes offered the best balance between context and
efficiency. Additionally, moderate magnifications enhanced generalization by
reducing overfitting. Through systematic comparison of these approaches on a
multi-class diseased glomerular dataset, our aim is to advance the
understanding of model strengths and limitations, and to offer actionable
insights for the refinement of automated detection strategies and clinical
workflows in the digital pathology domain.

</details>


### [244] [Hierarchical Vision-Language Learning for Medical Out-of-Distribution Detection](https://arxiv.org/abs/2508.17667)
*Runhe Lai,Xinhua Lu,Kanghao Chen,Qichao Chen,Wei-Shi Zheng,Ruixuan Wang*

Main category: cs.CV

TL;DR: A novel vision-language model framework for out-of-distribution detection in medical diagnosis that uses hierarchical visual fusion and hard pseudo-OOD sample generation to better identify unknown diseases resembling known ones.


<details>
  <summary>Details</summary>
Motivation: To improve trustworthy medical diagnosis systems by detecting unknown diseases (OOD samples) that could lead to misdiagnosis, especially challenging cases that resemble known diseases.

Method: Proposes a cross-scale visual fusion strategy to combine visual embeddings from multiple scales, enriching medical image representations. Also introduces cross-scale hard pseudo-OOD sample generation to maximize OOD detection performance.

Result: Experimental evaluations on three public medical datasets show superior OOD detection performance compared to existing methods.

Conclusion: The proposed vision-language model framework effectively integrates hierarchical visual information and generates challenging pseudo-OOD samples, achieving state-of-the-art performance in medical OOD detection.

Abstract: In trustworthy medical diagnosis systems, integrating out-of-distribution
(OOD) detection aims to identify unknown diseases in samples, thereby
mitigating the risk of misdiagnosis. In this study, we propose a novel OOD
detection framework based on vision-language models (VLMs), which integrates
hierarchical visual information to cope with challenging unknown diseases that
resemble known diseases. Specifically, a cross-scale visual fusion strategy is
proposed to couple visual embeddings from multiple scales. This enriches the
detailed representation of medical images and thus improves the discrimination
of unknown diseases. Moreover, a cross-scale hard pseudo-OOD sample generation
strategy is proposed to benefit OOD detection maximally. Experimental
evaluations on three public medical datasets support that the proposed
framework achieves superior OOD detection performance compared to existing
methods. The source code is available at https://openi.pcl.ac.cn/OpenMedIA/HVL.

</details>


### [245] [Language-Guided Temporal Token Pruning for Efficient VideoLLM Processing](https://arxiv.org/abs/2508.17686)
*Yogesh Kumar*

Main category: cs.CV

TL;DR: LGTTP is a model-agnostic framework that uses language queries to adaptively prune video tokens, reducing computation by 65% while maintaining 97-99% of original performance on VLMs.


<details>
  <summary>Details</summary>
Motivation: Vision Language Models struggle with long-form videos due to quadratic attention complexity, needing efficient methods to handle temporal information without losing performance.

Method: Language-Guided Temporal Token Pruning (LGTTP) leverages temporal cues from queries to adaptively prune video tokens, preserving higher token density in relevant segments rather than using uniform pruning.

Result: Achieves 65% computation reduction while preserving 97-99% performance. Improves HIT@1 by +9.5% on QVHighlights and retains 99.6% of R@1 on Charades-STA. Works well with explicit temporal markers and general tasks.

Conclusion: LGTTP effectively addresses computational bottlenecks in VLMs for long videos through adaptive token pruning guided by language queries, maintaining high performance with significant efficiency gains.

Abstract: Vision Language Models (VLMs) struggle with long-form videos due to the
quadratic complexity of attention mechanisms. We propose Language-Guided
Temporal Token Pruning (LGTTP), which leverages temporal cues from queries to
adaptively prune video tokens, preserving contextual continuity while reducing
computational overhead. Unlike uniform pruning or keyframe selection, LGTTP
retains higher token density in temporally relevant segments. Our
model-agnostic framework integrates with TimeChat and LLaVA-Video, achieving a
65% reduction in computation while preserving 97-99% of the original
performance. On QVHighlights, LGTTP improves HIT@1 by +9.5%, and on
Charades-STA, it retains 99.6% of R@1. It excels on queries with explicit
temporal markers and remains effective across general video understanding
tasks.

</details>


### [246] [Benchmarking Class Activation Map Methods for Explainable Brain Hemorrhage Classification on Hemorica Dataset](https://arxiv.org/abs/2508.17699)
*Z. Rafati,M. Hoseyni,J. Khoramdel,A. Nikoofard*

Main category: cs.CV

TL;DR: This study compares 9 Class Activation Mapping (CAM) techniques for brain hemorrhage diagnosis, finding HiResCAM and AblationCAM perform best for localization and segmentation respectively, establishing quantitative benchmarks for XAI in medical imaging.


<details>
  <summary>Details</summary>
Motivation: To increase transparency and clinical trust in deep learning models for medical imaging by investigating explainability through CAM techniques for brain hemorrhage diagnosis.

Method: Developed a pipeline to extract pixel-level annotations from classification models using 9 CAM algorithms applied across multiple network stages, evaluated on Hemorica dataset with slice-level labels and segmentation masks using Dice, IoU, and pixel-wise overlap metrics.

Result: Best localization at stage 5 of EfficientNetV2S, with HiResCAM achieving highest bounding-box alignment and AblationCAM achieving best pixel-level Dice (0.57) and IoU (0.40) despite models being trained only for classification.

Conclusion: Establishes reproducible benchmark for CAM methods in brain hemorrhage detection and demonstrates potential of XAI-driven pipelines for clinically meaningful AI-assisted diagnosis.

Abstract: Explainable Artificial Intelligence (XAI) has become an essential component
of medical imaging research, aiming to increase transparency and clinical trust
in deep learning models. This study investigates brain hemorrhage diagnosis
with a focus on explainability through Class Activation Mapping (CAM)
techniques. A pipeline was developed to extract pixellevel segmentation and
detection annotations from classification models using nine state-of-the-art
CAM algorithms, applied across multiple network stages, and quantitatively
evaluated on the Hemorica dataset, which uniquely provides both slice-level
labels and high-quality segmentation masks. Metrics including Dice, IoU, and
pixel-wise overlap were employed to benchmark CAM variants. Results show that
the strongest localization performance occurred at stage 5 of EfficientNetV2S,
with HiResCAM yielding the highest bounding-box alignment and AblationCAM
achieving the best pixel-level Dice (0.57) and IoU (0.40), representing strong
accuracy given that models were trained solely for classification without
segmentation supervision. To the best of current knowledge, this is among the f
irst works to quantitatively compare CAM methods for brain hemorrhage
detection, establishing a reproducible benchmark and underscoring the potential
of XAI-driven pipelines for clinically meaningful AI-assisted diagnosis.

</details>


### [247] [CATformer: Contrastive Adversarial Transformer for Image Super-Resolution](https://arxiv.org/abs/2508.17708)
*Qinyi Tian,Spence Cox,Laura E. Dalton*

Main category: cs.CV

TL;DR: CATformer is a novel neural network that combines diffusion-inspired transformers with adversarial and contrastive learning for super-resolution, achieving state-of-the-art performance in both efficiency and image quality.


<details>
  <summary>Details</summary>
Motivation: To bridge the performance gap between transformer-based, diffusion-based, and GAN-based methods in super-resolution by integrating their strengths into a unified architecture.

Method: Uses a dual-branch architecture with a primary diffusion-inspired transformer for progressive feature refinement and an auxiliary transformer branch for noise robustness through learned latent contrasts, fused with Residual-in-Residual Dense Blocks for reconstruction.

Result: Outperforms recent transformer-based and diffusion-inspired methods on benchmark datasets in both efficiency and visual image quality.

Conclusion: CATformer successfully integrates multiple advanced techniques and lays a foundation for practical applications of diffusion-inspired transformers in super-resolution tasks.

Abstract: Super-resolution remains a promising technique to enhance the quality of
low-resolution images. This study introduces CATformer (Contrastive Adversarial
Transformer), a novel neural network integrating diffusion-inspired feature
refinement with adversarial and contrastive learning. CATformer employs a
dual-branch architecture combining a primary diffusion-inspired transformer,
which progressively refines latent representations, with an auxiliary
transformer branch designed to enhance robustness to noise through learned
latent contrasts. These complementary representations are fused and decoded
using deep Residual-in-Residual Dense Blocks for enhanced reconstruction
quality. Extensive experiments on benchmark datasets demonstrate that CATformer
outperforms recent transformer-based and diffusion-inspired methods both in
efficiency and visual image quality. This work bridges the performance gap
among transformer-, diffusion-, and GAN-based methods, laying a foundation for
practical applications of diffusion-inspired transformers in super-resolution.

</details>


### [248] [NGD: Neural Gradient Based Deformation for Monocular Garment Reconstruction](https://arxiv.org/abs/2508.17712)
*Soham Dasgupta,Shanthika Naik,Preet Savalia,Sujay Kumar Ingle,Avinash Sharma*

Main category: cs.CV

TL;DR: NGD: Neural Gradient-based Deformation method for dynamic garment reconstruction from monocular videos, featuring adaptive remeshing for wrinkles and dynamic texture maps for lighting effects.


<details>
  <summary>Details</summary>
Motivation: Existing methods have limitations - implicit representations provide smooth geometry without high-frequency details, while template methods using vertex displacement cause artifacts. Need better approach for complex garment dynamics.

Method: Proposes Neural Gradient-based Deformation (NGD) with adaptive remeshing strategy for dynamically evolving surfaces like wrinkles, and learns dynamic texture maps to capture per-frame lighting and shadow effects.

Result: Significant improvements over state-of-the-art methods, achieving high-quality garment reconstructions with detailed geometry and realistic textures.

Conclusion: NGD effectively addresses limitations of previous approaches, enabling high-quality dynamic garment reconstruction from monocular videos with detailed surface features and lighting effects.

Abstract: Dynamic garment reconstruction from monocular video is an important yet
challenging task due to the complex dynamics and unconstrained nature of the
garments. Recent advancements in neural rendering have enabled high-quality
geometric reconstruction with image/video supervision. However, implicit
representation methods that use volume rendering often provide smooth geometry
and fail to model high-frequency details. While template reconstruction methods
model explicit geometry, they use vertex displacement for deformation, which
results in artifacts. Addressing these limitations, we propose NGD, a Neural
Gradient-based Deformation method to reconstruct dynamically evolving textured
garments from monocular videos. Additionally, we propose a novel adaptive
remeshing strategy for modelling dynamically evolving surfaces like wrinkles
and pleats of the skirt, leading to high-quality reconstruction. Finally, we
learn dynamic texture maps to capture per-frame lighting and shadow effects. We
provide extensive qualitative and quantitative evaluations to demonstrate
significant improvements over existing SOTA methods and provide high-quality
garment reconstructions.

</details>


### [249] [F2RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Model](https://arxiv.org/abs/2508.17714)
*Hanbo Bi,Zhiqiang Yuan,Zexi Jia,Jiapei Zhang,Chongyang Li,Peixiang Luo,Ying Deng,Xiaoyue Duan,Jinchao Zhang*

Main category: cs.CV

TL;DR: The paper introduces Fine-grained Fragment Retrieval (FFR) task for retrieving semantically coherent multimodal fragments from long conversations, creates the MLDR dataset, and proposes F2RVLM model with two-stage training and curriculum learning that outperforms existing VLMs.


<details>
  <summary>Details</summary>
Motivation: Traditional dialogue retrieval fails to meet users' needs for retrieving semantically coherent content scattered across long multimodal conversations, requiring a more fine-grained approach.

Method: Proposes F2RVLM - a generative retrieval model trained with supervised fine-tuning followed by GRPO-based reinforcement learning with multi-objective rewards, plus difficulty-aware curriculum sampling to handle varying fragment complexity.

Result: F2RVLM outperforms popular Vision-Language Models in both in-domain and real-domain settings, demonstrating superior retrieval performance on the constructed MLDR dataset and real-world WeChat test set.

Conclusion: The proposed FFR task and F2RVLM model effectively address the challenge of retrieving coherent multimodal fragments from long-form dialogues, showing significant improvements over existing approaches.

Abstract: Traditional dialogue retrieval aims to select the most appropriate utterance
or image from recent dialogue history. However, they often fail to meet users'
actual needs for revisiting semantically coherent content scattered across
long-form conversations. To fill this gap, we define the Fine-grained Fragment
Retrieval (FFR) task, requiring models to locate query-relevant fragments,
comprising both utterances and images, from multimodal long-form dialogues. As
a foundation for FFR, we construct MLDR, the longest-turn multimodal dialogue
retrieval dataset to date, averaging 25.45 turns per dialogue, with each
naturally spanning three distinct topics. To evaluate generalization in
real-world scenarios, we curate and annotate a WeChat-based test set comprising
real-world multimodal dialogues with an average of 75.38 turns. Building on
these resources, we explore existing generation-based Vision-Language Models
(VLMs) on FFR and observe that they often retrieve incoherent utterance-image
fragments. While optimized for generating responses from visual-textual inputs,
these models lack explicit supervision to ensure semantic coherence within
retrieved fragments. To this end, we propose F2RVLM, a generative retrieval
model trained in a two-stage paradigm: (1) supervised fine-tuning to inject
fragment-level retrieval knowledge, and (2) GRPO-based reinforcement learning
with multi-objective rewards promoting semantic precision, relevance, and
contextual coherence. To handle varying intra-fragment complexity, from locally
dense to sparsely distributed, we introduce difficulty-aware curriculum
sampling that ranks training instances by model-predicted difficulty and
gradually exposes the model to harder samples. This boosts reasoning ability in
long, multi-turn contexts. F2RVLM outperforms popular VLMs in both in-domain
and real-domain settings, demonstrating superior retrieval performance.

</details>


### [250] [Instant Preference Alignment for Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.17718)
*Yang Li,Songlin Yang,Xiaoxuan Han,Wei Wang,Jing Dong,Yueming Lyu,Ziyu Xue*

Main category: cs.CV

TL;DR: Training-free framework using MLLM priors for instant preference-aligned text-to-image generation, achieving better results than previous methods through preference understanding and guided generation.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image generation methods rely on static preferences or fine-tuning, limiting adaptability to evolving user intents. Need for real-time, training-free preference alignment.

Method: Decouples into preference understanding (using MLLMs to extract global signals from reference images) and preference-guided generation (global keyword control + local cross-attention modulation). Supports multi-round interactive refinement.

Result: Outperforms prior approaches on Viper dataset and collected benchmark in both quantitative metrics and human evaluations. Enables dialog-based generation and MLLM-diffusion integration.

Conclusion: Proposed framework successfully achieves instant preference-aligned T2I generation without training, supporting broader and more fine-grained coverage of user preferences through MLLM integration.

Abstract: Text-to-image (T2I) generation has greatly enhanced creative expression, yet
achieving preference-aligned generation in a real-time and training-free manner
remains challenging. Previous methods often rely on static, pre-collected
preferences or fine-tuning, limiting adaptability to evolving and nuanced user
intents. In this paper, we highlight the need for instant preference-aligned
T2I generation and propose a training-free framework grounded in multimodal
large language model (MLLM) priors. Our framework decouples the task into two
components: preference understanding and preference-guided generation. For
preference understanding, we leverage MLLMs to automatically extract global
preference signals from a reference image and enrich a given prompt using
structured instruction design. Our approach supports broader and more
fine-grained coverage of user preferences than existing methods. For
preference-guided generation, we integrate global keyword-based control and
local region-aware cross-attention modulation to steer the diffusion model
without additional training, enabling precise alignment across both global
attributes and local elements. The entire framework supports multi-round
interactive refinement, facilitating real-time and context-aware image
generation. Extensive experiments on the Viper dataset and our collected
benchmark demonstrate that our method outperforms prior approaches in both
quantitative metrics and human evaluations, and opens up new possibilities for
dialog-based generation and MLLM-diffusion integration.

</details>


### [251] [Few-shot Human Action Anomaly Detection via a Unified Contrastive Learning Framework](https://arxiv.org/abs/2508.17726)
*Koichiro Kamide,Shunsuke Sakai,Shun Maeda,Chunzhi Gu,Chao Zhang*

Main category: cs.CV

TL;DR: A unified framework for Human Action Anomaly Detection that works with few-shot scenarios using contrastive learning and diffusion-based motion augmentation, achieving state-of-the-art results on seen and unseen action categories.


<details>
  <summary>Details</summary>
Motivation: Existing HAAD methods require separate training for each action category with large normal samples, limiting scalability and real-world applicability where data is scarce or novel categories frequently appear.

Method: Constructs category-agnostic representation space via contrastive learning, compares test samples with small support sets, and uses generative motion augmentation with diffusion models to create diverse training samples.

Result: Extensive experiments on HumanAct12 dataset demonstrate state-of-the-art effectiveness under both seen and unseen category settings, with improved training efficiency and model scalability for few-shot HAAD.

Conclusion: The proposed framework successfully addresses scalability limitations of existing methods and enables effective anomaly detection in few-shot scenarios with both known and novel action categories.

Abstract: Human Action Anomaly Detection (HAAD) aims to identify anomalous actions
given only normal action data during training. Existing methods typically
follow a one-model-per-category paradigm, requiring separate training for each
action category and a large number of normal samples. These constraints hinder
scalability and limit applicability in real-world scenarios, where data is
often scarce or novel categories frequently appear. To address these
limitations, we propose a unified framework for HAAD that is compatible with
few-shot scenarios. Our method constructs a category-agnostic representation
space via contrastive learning, enabling AD by comparing test samples with a
given small set of normal examples (referred to as the support set). To improve
inter-category generalization and intra-category robustness, we introduce a
generative motion augmentation strategy harnessing a diffusion-based foundation
model for creating diverse and realistic training samples. Notably, to the best
of our knowledge, our work is the first to introduce such a strategy
specifically tailored to enhance contrastive learning for action AD. Extensive
experiments on the HumanAct12 dataset demonstrate the state-of-the-art
effectiveness of our approach under both seen and unseen category settings,
regarding training efficiency and model scalability for few-shot HAAD.

</details>


### [252] [CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text-to-Image Generation](https://arxiv.org/abs/2508.17760)
*Mingyue Yang,Dianxi Shi,Jialu Zhou,Xinyu Wei,Leqian Li,Shaowu Yang,Chunping Qiu*

Main category: cs.CV

TL;DR: CEIDM is a diffusion-based T2I method with dual controls for entities and interactions, using LLM-based relationship mining, action clustering/offsetting, and entity control networks to generate higher quality images with better entity and interaction accuracy.


<details>
  <summary>Details</summary>
Motivation: Current T2I diffusion models struggle with effectively controlling complex entities and their intricate interactions to produce high-quality images that follow realistic logic.

Method: 1) LLM-based entity interactive relationship mining using chain of thought; 2) Interactive action clustering and offset method with global/local bidirectional offsets; 3) Entity control network with semantic-guided masks and multi-scale convolutional/dynamic networks.

Result: Experiments show CEIDM outperforms existing methods in both entity control and interaction control, generating images with more accurate interactive actions and closer adherence to realistic logic.

Conclusion: CEIDM successfully addresses the challenge of controlling entities and their interactions in T2I generation through its dual control approach, producing higher quality and more logically consistent images than current state-of-the-art methods.

Abstract: In Text-to-Image (T2I) generation, the complexity of entities and their
intricate interactions pose a significant challenge for T2I method based on
diffusion model: how to effectively control entity and their interactions to
produce high-quality images. To address this, we propose CEIDM, a image
generation method based on diffusion model with dual controls for entity and
interaction. First, we propose an entity interactive relationships mining
approach based on Large Language Models (LLMs), extracting reasonable and rich
implicit interactive relationships through chain of thought to guide diffusion
models to generate high-quality images that are closer to realistic logic and
have more reasonable interactive relationships. Furthermore, We propose an
interactive action clustering and offset method to cluster and offset the
interactive action features contained in each text prompts. By constructing
global and local bidirectional offsets, we enhance semantic understanding and
detail supplementation of original actions, making the model's understanding of
the concept of interactive "actions" more accurate and generating images with
more accurate interactive actions. Finally, we design an entity control network
which generates masks with entity semantic guidance, then leveraging
multi-scale convolutional network to enhance entity feature and dynamic network
to fuse feature. It effectively controls entities and significantly improves
image quality. Experiments show that the proposed CEIDM method is better than
the most representative existing methods in both entity control and their
interaction control.

</details>


### [253] [Segmentation and Classification of Pap Smear Images for Cervical Cancer Detection Using Deep Learning](https://arxiv.org/abs/2508.17728)
*Nisreen Albzour,Sarah S. Lam*

Main category: cs.CV

TL;DR: Deep learning framework using U-Net segmentation and classification for cervical cancer detection from Pap smear images, showing marginal performance improvement with segmentation.


<details>
  <summary>Details</summary>
Motivation: Manual Pap smear examination is time-consuming and error-prone, requiring automated solutions for early cervical cancer detection to reduce mortality rates.

Method: Integrated U-Net for segmentation and classification model using Herlev Pap Smear Dataset, comparing performance on segmented vs non-segmented images.

Result: Segmented images showed marginal improvement: 0.41% higher precision and 1.30% higher F1-score, indicating limited impact on classification performance.

Conclusion: Segmentation helps feature extraction but has limited impact on classification; framework serves as supplemental tool for pathologists in early diagnosis.

Abstract: Cervical cancer remains a significant global health concern and a leading
cause of cancer-related deaths among women. Early detection through Pap smear
tests is essential to reduce mortality rates; however, the manual examination
is time consuming and prone to human error. This study proposes a deep learning
framework that integrates U-Net for segmentation and a classification model to
enhance diagnostic performance. The Herlev Pap Smear Dataset, a publicly
available cervical cell dataset, was utilized for training and evaluation. The
impact of segmentation on classification performance was evaluated by comparing
the model trained on segmented images and another trained on non-segmented
images. Experimental results showed that the use of segmented images marginally
improved the model performance on precision (about 0.41 percent higher) and
F1-score (about 1.30 percent higher), which suggests a slightly more balanced
classification performance. While segmentation helps in feature extraction, the
results showed that its impact on classification performance appears to be
limited. The proposed framework offers a supplemental tool for clinical
applications, which may aid pathologists in early diagnosis.

</details>


### [254] [CMFDNet: Cross-Mamba and Feature Discovery Network for Polyp Segmentation](https://arxiv.org/abs/2508.17729)
*Feng Jiang,Zongfei Zhang,Xin Xu*

Main category: cs.CV

TL;DR: CMFDNet is a novel polyp segmentation architecture that addresses three key challenges: shape/size variation, indistinct boundaries, and small polyp detection, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing polyp segmentation methods struggle with significant shape/size variations, blurry boundaries between polyps and adjacent tissues, and the tendency to overlook small polyps during segmentation.

Method: Proposes CMFDNet with three modules: CMD module (cross-scanning decoder to reduce blurry boundaries), MSA module (multi-branch parallel structure for diverse geometry recognition), and FD module (establishes decoder feature dependencies for small polyp detection).

Result: Outperforms six state-of-the-art methods, achieving mDice scores that exceed the best SOTA by 1.83% on ETIS dataset and 1.55% on ColonDB dataset.

Conclusion: CMFDNet effectively addresses key limitations in polyp segmentation through its innovative architecture design, demonstrating superior performance particularly in handling boundary clarity and small polyp detection.

Abstract: Automated colonic polyp segmentation is crucial for assisting doctors in
screening of precancerous polyps and diagnosis of colorectal neoplasms.
Although existing methods have achieved promising results, polyp segmentation
remains hindered by the following limitations,including: (1) significant
variation in polyp shapes and sizes, (2) indistinct boundaries between polyps
and adjacent tissues, and (3) small-sized polyps are easily overlooked during
the segmentation process. Driven by these practical difficulties, an innovative
architecture, CMFDNet, is proposed with the CMD module, MSA module, and FD
module. The CMD module, serving as an innovative decoder, introduces a
cross-scanning method to reduce blurry boundaries. The MSA module adopts a
multi-branch parallel structure to enhance the recognition ability for polyps
with diverse geometries and scale distributions. The FD module establishes
dependencies among all decoder features to alleviate the under-detection of
polyps with small-scale features. Experimental results show that CMFDNet
outperforms six SOTA methods used for comparison, especially on ETIS and
ColonDB datasets, where mDice scores exceed the best SOTA method by 1.83% and
1.55%, respectively.

</details>


### [255] [Designing Practical Models for Isolated Word Visual Speech Recognition](https://arxiv.org/abs/2508.17894)
*Iason Ioannis Panagos,Giorgos Sfikas,Christophoros Nikou*

Main category: cs.CV

TL;DR: Lightweight visual speech recognition architectures that reduce hardware costs while maintaining strong performance, using efficient image classification models and temporal convolution networks.


<details>
  <summary>Details</summary>
Motivation: Deep VSR systems have high computation costs that limit practical deployment in resource-constrained scenarios, preventing wider adoption in real-world applications.

Method: Following the two-network design paradigm, benchmark efficient models from image classification literature and adopt lightweight block designs in temporal convolution network backbones.

Result: Developed several unified models with low resource requirements but strong recognition performance, validated on the largest public English words database.

Conclusion: The lightweight architectures effectively address hardware cost issues while maintaining recognition accuracy, making VSR systems more practical for real-world deployment.

Abstract: Visual speech recognition (VSR) systems decode spoken words from an input
sequence using only the video data. Practical applications of such systems
include medical assistance as well as human-machine interactions. A VSR system
is typically employed in a complementary role in cases where the audio is
corrupt or not available. In order to accurately predict the spoken words,
these architectures often rely on deep neural networks in order to extract
meaningful representations from the input sequence. While deep architectures
achieve impressive recognition performance, relying on such models incurs
significant computation costs which translates into increased resource demands
in terms of hardware requirements and results in limited applicability in
real-world scenarios where resources might be constrained. This factor prevents
wider adoption and deployment of speech recognition systems in more practical
applications. In this work, we aim to alleviate this issue by developing
architectures for VSR that have low hardware costs. Following the standard
two-network design paradigm, where one network handles visual feature
extraction and another one utilizes the extracted features to classify the
entire sequence, we develop lightweight end-to-end architectures by first
benchmarking efficient models from the image classification literature, and
then adopting lightweight block designs in a temporal convolution network
backbone. We create several unified models with low resource requirements but
strong recognition performance. Experiments on the largest public database for
English words demonstrate the effectiveness and practicality of our developed
models. Code and trained models will be made publicly available.

</details>


### [256] [DroneKey: Drone 3D Pose Estimation in Image Sequences using Gated Key-representation and Pose-adaptive Learning](https://arxiv.org/abs/2508.17746)
*Seo-Bin Hwang,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: DroneKey is a framework for drone 3D pose estimation that combines 2D keypoint detection with 3D pose estimation, achieving state-of-the-art performance with real-time processing at 44 FPS.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with drone keypoint detection due to the visual similarity and pose diversity of drone propellers, which serve as keypoints but are difficult to detect accurately.

Method: Proposes a framework with 2D keypoint detector and 3D pose estimator. Uses transformer encoder layers to extract two key-representations combined via gated sum, and introduces pose-adaptive Mahalanobis distance in loss function for stable predictions. Built new datasets for training and evaluation.

Result: Achieves 99.68% AP in keypoint detection, outperforming existing methods. For 3D pose estimation: MAE-angle of 10.62Â°, RMSE of 0.221m, and MAE-absolute of 0.076m. Real-time processing at 44 FPS. Ablation studies confirm improved stability and accuracy.

Conclusion: DroneKey effectively addresses drone keypoint detection challenges and achieves high accuracy in 3D pose estimation with real-time performance. The method and datasets are publicly available for further research.

Abstract: Estimating the 3D pose of a drone is important for anti-drone systems, but
existing methods struggle with the unique challenges of drone keypoint
detection. Drone propellers serve as keypoints but are difficult to detect due
to their high visual similarity and diversity of poses. To address these
challenges, we propose DroneKey, a framework that combines a 2D keypoint
detector and a 3D pose estimator specifically designed for drones. In the
keypoint detection stage, we extract two key-representations (intermediate and
compact) from each transformer encoder layer and optimally combine them using a
gated sum. We also introduce a pose-adaptive Mahalanobis distance in the loss
function to ensure stable keypoint predictions across extreme poses. We built
new datasets of drone 2D keypoints and 3D pose to train and evaluate our
method, which have been publicly released. Experiments show that our method
achieves an AP of 99.68% (OKS) in keypoint detection, outperforming existing
methods. Ablation studies confirm that the pose-adaptive Mahalanobis loss
function improves keypoint prediction stability and accuracy. Additionally,
improvements in the encoder design enable real-time processing at 44 FPS. For
3D pose estimation, our method achieved an MAE-angle of 10.62{\deg}, an RMSE of
0.221m, and an MAE-absolute of 0.076m, demonstrating high accuracy and
reliability. The code and dataset are available at
https://github.com/kkanuseobin/DroneKey.

</details>


### [257] [From Global to Local: Social Bias Transfer in CLIP](https://arxiv.org/abs/2508.17750)
*Ryan Ramos,Yusuke Hirota,Yuta Nakashima,Noa Garcia*

Main category: cs.CV

TL;DR: This paper investigates how social biases learned during CLIP model pre-training transfer to downstream applications, finding inconsistent bias transfer patterns due to representation space convergence during adaptation.


<details>
  <summary>Details</summary>
Motivation: To understand how social biases and human stereotypes learned during CLIP pre-training propagate to downstream tasks like visual question answering and image captioning, and whether bias transfer occurs consistently.

Method: Comprehensive empirical analysis including: 1) examining pre-training bias variation between global and local data views, 2) analyzing correlations between pre-training and downstream task biases across varying bias levels, and 3) exploring representation space convergence during downstream adaptation.

Result: Bias measurement is highly dependent on data subsets used; inconsistent trends in bias transfer were found; representation spaces of different pre-trained CLIPs tend to converge when adapted for downstream tasks.

Conclusion: Current paradigm shows difficulty in discovering consistent bias transfer patterns, offering insights for better bias mitigation practices in future research.

Abstract: The recycling of contrastive language-image pre-trained (CLIP) models as
backbones for a large number of downstream tasks calls for a thorough analysis
of their transferability implications, especially their well-documented
reproduction of social biases and human stereotypes. How do such biases,
learned during pre-training, propagate to downstream applications like visual
question answering or image captioning? Do they transfer at all?
  We investigate this phenomenon, referred to as bias transfer in prior
literature, through a comprehensive empirical analysis. Firstly, we examine how
pre-training bias varies between global and local views of data, finding that
bias measurement is highly dependent on the subset of data on which it is
computed. Secondly, we analyze correlations between biases in the pre-trained
models and the downstream tasks across varying levels of pre-training bias,
finding difficulty in discovering consistent trends in bias transfer. Finally,
we explore why this inconsistency occurs, showing that under the current
paradigm, representation spaces of different pre-trained CLIPs tend to converge
when adapted for downstream tasks. We hope this work offers valuable insights
into bias behavior and informs future research to promote better bias
mitigation practices.

</details>


### [258] [Robust Anomaly Detection in Industrial Environments via Meta-Learning](https://arxiv.org/abs/2508.17789)
*Muhammad Aqeel,Shakiba Sharifi,Marco Cristani,Francesco Setti*

Main category: cs.CV

TL;DR: RAD is a robust anomaly detection framework that combines Normalizing Flows with Meta-Learning to handle label noise in industrial settings, achieving high detection accuracy even with 50% mislabeled training data.


<details>
  <summary>Details</summary>
Motivation: Conventional anomaly detection methods struggle with mislabeled training samples, which are common in real-world industrial environments where perfect data curation is challenging.

Method: Integrates Normalizing Flows with Model-Agnostic Meta-Learning, using bi-level optimization for rapid adaptation to noise conditions. Employs uncertainty quantification for adaptive L2 regularization and multiscale feature processing with pretrained feature extractors.

Result: Achieved I-AUROC scores of 95.4% on MVTec-AD and 94.6% on KSDD2 under clean conditions, maintaining robust performance above 86.8% and 92.1% respectively with 50% mislabeled training samples.

Conclusion: RAD demonstrates exceptional resilience to noisy training conditions and effectively detects subtle anomalies across diverse industrial scenarios, making it a practical solution for real-world applications.

Abstract: Anomaly detection is fundamental for ensuring quality control and operational
efficiency in industrial environments, yet conventional approaches face
significant challenges when training data contains mislabeled samples-a common
occurrence in real-world scenarios. This paper presents RAD, a robust anomaly
detection framework that integrates Normalizing Flows with Model-Agnostic
Meta-Learning to address the critical challenge of label noise in industrial
settings. Our approach employs a bi-level optimization strategy where
meta-learning enables rapid adaptation to varying noise conditions, while
uncertainty quantification guides adaptive L2 regularization to maintain model
stability. The framework incorporates multiscale feature processing through
pretrained feature extractors and leverages the precise likelihood estimation
capabilities of Normalizing Flows for robust anomaly scoring. Comprehensive
evaluation on MVTec-AD and KSDD2 datasets demonstrates superior performance,
achieving I-AUROC scores of 95.4% and 94.6% respectively under clean
conditions, while maintaining robust detection capabilities above 86.8% and
92.1% even when 50% of training samples are mislabeled. The results highlight
RAD's exceptional resilience to noisy training conditions and its ability to
detect subtle anomalies across diverse industrial scenarios, making it a
practical solution for real-world anomaly detection applications where perfect
data curation is challenging.

</details>


### [259] [Sketchpose: Learning to Segment Cells with Partial Annotations](https://arxiv.org/abs/2508.17798)
*ClÃ©ment Cazorla,NathanaÃ«l Munier,Renaud Morin,Pierre Weiss*

Main category: cs.CV

TL;DR: A method for cell segmentation that uses distance maps but works with partially annotated objects, enabling frugal learning and transfer learning without full annotations.


<details>
  <summary>Details</summary>
Motivation: Current cell segmentation networks require fully annotated datasets, which limits training set generation and transfer learning capabilities.

Method: Proposes an approach that still relies on distance maps but can handle partially annotated objects, making it suitable for scenarios with incomplete annotations.

Result: The method achieves substantial time and resource savings without compromising segmentation quality, as demonstrated in frugal learning, transfer learning, and regular learning contexts.

Conclusion: The approach successfully addresses the limitation of requiring full annotations in cell segmentation, providing a user-friendly solution through a Napari plugin while maintaining high accuracy.

Abstract: The most popular networks used for cell segmentation (e.g. Cellpose,
Stardist, HoverNet,...) rely on a prediction of a distance map. It yields
unprecedented accuracy but hinges on fully annotated datasets. This is a
serious limitation to generate training sets and perform transfer learning. In
this paper, we propose a method that still relies on the distance map and
handles partially annotated objects. We evaluate the performance of the
proposed approach in the contexts of frugal learning, transfer learning and
regular learning on regular databases. Our experiments show that it can lead to
substantial savings in time and resources without sacrificing segmentation
quality. The proposed algorithm is embedded in a user-friendly Napari plugin.

</details>


### [260] [PoRe: Position-Reweighted Visual Token Pruning for Vision Language Models](https://arxiv.org/abs/2508.17807)
*Kai Zhao,Wubang Yuan,Alex Lingyu Hung,Dan Zeng*

Main category: cs.CV

TL;DR: Simple position-based reweighting method to fix recency bias in visual token pruning for VLMs, improving pruning performance without architecture changes.


<details>
  <summary>Details</summary>
Motivation: VLMs suffer from recency bias where lower image regions get inflated attention scores, leading to suboptimal visual token pruning that disproportionately retains bottom tokens.

Method: Propose position-reweighted visual token pruning - a plug-and-play reweighting mechanism that adjusts attention scores based on spatial positions to alleviate recency bias.

Result: Extensive experiments show improved performance of visual token pruning with minimal computational overhead across LVLMs.

Conclusion: Simple position-based reweighting effectively addresses recency bias in visual token pruning, enhancing VLM efficiency without requiring architectural changes or extra training.

Abstract: Vision-Language Models (VLMs) typically process a significantly larger number
of visual tokens compared to text tokens due to the inherent redundancy in
visual signals. Visual token pruning is a promising direction to reduce the
computational cost of VLMs by eliminating redundant visual tokens. The
text-visual attention score is a widely adopted criterion for visual token
pruning as it reflects the relevance of visual tokens to the text input.
However, many sequence models exhibit a recency bias, where tokens appearing
later in the sequence exert a disproportionately large influence on the model's
output. In VLMs, this bias manifests as inflated attention scores for tokens
corresponding to the lower regions of the image, leading to suboptimal pruning
that disproportionately retains tokens from the image bottom. In this paper, we
present an extremely simple yet effective approach to alleviate the recency
bias in visual token pruning. We propose a straightforward reweighting
mechanism that adjusts the attention scores of visual tokens according to their
spatial positions in the image. Our method, termed Position-reweighted Visual
Token Pruning, is a plug-and-play solution that can be seamlessly incorporated
into existing visual token pruning frameworks without any changes to the model
architecture or extra training. Extensive experiments on LVLMs demonstrate that
our method improves the performance of visual token pruning with minimal
computational overhead.

</details>


### [261] [UniSino: Physics-Driven Foundational Model for Universal CT Sinogram Standardization](https://arxiv.org/abs/2508.17816)
*Xingyu Ai,Shaoyu Wang,Zhiyuan Jia,Ao Xu,Hongming Shan,Jianhua Ma,Qiegen Liu*

Main category: cs.CV

TL;DR: UniSino is a foundation model that standardizes CT sinograms in the projection domain to address undersampling and noise artifacts, achieving superior reconstruction quality and generalization across diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: Conventional CT sinogram correction methods lack generalizability across heterogeneous artifact types and rely on manually designed algorithms with fixed parameters, leading to compromised diagnostic accuracy from undersampling and noise artifacts.

Method: UniSino operates directly in the projection domain rather than image domain, incorporating physical characteristics of sinograms in its training framework to enhance generalization across multiple undersampling scenarios and subtasks.

Result: Experimental results show UniSino achieves superior reconstruction quality in both single and mixed undersampling cases, demonstrating exceptional robustness and generalization across four benchmark datasets.

Conclusion: UniSino provides a universal CT sinogram standardization solution with strong generalization capabilities, outperforming conventional methods and offering robust performance for diverse undersampling scenarios in CT imaging.

Abstract: During raw-data acquisition in CT imaging, diverse factors can degrade the
collected sinograms, with undersampling and noise leading to severe artifacts
and noise in reconstructed images and compromising diagnostic accuracy.
Conventional correction methods rely on manually designed algorithms or fixed
empirical parameters, but these approaches often lack generalizability across
heterogeneous artifact types. To address these limitations, we propose UniSino,
a foundation model for universal CT sinogram standardization. Unlike existing
foundational models that operate in image domain, UniSino directly standardizes
data in the projection domain, which enables stronger generalization across
diverse undersampling scenarios. Its training framework incorporates the
physical characteristics of sinograms, enhancing generalization and enabling
robust performance across multiple subtasks spanning four benchmark datasets.
Experimental results demonstrate thatUniSino achieves superior reconstruction
quality both single and mixed undersampling case, demonstrating exceptional
robustness and generalization in sinogram enhancement for CT imaging. The code
is available at: https://github.com/yqx7150/UniSino.

</details>


### [262] [TemCoCo: Temporally Consistent Multi-modal Video Fusion with Visual-Semantic Collaboration](https://arxiv.org/abs/2508.17817)
*Meiqi Gong,Hao Zhang,Xunpeng Yi,Linfeng Tang,Jiayi Ma*

Main category: cs.CV

TL;DR: First video fusion framework with temporal modeling and visual-semantic collaboration to ensure visual fidelity, semantic accuracy, and temporal consistency across video frames.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal fusion methods use static frame-based techniques for video tasks, neglecting temporal dependencies and causing inconsistent results across frames.

Method: Visual-semantic interaction module with Dinov2 and VGG19 for targeted distillation; temporal cooperative module for video degradation enhancement; temporal-enhanced mechanism with temporal loss; two new evaluation metrics for temporal consistency.

Result: Extensive experiments on public video datasets demonstrate superior performance compared to existing methods.

Conclusion: The proposed framework effectively addresses temporal consistency issues in video fusion while maintaining visual and semantic quality, with code publicly available.

Abstract: Existing multi-modal fusion methods typically apply static frame-based image
fusion techniques directly to video fusion tasks, neglecting inherent temporal
dependencies and leading to inconsistent results across frames. To address this
limitation, we propose the first video fusion framework that explicitly
incorporates temporal modeling with visual-semantic collaboration to
simultaneously ensure visual fidelity, semantic accuracy, and temporal
consistency. First, we introduce a visual-semantic interaction module
consisting of a semantic branch and a visual branch, with Dinov2 and VGG19
employed for targeted distillation, allowing simultaneous enhancement of both
the visual and semantic representations. Second, we pioneer integrate the video
degradation enhancement task into the video fusion pipeline by constructing a
temporal cooperative module, which leverages temporal dependencies to
facilitate weak information recovery. Third, to ensure temporal consistency, we
embed a temporal-enhanced mechanism into the network and devise a temporal loss
to guide the optimization process. Finally, we introduce two innovative
evaluation metrics tailored for video fusion, aimed at assessing the temporal
consistency of the generated fused videos. Extensive experimental results on
public video datasets demonstrate the superiority of our method. Our code is
released at https://github.com/Meiqi-Gong/TemCoCo.

</details>


### [263] [A Contrastive Learning-Guided Confident Meta-learning for Zero Shot Anomaly Detection](https://arxiv.org/abs/2508.17827)
*Muhammad Aqeel,Danijel Skocaj,Marco Cristani,Francesco Setti*

Main category: cs.CV

TL;DR: CoZAD is a zero-shot anomaly detection framework that combines soft confident learning with meta-learning and contrastive representation, achieving state-of-the-art performance without vision-language models or ensembles.


<details>
  <summary>Details</summary>
Motivation: Address data scarcity and high annotation costs in industrial and medical anomaly detection, particularly in evolving manufacturing and healthcare settings where traditional methods struggle.

Method: Integrates soft confident learning (assigning confidence-based weights instead of discarding uncertain samples), meta-learning (MAML framework with covariance regularization), and contrastive learning for discriminative feature spaces. Uses IQR-based thresholding for data uncertainty and covariance regularization for model uncertainty.

Result: Outperforms existing methods on 6/7 industrial benchmarks, achieving 99.2% I-AUROC on DTD-Synthetic, 97.2% on BTAD, and 96.3% P-AUROC on MVTec-AD for pixel-level localization. Demonstrates strong performance across 10 industrial and medical datasets.

Conclusion: CoZAD provides an effective zero-shot solution for anomaly detection that eliminates dependence on vision-language alignments or model ensembles, making it suitable for resource-constrained environments requiring rapid deployment.

Abstract: Industrial and medical anomaly detection faces critical challenges from data
scarcity and prohibitive annotation costs, particularly in evolving
manufacturing and healthcare settings. To address this, we propose CoZAD, a
novel zero-shot anomaly detection framework that integrates soft confident
learning with meta-learning and contrastive feature representation. Unlike
traditional confident learning that discards uncertain samples, our method
assigns confidence-based weights to all training data, preserving boundary
information while emphasizing prototypical normal patterns. The framework
quantifies data uncertainty through IQR-based thresholding and model
uncertainty via covariance based regularization within a Model-Agnostic
Meta-Learning. Contrastive learning creates discriminative feature spaces where
normal patterns form compact clusters, enabling rapid domain adaptation.
Comprehensive evaluation across 10 datasets spanning industrial and medical
domains demonstrates state-of-the-art performance, outperforming existing
methods on 6 out of 7 industrial benchmarks with notable improvements on
texture-rich datasets (99.2% I-AUROC on DTD-Synthetic, 97.2% on BTAD) and
pixellevel localization (96.3% P-AUROC on MVTec-AD). The framework eliminates
dependence on vision-language alignments or model ensembles, making it valuable
for resourceconstrained environments requiring rapid deployment.

</details>


### [264] [HLG: Comprehensive 3D Room Construction via Hierarchical Layout Generation](https://arxiv.org/abs/2508.17832)
*Xiping Wang,Yuxi Wang,Mengqi Zhou,Junsong Fan,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: HLG is a hierarchical method for fine-grained 3D indoor scene generation that uses coarse-to-fine refinement and layout optimization to create realistic object placements.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with fine-grained object placements, limiting realism and utility for VR, interior design, and embodied AI applications that require detailed scene comprehension.

Method: Hierarchical Layout Generation (HLG) with coarse-to-fine approach, fine-grained layout alignment module (vertical/horizontal decoupling), and trainable layout optimization network to fix placement issues like positioning errors and object intersections.

Result: Superior performance in generating realistic indoor scenes compared to existing methods, demonstrated through extensive experiments.

Conclusion: HLG advances scene generation field and enables new applications requiring detailed 3D environments, with code to be released for future research.

Abstract: Realistic 3D indoor scene generation is crucial for virtual reality, interior
design, embodied intelligence, and scene understanding. While existing methods
have made progress in coarse-scale furniture arrangement, they struggle to
capture fine-grained object placements, limiting the realism and utility of
generated environments. This gap hinders immersive virtual experiences and
detailed scene comprehension for embodied AI applications. To address these
issues, we propose Hierarchical Layout Generation (HLG), a novel method for
fine-grained 3D scene generation. HLG is the first to adopt a coarse-to-fine
hierarchical approach, refining scene layouts from large-scale furniture
placement to intricate object arrangements. Specifically, our fine-grained
layout alignment module constructs a hierarchical layout through vertical and
horizontal decoupling, effectively decomposing complex 3D indoor scenes into
multiple levels of granularity. Additionally, our trainable layout optimization
network addresses placement issues, such as incorrect positioning, orientation
errors, and object intersections, ensuring structurally coherent and physically
plausible scene generation. We demonstrate the effectiveness of our approach
through extensive experiments, showing superior performance in generating
realistic indoor scenes compared to existing methods. This work advances the
field of scene generation and opens new possibilities for applications
requiring detailed 3D environments. We will release our code upon publication
to encourage future research.

</details>


### [265] [SCOUT: Semi-supervised Camouflaged Object Detection by Utilizing Text and Adaptive Data Selection](https://arxiv.org/abs/2508.17843)
*Weiqi Yan,Lvhai Chen,Shengchuan Zhang,Yan Zhang,Liujuan Cao*

Main category: cs.CV

TL;DR: SCOUT introduces a semi-supervised camouflaged object detection method with adaptive data selection and text fusion, achieving state-of-the-art performance while reducing annotation costs.


<details>
  <summary>Details</summary>
Motivation: Pixel-level annotation is costly and hinders COD development. Existing semi-supervised methods don't effectively utilize unlabeled data, leaving room for improvement.

Method: Uses Adaptive Data Augment and Selection (ADAS) module for valuable data selection via adversarial augment/sampling, and Text Fusion Module (TFM) that combines camouflage knowledge with text-visual interaction. Built new RefTextCOD dataset.

Result: Extensive experiments show SCOUT surpasses previous semi-supervised COD methods and achieves state-of-the-art performance.

Conclusion: The proposed SCOUT framework effectively addresses annotation cost issues in COD through adaptive data selection and text-visual fusion, demonstrating superior performance over existing approaches.

Abstract: The difficulty of pixel-level annotation has significantly hindered the
development of the Camouflaged Object Detection (COD) field. To save on
annotation costs, previous works leverage the semi-supervised COD framework
that relies on a small number of labeled data and a large volume of unlabeled
data. We argue that there is still significant room for improvement in the
effective utilization of unlabeled data. To this end, we introduce a
Semi-supervised Camouflaged Object Detection by Utilizing Text and Adaptive
Data Selection (SCOUT). It includes an Adaptive Data Augment and Selection
(ADAS) module and a Text Fusion Module (TFM). The ADSA module selects valuable
data for annotation through an adversarial augment and sampling strategy. The
TFM module further leverages the selected valuable data by combining
camouflage-related knowledge and text-visual interaction. To adapt to this
work, we build a new dataset, namely RefTextCOD. Extensive experiments show
that the proposed method surpasses previous semi-supervised methods in the COD
field and achieves state-of-the-art performance. Our code will be released at
https://github.com/Heartfirey/SCOUT.

</details>


### [266] [Diffusion-Based Data Augmentation for Medical Image Segmentation](https://arxiv.org/abs/2508.17844)
*Maham Nazir,Muhammad Aqeel,Francesco Setti*

Main category: cs.CV

TL;DR: DiffAug is a novel framework that uses text-guided diffusion models to generate synthetic medical abnormalities with automatic segmentation validation, achieving state-of-the-art performance with 8-10% Dice improvements and 28% reduction in false negatives.


<details>
  <summary>Details</summary>
Motivation: Medical image segmentation models struggle with rare abnormalities due to scarce annotated pathological data, creating a need for synthetic data generation methods that can produce diverse and accurate pathological samples.

Method: Combines text-guided diffusion models conditioned on medical text descriptions and spatial masks to synthesize abnormalities via inpainting on normal images, with dynamic quality validation through a latent-space segmentation network for accurate localization and single-step inference.

Result: Achieves state-of-the-art performance on three medical imaging benchmarks (CVC-ClinicDB, Kvasir-SEG, REFUGE2) with 8-10% Dice improvements over baselines and reduces false negative rates by up to 28% for challenging cases like small polyps and flat lesions.

Conclusion: The proposed DiffAug framework effectively addresses the scarcity of annotated pathological data by generating high-quality synthetic abnormalities through text-guided diffusion and automatic validation, significantly improving segmentation performance for rare medical conditions.

Abstract: Medical image segmentation models struggle with rare abnormalities due to
scarce annotated pathological data. We propose DiffAug a novel framework that
combines textguided diffusion-based generation with automatic segmentation
validation to address this challenge. Our proposed approach uses latent
diffusion models conditioned on medical text descriptions and spatial masks to
synthesize abnormalities via inpainting on normal images. Generated samples
undergo dynamic quality validation through a latentspace segmentation network
that ensures accurate localization while enabling single-step inference. The
text prompts, derived from medical literature, guide the generation of diverse
abnormality types without requiring manual annotation. Our validation mechanism
filters synthetic samples based on spatial accuracy, maintaining quality while
operating efficiently through direct latent estimation. Evaluated on three
medical imaging benchmarks (CVC-ClinicDB, Kvasir-SEG, REFUGE2), our framework
achieves state-of-the-art performance with 8-10% Dice improvements over
baselines and reduces false negative rates by up to 28% for challenging cases
like small polyps and flat lesions critical for early detection in screening
applications.

</details>


### [267] [Alternating Training-based Label Smoothing Enhances Prompt Generalization](https://arxiv.org/abs/2508.17846)
*Yang Chen,Yanbin Wei,Ke Jin,Yi Kong,James Kwok,Yu Zhang*

Main category: cs.CV

TL;DR: ATLaS method combines label smoothing with prompt tuning through alternating training with one-hot and soft labels, improving generalization performance of vision-language models.


<details>
  <summary>Details</summary>
Motivation: Prompt tuning has limited generalization despite being parameter-efficient, and vanilla label smoothing weakens prompt tuning performance. The authors want to integrate label smoothing effectively with prompt tuning.

Method: Alternating Training-based Label Smoothing (ATLaS) method that alternately trains with standard one-hot labels and soft labels from label smoothing. Also introduces Class-wise Soft Labels (CSL) and Instance-wise Soft Labels (ISL) to provide inter-class and instance-class relationships.

Result: Extensive experiments show ATLaS consistently enhances generalization performance of prompt tuning and exhibits high compatibility with prevalent prompt tuning methods.

Conclusion: ATLaS successfully integrates label smoothing with prompt tuning, overcoming the limitations of vanilla label smoothing and improving model generalization across various downstream tasks.

Abstract: Recent advances in pre-trained vision-language models have demonstrated
remarkable zero-shot generalization capabilities. To further enhance these
models' adaptability to various downstream tasks, prompt tuning has emerged as
a parameter-efficient fine-tuning method. However, despite its efficiency, the
generalization ability of prompt remains limited. In contrast, label smoothing
(LS) has been widely recognized as an effective regularization technique that
prevents models from becoming over-confident and improves their generalization.
This inspires us to explore the integration of LS with prompt tuning. However,
we have observed that the vanilla LS even weakens the generalization ability of
prompt tuning. To address this issue, we propose the Alternating Training-based
Label Smoothing (ATLaS) method, which alternately trains with standard one-hot
labels and soft labels generated by LS to supervise the prompt tuning.
Moreover, we introduce two types of efficient offline soft labels, including
Class-wise Soft Labels (CSL) and Instance-wise Soft Labels (ISL), to provide
inter-class or instance-class relationships for prompt tuning. The theoretical
properties of the proposed ATLaS method are analyzed. Extensive experiments
demonstrate that the proposed ATLaS method, combined with CSL and ISL,
consistently enhances the generalization performance of prompt tuning.
Moreover, the proposed ATLaS method exhibits high compatibility with prevalent
prompt tuning methods, enabling seamless integration into existing methods.

</details>


### [268] [Box-Level Class-Balanced Sampling for Active Object Detection](https://arxiv.org/abs/2508.17849)
*Jingyi Liao,Xun Xu,Chuan-Sheng Foo,Lile Cai*

Main category: cs.CV

TL;DR: Class-balanced active learning for object detection that addresses class imbalance in pseudo labels through balanced sampling and task-aware soft pseudo labeling.


<details>
  <summary>Details</summary>
Motivation: Training deep object detectors requires expensive bounding box annotation, and existing box-level active learning approaches suffer from severe class imbalance in pseudo labels during early training stages where models only perform well on majority classes.

Method: Proposes a class-balanced sampling strategy to select more objects from minority classes for labeling, and a task-aware soft pseudo labeling strategy to improve pseudo label accuracy.

Result: Achieves state-of-the-art performance on public benchmarking datasets.

Conclusion: The proposed class-balanced active learning approach effectively addresses class imbalance in pseudo labels and improves object detection performance with reduced annotation costs.

Abstract: Training deep object detectors demands expensive bounding box annotation.
Active learning (AL) is a promising technique to alleviate the annotation
burden. Performing AL at box-level for object detection, i.e., selecting the
most informative boxes to label and supplementing the sparsely-labelled image
with pseudo labels, has been shown to be more cost-effective than selecting and
labelling the entire image. In box-level AL for object detection, we observe
that models at early stage can only perform well on majority classes, making
the pseudo labels severely class-imbalanced. We propose a class-balanced
sampling strategy to select more objects from minority classes for labelling,
so as to make the final training data, \ie, ground truth labels obtained by AL
and pseudo labels, more class-balanced to train a better model. We also propose
a task-aware soft pseudo labelling strategy to increase the accuracy of pseudo
labels. We evaluate our method on public benchmarking datasets and show that
our method achieves state-of-the-art performance.

</details>


### [269] [VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inference](https://arxiv.org/abs/2508.17857)
*Pengfei Jiang,Hanjun Li,Linglan Zhao,Fei Chao,Ke Yan,Shouhong Ding,Rongrong Ji*

Main category: cs.CV

TL;DR: VISA is a novel method that compresses visual tokens in multimodal LLMs using graph-based aggregation and group-wise selection to improve inference efficiency while preserving visual information.


<details>
  <summary>Details</summary>
Motivation: Address inefficient inference caused by excessive visual tokens in multimodal large language models (MLLMs) while maintaining visual information quality.

Method: Combines graph-based visual token aggregation (VTA) that treats tokens as nodes in a semantic similarity graph, and group-wise token selection (GTS) that divides tokens into kept/removed groups guided by text tokens.

Result: Outperforms previous methods on LLaVA-1.5, LLaVA-NeXT, and Video-LLaVA benchmarks, achieving superior trade-off between performance and inference speed.

Conclusion: VISA effectively compresses visual tokens while preserving more information than previous pruning approaches, making MLLM inference more efficient without sacrificing performance.

Abstract: In this study, we introduce a novel method called group-wise \textbf{VI}sual
token \textbf{S}election and \textbf{A}ggregation (VISA) to address the issue
of inefficient inference stemming from excessive visual tokens in multimoal
large language models (MLLMs). Compared with previous token pruning approaches,
our method can preserve more visual information while compressing visual
tokens. We first propose a graph-based visual token aggregation (VTA) module.
VTA treats each visual token as a node, forming a graph based on semantic
similarity among visual tokens. It then aggregates information from removed
tokens into kept tokens based on this graph, producing a more compact visual
token representation. Additionally, we introduce a group-wise token selection
strategy (GTS) to divide visual tokens into kept and removed ones, guided by
text tokens from the final layers of each group. This strategy progressively
aggregates visual information, enhancing the stability of the visual
information extraction process. We conduct comprehensive experiments on
LLaVA-1.5, LLaVA-NeXT, and Video-LLaVA across various benchmarks to validate
the efficacy of VISA. Our method consistently outperforms previous methods,
achieving a superior trade-off between model performance and inference speed.
The code is available at https://github.com/mobiushy/VISA.

</details>


### [270] [AVAM: Universal Training-free Adaptive Visual Anchoring Embedded into Multimodal Large Language Model for Multi-image Question Answering](https://arxiv.org/abs/2508.17860)
*Kang Zeng,Guojin Zhong,Jintao Cheng,Jin Yuan,Zhiyong Li*

Main category: cs.CV

TL;DR: Proposes Adaptive Visual Anchoring strategy to compress redundant visual information in Multi-Image VQA, improving accuracy and efficiency through adaptive compression and collaborative decoding.


<details>
  <summary>Details</summary>
Motivation: Multi-Image VQA introduces substantial visual redundancy that negatively impacts accuracy and efficiency, and existing methods lack flexibility in controlling compressed visual tokens and produce discrete fragments that hinder holistic image comprehension.

Method: Adaptive Visual Anchoring strategy that can be integrated into existing MLLMs for adaptive compression, plus a collaborative decoding mechanism to balance results from global and compressed visual input.

Result: Extensive experiments show consistent performance improvements across various MLLMs, validating the method's effectiveness.

Conclusion: The proposed approach effectively addresses visual redundancy in MVQA through adaptive compression and collaborative decoding, achieving significant accuracy improvements while maintaining flexibility.

Abstract: The advancement of Multimodal Large Language Models (MLLMs) has driven
significant progress in Visual Question Answering (VQA), evolving from Single
to Multi Image VQA (MVQA). However, the increased number of images in MVQA
inevitably introduces substantial visual redundancy that is irrelevant to
question answering, negatively impacting both accuracy and efficiency. To
address this issue, existing methods lack flexibility in controlling the number
of compressed visual tokens and tend to produce discrete visual fragments,
which hinder MLLMs' ability to comprehend images holistically. In this paper,
we propose a straightforward yet universal Adaptive Visual Anchoring strategy,
which can be seamlessly integrated into existing MLLMs, offering significant
accuracy improvements through adaptive compression. Meanwhile, to balance the
results derived from both global and compressed visual input, we further
introduce a novel collaborative decoding mechanism, enabling optimal
performance. Extensive experiments validate the effectiveness of our method,
demonstrating consistent performance improvements across various MLLMs. The
code will be publicly available.

</details>


### [271] [Camera Pose Refinement via 3D Gaussian Splatting](https://arxiv.org/abs/2508.17876)
*Lulu Hao,Lipu Zhou,Zhenzhong Wei,Xu Wang*

Main category: cs.CV

TL;DR: GS-SMC is a novel camera pose refinement framework that leverages 3D Gaussian Splatting to improve pose accuracy without requiring scene-specific retraining, using epipolar geometric constraints between query and rendered images.


<details>
  <summary>Details</summary>
Motivation: Existing pose refinement methods require reconstructing scenes for different descriptors or retraining networks for each scene, while geometry-free methods lack accuracy. There's a need for a lightweight solution that works across diverse scenes without additional training.

Method: Uses existing 3DGS models to render novel views, then applies iterative optimization with epipolar geometric constraints between query images and multiple rendered images. Allows flexible choice of feature extractors and matchers.

Result: Outperforms state-of-the-art methods with 53.3% and 56.9% reductions in median translation and rotation errors on 7-Scenes, and 40.7% and 53.2% on Cambridge Landmarks datasets.

Conclusion: The proposed GS-SMC framework provides an effective and flexible camera pose refinement solution that leverages existing 3DGS models without requiring scene-specific training, achieving significant accuracy improvements over current methods.

Abstract: Camera pose refinement aims at improving the accuracy of initial pose
estimation for applications in 3D computer vision. Most refinement approaches
rely on 2D-3D correspondences with specific descriptors or dedicated networks,
requiring reconstructing the scene again for a different descriptor or fully
retraining the network for each scene. Some recent methods instead infer pose
from feature similarity, but their lack of geometry constraints results in less
accuracy. To overcome these limitations, we propose a novel camera pose
refinement framework leveraging 3D Gaussian Splatting (3DGS), referred to as
GS-SMC. Given the widespread usage of 3DGS, our method can employ an existing
3DGS model to render novel views, providing a lightweight solution that can be
directly applied to diverse scenes without additional training or fine-tuning.
Specifically, we introduce an iterative optimization approach, which refines
the camera pose using epipolar geometric constraints among the query and
multiple rendered images. Our method allows flexibly choosing feature
extractors and matchers to establish these constraints. Extensive empirical
evaluations on the 7-Scenes and the Cambridge Landmarks datasets demonstrate
that our method outperforms state-of-the-art camera pose refinement approaches,
achieving 53.3% and 56.9% reductions in median translation and rotation errors
on 7-Scenes, and 40.7% and 53.2% on Cambridge.

</details>


### [272] [Edge-Enhanced Vision Transformer Framework for Accurate AI-Generated Image Detection](https://arxiv.org/abs/2508.17877)
*Dabbrata Das,Mahshar Yahan,Md Tareq Zaman,Md Rishadul Bayesh*

Main category: cs.CV

TL;DR: A hybrid framework combining fine-tuned Vision Transformer with edge-based processing achieves state-of-the-art AI-generated image detection with high accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Address limitations of conventional deep learning methods that overlook subtle structural inconsistencies and require substantial computational resources for detecting AI-generated images.

Method: Hybrid framework with fine-tuned Vision Transformer and novel edge-based module that computes variance from edge-difference maps before/after smoothing, exploiting texture and edge differences between real and AI-generated images.

Result: Achieves 97.75% accuracy and 97.77% F1-score on CIFAKE dataset, surpassing state-of-the-art models across multiple benchmarks including Artistic and Custom Curated datasets.

Conclusion: The proposed method provides a lightweight, interpretable, and effective solution suitable for real-world applications in automated content verification and digital forensics for both images and video frames.

Abstract: The rapid advancement of generative models has led to a growing prevalence of
highly realistic AI-generated images, posing significant challenges for digital
forensics and content authentication. Conventional detection methods mainly
rely on deep learning models that extract global features, which often overlook
subtle structural inconsistencies and demand substantial computational
resources. To address these limitations, we propose a hybrid detection
framework that combines a fine-tuned Vision Transformer (ViT) with a novel
edge-based image processing module. The edge-based module computes variance
from edge-difference maps generated before and after smoothing, exploiting the
observation that AI-generated images typically exhibit smoother textures,
weaker edges, and reduced noise compared to real images. When applied as a
post-processing step on ViT predictions, this module enhances sensitivity to
fine-grained structural cues while maintaining computational efficiency.
Extensive experiments on the CIFAKE, Artistic, and Custom Curated datasets
demonstrate that the proposed framework achieves superior detection performance
across all benchmarks, attaining 97.75% accuracy and a 97.77% F1-score on
CIFAKE, surpassing widely adopted state-of-the-art models. These results
establish the proposed method as a lightweight, interpretable, and effective
solution for both still images and video frames, making it highly suitable for
real-world applications in automated content verification and digital
forensics.

</details>


### [273] [ISALux: Illumination and Segmentation Aware Transformer Employing Mixture of Experts for Low Light Image Enhancement](https://arxiv.org/abs/2508.17885)
*Raul Balmez,Alexandru Brateanu,Ciprian Orhei,Codruta Ancuti,Cosmin Ancuti*

Main category: cs.CV

TL;DR: ISALux is a transformer-based low-light image enhancement method that integrates illumination and semantic priors using a novel self-attention block and MoE-based FFN with LoRA adaptations to prevent overfitting.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of low-light image enhancement by integrating both illumination and semantic information, while overcoming overfitting issues caused by distinct light patterns in benchmarking datasets.

Method: Uses Hybrid Illumination and Semantics-Aware Multi-Headed Self-Attention (HISA-MSA) with two self-attention modules for independent processing of illumination and semantic features, combined with Mixture of Experts-based Feed-Forward Network and low-rank matrix adaptations (LoRA).

Result: Extensive evaluations show ISALux is competitive with state-of-the-art methods across multiple specialized datasets, with ablation studies confirming the contribution of each component.

Conclusion: ISALux effectively integrates illumination and semantic priors for low-light image enhancement, demonstrating strong performance while addressing overfitting through innovative architectural components.

Abstract: We introduce ISALux, a novel transformer-based approach for Low-Light Image
Enhancement (LLIE) that seamlessly integrates illumination and semantic priors.
Our architecture includes an original self-attention block, Hybrid Illumination
and Semantics-Aware Multi-Headed Self- Attention (HISA-MSA), which integrates
illumination and semantic segmentation maps for en- hanced feature extraction.
ISALux employs two self-attention modules to independently process illumination
and semantic features, selectively enriching each other to regulate luminance
and high- light structural variations in real-world scenarios. A Mixture of
Experts (MoE)-based Feed-Forward Network (FFN) enhances contextual learning,
with a gating mechanism conditionally activating the top K experts for
specialized processing. To address overfitting in LLIE methods caused by
distinct light patterns in benchmarking datasets, we enhance the HISA-MSA
module with low-rank matrix adaptations (LoRA). Extensive qualitative and
quantitative evaluations across multiple specialized datasets demonstrate that
ISALux is competitive with state-of-the-art (SOTA) methods. Addition- ally, an
ablation study highlights the contribution of each component in the proposed
model. Code will be released upon publication.

</details>


### [274] [UniAPO: Unified Multimodal Automated Prompt Optimization](https://arxiv.org/abs/2508.17890)
*Qipeng Zhu,Yanzhe Chen,Huasong Zhong,Yan Li,Jie Chen,Zhixin Zhang,Junping Zhang,Zhenheng Yang*

Main category: cs.CV

TL;DR: UniAPO is a unified multimodal automated prompt optimization framework that addresses visual token inflation and lack of process-level supervision challenges in multimodal tasks through EM-inspired optimization and short-long term memory mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing automatic prompt optimization methods work well for text-only inputs but face challenges in multimodal tasks due to visual token inflation (long visual sequences restricting context capacity) and lack of process-level supervision (focusing only on outcome-level feedback).

Method: UniAPO uses an EM-inspired optimization process that decouples feedback modeling and prompt refinement, plus a short-long term memory mechanism where historical feedback mitigates context limitations and historical prompts provide directional guidance.

Result: UniAPO achieves consistent performance gains across text, image, and video benchmarks, demonstrating effectiveness as a unified framework for efficient and transferable prompt optimization.

Conclusion: The framework successfully addresses multimodal prompt optimization challenges and establishes a unified approach that works across different modalities including text, images, and videos.

Abstract: Prompting is fundamental to unlocking the full potential of large language
models. To automate and enhance this process, automatic prompt optimization
(APO) has been developed, demonstrating effectiveness primarily in text-only
input scenarios. However, extending existing APO methods to multimodal tasks,
such as video-language generation introduces two core challenges: (i) visual
token inflation, where long visual token sequences restrict context capacity
and result in insufficient feedback signals; (ii) a lack of process-level
supervision, as existing methods focus on outcome-level supervision and
overlook intermediate supervision, limiting prompt optimization. We present
UniAPO: Unified Multimodal Automated Prompt Optimization, the first framework
tailored for multimodal APO. UniAPO adopts an EM-inspired optimization process
that decouples feedback modeling and prompt refinement, making the optimization
more stable and goal-driven. To further address the aforementioned challenges,
we introduce a short-long term memory mechanism: historical feedback mitigates
context limitations, while historical prompts provide directional guidance for
effective prompt optimization. UniAPO achieves consistent gains across text,
image, and video benchmarks, establishing a unified framework for efficient and
transferable prompt optimization.

</details>


### [275] [EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images](https://arxiv.org/abs/2508.17916)
*Xinning Yao,Bo Liu,Bojian Li,Jingjing Wang,Jinghua Yue,Fugen Zhou*

Main category: cs.CV

TL;DR: EndoUFM is an unsupervised monocular depth estimation framework for endoscopic surgeries that integrates dual foundation models with adaptive fine-tuning and novel architectural components to overcome domain adaptation challenges in surgical environments.


<details>
  <summary>Details</summary>
Motivation: Existing monocular depth estimation techniques perform poorly in surgical environments due to varying illumination and complex textures. Visual foundation models trained on natural images have domain adaptability limitations when applied to endoscopy.

Method: Integrates dual foundation models with Random Vector Low-Rank Adaptation (RVLoRA) for adaptive fine-tuning, uses Residual blocks based on Depthwise Separable Convolution (Res-DSC) for local feature capture, and employs mask-guided smoothness loss for depth consistency within tissue structures.

Result: Achieves state-of-the-art performance on SCARED, Hamlyn, SERV-CT, and EndoNeRF datasets while maintaining efficient model size.

Conclusion: The framework enhances surgeons' spatial perception during minimally invasive procedures, improving surgical precision and safety with implications for augmented reality and navigation systems.

Abstract: Depth estimation is a foundational component for 3D reconstruction in
minimally invasive endoscopic surgeries. However, existing monocular depth
estimation techniques often exhibit limited performance to the varying
illumination and complex textures of the surgical environment. While powerful
visual foundation models offer a promising solution, their training on natural
images leads to significant domain adaptability limitations and semantic
perception deficiencies when applied to endoscopy. In this study, we introduce
EndoUFM, an unsupervised monocular depth estimation framework that innovatively
integrating dual foundation models for surgical scenes, which enhance the depth
estimation performance by leveraging the powerful pre-learned priors. The
framework features a novel adaptive fine-tuning strategy that incorporates
Random Vector Low-Rank Adaptation (RVLoRA) to enhance model adaptability, and a
Residual block based on Depthwise Separable Convolution (Res-DSC) to improve
the capture of fine-grained local features. Furthermore, we design a
mask-guided smoothness loss to enforce depth consistency within anatomical
tissue structures. Extensive experiments on the SCARED, Hamlyn, SERV-CT, and
EndoNeRF datasets confirm that our method achieves state-of-the-art performance
while maintaining an efficient model size. This work contributes to augmenting
surgeons' spatial perception during minimally invasive procedures, thereby
enhancing surgical precision and safety, with crucial implications for
augmented reality and navigation systems.

</details>


### [276] [Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health Biomarkers Estimation](https://arxiv.org/abs/2508.17924)
*Konstantin Egorov,Stepan Botman,Pavel Blinov,Galina Zubkova,Anton Ivaschenko,Alexander Kolsanov,Andrey Savchenko*

Main category: cs.CV

TL;DR: A large-scale multi-view video dataset for remote photoplethysmography (rPPG) with 3600 recordings from 600 subjects, captured under varied conditions with multiple cameras and paired with comprehensive physiological data.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing rPPG datasets: small size, privacy concerns with facial videos, and lack of diversity in conditions.

Method: Created a comprehensive dataset with 3600 synchronized video recordings from 600 subjects using multiple consumer-grade cameras at different angles, paired with 100 Hz PPG signals and extended health metrics including ECG, blood pressure, biomarkers, temperature, oxygen saturation, respiratory rate, and stress levels.

Result: Trained an efficient rPPG model and compared its quality with existing approaches in cross-dataset scenarios.

Conclusion: The public release of this dataset and model should significantly accelerate progress in developing AI medical assistants.

Abstract: Progress in remote PhotoPlethysmoGraphy (rPPG) is limited by the critical
issues of existing publicly available datasets: small size, privacy concerns
with facial videos, and lack of diversity in conditions. The paper introduces a
novel comprehensive large-scale multi-view video dataset for rPPG and health
biomarkers estimation. Our dataset comprises 3600 synchronized video recordings
from 600 subjects, captured under varied conditions (resting and post-exercise)
using multiple consumer-grade cameras at different angles. To enable multimodal
analysis of physiological states, each recording is paired with a 100 Hz PPG
signal and extended health metrics, such as electrocardiogram, arterial blood
pressure, biomarkers, temperature, oxygen saturation, respiratory rate, and
stress level. Using this data, we train an efficient rPPG model and compare its
quality with existing approaches in cross-dataset scenarios. The public release
of our dataset and model should significantly speed up the progress in the
development of AI medical assistants.

</details>


### [277] [See What You Need: Query-Aware Visual Intelligence through Reasoning-Perception Loops](https://arxiv.org/abs/2508.17932)
*Zixuan Dong,Baoyun Peng,Yufei Wang,Lin Liu,Xinxin Dong,Yunlong Cao,Xiaodong Wang*

Main category: cs.CV

TL;DR: CAVIA is a training-free framework that coordinates reasoning and visual attention for long-form video question answering, achieving state-of-the-art performance on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current video QA systems decouple reasoning from perception, leading to information loss or computational inefficiency. Different queries require different visual evidence from the same video content, but existing approaches lack adaptive visual extraction.

Method: CAVIA creates a closed-loop system with three innovations: hierarchical reasoning for precise frame localization, cross-modal semantic bridging for targeted extraction, and confidence-driven iterative synthesis where reasoning continuously guides visual extraction based on information gaps.

Result: Achieves SOTA performance on EgoSchema (65.7%, +5.3%), NExT-QA (76.1%, +2.6%), and IntentQA (73.8%, +6.9%) benchmarks.

Conclusion: Dynamic reasoning-perception coordination provides a scalable paradigm for video understanding, demonstrating that adaptive visual extraction guided by reasoning requirements significantly improves performance.

Abstract: Human video comprehension demonstrates dynamic coordination between reasoning
and visual attention, adaptively focusing on query-relevant details. However,
current long-form video question answering systems employ rigid pipelines that
decouple reasoning from perception, leading to either information loss through
premature visual abstraction or computational inefficiency through exhaustive
processing. The core limitation lies in the inability to adapt visual
extraction to specific reasoning requirements, different queries demand
fundamentally different visual evidence from the same video content. In this
work, we present CAVIA, a training-free framework that revolutionizes video
understanding through reasoning, perception coordination. Unlike conventional
approaches where visual processing operates independently of reasoning, CAVIA
creates a closed-loop system where reasoning continuously guides visual
extraction based on identified information gaps. CAVIA introduces three
innovations: (1) hierarchical reasoning, guided localization to precise frames;
(2) cross-modal semantic bridging for targeted extraction; (3)
confidence-driven iterative synthesis. CAVIA achieves state-of-the-art
performance on challenging benchmarks: EgoSchema (65.7%, +5.3%), NExT-QA
(76.1%, +2.6%), and IntentQA (73.8%, +6.9%), demonstrating that dynamic
reasoning-perception coordination provides a scalable paradigm for video
understanding.

</details>


### [278] [Beam Geometry and Input Dimensionality: Impact on Sparse-Sampling Artifact Correction for Clinical CT with U-Nets](https://arxiv.org/abs/2508.17961)
*Tina Dorosti,Johannes Thalhammer,Sebastian Peterhansl,Daniela Pfeiffer,Franz Pfeiffer,Florian Schaff*

Main category: cs.CV

TL;DR: Study compares 2D, 2.5D, and 3D U-Net approaches for CT streak artifact correction, finding 2D U-Net with axial slices performs best across all beam geometries.


<details>
  <summary>Details</summary>
Motivation: To investigate how different beam geometries and data dimensions affect sparse-sampling streak artifact correction in clinical CT scans, aiming to incorporate volumetric context to improve model performance.

Method: Used 22 clinical CT scans to simulate sparse sampling with parallel, fan, and cone beam geometries. Trained 2D and 3D U-Nets on different data formats: 2D axial slices, 2.5D (three orthogonal slices combined), and 3D voxel blocks. Evaluated performance using MSE and SSIM metrics.

Result: 2D U-Net trained on axial 2D slices achieved the best performance in both MSE and SSIM across all beam geometries, outperforming both 2.5D and 3D input data approaches.

Conclusion: Despite attempts to incorporate volumetric context through 2.5D and 3D data, traditional 2D U-Net with axial slices remains the most effective approach for CT streak artifact correction tasks.

Abstract: This study aims to investigate the effect of various beam geometries and
dimensions of input data on the sparse-sampling streak artifact correction task
with U-Nets for clinical CT scans as a means of incorporating the volumetric
context into artifact reduction tasks to improve model performance. A total of
22 subjects were retrospectively selected (01.2016-12.2018) from the Technical
University of Munich's research hospital, TUM Klinikum rechts der Isar.
Sparsely-sampled CT volumes were simulated with the Astra toolbox for parallel,
fan, and cone beam geometries. 2048 views were taken as full-view scans. 2D and
3D U-Nets were trained and validated on 14, and tested on 8 subjects,
respectively. For the dimensionality study, in addition to the 512x512 2D CT
images, the CT scans were further pre-processed to generate a so-called '2.5D',
and 3D data: Each CT volume was divided into 64x64x64 voxel blocks. The 3D data
refers to individual 64-voxel blocks. An axial, coronal, and sagittal cut
through the center of each block resulted in three 64x64 2D patches that were
rearranged as a single 64x64x3 image, proposed as 2.5D data. Model performance
was assessed with the mean squared error (MSE) and structural similarity index
measure (SSIM). For all geometries, the 2D U-Net trained on axial 2D slices
results in the best MSE and SSIM values, outperforming the 2.5D and 3D input
data dimensions.

</details>


### [279] [SAIL-Recon: Large SfM by Augmenting Scene Regression with Localization](https://arxiv.org/abs/2508.17972)
*Junyuan Deng,Heng Li,Tao Xie,Weiqiang Ren,Qian Zhang,Ping Tan,Xiaoyang Guo*

Main category: cs.CV

TL;DR: SAIL-Recon is a feed-forward Transformer that addresses the scalability limitations of scene regression methods for large-scale Structure-from-Motion by combining neural scene representation with visual localization capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing scene regression methods like VGGT struggle to handle large numbers of input images despite their impressive performance with extreme viewpoint changes.

Method: The method first computes a neural scene representation from anchor images, then fine-tunes a regression network to reconstruct all input images conditioned on this representation.

Result: Achieves state-of-the-art results on camera pose estimation and novel view synthesis benchmarks including TUM-RGBD, CO3Dv2, and Tanks & Temples, while scaling efficiently to large-scale scenes.

Conclusion: SAIL-Recon successfully overcomes the scalability limitations of previous scene regression methods and demonstrates superior performance across multiple benchmarks, with code and models made publicly available.

Abstract: Scene regression methods, such as VGGT, solve the Structure-from-Motion (SfM)
problem by directly regressing camera poses and 3D scene structures from input
images. They demonstrate impressive performance in handling images under
extreme viewpoint changes. However, these methods struggle to handle a large
number of input images. To address this problem, we introduce SAIL-Recon, a
feed-forward Transformer for large scale SfM, by augmenting the scene
regression network with visual localization capabilities. Specifically, our
method first computes a neural scene representation from a subset of anchor
images. The regression network is then fine-tuned to reconstruct all input
images conditioned on this neural scene representation. Comprehensive
experiments show that our method not only scales efficiently to large-scale
scenes, but also achieves state-of-the-art results on both camera pose
estimation and novel view synthesis benchmarks, including TUM-RGBD, CO3Dv2, and
Tanks & Temples. We will publish our model and code. Code and models are
publicly available at: https://hkust-sail.github.io/ sail-recon/.

</details>


### [280] [Enhanced Drift-Aware Computer Vision Architecture for Autonomous Driving](https://arxiv.org/abs/2508.17975)
*Md Shahi Amran Hossain,Abu Shad Ahammed,Sayeri Mukherjee,Roman Obermaisser*

Main category: cs.CV

TL;DR: Hybrid computer vision architecture combining YOLOv8 and 5-layer CNN improves object detection accuracy by over 90% in drifted road environments using synthetic training data.


<details>
  <summary>Details</summary>
Motivation: Address safety concerns in autonomous driving by improving object detection robustness against data drift from adverse weather and low lighting conditions, aligning with ISO 8800 standards.

Method: Dual-mode framework using YOLOv8 for fast detection and a five-layer CNN for verification, trained with thousands of synthetic road environment images to handle drifted scenarios.

Result: Achieved over 90% improvement in detection accuracy when tested with drift-augmented road images compared to baseline models.

Conclusion: Hybrid computer vision architecture demonstrates significantly enhanced road safety performance in challenging drifted environments, providing a robust solution for autonomous driving applications.

Abstract: The use of computer vision in automotive is a trending research in which
safety and security are a primary concern. In particular, for autonomous
driving, preventing road accidents requires highly accurate object detection
under diverse conditions. To address this issue, recently the International
Organization for Standardization (ISO) released the 8800 norm, providing
structured frameworks for managing associated AI relevant risks. However,
challenging scenarios such as adverse weather or low lighting often introduce
data drift, leading to degraded model performance and potential safety
violations. In this work, we present a novel hybrid computer vision
architecture trained with thousands of synthetic image data from the road
environment to improve robustness in unseen drifted environments. Our dual mode
framework utilized YOLO version 8 for swift detection and incorporated a
five-layer CNN for verification. The system functioned in sequence and improved
the detection accuracy by more than 90\% when tested with drift-augmented road
images. The focus was to demonstrate how such a hybrid model can provide better
road safety when working together in a hybrid structure.

</details>


### [281] [Propose and Rectify: A Forensics-Driven MLLM Framework for Image Manipulation Localization](https://arxiv.org/abs/2508.17976)
*Keyang Zhang,Chenqi Kong,Hui Liu,Bo Ding,Xinghao Jiang,Haoliang Li*

Main category: cs.CV

TL;DR: A Propose-Rectify framework that combines MLLMs' semantic reasoning with forensic analysis for precise image manipulation detection and localization, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with perceiving subtle forensic artifacts needed for accurate manipulation localization despite their semantic understanding capabilities.

Method: Two-stage framework: 1) Forensic-adapted LLaVA generates initial proposals using semantic reasoning; 2) Forensics Rectification Module validates proposals through multi-scale forensic feature analysis and Enhanced Segmentation Module incorporates forensic cues into SAM for precise delineation.

Result: State-of-the-art performance across diverse datasets with exceptional robustness and generalization capabilities.

Conclusion: The framework effectively bridges semantic reasoning with forensic-specific analysis, ensuring comprehensive detection accuracy and precise localization of manipulated regions.

Abstract: The increasing sophistication of image manipulation techniques demands robust
forensic solutions that can both reliably detect alterations and precisely
localize tampered regions. Recent Multimodal Large Language Models (MLLMs) show
promise by leveraging world knowledge and semantic understanding for
context-aware detection, yet they struggle with perceiving subtle, low-level
forensic artifacts crucial for accurate manipulation localization. This paper
presents a novel Propose-Rectify framework that effectively bridges semantic
reasoning with forensic-specific analysis. In the proposal stage, our approach
utilizes a forensic-adapted LLaVA model to generate initial manipulation
analysis and preliminary localization of suspicious regions based on semantic
understanding and contextual reasoning. In the rectification stage, we
introduce a Forensics Rectification Module that systematically validates and
refines these initial proposals through multi-scale forensic feature analysis,
integrating technical evidence from several specialized filters. Additionally,
we present an Enhanced Segmentation Module that incorporates critical forensic
cues into SAM's encoded image embeddings, thereby overcoming inherent semantic
biases to achieve precise delineation of manipulated regions. By
synergistically combining advanced multimodal reasoning with established
forensic methodologies, our framework ensures that initial semantic proposals
are systematically validated and enhanced through concrete technical evidence,
resulting in comprehensive detection accuracy and localization precision.
Extensive experimental validation demonstrates state-of-the-art performance
across diverse datasets with exceptional robustness and generalization
capabilities.

</details>


### [282] [Fence off Anomaly Interference: Cross-Domain Distillation for Fully Unsupervised Anomaly Detection](https://arxiv.org/abs/2508.18007)
*Xinyue Liu,Jianyuan Wang,Biao Leng,Shuo Zhang*

Main category: cs.CV

TL;DR: Proposes Cross-Domain Distillation framework for Fully Unsupervised Anomaly Detection where training data may contain anomalies, using domain-specific training and cross-domain knowledge aggregation to prevent learning anomalous representations.


<details>
  <summary>Details</summary>
Motivation: Fully Unsupervised Anomaly Detection (FUAD) aims to detect anomalies without any labels, even when training data contains anomalous samples. Traditional knowledge distillation methods risk learning anomalous representations in this setting.

Method: Cross-Domain Distillation framework with Domain-Specific Training (dividing training set into low-anomaly domains) and Cross-Domain Knowledge Aggregation (using pseudo-normal features from domain-specific students to train a global student).

Result: Significant performance improvements over baseline methods on noisy versions of MVTec AD and VisA datasets.

Conclusion: The proposed CDD framework effectively addresses the challenge of FUAD by preventing learning of anomalous representations and enabling generalized normal feature learning across domains.

Abstract: Fully Unsupervised Anomaly Detection (FUAD) is a practical extension of
Unsupervised Anomaly Detection (UAD), aiming to detect anomalies without any
labels even when the training set may contain anomalous samples. To achieve
FUAD, we pioneer the introduction of Knowledge Distillation (KD) paradigm based
on teacher-student framework into the FUAD setting. However, due to the
presence of anomalies in the training data, traditional KD methods risk
enabling the student to learn the teacher's representation of anomalies under
FUAD setting, thereby resulting in poor anomaly detection performance. To
address this issue, we propose a novel Cross-Domain Distillation (CDD)
framework based on the widely studied reverse distillation (RD) paradigm.
Specifically, we design a Domain-Specific Training, which divides the training
set into multiple domains with lower anomaly ratios and train a domain-specific
student for each. Cross-Domain Knowledge Aggregation is then performed, where
pseudo-normal features generated by domain-specific students collaboratively
guide a global student to learn generalized normal representations across all
samples. Experimental results on noisy versions of the MVTec AD and VisA
datasets demonstrate that our method achieves significant performance
improvements over the baseline, validating its effectiveness under FUAD
setting.

</details>


### [283] [Development of a Neural Network Model for Currency Detection to aid visually impaired people in Nigeria](https://arxiv.org/abs/2508.18012)
*Sochukwuma Nwokoye,Desmond Moru*

Main category: cs.CV

TL;DR: Neural network system for identifying Nigerian currency to assist visually impaired individuals with cash transactions, achieving over 90% accuracy.


<details>
  <summary>Details</summary>
Motivation: To help visually impaired individuals differentiate various forms of cash and streamline commercial transactions using artificial intelligence.

Method: Built a custom dataset of 3,468 images and trained an SSD (Single Shot MultiBox Detector) neural network model for cash recognition.

Result: The system achieved a Mean Average Precision score of over 90% in accurately identifying Nigerian currency.

Conclusion: The proposed system has significant potential to contribute to assistive technology and improve quality of life for visually impaired people in Nigeria and beyond.

Abstract: Neural networks in assistive technology for visually impaired leverage
artificial intelligence's capacity to recognize patterns in complex data. They
are used for converting visual data into auditory or tactile representations,
helping the visually impaired understand their surroundings. The primary aim of
this research is to explore the potential of artificial neural networks to
facilitate the differentiation of various forms of cash for individuals with
visual impairments. In this study, we built a custom dataset of 3,468 images,
which was subsequently used to train an SSD neural network model. The proposed
system can accurately identify Nigerian cash, thereby streamlining commercial
transactions. The performance of the system in terms of accuracy was assessed,
and the Mean Average Precision score was over 90%. We believe that our system
has the potential to make a substantial contribution to the field of assistive
technology while also improving the quality of life of visually challenged
persons in Nigeria and beyond.

</details>


### [284] [Towards Continual Visual Anomaly Detection in the Medical Domain](https://arxiv.org/abs/2508.18013)
*Manuel Barusco,Francesco Borsatti,Nicola Beda,Davide Dalle Pezze,Gian Antonio Susto*

Main category: cs.CV

TL;DR: First study applying continual learning to visual anomaly detection in medical imaging using PatchCoreCL, achieving comparable performance to task-specific models with minimal forgetting.


<details>
  <summary>Details</summary>
Motivation: Visual anomaly detection is critical in medical imaging but faces performance degradation from evolving data distributions over time. Continual learning provides a framework to adapt models incrementally while preserving knowledge.

Method: Used PatchCoreCL (continual learning version of PatchCore model) evaluated on BMAD dataset with image-level and pixel-level annotations for medical anomaly detection.

Result: PatchCoreCL achieved performance comparable to task-specific models with forgetting value less than 1%, demonstrating effectiveness for adaptive VAD.

Conclusion: CL is feasible and promising for adaptive visual anomaly detection in medical imaging, with PatchCoreCL showing strong performance and minimal knowledge forgetting.

Abstract: Visual Anomaly Detection (VAD) seeks to identify abnormal images and
precisely localize the corresponding anomalous regions, relying solely on
normal data during training. This approach has proven essential in domains such
as manufacturing and, more recently, in the medical field, where accurate and
explainable detection is critical. Despite its importance, the impact of
evolving input data distributions over time has received limited attention,
even though such changes can significantly degrade model performance. In
particular, given the dynamic and evolving nature of medical imaging data,
Continual Learning (CL) provides a natural and effective framework to
incrementally adapt models while preserving previously acquired knowledge. This
study explores for the first time the application of VAD models in a CL
scenario for the medical field. In this work, we utilize a CL version of the
well-established PatchCore model, called PatchCoreCL, and evaluate its
performance using BMAD, a real-world medical imaging dataset with both
image-level and pixel-level annotations. Our results demonstrate that
PatchCoreCL is an effective solution, achieving performance comparable to the
task-specific models, with a forgetting value less than a 1%, highlighting the
feasibility and potential of CL for adaptive VAD in medical imaging.

</details>


### [285] [FCR: Investigating Generative AI models for Forensic Craniofacial Reconstruction](https://arxiv.org/abs/2508.18031)
*Ravi Shankar Prasad,Dinesh Singh*

Main category: cs.CV

TL;DR: Proposes a generative framework using CycleGANs and cGANs for craniofacial reconstruction from 2D X-ray images, addressing limitations of traditional methods and achieving realistic face generation for forensic identification.


<details>
  <summary>Details</summary>
Motivation: Traditional craniofacial reconstruction methods require expert knowledge and are time-consuming, while existing probabilistic models fail to capture cross-domain skull-face attributes. The need for efficient forensic identification drives this research.

Method: Uses generative models (CycleGANs, cGANs) fine-tuned to generate realistic images across skull and face domains from 2D X-ray inputs. First approach using 2D X-rays as skull representation with generative models.

Result: Evaluated generated faces using FID, IS, and SSIM scores. Proposed retrieval framework where generated faces query real face databases. Experimental results show effectiveness for forensic applications.

Conclusion: The framework provides an effective tool for forensic science, demonstrating successful craniofacial reconstruction from 2D X-ray images using generative models, offering a practical alternative to traditional methods.

Abstract: Craniofacial reconstruction in forensics is one of the processes to identify
victims of crime and natural disasters. Identifying an individual from their
remains plays a crucial role when all other identification methods fail.
Traditional methods for this task, such as clay-based craniofacial
reconstruction, require expert domain knowledge and are a time-consuming
process. At the same time, other probabilistic generative models like the
statistical shape model or the Basel face model fail to capture the skull and
face cross-domain attributes. Looking at these limitations, we propose a
generic framework for craniofacial reconstruction from 2D X-ray images. Here,
we used various generative models (i.e., CycleGANs, cGANs, etc) and fine-tune
the generator and discriminator parts to generate more realistic images in two
distinct domains, which are the skull and face of an individual. This is the
first time where 2D X-rays are being used as a representation of the skull by
generative models for craniofacial reconstruction. We have evaluated the
quality of generated faces using FID, IS, and SSIM scores. Finally, we have
proposed a retrieval framework where the query is the generated face image and
the gallery is the database of real faces. By experimental results, we have
found that this can be an effective tool for forensic science.

</details>


### [286] [Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation](https://arxiv.org/abs/2508.18032)
*Yaqi Li,Peng Chen,Mingyang Han,Bu Pi,Haoxiang Shi,Runzhou Zhao,Yang Yao,Xuan Zhang,Jun Song*

Main category: cs.CV

TL;DR: Visual-CoG introduces stage-aware rewards throughout image generation pipeline to improve multi-attribute and ambiguous prompt handling, achieving significant performance gains on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing autoregressive text-to-image models struggle with multi-attribute and ambiguous prompts, and current reinforcement learning approaches only provide final-stage rewards, making it difficult to identify which generation stages contribute positively.

Method: Proposes Visual-Chain of Guidance (Visual-CoG) paradigm with three stages: semantic reasoning, process refining, and outcome evaluation, using stage-aware rewards to provide immediate guidance throughout the image generation pipeline.

Result: Achieves improvements of 15% on GenEval, 5% on T2I-CompBench, and 19% on the proposed VisCog-Bench benchmark across four subtasks evaluating semantic reasoning effectiveness.

Conclusion: Visual-CoG demonstrates superior performance by providing stage-aware rewards throughout the generation process, effectively addressing limitations of final-only reward guidance in text-to-image generation.

Abstract: Despite the promising progress of recent autoregressive models in
text-to-image (T2I) generation, their ability to handle multi-attribute and
ambiguous prompts remains limited. To address these limitations, existing works
have applied chain-of-thought (CoT) to enable stage-aware visual synthesis and
employed reinforcement learning (RL) to improve reasoning capabilities.
However, most models provide reward signals only at the end of the generation
stage. This monolithic final-only guidance makes it difficult to identify which
stages contribute positively to the final outcome and may lead to suboptimal
policies. To tackle this issue, we propose a Visual-Chain of Guidance
(Visual-CoG) paradigm consisting of three stages: semantic reasoning, process
refining, and outcome evaluation, with stage-aware rewards providing immediate
guidance throughout the image generation pipeline. We further construct a
visual cognition benchmark, VisCog-Bench, which comprises four subtasks to
evaluate the effectiveness of semantic reasoning. Comprehensive evaluations on
GenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%,
5%, and 19%, respectively, demonstrating the superior performance of the
proposed Visual-CoG. We will release all the resources soon.

</details>


### [287] [ArgusCogito: Chain-of-Thought for Cross-Modal Synergy and Omnidirectional Reasoning in Camouflaged Object Segmentation](https://arxiv.org/abs/2508.18050)
*Jianwen Tan,Huiyao Zhang,Rui Xiong,Han Zhou,Hongfei Wang,Ye Li*

Main category: cs.CV

TL;DR: ArgusCogito is a novel zero-shot chain-of-thought framework for camouflaged object segmentation that uses cross-modal reasoning and omnidirectional attention inspired by the Hundred-eyed Giant, achieving SOTA performance on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current camouflaged object segmentation methods suffer from shallow feature representation, inadequate reasoning, and weak cross-modal integration, leading to incomplete target separation and imprecise segmentation.

Method: Three-stage framework: 1) Conjecture - global reasoning with cross-modal fusion (RGB, depth, semantic maps), 2) Focus - omnidirectional attention-driven scanning, 3) Sculpting - iterative mask generation with dense point prompts.

Result: Achieves state-of-the-art performance on four COS benchmarks and three medical image segmentation benchmarks, demonstrating exceptional efficacy, superior generalization, and robustness.

Conclusion: The cognitively-inspired framework successfully addresses COS challenges through holistic observation, omnidirectional focus, and intensive scrutiny, validating the effectiveness of cross-modal synergy and reasoning in VLMs.

Abstract: Camouflaged Object Segmentation (COS) poses a significant challenge due to
the intrinsic high similarity between targets and backgrounds, demanding models
capable of profound holistic understanding beyond superficial cues. Prevailing
methods, often limited by shallow feature representation, inadequate reasoning
mechanisms, and weak cross-modal integration, struggle to achieve this depth of
cognition, resulting in prevalent issues like incomplete target separation and
imprecise segmentation. Inspired by the perceptual strategy of the Hundred-eyed
Giant-emphasizing holistic observation, omnidirectional focus, and intensive
scrutiny-we introduce ArgusCogito, a novel zero-shot, chain-of-thought
framework underpinned by cross-modal synergy and omnidirectional reasoning
within Vision-Language Models (VLMs). ArgusCogito orchestrates three
cognitively-inspired stages: (1) Conjecture: Constructs a strong cognitive
prior through global reasoning with cross-modal fusion (RGB, depth, semantic
maps), enabling holistic scene understanding and enhanced target-background
disambiguation. (2) Focus: Performs omnidirectional, attention-driven scanning
and focused reasoning, guided by semantic priors from Conjecture, enabling
precise target localization and region-of-interest refinement. (3) Sculpting:
Progressively sculpts high-fidelity segmentation masks by integrating
cross-modal information and iteratively generating dense positive/negative
point prompts within focused regions, emulating Argus' intensive scrutiny.
Extensive evaluations on four challenging COS benchmarks and three Medical
Image Segmentation (MIS) benchmarks demonstrate that ArgusCogito achieves
state-of-the-art (SOTA) performance, validating the framework's exceptional
efficacy, superior generalization capability, and robustness.

</details>


### [288] [Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Images](https://arxiv.org/abs/2508.18067)
*Kaiyu Li,Xiangyong Cao,Ruixun Liu,Shihong Wang,Zixuan Jiang,Zhi Wang,Deyu Meng*

Main category: cs.CV

TL;DR: SegEarth-OV is the first annotation-free open-vocabulary semantic segmentation framework for remote sensing images, featuring SimFeatUp for high-resolution detail restoration and Global Bias Alleviation for enhanced local semantics, with AlignEarth enabling cross-modal knowledge transfer to SAR images.


<details>
  <summary>Details</summary>
Motivation: Existing open-vocabulary segmentation frameworks for natural images are inadequate for remote sensing data due to scale variations, fine-grained details, and high annotation costs, creating a critical gap for Earth observation applications.

Method: Proposes SegEarth-OV with SimFeatUp universal upsampler for high-resolution spatial detail restoration, Global Bias Alleviation operation for local semantic enhancement, and AlignEarth for cross-modal knowledge transfer from optical to SAR encoders without building foundation models from scratch.

Result: Extensive experiments on both optical and SAR datasets show dramatic improvements over state-of-the-art methods, validating the framework's effectiveness for annotation-free open-world Earth observation.

Conclusion: SegEarth-OV establishes a robust foundation for annotation-free and open-vocabulary semantic segmentation across diverse remote sensing modalities, overcoming limitations of existing natural image frameworks and enabling comprehensive Earth observation without costly manual annotations.

Abstract: Semantic segmentation of remote sensing (RS) images is pivotal for
comprehensive Earth observation, but the demand for interpreting new object
categories, coupled with the high expense of manual annotation, poses
significant challenges. Although open-vocabulary semantic segmentation (OVSS)
offers a promising solution, existing frameworks designed for natural images
are insufficient for the unique complexities of RS data. They struggle with
vast scale variations and fine-grained details, and their adaptation often
relies on extensive, costly annotations. To address this critical gap, this
paper introduces SegEarth-OV, the first framework for annotation-free
open-vocabulary segmentation of RS images. Specifically, we propose SimFeatUp,
a universal upsampler that robustly restores high-resolution spatial details
from coarse features, correcting distorted target shapes without any
task-specific post-training. We also present a simple yet effective Global Bias
Alleviation operation to subtract the inherent global context from patch
features, significantly enhancing local semantic fidelity. These components
empower SegEarth-OV to effectively harness the rich semantics of pre-trained
VLMs, making OVSS possible in optical RS contexts. Furthermore, to extend the
framework's universality to other challenging RS modalities like SAR images,
where large-scale VLMs are unavailable and expensive to create, we introduce
AlignEarth, which is a distillation-based strategy and can efficiently transfer
semantic knowledge from an optical VLM encoder to an SAR encoder, bypassing the
need to build SAR foundation models from scratch and enabling universal OVSS
across diverse sensor types. Extensive experiments on both optical and SAR
datasets validate that SegEarth-OV can achieve dramatic improvements over the
SOTA methods, establishing a robust foundation for annotation-free and
open-world Earth observation.

</details>


### [289] [EventTracer: Fast Path Tracing-based Event Stream Rendering](https://arxiv.org/abs/2508.18071)
*Zhenyang Li,Xiaoyang Bai,Jinfan Lu,Pengfei Shen,Edmund Y. Lam,Yifan Peng*

Main category: cs.CV

TL;DR: EventTracer is a path tracing-based rendering pipeline that efficiently simulates high-fidelity event sequences from 3D scenes using low SPP path tracing and a lightweight event spiking network with BiLIF units and bidirectional EMD loss.


<details>
  <summary>Details</summary>
Motivation: Existing event stream simulation methods work with expensive noiseless RGB frames and achieve only 100-300 FPS temporal resolution, far lower than real-world event data, creating a need for more efficient and physics-aware simulation.

Method: Uses low sample-per-pixel path tracing for efficient rendering, trains a lightweight event spiking network with bipolar leaky integrate-and-fired units and bidirectional earth mover distance loss to denoise RGB videos into realistic event sequences.

Result: EventTracer runs at about 4 minutes per second of 720p video, captures better scene details, shows greater similarity to real-world event data than other simulators, and provides accurate spatiotemporal modeling.

Conclusion: EventTracer is a promising tool for creating large-scale event-RGB datasets at low cost, narrowing the sim-to-real gap in event-based vision, and boosting applications in robotics, autonomous driving, and VR/AR.

Abstract: Simulating event streams from 3D scenes has become a common practice in
event-based vision research, as it meets the demand for large-scale, high
temporal frequency data without setting up expensive hardware devices or
undertaking extensive data collections. Yet existing methods in this direction
typically work with noiseless RGB frames that are costly to render, and
therefore they can only achieve a temporal resolution equivalent to 100-300
FPS, far lower than that of real-world event data. In this work, we propose
EventTracer, a path tracing-based rendering pipeline that simulates
high-fidelity event sequences from complex 3D scenes in an efficient and
physics-aware manner. Specifically, we speed up the rendering process via low
sample-per-pixel (SPP) path tracing, and train a lightweight event spiking
network to denoise the resulting RGB videos into realistic event sequences. To
capture the physical properties of event streams, the network is equipped with
a bipolar leaky integrate-and-fired (BiLIF) spiking unit and trained with a
bidirectional earth mover distance (EMD) loss. Our EventTracer pipeline runs at
a speed of about 4 minutes per second of 720p video, and it inherits the merit
of accurate spatiotemporal modeling from its path tracing backbone. We show in
two downstream tasks that EventTracer captures better scene details and
demonstrates a greater similarity to real-world event data than other event
simulators, which establishes it as a promising tool for creating large-scale
event-RGB datasets at a low cost, narrowing the sim-to-real gap in event-based
vision, and boosting various application scenarios such as robotics, autonomous
driving, and VRAR.

</details>


### [290] [Few-shot Unknown Class Discovery of Hyperspectral Images with Prototype Learning and Clustering](https://arxiv.org/abs/2508.18075)
*Chun Liu,Chen Zhang,Zhuo Li,Zheng Li,Wei Yang*

Main category: cs.CV

TL;DR: A prototype learning and clustering method for open-set few-shot HSI classification that not only rejects unknown samples but also discovers and clusters them into new classes using inferred unknown class prototypes.


<details>
  <summary>Details</summary>
Motivation: Current open-set HSI classification methods only reject unknown class samples without identifying or discovering the unknown classes among them, limiting their practical utility.

Method: Proposes a prototype learning approach that uses few labeled samples to infer unknown class prototypes while distinguishing known classes. After rejection by the known class classifier, unknown samples are clustered into different classes based on distance to the inferred unknown prototypes.

Result: Extensive experiments on four benchmark HSI datasets demonstrate competitive performance compared to state-of-the-art methods in open-set few-shot HSI classification tasks.

Conclusion: The proposed method effectively addresses the limitation of existing approaches by not just rejecting but also discovering and clustering unknown classes, showing strong performance in open-set few-shot HSI classification.

Abstract: Open-set few-shot hyperspectral image (HSI) classification aims to classify
image pixels by using few labeled pixels per class, where the pixels to be
classified may be not all from the classes that have been seen. To address the
open-set HSI classification challenge, current methods focus mainly on
distinguishing the unknown class samples from the known class samples and
rejecting them to increase the accuracy of identifying known class samples.
They fails to further identify or discovery the unknow classes among the
samples. This paper proposes a prototype learning and clustering method for
discoverying unknown classes in HSIs under the few-shot environment. Using few
labeled samples, it strives to develop the ability of infering the prototypes
of unknown classes while distinguishing unknown classes from known classes.
Once the unknown class samples are rejected by the learned known class
classifier, the proposed method can further cluster the unknown class samples
into different classes according to their distance to the inferred unknown
class prototypes. Compared to existing state-of-the-art methods, extensive
experiments on four benchmark HSI datasets demonstrate that our proposed method
exhibits competitive performance in open-set few-shot HSI classification tasks.
All the codes are available at \href{https://github.com/KOBEN-ff/OpenFUCD-main}
{https://github.com/KOBEN-ff/OpenFUCD-main}

</details>


### [291] [Incorporating Pre-trained Diffusion Models in Solving the SchrÃ¶dinger Bridge Problem](https://arxiv.org/abs/2508.18095)
*Zhicong Tang,Tiankai Hang,Shuyang Gu,Dong Chen,Baining Guo*

Main category: cs.CV

TL;DR: This paper unifies Score-based Generative Models (SGMs/Diffusion models) and SchrÃ¶dinger Bridge problems through three reparameterization techniques that accelerate and stabilize training, using pre-trained SGMs for effective initialization of SB-based models.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between Score-based Generative Models and SchrÃ¶dinger Bridge problems, addressing training efficiency and stability issues in SB-based models while leveraging the strengths of both approaches.

Method: Proposes three reparameterization techniques: IPMM, IPTM, and IPFM for accelerated training. Introduces novel initialization strategies using pre-trained SGMs to train SB-based models effectively.

Result: Significant acceleration and stabilization of SB-based model training. Improved performance of both SB-based models and SGMs through the unified approach. Extensive experiments demonstrate effectiveness.

Conclusion: The work successfully unifies SGMs and SB problems, providing efficient training methods and paving the way for future research in generative models by combining the advantages of both approaches.

Abstract: This paper aims to unify Score-based Generative Models (SGMs), also known as
Diffusion models, and the Schr\"odinger Bridge (SB) problem through three
reparameterization techniques: Iterative Proportional Mean-Matching (IPMM),
Iterative Proportional Terminus-Matching (IPTM), and Iterative Proportional
Flow-Matching (IPFM). These techniques significantly accelerate and stabilize
the training of SB-based models. Furthermore, the paper introduces novel
initialization strategies that use pre-trained SGMs to effectively train
SB-based models. By using SGMs as initialization, we leverage the advantages of
both SB-based models and SGMs, ensuring efficient training of SB-based models
and further improving the performance of SGMs. Extensive experiments
demonstrate the significant effectiveness and improvements of the proposed
methods. We believe this work contributes to and paves the way for future
research on generative models.

</details>


### [292] [BirdRecorder's AI on Sky: Safeguarding birds of prey by detection and classification of tiny objects around wind turbines](https://arxiv.org/abs/2508.18136)
*Nico Klar,Nizam Gifary,Felix P. G. Ziegler,Frank Sehnke,Anton Kaifel,Eric Price,Aamir Ahmad*

Main category: cs.CV

TL;DR: BirdRecorder is an AI-based anti-collision system that uses SSD detection, hardware acceleration, and tracking algorithms to protect endangered birds from wind turbine collisions in real-time.


<details>
  <summary>Details</summary>
Motivation: Address the conflict between renewable energy expansion (wind power) and wildlife conservation by preventing bird-turbine collisions, particularly for endangered species like the red kite.

Method: Integrates robotics, telemetry, and AI algorithms with Single Shot Detector (SSD) for detection, specialized hardware acceleration, and tracking algorithms for real-time image processing within 800m range.

Result: Outperforms existing approaches in both accuracy and efficiency, achieving high detection precision while maintaining necessary speed for real-time decision-making.

Conclusion: BirdRecorder successfully bridges renewable energy expansion with wildlife conservation, enabling sustainable coexistence of technology and nature through effective bird protection.

Abstract: The urgent need for renewable energy expansion, particularly wind power, is
hindered by conflicts with wildlife conservation. To address this, we developed
BirdRecorder, an advanced AI-based anti-collision system to protect endangered
birds, especially the red kite (Milvus milvus). Integrating robotics,
telemetry, and high-performance AI algorithms, BirdRecorder aims to detect,
track, and classify avian species within a range of 800 m to minimize
bird-turbine collisions.
  BirdRecorder integrates advanced AI methods with optimized hardware and
software architectures to enable real-time image processing. Leveraging Single
Shot Detector (SSD) for detection, combined with specialized hardware
acceleration and tracking algorithms, our system achieves high detection
precision while maintaining the speed necessary for real-time decision-making.
By combining these components, BirdRecorder outperforms existing approaches in
both accuracy and efficiency.
  In this paper, we summarize results on field tests and performance of the
BirdRecorder system. By bridging the gap between renewable energy expansion and
wildlife conservation, BirdRecorder contributes to a more sustainable
coexistence of technology and nature.

</details>


### [293] [Assessing the Noise Robustness of Class Activation Maps: A Framework for Reliable Model Interpretability](https://arxiv.org/abs/2508.18154)
*Syamantak Sarkar,Revoti P. Bora,Bhupender Kaushal,Sudhish N George,Kiran Raja*

Main category: cs.CV

TL;DR: Evaluation of CAM methods' robustness to noise perturbations, proposing a new metric for consistency and responsiveness across models and datasets.


<details>
  <summary>Details</summary>
Motivation: Class Activation Maps (CAMs) are important for visualizing deep learning model regions, but their robustness to different noise types remains underexplored.

Method: Evaluated various CAM methods' resilience to noise perturbations across multiple architectures and datasets. Proposed a robustness metric measuring consistency (stability under non-class-altering perturbations) and responsiveness (sensitivity to prediction changes).

Result: Found considerable variability in noise sensitivity for different CAMs. The proposed metric was empirically evaluated across models, perturbations, and datasets with complementary statistical tests.

Conclusion: The study highlights the need for robust CAM evaluation and provides a practical metric to assess explanation stability under noise perturbations.

Abstract: Class Activation Maps (CAMs) are one of the important methods for visualizing
regions used by deep learning models. Yet their robustness to different noise
remains underexplored. In this work, we evaluate and report the resilience of
various CAM methods for different noise perturbations across multiple
architectures and datasets. By analyzing the influence of different noise types
on CAM explanations, we assess the susceptibility to noise and the extent to
which dataset characteristics may impact explanation stability. The findings
highlight considerable variability in noise sensitivity for various CAMs. We
propose a robustness metric for CAMs that captures two key properties:
consistency and responsiveness. Consistency reflects the ability of CAMs to
remain stable under input perturbations that do not alter the predicted class,
while responsiveness measures the sensitivity of CAMs to changes in the
prediction caused by such perturbations. The metric is evaluated empirically
across models, different perturbations, and datasets along with complementary
statistical tests to exemplify the applicability of our proposed approach.

</details>


### [294] [SpotEdit: Evaluating Visually-Guided Image Editing Methods](https://arxiv.org/abs/2508.18159)
*Sara Ghazanfari,Wei-An Lin,Haitong Tian,Ersin Yumer*

Main category: cs.CV

TL;DR: SpotEdit is a comprehensive benchmark for evaluating visually-guided image editing methods across different generative models, revealing performance gaps and addressing the critical issue of hallucination where models incorrectly perceive visual cues.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations for visually-guided image editing are too simple and don't adequately represent real-world editing challenges, leaving performance disparities and critical issues like hallucination underexplored.

Method: Developed SpotEdit benchmark to systematically assess various generative models (diffusion, autoregressive, hybrid) and included a dedicated component to evaluate hallucination where models incorrectly perceive visual cues.

Result: Uncovered substantial performance disparities across different generative models and found that leading models like GPT-4o often hallucinate visual cues and erroneously perform editing tasks.

Conclusion: SpotEdit provides a comprehensive evaluation framework that reveals critical weaknesses in current visually-guided image editing methods, particularly the hallucination problem, and is publicly available for further research.

Abstract: Visually-guided image editing, where edits are conditioned on both visual
cues and textual prompts, has emerged as a powerful paradigm for fine-grained,
controllable content generation. Although recent generative models have shown
remarkable capabilities, existing evaluations remain simple and insufficiently
representative of real-world editing challenges. We present SpotEdit, a
comprehensive benchmark designed to systematically assess visually-guided image
editing methods across diverse diffusion, autoregressive, and hybrid generative
models, uncovering substantial performance disparities. To address a critical
yet underexplored challenge, our benchmark includes a dedicated component on
hallucination, highlighting how leading models, such as GPT-4o, often
hallucinate the existence of a visual cue and erroneously perform the editing
task. Our code and benchmark are publicly released at
https://github.com/SaraGhazanfari/SpotEdit.

</details>


### [295] [Scene-Aware Vectorized Memory Multi-Agent Framework with Cross-Modal Differentiated Quantization VLMs for Visually Impaired Assistance](https://arxiv.org/abs/2508.18177)
*Xiangxiang Wang,Xuanyu Wang,YiJia Luo,Yongbin Yu,Manping Fan,Jingtao Zhang,Liyong Ren*

Main category: cs.CV

TL;DR: Proposes dual innovation framework: cross-modal quantization for VLMs and scene-aware multi-agent system for visually impaired assistance, reducing memory from 38GB to 16GB with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: To develop efficient vision-language models and assistive technology for visually impaired users, addressing high memory requirements and enabling comprehensive real-time assistance in scene perception and navigation.

Method: Developed a modular framework with differentiated processing strategies, including cross-modal differentiated quantization and a multi-agent architecture combining scene classification, vectorized memory, and multimodal interaction with perception-memory-reasoning workflows.

Result: Quantized 19B-parameter model shows only 2.05% performance drop on MMBench, maintains 63.7 accuracy on OCR-VQA, reduces memory from 38GB to 16GB, and achieves response latency of 2.83-3.52 seconds, outperforming smaller models with equivalent memory.

Conclusion: The research advances computational efficiency and assistive technology, providing visually impaired users with comprehensive real-time assistance while significantly reducing memory requirements and maintaining performance.

Abstract: This study proposes the dual technological innovation framework, including a
cross-modal differ entiated quantization framework for vision-language models
(VLMs) and a scene-aware vectorized
  memory multi-agent system for visually impaired assistance. The modular
framework was developed
  implementing differentiated processing strategies, effectively reducing
memory requirements from
  38GB to 16GB while maintaining model performance. The multi-agent
architecture combines
  scene classification, vectorized memory, and multimodal interaction, enabling
persistent storage
  and efficient retrieval of scene memories. Through
perception-memory-reasoning workflows, the
  system provides environmental information beyond the current view using
historical memories.
  Experiments show the quantized 19B-parameter model only experiences a 2.05%
performance drop
  on MMBench and maintains 63.7 accuracy on OCR-VQA (original: 64.9),
outperforming smaller
  models with equivalent memory requirements like the Molmo-7B series. The
system maintains
  response latency between 2.83-3.52 seconds from scene analysis to initial
speech output, substantially
  faster than non-streaming methods. This research advances computational
efficiency and assistive
  technology, offering visually impaired users comprehensive real-time
assistance in scene perception,
  text recognition, and navigation.

</details>


### [296] [Emerging Semantic Segmentation from Positive and Negative Coarse Label Learning](https://arxiv.org/abs/2508.18186)
*Le Zhang,Fuping Wu,Arun Thirunavukarasu,Kevin Bronik,Thomas Nichols,Bartlomiej W. Papiez*

Main category: cs.CV

TL;DR: A method using coarse, noisy annotations to train semantic segmentation CNNs, outperforming state-of-the-art methods especially when coarse annotations are limited.


<details>
  <summary>Details</summary>
Motivation: Pixel-level labeling is time-consuming and requires experts, while coarse annotations are quicker and cheaper to produce even by non-experts.

Method: Two coupled CNNs learn true segmentation distributions from noisy coarse annotations using complementary label learning to estimate negative label distributions.

Result: Outperforms state-of-the-art methods on MNIST, Cityscapes, and retinal image datasets, particularly when coarse annotations are scarce.

Conclusion: Coarse annotations can effectively train segmentation models, reducing dependency on expensive pixel-level expert labeling.

Abstract: Large annotated datasets are vital for training segmentation models, but
pixel-level labeling is time-consuming, error-prone, and often requires scarce
expert annotators, especially in medical imaging. In contrast, coarse
annotations are quicker, cheaper, and easier to produce, even by non-experts.
In this paper, we propose to use coarse drawings from both positive (target)
and negative (background) classes in the image, even with noisy pixels, to
train a convolutional neural network (CNN) for semantic segmentation. We
present a method for learning the true segmentation label distributions from
purely noisy coarse annotations using two coupled CNNs. The separation of the
two CNNs is achieved by high fidelity with the characters of the noisy training
annotations. We propose to add a complementary label learning that encourages
estimating negative label distribution. To illustrate the properties of our
method, we first use a toy segmentation dataset based on MNIST. We then present
the quantitative results of experiments using publicly available datasets:
Cityscapes dataset for multi-class segmentation, and retinal images for medical
applications. In all experiments, our method outperforms state-of-the-art
methods, particularly in the cases where the ratio of coarse annotations is
small compared to the given dense annotations.

</details>


### [297] [BRAIN: Bias-Mitigation Continual Learning Approach to Vision-Brain Understanding](https://arxiv.org/abs/2508.18187)
*Xuan-Bac Nguyen,Thanh-Dat Truong,Pawan Sinha,Khoa Luu*

Main category: cs.CV

TL;DR: Proposes BRAIN approach to mitigate memory decay effects in brain signals for vision-brain understanding models using continual learning and debiasing techniques.


<details>
  <summary>Details</summary>
Motivation: Memory decay causes brain signals to weaken and lose visual context over time, creating inconsistency and bias that degrades model performance in vision-brain understanding tasks.

Method: Bias-Mitigation Continual Learning (BRAIN) approach with De-bias Contrastive Learning loss function and Angular-based Forgetting Mitigation to address signal shift and prevent catastrophic forgetting.

Result: Achieves State-of-the-Art performance across various benchmarks, outperforming both prior methods and non-continual learning approaches.

Conclusion: The proposed BRAIN framework effectively addresses brain signal inconsistency and bias problems in continual learning settings, demonstrating significant performance improvements for vision-brain understanding models.

Abstract: Memory decay makes it harder for the human brain to recognize visual objects
and retain details. Consequently, recorded brain signals become weaker,
uncertain, and contain poor visual context over time. This paper presents one
of the first vision-learning approaches to address this problem. First, we
statistically and experimentally demonstrate the existence of inconsistency in
brain signals and its impact on the Vision-Brain Understanding (VBU) model. Our
findings show that brain signal representations shift over recording sessions,
leading to compounding bias, which poses challenges for model learning and
degrades performance. Then, we propose a new Bias-Mitigation Continual Learning
(BRAIN) approach to address these limitations. In this approach, the model is
trained in a continual learning setup and mitigates the growing bias from each
learning step. A new loss function named De-bias Contrastive Learning is also
introduced to address the bias problem. In addition, to prevent catastrophic
forgetting, where the model loses knowledge from previous sessions, the new
Angular-based Forgetting Mitigation approach is introduced to preserve learned
knowledge in the model. Finally, the empirical experiments demonstrate that our
approach achieves State-of-the-Art (SOTA) performance across various
benchmarks, surpassing prior and non-continual learning methods.

</details>


### [298] [Explain and Monitor Deep Learning Models for Computer Vision using Obz AI](https://arxiv.org/abs/2508.18188)
*Neo Christopher Chung,Jakub Binda*

Main category: cs.CV

TL;DR: Obz AI is a comprehensive software ecosystem that bridges the gap between explainable AI (XAI) techniques and practical computer vision deployments by providing integrated explainability, observability, and monitoring capabilities.


<details>
  <summary>Details</summary>
Motivation: Deep learning models in computer vision are often black boxes with limited transparency, and despite recent XAI advancements, explainability remains underutilized in practical deployments due to lack of integrated software solutions connecting XAI with knowledge management and monitoring frameworks.

Method: Developed Obz AI - a comprehensive software ecosystem with seamless integration pipeline from Python client library to full-stack analytics dashboard, enabling incorporation of advanced XAI methodologies, feature extraction for outlier detection, and real-time model monitoring.

Result: Obz AI facilitates state-of-the-art explainability and observability for vision AI systems, making deep model decision-making mechanisms interpretable and promoting responsible deployment.

Conclusion: Obz AI successfully closes the gap between XAI research and practical CV deployments by providing an integrated solution that enhances transparency, observability, and responsible use of computer vision AI systems.

Abstract: Deep learning has transformed computer vision (CV), achieving outstanding
performance in classification, segmentation, and related tasks. Such AI-based
CV systems are becoming prevalent, with applications spanning from medical
imaging to surveillance. State of the art models such as convolutional neural
networks (CNNs) and vision transformers (ViTs) are often regarded as ``black
boxes,'' offering limited transparency into their decision-making processes.
Despite a recent advancement in explainable AI (XAI), explainability remains
underutilized in practical CV deployments. A primary obstacle is the absence of
integrated software solutions that connect XAI techniques with robust knowledge
management and monitoring frameworks. To close this gap, we have developed Obz
AI, a comprehensive software ecosystem designed to facilitate state-of-the-art
explainability and observability for vision AI systems. Obz AI provides a
seamless integration pipeline, from a Python client library to a full-stack
analytics dashboard. With Obz AI, a machine learning engineer can easily
incorporate advanced XAI methodologies, extract and analyze features for
outlier detection, and continuously monitor AI models in real time. By making
the decision-making mechanisms of deep models interpretable, Obz AI promotes
observability and responsible deployment of computer vision systems.

</details>


### [299] [Follow My Hold: Hand-Object Interaction Reconstruction through Geometric Guidance](https://arxiv.org/abs/2508.18213)
*Ayce Idil Aytekin,Helge Rhodin,Rishabh Dabral,Christian Theobalt*

Main category: cs.CV

TL;DR: Diffusion-based 3D object reconstruction from monocular RGB images using hand-object interaction as geometric guidance, with inference-time optimization for high-quality results.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of prior methods that require extensive post-processing or produce low-quality reconstructions by leveraging hand-object interactions as geometric constraints for more accurate 3D reconstruction.

Method: Uses latent diffusion model conditioned on inpainted object appearance with inference-time guidance. Applies supervision to velocity field while optimizing hand and object transformations using multi-modal geometric cues (normal/depth alignment, silhouette consistency, 2D keypoint reprojection). Incorporates signed distance field supervision and contact/non-intersection constraints.

Result: Produces accurate, robust, and coherent reconstructions under occlusion while generalizing well to in-the-wild scenarios.

Conclusion: The proposed diffusion framework with geometric guidance and optimization-in-the-loop design enables high-quality 3D object reconstruction from monocular images by effectively leveraging hand-object interaction constraints.

Abstract: We propose a novel diffusion-based framework for reconstructing 3D geometry
of hand-held objects from monocular RGB images by leveraging hand-object
interaction as geometric guidance. Our method conditions a latent diffusion
model on an inpainted object appearance and uses inference-time guidance to
optimize the object reconstruction, while simultaneously ensuring plausible
hand-object interactions. Unlike prior methods that rely on extensive
post-processing or produce low-quality reconstructions, our approach directly
generates high-quality object geometry during the diffusion process by
introducing guidance with an optimization-in-the-loop design. Specifically, we
guide the diffusion model by applying supervision to the velocity field while
simultaneously optimizing the transformations of both the hand and the object
being reconstructed. This optimization is driven by multi-modal geometric cues,
including normal and depth alignment, silhouette consistency, and 2D keypoint
reprojection. We further incorporate signed distance field supervision and
enforce contact and non-intersection constraints to ensure physical
plausibility of hand-object interaction. Our method yields accurate, robust and
coherent reconstructions under occlusion while generalizing well to in-the-wild
scenarios.

</details>


### [300] [GM-Skip: Metric-Guided Transformer Block Skipping for Efficient Vision-Language Models](https://arxiv.org/abs/2508.18227)
*Lianming Huang,Haibo Hu,Qiao Li,Xin He,Nan Guan,Chun Jason Xue*

Main category: cs.CV

TL;DR: GM-Skip is a framework that accelerates Vision-Language Model inference by adaptively skipping redundant Transformer blocks while maintaining output quality, achieving significant speed improvements with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Transformer-based VLMs have high computational costs that hinder deployment in latency-sensitive applications like autonomous driving, requiring efficient inference methods.

Method: Uses greedy metric-guided block selection with metric feedback (accuracy, CIDEr) to identify redundant layers, reverse-order deletion to preserve early blocks, and tunable sparsity-performance trade-off.

Result: Improves single-object classification accuracy from 19.1% to 87.3% while skipping >40% blocks on COCO, achieves 45.4% latency reduction in autonomous vehicle deployment.

Conclusion: GM-Skip effectively accelerates VLM inference while preserving task performance, demonstrating practical value for real-world applications like autonomous driving.

Abstract: Transformer-based Vision-Language Models (VLMs) have achieved impressive
performance on tasks such as image captioning, object recognition, and visual
reasoning, but their high computational cost hinders deployment in
latency-sensitive applications like autonomous driving. We introduce GM-Skip, a
flexible and metric-adaptive framework for Transformer block skipping that
accelerates VLM inference while preserving output quality. GM-Skip features a
greedy, metric-guided block selection strategy that uses metric feedback (e.g.,
accuracy, CIDEr) to identify redundant layers, along with a reverse-order
deletion mechanism that preserves early foundational blocks to avoid
performance collapse. To support diverse deployment needs, it incorporates a
tunable trade-off between sparsity and performance via a score-sparsity balance
objective. Experiments across multiple tasks and datasets, including COCO and
CODA, show that GM-Skip consistently improves inference speed while maintaining
task performance. On the COCO dataset, GM-Skip improves single-object
classification accuracy on the Person category from 19.1 percent to 87.3
percent while skipping more than 40 percent of Transformer blocks. In
real-world deployment, it achieves up to 45.4 percent latency reduction on
single-object detection when integrated into an autonomous vehicle running
Autoware.Universe, validating the effectiveness of its skip configurations and
confirming its practical value in accelerating real-world inference.

</details>


### [301] [Sealing The Backdoor: Unlearning Adversarial Text Triggers In Diffusion Models Using Knowledge Distillation](https://arxiv.org/abs/2508.18235)
*Ashwath Vaithinathan Aravindan,Abha Jha,Matthew Salaway,Atharva Sandeep Bhide,Duygu Nur Yaldiz*

Main category: cs.CV

TL;DR: SKD-CAG method effectively removes text-based backdoor attacks from diffusion models while preserving image quality, achieving near-perfect removal rates.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models are vulnerable to backdoor attacks where adversaries inject imperceptible textual triggers, but current defenses are inadequate for generative models.

Method: Self-Knowledge Distillation with Cross-Attention Guidance (SKD-CAG) uses knowledge distillation and cross-attention mechanisms to selectively erase adversarial associations while maintaining clean outputs.

Result: Achieved 100% removal accuracy for pixel backdoors and 93% for style-based attacks without compromising robustness or image fidelity.

Conclusion: Targeted unlearning through SKD-CAG is an effective defense strategy to secure generative models against text-based backdoor attacks.

Abstract: Text-to-image diffusion models have revolutionized generative AI, but their
vulnerability to backdoor attacks poses significant security risks. Adversaries
can inject imperceptible textual triggers into training data, causing models to
generate manipulated outputs. Although text-based backdoor defenses in
classification models are well-explored, generative models lack effective
mitigation techniques against. We address this by selectively erasing the
model's learned associations between adversarial text triggers and poisoned
outputs, while preserving overall generation quality. Our approach,
Self-Knowledge Distillation with Cross-Attention Guidance (SKD-CAG), uses
knowledge distillation to guide the model in correcting responses to poisoned
prompts while maintaining image quality by exploiting the fact that the
backdoored model still produces clean outputs in the absence of triggers. Using
the cross-attention mechanism, SKD-CAG neutralizes backdoor influences at the
attention level, ensuring the targeted removal of adversarial effects.
Extensive experiments show that our method outperforms existing approaches,
achieving removal accuracy 100\% for pixel backdoors and 93\% for style-based
attacks, without sacrificing robustness or image fidelity. Our findings
highlight targeted unlearning as a promising defense to secure generative
models. Code and model weights can be found at
https://github.com/Mystic-Slice/Sealing-The-Backdoor .

</details>


### [302] [Interpretable Evaluation of AI-Generated Content with Language-Grounded Sparse Encoders](https://arxiv.org/abs/2508.18236)
*Yiming Tang,Arash Lagzian,Srinivas Anumasa,Qiran Zou,Trang Nguyen,Ehsan Adeli,Ching-Yu Cheng,Yilun Du,Dianbo Liu*

Main category: cs.CV

TL;DR: LanSE introduces interpretable evaluation metrics for AI-generated images by identifying visual patterns and describing them in natural language, enabling fine-grained assessment of generation quality across four key dimensions.


<details>
  <summary>Details</summary>
Motivation: Current AI-generated content evaluation metrics provide only coarse-grained assessments, failing to identify specific strengths and weaknesses needed for model selection and scientific understanding, limiting commercial deployment.

Method: Language-Grounded Sparse Encoders (LanSE) architecture that identifies interpretable visual patterns and automatically describes them in natural language, validated through large-scale human evaluation (11,000+ annotations) and LMM-based analysis.

Result: LanSE achieves over 93% accuracy in detecting interpretable visual patterns in natural images, reveals nuanced model differences invisible to existing metrics (e.g., FLUX's superior physical plausibility, SDXL-medium's strong content diversity), and aligns with human judgments.

Conclusion: LanSE bridges interpretability with practical evaluation needs, providing a powerful tool for model selection, quality control, and improvement, addressing the need for public confidence and safety in AI-generated content.

Abstract: While the quality of AI-generated contents, such as synthetic images, has
become remarkably high, current evaluation metrics provide only coarse-grained
assessments, failing to identify specific strengths and weaknesses that
researchers and practitioners need for model selection and development, further
limiting the scientific understanding and commercial deployment of these
generative models. To address this, we introduce Language-Grounded Sparse
Encoders (LanSE), a novel architecture that creates interpretable evaluation
metrics by identifying interpretable visual patterns and automatically
describing them in natural language. Through large-scale human evaluation (more
than 11,000 annotations) and large multimodal model (LMM) based analysis, LanSE
demonstrates reliable capabilities to detect interpretable visual patterns in
synthetic images with more than 93\% accuracy in natural images. LanSE further
provides a fine-grained evaluation framework that quantifies four key
dimensions of generation quality, prompt match, visual realism, physical
plausibility, and content diversity. LanSE reveals nuanced model differences
invisible to existing metrics, for instance, FLUX's superior physical
plausibility and SDXL-medium's strong content diversity, while aligning with
human judgments. By bridging interpretability with practical evaluation needs,
LanSE offers all users of generative AI models a powerful tool for model
selection, quality control of synthetic content, and model improvement. These
capabilities directly address the need for public confidence and safety in
AI-generated content, both critical for the future of generative AI
applications.

</details>


### [303] [PriorFormer: A Transformer for Real-time Monocular 3D Human Pose Estimation with Versatile Geometric Priors](https://arxiv.org/abs/2508.18238)
*Mohamed Adjel,Vincent Bonnet*

Main category: cs.CV

TL;DR: Lightweight Transformer model for 2D-to-3D human pose estimation that handles both calibrated and uncalibrated camera settings using geometric priors with masking mechanism.


<details>
  <summary>Details</summary>
Motivation: To create a versatile 3D pose estimation model that works in various deployment scenarios from calibrated lab environments to uncalibrated in-the-wild videos, while maintaining low computational cost for embedded platforms.

Method: Transformer-based architecture that takes 2D joint sequences and geometric priors (segment lengths, camera intrinsics) with masking to handle missing priors. Trained on AMASS dataset with synthetic 2D data from random camera poses.

Result: Achieved 36mm average 3D joint position error, improving state-of-the-art by 0.5cm. Runs in 380Î¼s on GPU and 1800Î¼s on CPU. Versatile model outperforms expert model even with all priors available.

Conclusion: The proposed lightweight Transformer successfully handles both calibrated and uncalibrated settings, maintains high accuracy with missing priors, and achieves state-of-the-art performance with low computational cost suitable for embedded devices.

Abstract: This paper proposes a new lightweight Transformer-based lifter that maps
short sequences of human 2D joint positions to 3D poses using a single camera.
The proposed model takes as input geometric priors including segment lengths
and camera intrinsics and is designed to operate in both calibrated and
uncalibrated settings. To this end, a masking mechanism enables the model to
ignore missing priors during training and inference. This yields a single
versatile network that can adapt to different deployment scenarios, from fully
calibrated lab environments to in-the-wild monocular videos without
calibration. The model was trained using 3D keypoints from AMASS dataset with
corresponding 2D synthetic data generated by sampling random camera poses and
intrinsics. It was then compared to an expert model trained, only on complete
priors, and the validation was done by conducting an ablation study. Results
show that both, camera and segment length priors, improve performance and that
the versatile model outperforms the expert, even when all priors are available,
and maintains high accuracy when priors are missing. Overall the average 3D
joint center positions estimation accuracy was as low as 36mm improving state
of the art by half a centimeter and at a much lower computational cost. Indeed,
the proposed model runs in 380$\mu$s on GPU and 1800$\mu$s on CPU, making it
suitable for deployment on embedded platforms and low-power devices.

</details>


### [304] [GSVisLoc: Generalizable Visual Localization for Gaussian Splatting Scene Representations](https://arxiv.org/abs/2508.18242)
*Fadi Khatib,Dror Moran,Guy Trostianetsky,Yoni Kasten,Meirav Galun,Ronen Basri*

Main category: cs.CV

TL;DR: GSVisLoc is a visual localization method that uses 3D Gaussian Splatting scene representations to estimate camera pose from query images through robust feature matching without requiring scene modifications or retraining.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop a visual localization method that can leverage 3DGS scene representations directly without the need for modifications, retraining, or additional reference images, addressing the challenge of camera pose estimation in novel scenes.

Method: The method involves three steps: 1) coarse matching between scene features (from downsampled and encoded 3D Gaussians) and image features (from encoded image patches), 2) fine matching for more precise correspondences, and 3) pose refinement for accurate final camera position and orientation estimation.

Result: GSVisLoc demonstrates competitive localization performance on standard indoor and outdoor benchmarks, outperforming existing 3DGS-based baselines and showing effective generalization to novel scenes without additional training.

Conclusion: The proposed GSVisLoc method successfully enables visual localization using 3DGS representations through robust feature matching, achieving strong performance while maintaining generalization capabilities across different scenes without requiring retraining or modifications.

Abstract: We introduce GSVisLoc, a visual localization method designed for 3D Gaussian
Splatting (3DGS) scene representations. Given a 3DGS model of a scene and a
query image, our goal is to estimate the camera's position and orientation. We
accomplish this by robustly matching scene features to image features. Scene
features are produced by downsampling and encoding the 3D Gaussians while image
features are obtained by encoding image patches. Our algorithm proceeds in
three steps, starting with coarse matching, then fine matching, and finally by
applying pose refinement for an accurate final estimate. Importantly, our
method leverages the explicit 3DGS scene representation for visual localization
without requiring modifications, retraining, or additional reference images. We
evaluate GSVisLoc on both indoor and outdoor scenes, demonstrating competitive
localization performance on standard benchmarks while outperforming existing
3DGS-based baselines. Moreover, our approach generalizes effectively to novel
scenes without additional training.

</details>


### [305] [MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs](https://arxiv.org/abs/2508.18264)
*Sixun Dong,Juhua Hu,Mian Zhang,Ming Yin,Yanjie Fu,Qi Qian*

Main category: cs.CV

TL;DR: MMTok is a multimodal token selection method that uses both vision and text tokens to select informative vision tokens based on coverage criteria, improving VLM inference efficiency while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing vision token pruning methods use only unimodal information and ignore the multimodal nature of vision-language tasks, lacking a generic criterion for different modalities.

Method: Formulates subset selection as maximum coverage problem, optimizes vision tokens to cover both text tokens and original vision tokens, and uses VLM agent to improve text token quality for guiding vision pruning.

Result: Achieves 1.87x speedup with 98.7% original performance on LLaVA-NeXT-13B, and maintains 87.7% performance with only 4 vision tokens on LLaVA-1.5-7B.

Conclusion: Multimodal information combination surpasses unimodal baselines, and coverage criterion is effective for token selection in vision-language models.

Abstract: Vision-Language Models (VLMs) demonstrate impressive performance in
understanding visual content with language instruction by converting visual
input to vision tokens. However, redundancy in vision tokens results in the
degenerated inference efficiency of VLMs. While many algorithms have been
proposed to reduce the number of vision tokens, most of them apply only
unimodal information (i.e., vision/text) for pruning and ignore the inherent
multimodal property of vision-language tasks. Moreover, it lacks a generic
criterion that can be applied to different modalities. To mitigate this
limitation, in this work, we propose to leverage both vision and text tokens to
select informative vision tokens by the criterion of coverage. We first
formulate the subset selection problem as a maximum coverage problem.
Afterward, a subset of vision tokens is optimized to cover the text tokens and
the original set of vision tokens, simultaneously. Finally, a VLM agent can be
adopted to further improve the quality of text tokens for guiding vision
pruning. The proposed method MMTok is extensively evaluated on benchmark
datasets with different VLMs. The comparison illustrates that vision and text
information are complementary, and combining multimodal information can surpass
the unimodal baseline with a clear margin. Moreover, under the maximum coverage
criterion on the POPE dataset, our method achieves a 1.87x speedup while
maintaining 98.7% of the original performance on LLaVA-NeXT-13B. Furthermore,
with only four vision tokens, it still preserves 87.7% of the original
performance on LLaVA-1.5-7B. These results highlight the effectiveness of
coverage in token selection.

</details>


### [306] [InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency](https://arxiv.org/abs/2508.18265)
*Weiyun Wang,Zhangwei Gao,Lixin Gu,Hengjun Pu,Long Cui,Xingguang Wei,Zhaoyang Liu,Linglin Jing,Shenglong Ye,Jie Shao,Zhaokai Wang,Zhe Chen,Hongjie Zhang,Ganlin Yang,Haomin Wang,Qi Wei,Jinhui Yin,Wenhao Li,Erfei Cui,Guanzhou Chen,Zichen Ding,Changyao Tian,Zhenyu Wu,Jingjing Xie,Zehao Li,Bowen Yang,Yuchen Duan,Xuehui Wang,Songze Li,Xiangyu Zhao,Haodong Duan,Nianchen Deng,Bin Fu,Yinan He,Yi Wang,Conghui He,Botian Shi,Junjun He,Yingtong Xiong,Han Lv,Lijun Wu,Wenqi Shao,Kaipeng Zhang,Huipeng Deng,Biqing Qi,Jiaye Ge,Qipeng Guo,Wenwei Zhang,Wanli Ouyang,Limin Wang,Min Dou,Xizhou Zhu,Tong Lu,Dahua Lin,Jifeng Dai,Bowen Zhou,Weijie Su,Kai Chen,Yu Qiao,Wenhai Wang,Gen Luo*

Main category: cs.CV

TL;DR: InternVL 3.5 introduces Cascade RL framework and Visual Resolution Router to significantly improve reasoning performance (+16.0%) and inference speed (4.05Ã faster) compared to previous version, achieving state-of-the-art results among open-source multimodal models.


<details>
  <summary>Details</summary>
Motivation: To advance open-source multimodal models with better versatility, reasoning capability, and inference efficiency while narrowing the performance gap with commercial models like GPT-5.

Method: Uses Cascade Reinforcement Learning (offline RL + online RL) for reasoning enhancement, Visual Resolution Router (ViR) for dynamic resolution adjustment, and Decoupled Vision-Language Deployment (DvD) for GPU load balancing.

Result: Achieves +16.0% gain in reasoning performance, 4.05Ã inference speedup, state-of-the-art results across multimodal, reasoning, text, and agentic tasks, with support for GUI interaction and embodied agency.

Conclusion: InternVL 3.5 represents a significant advancement in open-source multimodal models, demonstrating that innovative training frameworks and deployment strategies can dramatically improve both performance and efficiency while maintaining open accessibility.

Abstract: We introduce InternVL 3.5, a new family of open-source multimodal models that
significantly advances versatility, reasoning capability, and inference
efficiency along the InternVL series. A key innovation is the Cascade
Reinforcement Learning (Cascade RL) framework, which enhances reasoning through
a two-stage process: offline RL for stable convergence and online RL for
refined alignment. This coarse-to-fine training strategy leads to substantial
improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To
optimize efficiency, we propose a Visual Resolution Router (ViR) that
dynamically adjusts the resolution of visual tokens without compromising
performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD)
strategy separates the vision encoder and language model across different GPUs,
effectively balancing computational load. These contributions collectively
enable InternVL3.5 to achieve up to a +16.0\% gain in overall reasoning
performance and a 4.05$\times$ inference speedup compared to its predecessor,
i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as
GUI interaction and embodied agency. Notably, our largest model, i.e.,
InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs
across general multimodal, reasoning, text, and agentic tasks -- narrowing the
performance gap with leading commercial models like GPT-5. All models and code
are publicly released.

</details>


### [307] [ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion Models](https://arxiv.org/abs/2508.18271)
*Haitang Feng,Jie Liu,Jie Tang,Gangshan Wu,Beiqi Chen,Jianhuang Lai,Guangcong Wang*

Main category: cs.CV

TL;DR: ObjFiller-3D is a novel 3D inpainting method that uses video editing models instead of traditional 2D image inpainting to achieve more consistent and high-quality 3D object completion with reduced artifacts.


<details>
  <summary>Details</summary>
Motivation: Current 3D inpainting methods relying on multi-view 2D image inpainting suffer from inconsistencies across views, leading to blurred textures, spatial discontinuities, and visual artifacts that hinder accurate 3D object completion.

Method: Leverages state-of-the-art video editing models to fill masked regions of 3D objects, analyzes the representation gap between 3D and videos, adapts video inpainting for 3D scenes, and introduces reference-based 3D inpainting to enhance reconstruction quality.

Result: Achieves superior performance with PSNR of 26.6 vs. NeRFiller (15.9) and LPIPS of 0.19 vs. Instant3dit (0.25), producing more faithful and fine-grained reconstructions across diverse datasets.

Conclusion: ObjFiller-3D demonstrates strong potential for practical deployment in real-world 3D editing applications, offering a significant improvement over previous methods in terms of consistency and quality.

Abstract: 3D inpainting often relies on multi-view 2D image inpainting, where the
inherent inconsistencies across different inpainted views can result in blurred
textures, spatial discontinuities, and distracting visual artifacts. These
inconsistencies pose significant challenges when striving for accurate and
realistic 3D object completion, particularly in applications that demand high
fidelity and structural coherence. To overcome these limitations, we propose
ObjFiller-3D, a novel method designed for the completion and editing of
high-quality and consistent 3D objects. Instead of employing a conventional 2D
image inpainting model, our approach leverages a curated selection of
state-of-the-art video editing model to fill in the masked regions of 3D
objects. We analyze the representation gap between 3D and videos, and propose
an adaptation of a video inpainting model for 3D scene inpainting. In addition,
we introduce a reference-based 3D inpainting method to further enhance the
quality of reconstruction. Experiments across diverse datasets show that
compared to previous methods, ObjFiller-3D produces more faithful and
fine-grained reconstructions (PSNR of 26.6 vs. NeRFiller (15.9) and LPIPS of
0.19 vs. Instant3dit (0.25)). Moreover, it demonstrates strong potential for
practical deployment in real-world 3D editing applications. Project page:
https://objfiller3d.github.io/ Code:
https://github.com/objfiller3d/ObjFiller-3D .

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [308] [How Do LLM-Generated Texts Impact Term-Based Retrieval Models?](https://arxiv.org/abs/2508.17715)
*Wei Huang,Keping Bi,Yinqiong Cai,Wei Chen,Jiafeng Guo,Xueqi Cheng*

Main category: cs.IR

TL;DR: This paper investigates how LLM-generated content affects term-based retrieval models like BM25, finding that these models prioritize documents with term distributions matching queries rather than showing inherent source bias.


<details>
  <summary>Details</summary>
Motivation: As LLM-generated content floods the Internet, IR systems need to handle mixed human and machine-generated texts. Recent studies show neural retrievers prefer LLM content while term-based models favor human documents.

Method: Conducted linguistic analysis of LLM-generated texts, examining Zipf slopes, term specificity, and document diversity. Explored whether term-based retrieval models demonstrate source bias by analyzing document-term distribution alignment.

Result: LLM-generated texts show smoother high-frequency and steeper low-frequency Zipf slopes, higher term specificity, and greater document diversity. Term-based models prioritize documents whose term distributions match queries rather than showing inherent source bias.

Conclusion: Term-based retrieval models do not exhibit inherent bias toward LLM-generated content but instead favor documents with term distributions that align with query characteristics, providing foundation for addressing potential biases in mixed-source IR systems.

Abstract: As more content generated by large language models (LLMs) floods into the
Internet, information retrieval (IR) systems now face the challenge of
distinguishing and handling a blend of human-authored and machine-generated
texts. Recent studies suggest that neural retrievers may exhibit a preferential
inclination toward LLM-generated content, while classic term-based retrievers
like BM25 tend to favor human-written documents. This paper investigates the
influence of LLM-generated content on term-based retrieval models, which are
valued for their efficiency and robust generalization across domains. Our
linguistic analysis reveals that LLM-generated texts exhibit smoother
high-frequency and steeper low-frequency Zipf slopes, higher term specificity,
and greater document-level diversity. These traits are aligned with LLMs being
trained to optimize reader experience through diverse and precise expressions.
Our study further explores whether term-based retrieval models demonstrate
source bias, concluding that these models prioritize documents whose term
distributions closely correspond to those of the queries, rather than
displaying an inherent source bias. This work provides a foundation for
understanding and addressing potential biases in term-based IR systems managing
mixed-source content.

</details>


### [309] [HLLM-Creator: Hierarchical LLM-based Personalized Creative Generation](https://arxiv.org/abs/2508.18118)
*Junyi Chen,Lu Chi,Siliang Xu,Shiwei Ran,Bingyue Peng,Zehuan Yuan*

Main category: cs.IR

TL;DR: HLLM-Creator is a hierarchical LLM framework that efficiently generates personalized content by modeling user interests through clustering and pruning strategies, achieving significant improvements in ad performance with scalable deployment.


<details>
  <summary>Details</summary>
Motivation: Current AIGC systems lack true personalization, failing to address that different users focus on different product features. Online advertising and similar applications require user-centric content generation that adapts to individual interests while maintaining factual accuracy.

Method: Proposes HLLM-Creator with hierarchical LLM framework using user clustering and user-ad-matching-prediction pruning for efficiency. Includes chain-of-thought reasoning data construction pipeline for generating high-quality personalized creative titles with factual consistency despite limited data.

Result: Extensive experiments on Douyin Search Ads show effectiveness. Online A/B test demonstrates 0.476% increase on Adss metric, proving the approach works for large-scale industrial deployment.

Conclusion: HLLM-Creator provides an effective and efficient solution for personalized content generation in industrial scenarios, addressing key challenges of user interest modeling, factual consistency, and scalability with limited training data.

Abstract: AI-generated content technologies are widely used in content creation.
However, current AIGC systems rely heavily on creators' inspiration, rarely
generating truly user-personalized content. In real-world applications such as
online advertising, a single product may have multiple selling points, with
different users focusing on different features. This underscores the
significant value of personalized, user-centric creative generation. Effective
personalized content generation faces two main challenges: (1) accurately
modeling user interests and integrating them into the content generation
process while adhering to factual constraints, and (2) ensuring high efficiency
and scalability to handle the massive user base in industrial scenarios.
Additionally, the scarcity of personalized creative data in practice
complicates model training, making data construction another key hurdle. We
propose HLLM-Creator, a hierarchical LLM framework for efficient user interest
modeling and personalized content generation. During inference, a combination
of user clustering and a user-ad-matching-prediction based pruning strategy is
employed to significantly enhance generation efficiency and reduce
computational overhead, making the approach suitable for large-scale
deployment. Moreover, we design a data construction pipeline based on
chain-of-thought reasoning, which generates high-quality, user-specific
creative titles and ensures factual consistency despite limited personalized
data. This pipeline serves as a critical foundation for the effectiveness of
our model. Extensive experiments on personalized title generation for Douyin
Search Ads show the effectiveness of HLLM-Creator. Online A/B test shows a
0.476% increase on Adss, paving the way for more effective and efficient
personalized generation in industrial scenarios. Codes for academic dataset are
available at https://github.com/bytedance/HLLM.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [310] [Humans Perceive Wrong Narratives from AI Reasoning Texts](https://arxiv.org/abs/2508.16599)
*Mosh Levy,Zohar Elyoseph,Yoav Goldberg*

Main category: cs.HC

TL;DR: Humans struggle to identify causal relationships in AI reasoning text, with only 29.3% accuracy, challenging the reliability of reasoning text as an interpretability tool.


<details>
  <summary>Details</summary>
Motivation: To investigate whether human understanding of AI reasoning text corresponds to the model's actual computational process, particularly the ability to identify causal influences between reasoning steps.

Method: Evaluated humans on identifying which steps in reasoning text causally influence later steps using questions based on counterfactual measurements.

Result: Human accuracy was only 29.3% (barely above chance at 25%), and remained low at 42% even with majority voting on high-agreement questions.

Conclusion: There's a fundamental gap between human interpretation and model computation in reasoning text, suggesting it should be treated as an artifact to investigate rather than taken at face value.

Abstract: A new generation of AI models generates step-by-step reasoning text before
producing an answer. This text appears to offer a human-readable window into
their computation process, and is increasingly relied upon for transparency and
interpretability. However, it is unclear whether human understanding of this
text matches the model's actual computational process. In this paper, we
investigate a necessary condition for correspondence: the ability of humans to
identify which steps in a reasoning text causally influence later steps. We
evaluated humans on this ability by composing questions based on counterfactual
measurements and found a significant discrepancy: participant accuracy was only
29.3%, barely above chance (25%), and remained low (42%) even when evaluating
the majority vote on questions with high agreement. Our results reveal a
fundamental gap between how humans interpret reasoning texts and how models use
it, challenging its utility as a simple interpretability tool. We argue that
reasoning texts should be treated as an artifact to be investigated, not taken
at face value, and that understanding the non-human ways these models use
language is a critical research direction.

</details>


### [311] [Can AI Have a Personality? Prompt Engineering for AI Personality Simulation: A Chatbot Case Study in Gender-Affirming Voice Therapy Training](https://arxiv.org/abs/2508.18234)
*Tailon D. Jackson,Byunggu Yu*

Main category: cs.HC

TL;DR: LLMs can simulate consistent personalities through prompt engineering, demonstrated with a transgender woman chatbot for SLP training.


<details>
  <summary>Details</summary>
Motivation: To investigate if prompt engineering can guide LLMs to maintain consistent personality traits for specialized chatbot applications like gender-affirming voice therapy training.

Method: Created a chatbot named Monae Jackson representing a 32-year-old transgender woman using prompt engineering, and evaluated personality consistency using Big Five Personality test within client-therapist conversation simulations.

Result: The chatbot maintained recognizable and consistent persona with distinct personality characteristics through effective prompt engineering.

Conclusion: Prompt engineering can successfully simulate stable personality traits in AI chatbots for specialized training applications.

Abstract: This thesis investigates whether large language models (LLMs) can be guided
to simulate a consistent personality through prompt engineering. The study
explores this concept within the context of a chatbot designed for
Speech-Language Pathology (SLP) student training, specifically focused on
gender-affirming voice therapy. The chatbot, named Monae Jackson, was created
to represent a 32-year-old transgender woman and engage in conversations
simulating client-therapist interactions. Findings suggest that with prompt
engineering, the chatbot maintained a recognizable and consistent persona and
had a distinct personality based on the Big Five Personality test. These
results support the idea that prompt engineering can be used to simulate stable
personality characteristics in AI chatbots.

</details>


### [312] [Predicting User Grasp Intentions in Virtual Reality](https://arxiv.org/abs/2508.16582)
*Linghao Zeng*

Main category: cs.HC

TL;DR: LSTM regression models outperform classification approaches for predicting VR grasping intentions, achieving timing errors within 0.25s and distance errors of 5-20cm in the critical pre-grasp window, though precise hand posture prediction remains challenging.


<details>
  <summary>Details</summary>
Motivation: Predicting user intentions in VR is crucial for immersive experiences, especially for complex grasping motions requiring accurate haptic feedback.

Method: Evaluated classification and regression approaches using time-series hand movement data from 810 trials with varied object types, sizes, and manipulations, with focus on LSTM networks.

Result: Classification models struggled with user generalization while regression (particularly LSTM) showed robust performance with timing errors <0.25s and distance errors 5-20cm in the critical 2-second pre-grasp window.

Conclusion: Regression models better accommodate dynamic user behavior in VR, showing potential for enhancing VR interactions through adaptive haptic feedback and laying groundwork for real-time prediction advancements.

Abstract: Predicting user intentions in virtual reality (VR) is crucial for creating
immersive experiences, particularly in tasks involving complex grasping motions
where accurate haptic feedback is essential. In this work, we leverage
time-series data from hand movements to evaluate both classification and
regression approaches across 810 trials with varied object types, sizes, and
manipulations. Our findings reveal that classification models struggle to
generalize across users, leading to inconsistent performance. In contrast,
regression-based approaches, particularly those using Long Short Term Memory
(LSTM) networks, demonstrate more robust performance, with timing errors within
0.25 seconds and distance errors around 5-20 cm in the critical two-second
window before a grasp. Despite these improvements, predicting precise hand
postures remains challenging. Through a comprehensive analysis of user
variability and model interpretability, we explore why certain models fail and
how regression models better accommodate the dynamic and complex nature of user
behavior in VR. Our results underscore the potential of machine learning models
to enhance VR interactions, particularly through adaptive haptic feedback, and
lay the groundwork for future advancements in real-time prediction of user
actions in VR.

</details>


### [313] [Negative Shanshui: Real-time Interactive Ink Painting Synthesis](https://arxiv.org/abs/2508.16612)
*Aven-Le Zhou*

Main category: cs.HC

TL;DR: Negative Shanshui is a real-time AI system that reinterprets Chinese landscape painting through gaze-controlled VR animations to address ecological crises.


<details>
  <summary>Details</summary>
Motivation: To engage with ecological crises in the Anthropocene by reinterpreting classical Chinese landscape ink painting (shanshui) through interactive AI technology.

Method: Optimizes a fine-tuned Stable Diffusion model for real-time inference, integrates gaze-driven inpainting and frame interpolation for dynamic morphing animations in VR.

Result: Developed a complete technical pipeline enabling real-time interactive VR experience, successfully deployed at an art festival with audience engagement analysis.

Conclusion: The work demonstrates how AI can reinterpret traditional art forms to provoke ecological awareness through interactive experiences, with participants engaging through empathy, ambivalence, and critical reflection.

Abstract: This paper presents Negative Shanshui, a real-time interactive AI synthesis
approach that reinterprets classical Chinese landscape ink painting, i.e.,
shanshui, to engage with ecological crises in the Anthropocene. Negative
Shanshui optimizes a fine-tuned Stable Diffusion model for real-time inferences
and integrates it with gaze-driven inpainting, frame interpolation; it enables
dynamic morphing animations in response to the viewer's gaze and presents as an
interactive virtual reality (VR) experience. The paper describes the complete
technical pipeline, covering the system framework, optimization strategies,
gaze-based interaction, and multimodal deployment in an art festival. Further
analysis of audience feedback collected during its public exhibition highlights
how participants variously engaged with the work through empathy, ambivalence,
and critical reflection.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [314] [RubikSQL: Lifelong Learning Agentic Knowledge Base as an Industrial NL2SQL System](https://arxiv.org/abs/2508.17590)
*Zui Chen,Han Li,Xinhao Zhang,Xiaoyu Chen,Chunyin Dong,Yifeng Wang,Xin Cai,Su Zhang,Ziqi Li,Chi Ding,Jinxu Li,Shuai Wang,Dousheng Zhao,Sanhai Gao,Guangyi Liu*

Main category: cs.DB

TL;DR: RubikSQL is a novel NL2SQL system that treats SQL generation as a lifelong learning task, combining knowledge base maintenance with multi-agent workflow to handle enterprise-level challenges like implicit intents and domain-specific terminology.


<details>
  <summary>Details</summary>
Motivation: Address key challenges in real-world enterprise-level NL2SQL, including handling implicit intents and domain-specific terminology that traditional systems struggle with.

Method: Frames NL2SQL as lifelong learning with systematic KB building through database profiling, structured information extraction, agentic rule mining, and CoT-enhanced SQL profiling. Uses multi-agent workflow to leverage curated knowledge base.

Result: Achieves state-of-the-art performance on both KaggleDBQA and BIRD Mini-Dev datasets.

Conclusion: Introduces RubikBench benchmark specifically designed for industrial NL2SQL scenarios, providing valuable resource for future research in enterprise-level SQL generation.

Abstract: We present RubikSQL, a novel NL2SQL system designed to address key challenges
in real-world enterprise-level NL2SQL, such as implicit intents and
domain-specific terminology. RubikSQL frames NL2SQL as a lifelong learning
task, demanding both Knowledge Base (KB) maintenance and SQL generation.
RubikSQL systematically builds and refines its KB through techniques including
database profiling, structured information extraction, agentic rule mining, and
Chain-of-Thought (CoT)-enhanced SQL profiling. RubikSQL then employs a
multi-agent workflow to leverage this curated KB, generating accurate SQLs.
RubikSQL achieves SOTA performance on both the KaggleDBQA and BIRD Mini-Dev
datasets. Finally, we release the RubikBench benchmark, a new benchmark
specifically designed to capture vital traits of industrial NL2SQL scenarios,
providing a valuable resource for future research.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [315] [BrainPath: Generating Subject-Specific Brain Aging Trajectories](https://arxiv.org/abs/2508.16667)
*Yifan Li,Javad Sohankar,Ji Luo,Jing Li,Yi Su*

Main category: q-bio.NC

TL;DR: BrainPath is a 3D generative framework that predicts personalized brain aging trajectories from a single baseline MRI scan, outperforming existing methods in accuracy and capturing realistic aging patterns.


<details>
  <summary>Details</summary>
Motivation: Current approaches for brain aging prediction are limited to predicting chronological age or generating synthetic MRIs that lack subject-specific trajectories, failing to capture the heterogeneity of biological aging and neurodegenerative disease progression.

Method: BrainPath integrates an age calibration loss, swap learning strategy, and age perceptual loss to learn longitudinal brain aging dynamics and predict anatomically faithful MRIs at arbitrary timepoints from a single baseline scan.

Result: BrainPath outperforms state-of-the-art models across ADNI and NACC datasets in structural similarity (SSIM), mean squared error (MSE), peak signal-to-noise ratio (PSNR), and MRI age-difference accuracy, while maintaining temporal consistency.

Conclusion: The framework enables personalized brain aging mapping, synthetic follow-up scan prediction, and trajectory-based analyses, providing a foundation for precision modeling of brain aging and supporting neurodegeneration research and aging interventions.

Abstract: Quantifying and forecasting individual brain aging trajectories is critical
for understanding neurodegenerative disease and the heterogeneity of aging, yet
current approaches remain limited. Most models predict chronological age, an
imperfect surrogate for biological aging, or generate synthetic MRIs that
enhance data diversity but fail to capture subject-specific trajectories. Here,
we present BrainPath, a 3D generative framework that learns longitudinal brain
aging dynamics during training and, at inference, predicts anatomically
faithful MRIs at arbitrary timepoints from a single baseline scan. BrainPath
integrates an age calibration loss, a swap learning strategy, and an age
perceptual loss to preserve subtle, biologically meaningful variations. Across
held-out ADNI and an independent NACC dataset, BrainPath outperforms
state-of-the-art reference models in structural similarity (SSIM), mean squared
error (MSE), peak signal-to-noise ratio (PSNR), and MRI age-difference
accuracy, while capturing realistic and temporally consistent aging patterns.
Beyond methodological innovation, BrainPath enables personalized mapping of
brain aging, synthetic follow-up scan prediction, and trajectory-based
analyses, providing a foundation for precision modeling of brain aging and
supporting research into neurodegeneration and aging interventions.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [316] [Talking to Robots: A Practical Examination of Speech Foundation Models for HRI Applications](https://arxiv.org/abs/2508.17753)
*Theresa Pekarek Rosin,Julia Gachot,Henri-Leon Kordt,Matthias Kerzel,Stefan Wermter*

Main category: cs.RO

TL;DR: Evaluation of 4 state-of-the-art ASR systems on 8 datasets covering 6 difficulty dimensions shows significant performance variations, hallucination issues, and biases despite similar benchmark scores, with serious implications for human-robot interaction.


<details>
  <summary>Details</summary>
Motivation: ASR systems in real-world settings need to handle imperfect audio from diverse user groups, particularly in human-robot interaction where recognition errors can impact task performance, user trust, and safety.

Method: Evaluated four state-of-the-art ASR systems on eight publicly available datasets that capture six dimensions of difficulty: domain-specific, accented, noisy, age-variant, impaired, and spontaneous speech.

Result: Analysis demonstrates significant variations in performance, hallucination tendencies, and inherent biases across different ASR systems, despite similar scores on standard benchmarks.

Conclusion: The limitations identified have serious implications for HRI applications, where recognition errors can interfere with task performance, user trust, and safety, highlighting the need for more robust ASR systems.

Abstract: Automatic Speech Recognition (ASR) systems in real-world settings need to
handle imperfect audio, often degraded by hardware limitations or environmental
noise, while accommodating diverse user groups. In human-robot interaction
(HRI), these challenges intersect to create a uniquely challenging recognition
environment. We evaluate four state-of-the-art ASR systems on eight publicly
available datasets that capture six dimensions of difficulty: domain-specific,
accented, noisy, age-variant, impaired, and spontaneous speech. Our analysis
demonstrates significant variations in performance, hallucination tendencies,
and inherent biases, despite similar scores on standard benchmarks. These
limitations have serious implications for HRI, where recognition errors can
interfere with task performance, user trust, and safety.

</details>


### [317] [Optimizing Grasping in Legged Robots: A Deep Learning Approach to Loco-Manipulation](https://arxiv.org/abs/2508.17466)
*Dilermando Almeida,Guilherme Lazzarini,Juliano Negri,Thiago H. Segreto,Ricardo V. Godoy,Marcelo Becker*

Main category: cs.RO

TL;DR: Deep learning framework for quadruped robots with arms to improve grasping precision using sim-to-real training with synthetic dataset and CNN model.


<details>
  <summary>Details</summary>
Motivation: Quadruped robots with manipulator arms need precise grasping capabilities for loco-manipulation tasks, but current methods require extensive real-world calibration and pre-programmed configurations.

Method: Sim-to-real methodology using Genesis simulation to generate synthetic dataset of grasp attempts. Custom CNN with U-Net architecture processes multi-modal input (RGB, depth, segmentation, surface normal maps) to output grasp-quality heatmaps.

Result: Successfully validated on quadruped robot performing full loco-manipulation: autonomous navigation, object perception, optimal grasp prediction, and precise grasping execution.

Conclusion: Simulated training with advanced sensing provides scalable and effective solution for object handling in quadruped robots.

Abstract: Quadruped robots have emerged as highly efficient and versatile platforms,
excelling in navigating complex and unstructured terrains where traditional
wheeled robots might fail. Equipping these robots with manipulator arms unlocks
the advanced capability of loco-manipulation to perform complex physical
interaction tasks in areas ranging from industrial automation to
search-and-rescue missions. However, achieving precise and adaptable grasping
in such dynamic scenarios remains a significant challenge, often hindered by
the need for extensive real-world calibration and pre-programmed grasp
configurations. This paper introduces a deep learning framework designed to
enhance the grasping capabilities of quadrupeds equipped with arms, focusing on
improved precision and adaptability. Our approach centers on a sim-to-real
methodology that minimizes reliance on physical data collection. We developed a
pipeline within the Genesis simulation environment to generate a synthetic
dataset of grasp attempts on common objects. By simulating thousands of
interactions from various perspectives, we created pixel-wise annotated
grasp-quality maps to serve as the ground truth for our model. This dataset was
used to train a custom CNN with a U-Net-like architecture that processes
multi-modal input from an onboard RGB and depth cameras, including RGB images,
depth maps, segmentation masks, and surface normal maps. The trained model
outputs a grasp-quality heatmap to identify the optimal grasp point. We
validated the complete framework on a four-legged robot. The system
successfully executed a full loco-manipulation task: autonomously navigating to
a target object, perceiving it with its sensors, predicting the optimal grasp
pose using our model, and performing a precise grasp. This work proves that
leveraging simulated training with advanced sensing offers a scalable and
effective solution for object handling.

</details>


### [318] [GWM: Towards Scalable Gaussian World Models for Robotic Manipulation](https://arxiv.org/abs/2508.17600)
*Guanxing Lu,Baoxiong Jia,Puhao Li,Yixin Chen,Ziwei Wang,Yansong Tang,Siyuan Huang*

Main category: cs.RO

TL;DR: Gaussian World Model (GWM) uses 3D Gaussian primitives and Diffusion Transformer to predict future scenes for robotic manipulation, outperforming state-of-the-art methods in both simulation and real-world experiments.


<details>
  <summary>Details</summary>
Motivation: Existing image-based world models lack robust geometric information and consistent 3D spatial understanding needed for robotic manipulation, even when pre-trained on large video datasets.

Method: Proposes GWM with latent Diffusion Transformer (DiT) and 3D variational autoencoder that reconstructs future states by inferring Gaussian primitive propagation under robot actions, using Gaussian Splatting for fine-grained scene reconstruction.

Result: GWM precisely predicts future scenes conditioned on diverse robot actions and trains policies that outperform state-of-the-art methods by significant margins, demonstrating strong 3D world model scaling potential.

Conclusion: The Gaussian World Model successfully bridges the gap in geometric understanding for robotic manipulation, providing both enhanced visual representation for imitation learning and serving as an effective neural simulator for model-based reinforcement learning.

Abstract: Training robot policies within a learned world model is trending due to the
inefficiency of real-world interactions. The established image-based world
models and policies have shown prior success, but lack robust geometric
information that requires consistent spatial and physical understanding of the
three-dimensional world, even pre-trained on internet-scale video sources. To
this end, we propose a novel branch of world model named Gaussian World Model
(GWM) for robotic manipulation, which reconstructs the future state by
inferring the propagation of Gaussian primitives under the effect of robot
actions. At its core is a latent Diffusion Transformer (DiT) combined with a 3D
variational autoencoder, enabling fine-grained scene-level future state
reconstruction with Gaussian Splatting. GWM can not only enhance the visual
representation for imitation learning agent by self-supervised future
prediction training, but can serve as a neural simulator that supports
model-based reinforcement learning. Both simulated and real-world experiments
depict that GWM can precisely predict future scenes conditioned on diverse
robot actions, and can be further utilized to train policies that outperform
the state-of-the-art by impressive margins, showcasing the initial data scaling
potential of 3D world model.

</details>


### [319] [SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulation](https://arxiv.org/abs/2508.17643)
*Krishna Vinod,Prithvi Jai Ramesh,Pavan Kumar B N,Bharatesh Chakravarthi*

Main category: cs.RO

TL;DR: Open-source ROS package for Gazebo that generates synthetic event streams from RGB cameras to enable event-based robotic policy evaluation in navigation and manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Event cameras offer superior latency, dynamic range and power efficiency but lack simulation support in mainstream robotics platforms, hindering evaluation of event-driven approaches.

Method: Developed v2e ROS package for Gazebo that converts RGB camera feeds to event streams. Trained Transformer-based event robotic policies via behavior cloning and compared against RGB-based policies in mobile robot following and manipulator grasping tasks.

Result: Event-guided policies consistently outperformed RGB-based counterparts across various operating conditions, demonstrating competitive advantages in real-time navigation and manipulation.

Conclusion: Event-driven perception shows strong potential to improve robotic policy learning, providing foundation for broader integration of event cameras into robotics systems.

Abstract: Event cameras offer microsecond latency, high dynamic range, and low power
consumption, making them ideal for real-time robotic perception under
challenging conditions such as motion blur, occlusion, and illumination
changes. However, despite their advantages, synthetic event-based vision
remains largely unexplored in mainstream robotics simulators. This lack of
simulation setup hinders the evaluation of event-driven approaches for robotic
manipulation and navigation tasks. This work presents an open-source,
user-friendly v2e robotics operating system (ROS) package for Gazebo simulation
that enables seamless event stream generation from RGB camera feeds. The
package is used to investigate event-based robotic policies (ERP) for real-time
navigation and manipulation. Two representative scenarios are evaluated: (1)
object following with a mobile robot and (2) object detection and grasping with
a robotic manipulator. Transformer-based ERPs are trained by behavior cloning
and compared to RGB-based counterparts under various operating conditions.
Experimental results show that event-guided policies consistently deliver
competitive advantages. The results highlight the potential of event-driven
perception to improve real-time robotic navigation and manipulation, providing
a foundation for broader integration of event cameras into robotic policy
learning. The GitHub repo for the dataset and code:
https://eventbasedvision.github.io/SEBVS/

</details>


### [320] [Egocentric Instruction-oriented Affordance Prediction via Large Multimodal Model](https://arxiv.org/abs/2508.17922)
*Bokai Ji,Jie Gu,Xiaokang Ma,Chu Tang,Jingmin Chen,Guangxia Li*

Main category: cs.RO

TL;DR: This paper introduces task-dependent affordance prediction, where manipulation regions and directions vary based on instructions for the same object. The authors create a dataset of 15K object-instruction-affordance triplets and develop a "search against verifiers" pipeline using large multimodal models to predict affordances through iterative reasoning.


<details>
  <summary>Details</summary>
Motivation: Current affordance prediction approaches overlook that affordance should be task-/instruction-dependent, meaning different instructions should lead to different manipulation regions and directions even for the same object.

Method: Created a dataset of 15,000 object-instruction-affordance triplets from egocentric viewpoints. Implemented a "search against verifiers" pipeline where large multimodal models progressively predict affordances with iterative self-verification, mimicking a reasoning process.

Result: The method successfully unlocks new instruction-oriented affordance prediction capabilities and achieves outstanding performance broadly across experiments.

Conclusion: The proposed approach demonstrates that affordance prediction should be instruction-dependent and that large multimodal models can effectively serve as affordance predictors through iterative reasoning processes with self-verification.

Abstract: Affordance is crucial for intelligent robots in the context of object
manipulation. In this paper, we argue that affordance should be
task-/instruction-dependent, which is overlooked by many previous works. That
is, different instructions can lead to different manipulation regions and
directions even for the same object. According to this observation, we present
a new dataset comprising fifteen thousand object-instruction-affordance
triplets. All scenes in the dataset are from an egocentric viewpoint, designed
to approximate the perspective of a human-like robot. Furthermore, we
investigate how to enable large multimodal models (LMMs) to serve as affordance
predictors by implementing a ``search against verifiers'' pipeline. An LMM is
asked to progressively predict affordances, with the output at each step being
verified by itself during the iterative process, imitating a reasoning process.
Experiments show that our method not only unlocks new instruction-oriented
affordance prediction capabilities, but also achieves outstanding performance
broadly.

</details>


### [321] [A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigm](https://arxiv.org/abs/2508.17969)
*Alexandros Gkillas,Christos Anagnostopoulos,Nikos Piperigkos,Dimitris Tsiktsiris,Theofilos Christodoulou,Theofanis Siamatras,Dimitrios Triantafyllou,Christos Basdekis,Theoktisti Marinopoulou,Panagiotis Lepentsiotis,Elefterios Blitsis,Aggeliki Zacharaki,Nearchos Stylianidis,Leonidas Katelaris,Lamberto Salvan,Aris S. Lalos,Christos Laoudias,Antonios Lalas,Konstantinos Votis*

Main category: cs.RO

TL;DR: A holistic perception system for autonomous vehicles combining internal monitoring (driver behavior, air quality, thermal comfort) and external monitoring (LiDAR-based semantic segmentation) using AI and multi-sensor fusion.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive AI-powered framework that optimizes both internal cabin experience and external environmental perception for autonomous vehicles, addressing both occupant comfort and safety.

Method: Uses multi-camera setup with facial recognition and LLM virtual assistant for internal monitoring, AI sensors for air quality/thermal analysis, and LiDAR-based semantic segmentation with super-resolution for external environment perception.

Result: Successfully integrated and deployed on a real electric vehicle, with experimental validation showing increased performance and efficiency of the modular perception architecture components.

Conclusion: The proposed holistic framework demonstrates effective AI-leveraged self-adaptive capabilities for both internal and external vehicle monitoring, providing optimized perception and experience for autonomous driving systems.

Abstract: This paper introduces a holistic perception system for internal and external
monitoring of autonomous vehicles, with the aim of demonstrating a novel
AI-leveraged self-adaptive framework of advanced vehicle technologies and
solutions that optimize perception and experience on-board. Internal monitoring
system relies on a multi-camera setup designed for predicting and identifying
driver and occupant behavior through facial recognition, exploiting in addition
a large language model as virtual assistant. Moreover, the in-cabin monitoring
system includes AI-empowered smart sensors that measure air-quality and perform
thermal comfort analysis for efficient on and off-boarding. On the other hand,
external monitoring system perceives the surrounding environment of vehicle,
through a LiDAR-based cost-efficient semantic segmentation approach, that
performs highly accurate and efficient super-resolution on low-quality raw 3D
point clouds. The holistic perception framework is developed in the context of
EU's Horizon Europe programm AutoTRUST, and has been integrated and deployed on
a real electric vehicle provided by ALKE. Experimental validation and
evaluation at the integration site of Joint Research Centre at Ispra, Italy,
highlights increased performance and efficiency of the modular blocks of the
proposed perception architecture.

</details>


### [322] [Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework](https://arxiv.org/abs/2508.18249)
*Zipeng Fang,Yanbo Wang,Lei Zhao,Weidong Chen*

Main category: cs.RO

TL;DR: Multimodal self-supervised framework for traversability estimation using footprint, LiDAR, and camera data with vision foundation models, achieving 88% IoU and 1.6-3.5% improvement over SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Existing self-supervised methods fail to capture non-traversable regions and overlook complementary strengths of heterogeneous sensory modalities for robust traversability estimation.

Method: Multimodal annotation pipeline integrating footprint, LiDAR, and camera data with vision foundation model, followed by dual-stream network training with sparse LiDAR supervision to mitigate pseudo-label noise.

Result: Automatic labeling achieves ~88% IoU across diverse datasets. Multimodal network yields consistently higher IoU, improving by 1.6-3.5% on all evaluated datasets compared to existing self-supervised SOTA methods.

Conclusion: The proposed multimodal framework effectively addresses limitations of single-modality approaches and captures diverse traversability patterns through integrated sensory data and foundation models.

Abstract: Traversability estimation is critical for enabling robots to navigate across
diverse terrains and environments. While recent self-supervised learning
methods achieve promising results, they often fail to capture the
characteristics of non-traversable regions. Moreover, most prior works
concentrate on a single modality, overlooking the complementary strengths
offered by integrating heterogeneous sensory modalities for more robust
traversability estimation. To address these limitations, we propose a
multimodal self-supervised framework for traversability labeling and
estimation. First, our annotation pipeline integrates footprint, LiDAR, and
camera data as prompts for a vision foundation model, generating traversability
labels that account for both semantic and geometric cues. Then, leveraging
these labels, we train a dual-stream network that jointly learns from different
modalities in a decoupled manner, enhancing its capacity to recognize diverse
traversability patterns. In addition, we incorporate sparse LiDAR-based
supervision to mitigate the noise introduced by pseudo labels. Finally,
extensive experiments conducted across urban, off-road, and campus environments
demonstrate the effectiveness of our approach. The proposed automatic labeling
method consistently achieves around 88% IoU across diverse datasets. Compared
to existing self-supervised state-of-the-art methods, our multimodal
traversability estimation network yields consistently higher IoU, improving by
1.6-3.5% on all evaluated datasets.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [323] [Predicting brain tumour enhancement from non-contrast MR imaging with artificial intelligence](https://arxiv.org/abs/2508.16650)
*James K Ruffle,Samia Mohinta,Guilherme Pombo,Asthik Biswas,Alan Campbell,Indran Davagnanam,David Doig,Ahmed Hamman,Harpreet Hyare,Farrah Jabeen,Emma Lim,Dermot Mallon,Stephanie Owen,Sophie Wilkinson,Sebastian Brandner,Parashkev Nachev*

Main category: eess.IV

TL;DR: Deep learning model predicts brain tumor contrast enhancement from non-contrast MRI alone, achieving 83% accuracy and outperforming expert radiologists.


<details>
  <summary>Details</summary>
Motivation: Gadolinium contrast administration in MRI is not always desirable due to frequent follow-up needs, renal impairment, allergies, or pediatric patients, creating a need for alternative methods.

Method: Trained deep learning models (nnU-Net, SegResNet, SwinUNETR) on 11,089 brain MRI studies from 10 international datasets to predict and segment enhancing tumor using only non-contrast T1-, T2-, and T2/FLAIR-weighted images.

Result: Best-performing nnU-Net achieved 83% balanced accuracy, 91.5% sensitivity, 74.4% specificity, outperforming expert radiologists (69.8% accuracy). Enhancement volume predictions strongly correlated with ground truth (RÂ² 0.859).

Conclusion: Deep learning can identify contrast-enhancing brain tumors from non-contrast MRI with clinically relevant performance, showing promise as screening tools to reduce gadolinium dependence in neuro-oncology imaging.

Abstract: Brain tumour imaging assessment typically requires both pre- and
post-contrast MRI, but gadolinium administration is not always desirable, such
as in frequent follow-up, renal impairment, allergy, or paediatric patients. We
aimed to develop and validate a deep learning model capable of predicting brain
tumour contrast enhancement from non-contrast MRI sequences alone. We assembled
11089 brain MRI studies from 10 international datasets spanning adult and
paediatric populations with various neuro-oncological states, including glioma,
meningioma, metastases, and post-resection appearances. Deep learning models
(nnU-Net, SegResNet, SwinUNETR) were trained to predict and segment enhancing
tumour using only non-contrast T1-, T2-, and T2/FLAIR-weighted images.
Performance was evaluated on 1109 held-out test patients using patient-level
detection metrics and voxel-level segmentation accuracy. Model predictions were
compared against 11 expert radiologists who each reviewed 100 randomly selected
patients. The best-performing nnU-Net achieved 83% balanced accuracy, 91.5%
sensitivity, and 74.4% specificity in detecting enhancing tumour. Enhancement
volume predictions strongly correlated with ground truth (R2 0.859). The model
outperformed expert radiologists, who achieved 69.8% accuracy, 75.9%
sensitivity, and 64.7% specificity. 76.8% of test patients had Dice over 0.3
(acceptable detection), 67.5% had Dice over 0.5 (good detection), and 50.2% had
Dice over 0.7 (excellent detection). Deep learning can identify
contrast-enhancing brain tumours from non-contrast MRI with clinically relevant
performance. These models show promise as screening tools and may reduce
gadolinium dependence in neuro-oncology imaging. Future work should evaluate
clinical utility alongside radiology experts.

</details>


### [324] [Analysis of Transferability Estimation Metrics for Surgical Phase Recognition](https://arxiv.org/abs/2508.16730)
*Prabhant Singh,Yiping Li,Yasmina Al Khalil*

Main category: eess.IV

TL;DR: This paper benchmarks transferability estimation metrics (LogME, H-Score, TransRate) for surgical phase recognition, finding LogME with minimum per-subset aggregation aligns best with fine-tuning accuracy.


<details>
  <summary>Details</summary>
Motivation: Expert annotations in surgical video analysis are time-consuming and costly, making it critical to identify the most suitable pre-trained model for downstream tasks without full retraining.

Method: Comprehensive benchmark of three transferability estimation metrics (LogME, H-Score, TransRate) on two surgical datasets (RAMIE and AutoLaparo), using source-independent transferability estimation approach.

Result: LogME with minimum per-subset aggregation aligns most closely with fine-tuning accuracy; H-Score has weak predictive power; TransRate often inverses true model rankings. Transferability estimates lose discriminative power when candidate models have similar performances.

Conclusion: Provides practical guidelines for model selection and outlines future directions for domain-specific metrics, theoretical foundations, and interactive benchmarking tools in surgical video analysis.

Abstract: Fine-tuning pre-trained models has become a cornerstone of modern machine
learning, allowing practitioners to achieve high performance with limited
labeled data. In surgical video analysis, where expert annotations are
especially time-consuming and costly, identifying the most suitable pre-trained
model for a downstream task is both critical and challenging.
Source-independent transferability estimation (SITE) offers a solution by
predicting how well a model will fine-tune on target data using only its
embeddings or outputs, without requiring full retraining. In this work, we
formalize SITE for surgical phase recognition and provide the first
comprehensive benchmark of three representative metrics, LogME, H-Score, and
TransRate, on two diverse datasets (RAMIE and AutoLaparo). Our results show
that LogME, particularly when aggregated by the minimum per-subset score,
aligns most closely with fine-tuning accuracy; H-Score yields only weak
predictive power; and TransRate often inverses true model rankings. Ablation
studies show that when candidate models have similar performances,
transferability estimates lose discriminative power, emphasizing the importance
of maintaining model diversity or using additional validation. We conclude with
practical guidelines for model selection and outline future directions toward
domain-specific metrics, theoretical foundations, and interactive benchmarking
tools.

</details>


### [325] [Multimodal Medical Endoscopic Image Analysis via Progressive Disentangle-aware Contrastive Learning](https://arxiv.org/abs/2508.16882)
*Junhao Wu,Yun Li,Junhao Li,Jingliang Bian,Xiaomao Fan,Wenbin Lei,Ruxin Wang*

Main category: eess.IV

TL;DR: A multi-modality framework using Align-Disentangle-Fusion mechanism integrates 2D WLI and NBI imaging to improve laryngo-pharyngeal tumor segmentation through multi-scale distribution alignment and progressive feature disentanglement.


<details>
  <summary>Details</summary>
Motivation: Traditional single-modality imaging methods fail to capture complex anatomical and pathological features of laryngo-pharyngeal tumors, necessitating better multimodal integration for accurate segmentation.

Method: Align-Disentangle-Fusion framework with multi-scale distribution alignment across transformer layers, progressive feature disentanglement strategy including preliminary disentanglement and disentangle-aware contrastive learning to separate modality-specific and shared features.

Result: The method consistently outperforms state-of-the-art approaches across multiple datasets, achieving superior accuracy in diverse real clinical scenarios.

Conclusion: The proposed multi-modality representation learning framework effectively integrates 2D WLI and NBI imaging pairs, demonstrating significant improvements in laryngo-pharyngeal tumor segmentation performance for clinical applications.

Abstract: Accurate segmentation of laryngo-pharyngeal tumors is crucial for precise
diagnosis and effective treatment planning. However, traditional
single-modality imaging methods often fall short of capturing the complex
anatomical and pathological features of these tumors. In this study, we present
an innovative multi-modality representation learning framework based on the
`Align-Disentangle-Fusion' mechanism that seamlessly integrates 2D White Light
Imaging (WLI) and Narrow Band Imaging (NBI) pairs to enhance segmentation
performance. A cornerstone of our approach is multi-scale distribution
alignment, which mitigates modality discrepancies by aligning features across
multiple transformer layers. Furthermore, a progressive feature disentanglement
strategy is developed with the designed preliminary disentanglement and
disentangle-aware contrastive learning to effectively separate
modality-specific and shared features, enabling robust multimodal contrastive
learning and efficient semantic fusion. Comprehensive experiments on multiple
datasets demonstrate that our method consistently outperforms state-of-the-art
approaches, achieving superior accuracy across diverse real clinical scenarios.

</details>


### [326] [Generating Synthetic Contrast-Enhanced Chest CT Images from Non-Contrast Scans Using Slice-Consistent Brownian Bridge Diffusion Network](https://arxiv.org/abs/2508.16897)
*Pouya Shiri,Xin Yi,Neel P. Mistry,Samaneh Javadinia,Mohammad Chegini,Seok-Bum Ko,Amirali Baniasadi,Scott J. Adams*

Main category: eess.IV

TL;DR: First bridge diffusion model for generating synthetic contrast-enhanced CT angiography from non-contrast CT scans, using slice-consistent Brownian Bridge Diffusion to maintain 3D anatomical integrity while operating in 2D.


<details>
  <summary>Details</summary>
Motivation: Contrast agents in CT imaging pose risks like nephrotoxicity and allergic reactions. Synthetic contrast-enhanced imaging would improve patient safety, accessibility, and reduce healthcare costs.

Method: Slice-Consistent Brownian Bridge Diffusion Model (SC-BBDM) with comprehensive preprocessing including resampling, Symmetric Normalization registration, and dilated segmentation masks. Tested on two datasets from Coltea-Lung dataset (aorta-only and aorta+heart).

Result: Demonstrated effectiveness in preserving vascular structures while enhancing contrast fidelity, outperforming baseline methods on both datasets.

Conclusion: The proposed diffusion-based approach successfully generates high-fidelity synthetic contrast-enhanced CTA images without actual contrast administration, maintaining 3D anatomical integrity with low memory requirements.

Abstract: Contrast-enhanced computed tomography (CT) imaging is essential for
diagnosing and monitoring thoracic diseases, including aortic pathologies.
However, contrast agents pose risks such as nephrotoxicity and allergic-like
reactions. The ability to generate high-fidelity synthetic contrast-enhanced CT
angiography (CTA) images without contrast administration would be
transformative, enhancing patient safety and accessibility while reducing
healthcare costs. In this study, we propose the first bridge diffusion-based
solution for synthesizing contrast-enhanced CTA images from non-contrast CT
scans. Our approach builds on the Slice-Consistent Brownian Bridge Diffusion
Model (SC-BBDM), leveraging its ability to model complex mappings while
maintaining consistency across slices. Unlike conventional slice-wise synthesis
methods, our framework preserves full 3D anatomical integrity while operating
in a high-resolution 2D fashion, allowing seamless volumetric interpretation
under a low memory budget. To ensure robust spatial alignment, we implement a
comprehensive preprocessing pipeline that includes resampling, registration
using the Symmetric Normalization method, and a sophisticated dilated
segmentation mask to extract the aorta and surrounding structures. We create
two datasets from the Coltea-Lung dataset: one containing only the aorta and
another including both the aorta and heart, enabling a detailed analysis of
anatomical context. We compare our approach against baseline methods on both
datasets, demonstrating its effectiveness in preserving vascular structures
while enhancing contrast fidelity.

</details>


### [327] [Deep Learning Architectures for Medical Image Denoising: A Comparative Study of CNN-DAE, CADTra, and DCMIEDNet](https://arxiv.org/abs/2508.17223)
*Asadullah Bin Rahman,Masud Ibn Afjal,Md. Abdulla Al Mamun*

Main category: eess.IV

TL;DR: Comparative evaluation of three deep learning models (CNN-DAE, CADTra, DCMIEDNet) for MRI brain image denoising shows DCMIEDNet excels at lower noise levels while CADTra is more robust under severe noise conditions, with both significantly outperforming traditional wavelet methods.


<details>
  <summary>Details</summary>
Motivation: Medical imaging modalities are inherently susceptible to noise contamination that degrades diagnostic utility and clinical assessment accuracy, necessitating effective denoising methods.

Method: Systematic evaluation of three deep learning architectures (CNN-DAE, CADTra, DCMIEDNet) across multiple Gaussian noise intensities (Ï=10,15,25) using the Figshare MRI Brain Dataset, comparing against traditional wavelet-based methods.

Result: DCMIEDNet achieved superior performance at lower noise levels (PSNR: 32.921Â±2.350 dB for Ï=10, 30.943Â±2.339 dB for Ï=15), while CADTra showed greater robustness under severe noise (PSNR: 27.671Â±2.091 dB for Ï=25). All deep learning approaches significantly outperformed traditional methods by 5-8 dB.

Conclusion: The study establishes quantitative benchmarks for medical image denoising and provides insights into architecture-specific strengths for varying noise intensities, demonstrating the superiority of deep learning approaches over traditional methods.

Abstract: Medical imaging modalities are inherently susceptible to noise contamination
that degrades diagnostic utility and clinical assessment accuracy. This paper
presents a comprehensive comparative evaluation of three state-of-the-art deep
learning architectures for MRI brain image denoising: CNN-DAE, CADTra, and
DCMIEDNet. We systematically evaluate these models across multiple Gaussian
noise intensities ($\sigma = 10, 15, 25$) using the Figshare MRI Brain Dataset.
Our experimental results demonstrate that DCMIEDNet achieves superior
performance at lower noise levels, with PSNR values of $32.921 \pm 2.350$ dB
and $30.943 \pm 2.339$ dB for $\sigma = 10$ and $15$ respectively. However,
CADTra exhibits greater robustness under severe noise conditions ($\sigma =
25$), achieving the highest PSNR of $27.671 \pm 2.091$ dB. All deep learning
approaches significantly outperform traditional wavelet-based methods, with
improvements ranging from 5-8 dB across tested conditions. This study
establishes quantitative benchmarks for medical image denoising and provides
insights into architecture-specific strengths for varying noise intensities.

</details>


### [328] [Semantic Diffusion Posterior Sampling for Cardiac Ultrasound Dehazing](https://arxiv.org/abs/2508.17326)
*Tristan S. W. Stevens,OisÃ­n Nolan,Ruud J. G. van Sloun*

Main category: eess.IV

TL;DR: A semantic-guided diffusion-based algorithm for echocardiography dehazing that uses semantic segmentation and diffusion posterior sampling to remove haze from ultrasound images.


<details>
  <summary>Details</summary>
Motivation: Echocardiography image quality is degraded by haze from multipath reverberations, especially in difficult-to-image patients, which affects diagnosis and monitoring.

Method: Integrates pixel-wise noise model from semantic segmentation of hazy inputs into diffusion posterior sampling framework guided by generative prior trained on clean ultrasound data.

Result: Quantitative evaluation shows strong performance across contrast and fidelity metrics on the challenge dataset.

Conclusion: The proposed semantic-guided diffusion approach effectively removes haze from echocardiography images, improving image quality for cardiac diagnosis.

Abstract: Echocardiography plays a central role in cardiac imaging, offering dynamic
views of the heart that are essential for diagnosis and monitoring. However,
image quality can be significantly degraded by haze arising from multipath
reverberations, particularly in difficult-to-image patients. In this work, we
propose a semantic-guided, diffusion-based dehazing algorithm developed for the
MICCAI Dehazing Echocardiography Challenge (DehazingEcho2025). Our method
integrates a pixel-wise noise model, derived from semantic segmentation of hazy
inputs into a diffusion posterior sampling framework guided by a generative
prior trained on clean ultrasound data. Quantitative evaluation on the
challenge dataset demonstrates strong performance across contrast and fidelity
metrics. Code for the submitted algorithm is available at
https://github.com/tristan-deep/semantic-diffusion-echo-dehazing.

</details>


### [329] [Towards Trustworthy Breast Tumor Segmentation in Ultrasound using Monte Carlo Dropout and Deep Ensembles for Epistemic Uncertainty Estimation](https://arxiv.org/abs/2508.17768)
*Toufiq Musah,Chinasa Kalaiwo,Maimoona Akram,Ubaida Napari Abdulai,Maruf Adewole,Farouk Dako,Adaobi Chiazor Emegoakor,Udunna C. Anazodo,Prince Ebenezer Adjei,Confidence Raymond*

Main category: eess.IV

TL;DR: Modified ResNet U-Net for breast ultrasound segmentation with uncertainty quantification, addressing dataset duplication issues and demonstrating state-of-the-art performance with reliable uncertainty estimates.


<details>
  <summary>Details</summary>
Motivation: Automated segmentation of breast ultrasound images is crucial for precise lesion delineation but faces challenges from inherent artifacts and dataset inconsistencies, requiring reliable uncertainty quantification for clinical trustworthiness.

Method: Used modified Residual Encoder U-Net with Monte Carlo dropout, deep ensembles, and their combination for uncertainty quantification. Identified and corrected data duplication in BUSI dataset, using deduplicated subset for evaluation. Benchmarked on both in-distribution and out-of-distribution datasets.

Result: Achieved state-of-the-art segmentation accuracy on Breast-Lesion-USG dataset with in-distribution validation. Provided calibrated uncertainty estimates that effectively signal low confidence regions. Performance declined with increased uncertainty in out-of-distribution evaluation.

Conclusion: The approach demonstrates effective uncertainty modeling for trustworthy clinical deployment, but highlights the persistent challenge of domain shift in medical imaging and the importance of integrated uncertainty quantification.

Abstract: Automated segmentation of BUS images is important for precise lesion
delineation and tumor characterization, but is challenged by inherent artifacts
and dataset inconsistencies. In this work, we evaluate the use of a modified
Residual Encoder U-Net for breast ultrasound segmentation, with a focus on
uncertainty quantification. We identify and correct for data duplication in the
BUSI dataset, and use a deduplicated subset for more reliable estimates of
generalization performance. Epistemic uncertainty is quantified using Monte
Carlo dropout, deep ensembles, and their combination. Models are benchmarked on
both in-distribution and out-of-distribution datasets to demonstrate how they
generalize to unseen cross-domain data. Our approach achieves state-of-the-art
segmentation accuracy on the Breast-Lesion-USG dataset with in-distribution
validation, and provides calibrated uncertainty estimates that effectively
signal regions of low model confidence. Performance declines and increased
uncertainty observed in out-of-distribution evaluation highlight the persistent
challenge of domain shift in medical imaging, and the importance of integrated
uncertainty modeling for trustworthy clinical deployment. \footnote{Code
available at: https://github.com/toufiqmusah/nn-uncertainty.git}

</details>


### [330] [TuningIQA: Fine-Grained Blind Image Quality Assessment for Livestreaming Camera Tuning](https://arxiv.org/abs/2508.17965)
*Xiangfei Sheng,Zhichao Duan,Xiaofeng Pan,Yipo Huang,Zhichao Yang,Pengfei Chen,Leida Li*

Main category: eess.IV

TL;DR: New fine-grained BIQA database FGLive-10K and TuningIQA metric for livestreaming camera tuning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing BIQA models only provide coarse quality scores, lacking fine-grained perceptual guidance needed for precise camera parameter tuning in livestreaming applications.

Method: Established FGLive-10K database with 10,185 high-res images and 50,925 multi-attribute annotations, then developed TuningIQA with human-aware feature extraction and graph-based parameter fusion.

Result: TuningIQA significantly outperforms state-of-the-art BIQA methods in both score regression and fine-grained quality ranking, achieving superior performance for camera tuning.

Conclusion: The proposed fine-grained BIQA approach effectively bridges the gap for precise camera parameter optimization in livestreaming, delivering superior user QoE through accurate quality assessment.

Abstract: Livestreaming has become increasingly prevalent in modern visual
communication, where automatic camera quality tuning is essential for
delivering superior user Quality of Experience (QoE). Such tuning requires
accurate blind image quality assessment (BIQA) to guide parameter optimization
decisions. Unfortunately, the existing BIQA models typically only predict an
overall coarse-grained quality score, which cannot provide fine-grained
perceptual guidance for precise camera parameter tuning. To bridge this gap, we
first establish FGLive-10K, a comprehensive fine-grained BIQA database
containing 10,185 high-resolution images captured under varying camera
parameter configurations across diverse livestreaming scenarios. The dataset
features 50,925 multi-attribute quality annotations and 19,234 fine-grained
pairwise preference annotations. Based on FGLive-10K, we further develop
TuningIQA, a fine-grained BIQA metric for livestreaming camera tuning, which
integrates human-aware feature extraction and graph-based camera parameter
fusion. Extensive experiments and comparisons demonstrate that TuningIQA
significantly outperforms state-of-the-art BIQA methods in both score
regression and fine-grained quality ranking, achieving superior performance
when deployed for livestreaming camera tuning.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [331] [MDD: A Dataset for Text-and-Music Conditioned Duet Dance Generation](https://arxiv.org/abs/2508.16911)
*Prerit Gupta,Jason Alexander Fotso-Puepi,Zhengyuan Li,Jay Mehta,Aniket Bera*

Main category: cs.GR

TL;DR: MDD is a multimodal benchmark dataset for text-controlled and music-conditioned 3D duet dance generation, featuring 620 minutes of motion capture data with 10K+ text descriptions, supporting two novel generation tasks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive datasets that integrate human motions, music, and text descriptions specifically for duet dance generation, enabling research in multimodal dance synthesis.

Method: Created a dataset with professional motion capture data synchronized with music and detailed natural language annotations describing spatial relationships, body movements, and rhythm. Introduced two tasks: Text-to-Duet and Text-to-Dance Accompaniment.

Result: A diverse multimodal benchmark with 620 minutes of high-quality motion data, 10K+ fine-grained text descriptions, and baseline evaluations for both proposed generation tasks.

Conclusion: MDD is the first dataset to seamlessly integrate motions, music, and text for duet dance generation, providing a foundation for future research in text-controlled and music-conditioned 3D dance synthesis.

Abstract: We introduce Multimodal DuetDance (MDD), a diverse multimodal benchmark
dataset designed for text-controlled and music-conditioned 3D duet dance motion
generation. Our dataset comprises 620 minutes of high-quality motion capture
data performed by professional dancers, synchronized with music, and detailed
with over 10K fine-grained natural language descriptions. The annotations
capture a rich movement vocabulary, detailing spatial relationships, body
movements, and rhythm, making MDD the first dataset to seamlessly integrate
human motions, music, and text for duet dance generation. We introduce two
novel tasks supported by our dataset: (1) Text-to-Duet, where given music and a
textual prompt, both the leader and follower dance motion are generated (2)
Text-to-Dance Accompaniment, where given music, textual prompt, and the
leader's motion, the follower's motion is generated in a cohesive, text-aligned
manner. We include baseline evaluations on both tasks to support future
research.

</details>


### [332] [A Survey of Deep Learning-based Point Cloud Denoising](https://arxiv.org/abs/2508.17011)
*Jinxi Wang,Ben Fei,Dasith de Silva Edirimuni,Zheng Liu,Ying He,Xuequan Lu*

Main category: cs.GR

TL;DR: A comprehensive survey of deep learning-based point cloud denoising methods, categorizing approaches by supervision level and modeling perspective, with unified benchmarking and analysis of current challenges.


<details>
  <summary>Details</summary>
Motivation: Raw point clouds in real-world applications are often corrupted by noise from various sources, reducing geometric fidelity and degrading downstream performance. Traditional optimization methods struggle with complex noise patterns, while deep learning approaches show promise but need systematic review.

Method: Organizes literature from two perspectives: (1) supervision level (supervised vs. unsupervised), and (2) modeling perspective with functional taxonomy. Establishes unified benchmark with consistent training settings and evaluates methods across denoising quality, surface fidelity, point distribution, and computational efficiency.

Result: Provides comprehensive review of deep learning methods up to August 2025, analyzing architectural trends structurally and chronologically, and benchmarking various approaches.

Conclusion: Identifies open challenges and outlines future research directions in the rapidly evolving field of point cloud denoising using deep learning techniques.

Abstract: Accurate 3D geometry acquisition is essential for a wide range of
applications, such as computer graphics, autonomous driving, robotics, and
augmented reality. However, raw point clouds acquired in real-world
environments are often corrupted with noise due to various factors such as
sensor, lighting, material, environment etc, which reduces geometric fidelity
and degrades downstream performance. Point cloud denoising is a fundamental
problem, aiming to recover clean point sets while preserving underlying
structures. Classical optimization-based methods, guided by hand-crafted
filters or geometric priors, have been extensively studied but struggle to
handle diverse and complex noise patterns. Recent deep learning approaches
leverage neural network architectures to learn distinctive representations and
demonstrate strong outcomes, particularly on complex and large-scale point
clouds. Provided these significant advances, this survey provides a
comprehensive and up-to-date review of deep learning-based point cloud
denoising methods up to August 2025. We organize the literature from two
perspectives: (1) supervision level (supervised vs. unsupervised), and (2)
modeling perspective, proposing a functional taxonomy that unifies diverse
approaches by their denoising principles. We further analyze architectural
trends both structurally and chronologically, establish a unified benchmark
with consistent training settings, and evaluate methods in terms of denoising
quality, surface fidelity, point distribution, and computational efficiency.
Finally, we discuss open challenges and outline directions for future research
in this rapidly evolving field.

</details>


### [333] [DanceEditor: Towards Iterative Editable Music-driven Dance Generation with Open-Vocabulary Descriptions](https://arxiv.org/abs/2508.17342)
*Hengyuan Zhang,Zhe Li,Xingqun Qi,Mengze Li,Muyi Sun,Man Zhang,Sirui Han*

Main category: cs.GR

TL;DR: DanceEditor is a novel framework for iterative and editable dance generation from music, featuring a prediction-then-editing paradigm with a Cross-modality Editing Module that integrates music and text prompts for coherent dance synthesis.


<details>
  <summary>Details</summary>
Motivation: Existing dance synthesis methods fail to support practical editing capabilities needed for real-world choreography scenarios, and there's a lack of high-quality datasets that incorporate iterative editing processes.

Method: Built on a prediction-then-editing paradigm, the framework first models dance movements from aligned music, then uses a Cross-modality Editing Module (CEM) to integrate text descriptions as conditioning information for iterative editing while maintaining music alignment.

Result: The method outperforms state-of-the-art models on the newly collected DanceRemix dataset (25.3M dance frames, 84.5K pairs) and generates dance motions that are both musically rhythmic and semantically aligned with text descriptions.

Conclusion: DanceEditor successfully enables iterative and editable dance generation that coherently aligns with music signals while preserving fine-grained semantic alignment with text descriptions, addressing practical choreography needs.

Abstract: Generating coherent and diverse human dances from music signals has gained
tremendous progress in animating virtual avatars. While existing methods
support direct dance synthesis, they fail to recognize that enabling users to
edit dance movements is far more practical in real-world choreography
scenarios. Moreover, the lack of high-quality dance datasets incorporating
iterative editing also limits addressing this challenge. To achieve this goal,
we first construct DanceRemix, a large-scale multi-turn editable dance dataset
comprising the prompt featuring over 25.3M dance frames and 84.5K pairs. In
addition, we propose a novel framework for iterative and editable dance
generation coherently aligned with given music signals, namely DanceEditor.
Considering the dance motion should be both musical rhythmic and enable
iterative editing by user descriptions, our framework is built upon a
prediction-then-editing paradigm unifying multi-modal conditions. At the
initial prediction stage, our framework improves the authority of generated
results by directly modeling dance movements from tailored, aligned music.
Moreover, at the subsequent iterative editing stages, we incorporate text
descriptions as conditioning information to draw the editable results through a
specifically designed Cross-modality Editing Module (CEM). Specifically, CEM
adaptively integrates the initial prediction with music and text prompts as
temporal motion cues to guide the synthesized sequences. Thereby, the results
display music harmonics while preserving fine-grained semantic alignment with
text descriptions. Extensive experiments demonstrate that our method
outperforms the state-of-the-art models on our newly collected DanceRemix
dataset. Code is available at https://lzvsdy.github.io/DanceEditor/.

</details>


### [334] [MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting](https://arxiv.org/abs/2508.17811)
*Hanzhi Chang,Ruijie Zhu,Wenjie Chang,Mulin Yu,Yanzhe Liang,Jiahao Lu,Zhuoyuan Li,Tianzhu Zhang*

Main category: cs.GR

TL;DR: MeshSplat is a novel framework for sparse-view surface reconstruction that uses 2D Gaussian Splatting as a bridge between novel view synthesis and geometric priors, achieving state-of-the-art performance without requiring direct 3D ground-truth supervision.


<details>
  <summary>Details</summary>
Motivation: Existing surface reconstruction methods struggle with accurate geometry recovery when input views are extremely sparse, creating a need for more effective sparse-view reconstruction techniques.

Method: Uses 2DGS as a bridge connecting novel view synthesis to learned geometric priors. Incorporates a feed-forward network for per-view pixel-aligned 2DGS prediction, Weighted Chamfer Distance Loss for depth regularization, and a normal prediction network to align 2DGS orientation with monocular normal estimates.

Result: Extensive experiments demonstrate state-of-the-art performance in generalizable sparse-view mesh reconstruction tasks, with improved accuracy in 2DGS position and orientation prediction.

Conclusion: MeshSplat effectively addresses sparse-view surface reconstruction challenges by leveraging 2DGS as an intermediate representation, eliminating the need for direct 3D supervision while achieving superior reconstruction quality.

Abstract: Surface reconstruction has been widely studied in computer vision and
graphics. However, existing surface reconstruction works struggle to recover
accurate scene geometry when the input views are extremely sparse. To address
this issue, we propose MeshSplat, a generalizable sparse-view surface
reconstruction framework via Gaussian Splatting. Our key idea is to leverage
2DGS as a bridge, which connects novel view synthesis to learned geometric
priors and then transfers these priors to achieve surface reconstruction.
Specifically, we incorporate a feed-forward network to predict per-view
pixel-aligned 2DGS, which enables the network to synthesize novel view images
and thus eliminates the need for direct 3D ground-truth supervision. To improve
the accuracy of 2DGS position and orientation prediction, we propose a Weighted
Chamfer Distance Loss to regularize the depth maps, especially in overlapping
areas of input views, and also a normal prediction network to align the
orientation of 2DGS with normal vectors predicted by a monocular normal
estimator. Extensive experiments validate the effectiveness of our proposed
improvement, demonstrating that our method achieves state-of-the-art
performance in generalizable sparse-view mesh reconstruction tasks. Project
Page: https://hanzhichang.github.io/meshsplat_web

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [335] [Empirical Analysis of the Effect of Context in the Task of Automated Essay Scoring in Transformer-Based Models](https://arxiv.org/abs/2508.16638)
*Abhirup Chakravarty*

Main category: cs.CY

TL;DR: Transformer-based AES models underperform compared to alternative deep learning architectures. This study enhances transformer models with contextual enrichment, achieving competitive results with state-of-the-art methods while being architecture-agnostic.


<details>
  <summary>Details</summary>
Motivation: Despite transformers dominating other NLP tasks, they underperform in Automated Essay Scoring compared to alternative deep learning architectures, creating a need for contextual enrichment to improve their performance.

Method: Augmented transformer-based models with multiple contextual dimensions using the ASAP-AES dataset. Analyzed diverse contextual factors and their impact on model performance.

Result: Achieved mean Quadratic Weighted Kappa score of 0.823 across entire dataset and 0.8697 on individual essay sets. Outperformed prior transformer models and showed superior performance in 3 out of 8 essay sets, trailing state-of-the-art by only 3.83% on average.

Conclusion: Contextual augmentation provides a versatile, architecture-agnostic technique for refining AES capabilities that can be seamlessly adapted to any AES model, contributing to the evolution of automated grading in education.

Abstract: Automated Essay Scoring (AES) has emerged to prominence in response to the
growing demand for educational automation. Providing an objective and
cost-effective solution, AES standardises the assessment of extended responses.
Although substantial research has been conducted in this domain, recent
investigations reveal that alternative deep-learning architectures outperform
transformer-based models. Despite the successful dominance in the performance
of the transformer architectures across various other tasks, this discrepancy
has prompted a need to enrich transformer-based AES models through contextual
enrichment.
  This study delves into diverse contextual factors using the ASAP-AES dataset,
analysing their impact on transformer-based model performance. Our most
effective model, augmented with multiple contextual dimensions, achieves a mean
Quadratic Weighted Kappa score of 0.823 across the entire essay dataset and
0.8697 when trained on individual essay sets. Evidently surpassing prior
transformer-based models, this augmented approach only underperforms relative
to the state-of-the-art deep learning model trained essay-set-wise by an
average of 3.83\% while exhibiting superior performance in three of the eight
sets.
  Importantly, this enhancement is orthogonal to architecture-based
advancements and seamlessly adaptable to any AES model. Consequently, this
contextual augmentation methodology presents a versatile technique for refining
AES capabilities, contributing to automated grading and evaluation evolution in
educational settings.

</details>


### [336] [Leveraging Multi-Source Textural UGC for Neighbourhood Housing Quality Assessment: A GPT-Enhanced Framework](https://arxiv.org/abs/2508.16657)
*Qiyuan Hong,Huimin Zhao,Ying Long*

Main category: cs.CY

TL;DR: GPT-4o analyzes user-generated content from multiple platforms to assess neighborhood housing quality with 92.5% accuracy, outperforming traditional methods and revealing platform-specific focus differences.


<details>
  <summary>Details</summary>
Motivation: To develop a scalable, resident-centric approach for urban housing quality assessment by leveraging AI analysis of multi-source user-generated content, addressing limitations of traditional objective assessment methods.

Method: Used GPT-4o to filter relevant texts from Dianping, Weibo, and Government Message Board, extract structured evaluation units, conduct sentiment scoring, and develop a refined housing quality assessment system with 46 indicators across 11 categories.

Result: GPT-4o achieved 92.5% accuracy in fine-tuned settings, outperforming rule-based and BERT models. The analysis revealed an objective-subjective method gap and platform-specific differences in focus areas.

Conclusion: Integration of UGC and GPT-driven analysis provides valuable, scalable solutions for resident-centric urban assessments, offering practical insights for policymakers and urban planners in housing quality evaluation.

Abstract: This study leverages GPT-4o to assess neighbourhood housing quality using
multi-source textural user-generated content (UGC) from Dianping, Weibo, and
the Government Message Board. The analysis involves filtering relevant texts,
extracting structured evaluation units, and conducting sentiment scoring. A
refined housing quality assessment system with 46 indicators across 11
categories was developed, highlighting an objective-subjective method gap and
platform-specific differences in focus. GPT-4o outperformed rule-based and BERT
models, achieving 92.5% accuracy in fine-tuned settings. The findings
underscore the value of integrating UGC and GPT-driven analysis for scalable,
resident-centric urban assessments, offering practical insights for
policymakers and urban planners.

</details>


### [337] [Invisible Filters: Cultural Bias in Hiring Evaluations Using Large Language Models](https://arxiv.org/abs/2508.16673)
*Pooja S. B. Rao,Laxminarayen Nagarajan Venkatesan,Mauro Cherubini,Dinesh Babu Jayagopi*

Main category: cs.CY

TL;DR: LLMs show cultural bias in hiring evaluations, giving lower scores to Indian job interviews than UK ones due to linguistic differences, but no significant name-based bias when isolated from context.


<details>
  <summary>Details</summary>
Motivation: To systematically examine potential biases in AI-driven hiring evaluations across different cultural contexts, addressing concerns about fairness and trust in LLM-assisted hiring decisions.

Method: Used two datasets (100 UK and 100 Indian interview transcripts), analyzed cross-cultural score differences, performed linguistic feature analysis, and conducted controlled identity substitutions (varying names by gender, caste, and region) within the Indian dataset.

Result: Indian transcripts received consistently lower hirability scores than UK transcripts, even when anonymized, with disparities linked to linguistic features like sentence complexity and lexical diversity. Name substitutions alone did not yield statistically significant bias effects.

Conclusion: LLM hiring evaluations exhibit cultural bias through linguistic features rather than names alone, highlighting the need for culturally sensitive AI design and evaluation of both linguistic and social dimensions in AI-assisted hiring.

Abstract: Artificial Intelligence (AI) is increasingly used in hiring, with large
language models (LLMs) having the potential to influence or even make hiring
decisions. However, this raises pressing concerns about bias, fairness, and
trust, particularly across diverse cultural contexts. Despite their growing
role, few studies have systematically examined the potential biases in
AI-driven hiring evaluation across cultures. In this study, we conduct a
systematic analysis of how LLMs assess job interviews across cultural and
identity dimensions. Using two datasets of interview transcripts, 100 from UK
and 100 from Indian job seekers, we first examine cross-cultural differences in
LLM-generated scores for hirability and related traits. Indian transcripts
receive consistently lower scores than UK transcripts, even when they were
anonymized, with disparities linked to linguistic features such as sentence
complexity and lexical diversity. We then perform controlled identity
substitutions (varying names by gender, caste, and region) within the Indian
dataset to test for name-based bias. These substitutions do not yield
statistically significant effects, indicating that names alone, when isolated
from other contextual signals, may not influence LLM evaluations. Our findings
underscore the importance of evaluating both linguistic and social dimensions
in LLM-driven evaluations and highlight the need for culturally sensitive
design and accountability in AI-assisted hiring.

</details>


### [338] [Citizen Centered Climate Intelligence: Operationalizing Open Tree Data for Urban Cooling and Eco-Routing in Indian Cities](https://arxiv.org/abs/2508.17648)
*Kaushik Ravi,Andreas BrÃ¼ck*

Main category: cs.CY

TL;DR: A citizen-centric framework for urban climate resilience using participatory sensing, AI-enhanced tree measurements, heat metrics, and eco-routing to transform urban planning into co-produced, localized practice.


<details>
  <summary>Details</summary>
Motivation: Urban climate resilience requires systems that embed data collection and action within citizens' daily lives, addressing ecological inequality and data centralization through participatory approaches.

Method: Three interlinked modules: 1) smartphone-based AI toolkit for tree measurements, 2) percentile-based model with Cooling Efficacy and Ambient Heat Relief metrics using satellite data, 3) eco-routing engine with Static Environmental Quality score based on tree density, diversity, and carbon sequestration.

Result: A closed feedback loop framework where citizens generate actionable data and benefit from personalized sustainable interventions, transforming open data into an active platform for shared governance.

Conclusion: The framework presents a replicable model for citizen-driven urban intelligence that reframes urban planning as co-produced, climate-resilient, and radically local practice for environmental equity.

Abstract: Urban climate resilience requires more than high-resolution data; it demands
systems that embed data collection, interpretation, and action within the daily
lives of citizens. This chapter presents a scalable, citizen-centric framework
that reimagines environmental infrastructure through participatory sensing,
open analytics, and prescriptive urban planning tools. Applied in Pune, India,
the framework comprises three interlinked modules: (1) a smartphone-based
measurement toolkit enhanced by AI segmentation to extract tree height, canopy
diameter, and trunk girth; (2) a percentile-based model using satellite-derived
Land Surface Temperature to calculate localized cooling through two new
metrics, Cooling Efficacy and Ambient Heat Relief; and (3) an eco-routing
engine that guides mobility using a Static Environmental Quality score, based
on tree density, species diversity, and cumulative carbon sequestration.
Together, these modules form a closed feedback loop where citizens generate
actionable data and benefit from personalized, sustainable interventions. This
framework transforms open data from a passive repository into an active
platform for shared governance and environmental equity. In the face of growing
ecological inequality and data centralization, this chapter presents a
replicable model for citizen-driven urban intelligence, reframing planning as a
co-produced, climate-resilient, and radically local practice.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [339] [THEME : Enhancing Thematic Investing with Semantic Stock Representations and Temporal Dynamics](https://arxiv.org/abs/2508.16936)
*Hoyoung Lee,Wonbin Ahn,Suhwan Park,Jaehoon Lee,Minjae Kim,Sungdong Yoo,Taeyoon Lim,Woohyung Lim,Yongjae Lee*

Main category: q-fin.PM

TL;DR: THEME is a hierarchical contrastive learning framework that combines thematic text embeddings with stock returns to improve thematic stock selection and portfolio construction.


<details>
  <summary>Details</summary>
Motivation: Thematic investing faces challenges in stock selection due to overlapping sector boundaries and evolving market dynamics. Existing thematic ETFs have coverage limitations that need to be addressed.

Method: Constructed Thematic Representation Set (TRS) dataset combining thematic ETFs, industry classifications, and financial news. Developed THEME framework with hierarchical contrastive learning that aligns theme-stock semantic embeddings and refines them with temporal stock return data.

Result: THEME outperforms strong baselines across multiple retrieval metrics and significantly improves performance in portfolio construction.

Conclusion: By jointly modeling thematic relationships from text and market dynamics from returns, THEME provides a scalable and adaptive solution for navigating complex investment themes.

Abstract: Thematic investing aims to construct portfolios aligned with structural
trends, yet selecting relevant stocks remains challenging due to overlapping
sector boundaries and evolving market dynamics. To address this challenge, we
construct the Thematic Representation Set (TRS), an extended dataset that
begins with real-world thematic ETFs and expands upon them by incorporating
industry classifications and financial news to overcome their coverage
limitations. The final dataset contains both the explicit mapping of themes to
their constituent stocks and the rich textual profiles for each. Building on
this dataset, we introduce \textsc{THEME}, a hierarchical contrastive learning
framework. By representing the textual profiles of themes and stocks as
embeddings, \textsc{THEME} first leverages their hierarchical relationship to
achieve semantic alignment. Subsequently, it refines these semantic embeddings
through a temporal refinement stage that incorporates individual stock returns.
The final stock representations are designed for effective retrieval of
thematically aligned assets with strong return potential. Empirical results
show that \textsc{THEME} outperforms strong baselines across multiple retrieval
metrics and significantly improves performance in portfolio construction. By
jointly modeling thematic relationships from text and market dynamics from
returns, \textsc{THEME} provides a scalable and adaptive solution for
navigating complex investment themes.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [340] [Unseen Speaker and Language Adaptation for Lightweight Text-To-Speech with Adapters](https://arxiv.org/abs/2508.18006)
*Alessio Falai,Ziyao Zhang,Akos Gangoly*

Main category: eess.AS

TL;DR: Cross-lingual TTS using adapters for lightweight synthesis of target voices in languages where they have no recordings, with effective adaptation while preventing catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: To enable Text-To-Speech synthesis of target voices in languages where those voices have no recordings, using lightweight adapter techniques for cross-lingual adaptation.

Method: Using adapter modules for language-specific and speaker-specific adaptation in pre-trained TTS models, comparing unseen speaker and language adaptation tasks, with analysis of adapter placement and configuration.

Result: Adapters effectively learn language-specific and speaker-specific information, allowing pre-trained models to adapt to unseen speakers or languages while preserving original capabilities. Proposed objective metric validates native accent quality.

Conclusion: Adapters provide an effective lightweight solution for cross-lingual TTS adaptation, enabling synthesis of target voices in new languages without recordings, with insights on optimal adapter configuration.

Abstract: In this paper we investigate cross-lingual Text-To-Speech (TTS) synthesis
through the lens of adapters, in the context of lightweight TTS systems. In
particular, we compare the tasks of unseen speaker and language adaptation with
the goal of synthesising a target voice in a target language, in which the
target voice has no recordings therein. Results from objective evaluations
demonstrate the effectiveness of adapters in learning language-specific and
speaker-specific information, allowing pre-trained models to learn unseen
speaker identities or languages, while avoiding catastrophic forgetting of the
original model's speaker or language information. Additionally, to measure how
native the generated voices are in terms of accent, we propose and validate an
objective metric inspired by mispronunciation detection techniques in
second-language (L2) learners. The paper also provides insights into the impact
of adapter placement, configuration and the number of speakers used.

</details>


### [341] [HunyuanVideo-Foley: Multimodal Diffusion with Representation Alignment for High-Fidelity Foley Audio Generation](https://arxiv.org/abs/2508.16930)
*Sizhe Shan,Qiulin Li,Yutao Cui,Miles Yang,Yuehai Wang,Qun Yang,Jin Zhou,Zhao Zhong*

Main category: eess.AS

TL;DR: HunyuanVideo-Foley is a text-video-to-audio framework that generates high-fidelity audio synchronized with video content using multimodal diffusion transformers and representation alignment.


<details>
  <summary>Details</summary>
Motivation: Current video generation lacks synchronized audio, which reduces immersion. Existing methods face challenges with multimodal data scarcity, modality imbalance, and limited audio quality.

Method: End-to-end framework with three innovations: (1) scalable data pipeline with 100k-hour automated annotation, (2) representation alignment using self-supervised audio features for latent diffusion training, (3) multimodal diffusion transformer with dual-stream audio-video fusion and textual semantic injection.

Result: Achieves state-of-the-art performance in audio fidelity, visual-semantic alignment, temporal alignment, and distribution matching.

Conclusion: The proposed framework successfully addresses key challenges in video-to-audio generation, producing high-quality synchronized audio that enhances video immersion.

Abstract: Recent advances in video generation produce visually realistic content, yet
the absence of synchronized audio severely compromises immersion. To address
key challenges in video-to-audio generation, including multimodal data
scarcity, modality imbalance and limited audio quality in existing methods, we
propose HunyuanVideo-Foley, an end-to-end text-video-to-audio framework that
synthesizes high-fidelity audio precisely aligned with visual dynamics and
semantic context. Our approach incorporates three core innovations: (1) a
scalable data pipeline curating 100k-hour multimodal datasets through automated
annotation; (2) a representation alignment strategy using self-supervised audio
features to guide latent diffusion training, efficiently improving audio
quality and generation stability; (3) a novel multimodal diffusion transformer
resolving modal competition, containing dual-stream audio-video fusion through
joint attention, and textual semantic injection via cross-attention.
Comprehensive evaluations demonstrate that HunyuanVideo-Foley achieves new
state-of-the-art performance across audio fidelity, visual-semantic alignment,
temporal alignment and distribution matching. The demo page is available at:
https://szczesnys.github.io/hunyuanvideo-foley/.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [342] [RephraseTTS: Dynamic Length Text based Speech Insertion with Speaker Style Transfer](https://arxiv.org/abs/2508.17031)
*Neeraj Matiyali,Siddharth Srivastava,Gaurav Sharma*

Main category: cs.SD

TL;DR: A transformer-based non-autoregressive method for text-conditioned speech insertion that dynamically determines insertion length based on text and tempo while preserving speaker characteristics.


<details>
  <summary>Details</summary>
Motivation: To enable speech audio updates when text transcripts are corrected, allowing seamless insertion of speech segments while maintaining original voice properties.

Method: Transformer-based non-autoregressive approach that dynamically determines variable-length speech insertions based on text transcript and input tempo.

Result: Outperforms baseline adaptive TTS methods in experiments and user studies on LibriTTS dataset, with high-quality output demonstrated.

Conclusion: The proposed method effectively handles text-conditioned speech insertion with variable lengths while preserving speaker voice characteristics and prosody.

Abstract: We propose a method for the task of text-conditioned speech insertion, i.e.
inserting a speech sample in an input speech sample, conditioned on the
corresponding complete text transcript. An example use case of the task would
be to update the speech audio when corrections are done on the corresponding
text transcript. The proposed method follows a transformer-based
non-autoregressive approach that allows speech insertions of variable lengths,
which are dynamically determined during inference, based on the text transcript
and tempo of the available partial input. It is capable of maintaining the
speaker's voice characteristics, prosody and other spectral properties of the
available speech input. Results from our experiments and user study on LibriTTS
show that our method outperforms baselines based on an existing adaptive text
to speech method. We also provide numerous qualitative results to appreciate
the quality of the output from the proposed method.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [343] [Neural Proteomics Fields for Super-resolved Spatial Proteomics Prediction](https://arxiv.org/abs/2508.17389)
*Bokai Zhao,Weiyang Shi,Hanqing Chao,Zijiang Yang,Yiyang Zhang,Ming Song,Tianzi Jiang*

Main category: q-bio.QM

TL;DR: Neural Proteomics Fields (NPF) is a deep learning model for spatial super-resolution in sequencing-based spatial proteomics that reconstructs protein distributions in continuous space using tissue-specific networks with spatial and morphological modules.


<details>
  <summary>Details</summary>
Motivation: Current sequencing-based spatial proteomics technologies suffer from low spatial resolution and inter-tissue variability in protein expression, which compromises existing prediction methods.

Method: NPF formulates spatial proteomics as a protein reconstruction problem using tissue-specific networks with Spatial Modeling Module (learns protein spatial distributions) and Morphology Modeling Module (extracts morphological features).

Result: NPF achieves state-of-the-art performance with fewer learnable parameters and includes an open-source benchmark dataset (Pseudo-Visium SP) for evaluation.

Conclusion: NPF demonstrates strong potential for advancing spatial proteomics research by providing high-resolution protein distribution mapping with efficient parameter usage.

Abstract: Spatial proteomics maps protein distributions in tissues, providing
transformative insights for life sciences. However, current sequencing-based
technologies suffer from low spatial resolution, and substantial inter-tissue
variability in protein expression further compromises the performance of
existing molecular data prediction methods. In this work, we introduce the
novel task of spatial super-resolution for sequencing-based spatial proteomics
(seq-SP) and, to the best of our knowledge, propose the first deep learning
model for this task--Neural Proteomics Fields (NPF). NPF formulates seq-SP as a
protein reconstruction problem in continuous space by training a dedicated
network for each tissue. The model comprises a Spatial Modeling Module, which
learns tissue-specific protein spatial distributions, and a Morphology Modeling
Module, which extracts tissue-specific morphological features. Furthermore, to
facilitate rigorous evaluation, we establish an open-source benchmark dataset,
Pseudo-Visium SP, for this task. Experimental results demonstrate that NPF
achieves state-of-the-art performance with fewer learnable parameters,
underscoring its potential for advancing spatial proteomics research. Our code
and dataset are publicly available at https://github.com/Bokai-Zhao/NPF.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [344] [Anemoi: A Semi-Centralized Multi-agent Systems Based on Agent-to-Agent Communication MCP server from Coral Protocol](https://arxiv.org/abs/2508.17068)
*Xinxing Ren,Caelum Forder,Qianbo Zang,Ahsen Tahir,Roman J. Georgio,Suman Deb,Peter Carroll,Ãnder GÃ¼rcan,Zekun Guo*

Main category: cs.MA

TL;DR: Anemoi is a semi-centralized multi-agent system that enables direct inter-agent communication through A2A MCP server, reducing dependency on a single powerful planner and improving collaboration efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional centralized MAS designs suffer from strong dependency on planner capability and limited inter-agent communication, leading to degraded performance with smaller LLMs and inefficient prompt concatenation.

Method: Built on Coral Protocol's A2A communication MCP server, Anemoi enables structured direct inter-agent collaboration where all agents can monitor progress, assess results, identify bottlenecks, and propose refinements in real time.

Result: Achieved 52.73% accuracy on GAIA benchmark using GPT-4.1-mini as planner, surpassing the strongest open-source baseline OWL (43.63%) by +9.09% under identical LLM settings.

Conclusion: Anemoi's semi-centralized approach reduces reliance on single planners, supports adaptive plan updates, minimizes redundant context passing, and enables more scalable and cost-efficient multi-agent execution.

Abstract: Recent advances in generalist multi-agent systems (MAS) have largely followed
a context-engineering plus centralized paradigm, where a planner agent
coordinates multiple worker agents through unidirectional prompt passing. While
effective under strong planner models, this design suffers from two critical
limitations: (1) strong dependency on the planner's capability, which leads to
degraded performance when a smaller LLM powers the planner; and (2) limited
inter-agent communication, where collaboration relies on costly prompt
concatenation and context injection, introducing redundancy and information
loss. To address these challenges, we propose Anemoi, a semi-centralized MAS
built on the Agent-to-Agent (A2A) communication MCP server from Coral Protocol.
Unlike traditional designs, Anemoi enables structured and direct inter-agent
collaboration, allowing all agents to monitor progress, assess results,
identify bottlenecks, and propose refinements in real time. This paradigm
reduces reliance on a single planner, supports adaptive plan updates, and
minimizes redundant context passing, resulting in more scalable and
cost-efficient execution. Evaluated on the GAIA benchmark, Anemoi achieved
52.73\% accuracy with a small LLM (GPT-4.1-mini) as the planner, surpassing the
strongest open-source baseline OWL (43.63\%) by +9.09\% under identical LLM
settings. Our implementation is publicly available at
https://github.com/Coral-Protocol/Anemoi.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [345] [Guarding Your Conversations: Privacy Gatekeepers for Secure Interactions with Cloud-Based AI Models](https://arxiv.org/abs/2508.16765)
*GodsGift Uzor,Hasan Al-Qudah,Ynes Ineza,Abdul Serwadda*

Main category: cs.CR

TL;DR: Proposes an LLM gatekeeper - a local lightweight model that filters sensitive information from user queries before sending to cloud LLMs, enhancing privacy with minimal performance impact.


<details>
  <summary>Details</summary>
Motivation: LLMs collect extensive user data and context, creating privacy risks even with opt-out settings, especially in jurisdictions with weak privacy laws, surveillance, or poor security practices.

Method: Developed a dual-model approach with a locally-run lightweight 'gatekeeper' model that screens queries for sensitive/PII information before forwarding to cloud-based LLMs.

Result: Human subject experiments showed the approach introduces minimal overhead while significantly enhancing user privacy without compromising LLM response quality.

Conclusion: The LLM gatekeeper concept provides effective privacy protection for cloud-based LLM interactions, offering a practical solution to mitigate data exposure risks in untrustworthy environments.

Abstract: The interactive nature of Large Language Models (LLMs), which closely track
user data and context, has prompted users to share personal and private
information in unprecedented ways. Even when users opt out of allowing their
data to be used for training, these privacy settings offer limited protection
when LLM providers operate in jurisdictions with weak privacy laws, invasive
government surveillance, or poor data security practices. In such cases, the
risk of sensitive information, including Personally Identifiable Information
(PII), being mishandled or exposed remains high. To address this, we propose
the concept of an "LLM gatekeeper", a lightweight, locally run model that
filters out sensitive information from user queries before they are sent to the
potentially untrustworthy, though highly capable, cloud-based LLM. Through
experiments with human subjects, we demonstrate that this dual-model approach
introduces minimal overhead while significantly enhancing user privacy, without
compromising the quality of LLM responses.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [346] [Learn to Memorize: Optimizing LLM-based Agents with Adaptive Memory Framework](https://arxiv.org/abs/2508.16629)
*Zeyu Zhang,Quanyu Dai,Rui Li,Xiaohe Bo,Xu Chen,Zhenhua Dong*

Main category: cs.LG

TL;DR: Proposes an adaptive, data-driven memory framework for LLM-based agents that learns optimal memory strategies through modeling memory cycles, replacing manual memory design with automated learning.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based agent memory mechanisms are manually predefined by experts, leading to high labor costs and suboptimal performance. They also ignore the memory cycle effect in interactive scenarios, which is crucial for optimizing agents in specific environments.

Method: Designed an MoE gate function for memory retrieval, learnable aggregation process for memory utilization, and task-specific reflection for memory storage. The framework enables agents to learn effective memorization strategies through both off-policy and on-policy optimization.

Result: Comprehensive experiments conducted across multiple aspects demonstrate the effectiveness of the proposed methods. The framework allows LLM-based agents to learn optimal memory strategies for specific environments.

Conclusion: The adaptive memory framework successfully addresses limitations of manual memory design by enabling data-driven learning of memory cycles, improving agent performance while reducing human labor costs. The project is open-sourced to benefit the research community.

Abstract: LLM-based agents have been extensively applied across various domains, where
memory stands out as one of their most essential capabilities. Previous memory
mechanisms of LLM-based agents are manually predefined by human experts,
leading to higher labor costs and suboptimal performance. In addition, these
methods overlook the memory cycle effect in interactive scenarios, which is
critical to optimizing LLM-based agents for specific environments. To address
these challenges, in this paper, we propose to optimize LLM-based agents with
an adaptive and data-driven memory framework by modeling memory cycles.
Specifically, we design an MoE gate function to facilitate memory retrieval,
propose a learnable aggregation process to improve memory utilization, and
develop task-specific reflection to adapt memory storage. Our memory framework
empowers LLM-based agents to learn how to memorize information effectively in
specific environments, with both off-policy and on-policy optimization. In
order to evaluate the effectiveness of our proposed methods, we conduct
comprehensive experiments across multiple aspects. To benefit the research
community in this area, we release our project at
https://github.com/nuster1128/learn_to_memorize.

</details>


### [347] [WISCA: A Lightweight Model Transition Method to Improve LLM Training via Weight Scaling](https://arxiv.org/abs/2508.16676)
*Jiacheng Li,Jianchao Tan,Zhidong Yang,Pingwei Sun,Feiye Huo,Jiayu Qin,Yerui Sun,Yuchen Xie,Xunliang Cai,Xiangyu Zhang,Maoxin He,Guangming Tan,Weile Jia,Tong Zhao*

Main category: cs.LG

TL;DR: WISCA: A weight scaling method that improves training efficiency and model quality by optimizing weight patterns without changing network architecture, achieving better convergence and generalization in LLMs.


<details>
  <summary>Details</summary>
Motivation: Current Transformer optimization approaches focus on architectural changes or optimizer adjustments but lack systematic optimization of weight patterns during training, which refers to the distribution and relative magnitudes of weight parameters.

Method: Proposes WISCA weight scaling method that rescales weights while preserving model outputs to indirectly optimize training trajectory, without modifying network structures.

Result: Significantly improves convergence quality with 5.6% average improvement on zero-shot validation tasks and 2.12% average reduction in training perplexity across multiple architectures, particularly effective for LLMs with GQA and LoRA fine-tuning.

Conclusion: WISCA provides an effective approach to enhance training efficiency and model performance through systematic weight pattern optimization, demonstrating substantial improvements in generalization capability and convergence quality for Transformer-based LLMs.

Abstract: Transformer architecture gradually dominates the LLM field. Recent advances
in training optimization for Transformer-based large language models (LLMs)
primarily focus on architectural modifications or optimizer adjustments.
However, these approaches lack systematic optimization of weight patterns
during training. Weight pattern refers to the distribution and relative
magnitudes of weight parameters in a neural network. To address this issue, we
propose a Weight Scaling method called WISCA to enhance training efficiency and
model quality by strategically improving neural network weight patterns without
changing network structures. By rescaling weights while preserving model
outputs, WISCA indirectly optimizes the model's training trajectory.
Experiments demonstrate that WISCA significantly improves convergence quality
(measured by generalization capability and loss reduction), particularly in
LLMs with Grouped Query Attention (GQA) architectures and LoRA fine-tuning
tasks. Empirical results show 5.6% average improvement on zero-shot validation
tasks and 2.12% average reduction in training perplexity across multiple
architectures.

</details>


### [348] [Recall-Extend Dynamics: Enhancing Small Language Models through Controlled Exploration and Refined Offline Integration](https://arxiv.org/abs/2508.16677)
*Zhong Guan,Likang Wu,Hongke Zhao,Jiahui Wang,Le Wu*

Main category: cs.LG

TL;DR: RED method enhances small language models by combining offline distillation with online RL, using entropy monitoring and policy shift mechanisms to address exploration and distribution issues.


<details>
  <summary>Details</summary>
Motivation: Small language models lack sufficient reasoning enhancement compared to large models, and existing approaches combining distilled data with RLVR face challenges with exploration space and data integration.

Method: Proposes RED framework with controlled exploration, entropy-based offline weight regulation, and sample-accuracy policy shift mechanism to balance offline imitation and online learning.

Result: Addresses insufficient exploration in small models and distillation complexity by dynamically managing offline-online data integration and policy learning.

Conclusion: RED provides an effective approach to enhance small language models' reasoning capabilities through optimized offline-online integration and adaptive policy management.

Abstract: Many existing studies have achieved significant improvements in the reasoning
capabilities of large language models (LLMs) through reinforcement learning
with verifiable rewards (RLVR), while the enhancement of reasoning abilities in
small language models (SLMs) has not yet been sufficiently explored. Combining
distilled data from larger models with RLVR on small models themselves is a
natural approach, but it still faces various challenges and issues. Therefore,
we propose \textit{\underline{R}}ecall-\textit{\underline{E}}xtend
\textit{\underline{D}}ynamics(RED): Enhancing Small Language Models through
Controlled Exploration and Refined Offline Integration. In this paper, we
explore the perspective of varying exploration spaces, balancing offline
distillation with online reinforcement learning. Simultaneously, we
specifically design and optimize for the insertion problem within offline data.
By monitoring the ratio of entropy changes in the model concerning offline and
online data, we regulate the weight of offline-SFT, thereby addressing the
issues of insufficient exploration space in small models and the redundancy and
complexity during the distillation process. Furthermore, to tackle the
distribution discrepancies between offline data and the current policy, we
design a sample-accuracy-based policy shift mechanism that dynamically chooses
between imitating offline distilled data and learning from its own policy.

</details>


### [349] [Hyperbolic Multimodal Representation Learning for Biological Taxonomies](https://arxiv.org/abs/2508.16744)
*ZeMing Gong,Chuanqi Tang,Xiaoliang Huo,Nicholas Pellegrino,Austin T. Wang,Graham W. Taylor,Angel X. Chang,Scott C. Lowe,Joakim Bruslund Haurum*

Main category: cs.LG

TL;DR: Hyperbolic networks for multimodal hierarchical classification in biodiversity research, achieving competitive performance on BIOSCAN-1M dataset with DNA barcodes but facing challenges in fine-grained classification.


<details>
  <summary>Details</summary>
Motivation: To investigate whether hyperbolic networks can provide better embedding space for hierarchical taxonomic classification using multimodal evidence (images and genetic information) in biodiversity research.

Method: Embeds multimodal inputs into shared hyperbolic space using contrastive learning and a novel stacked entailment-based objective function.

Result: Hyperbolic embedding achieves competitive performance with Euclidean baselines and outperforms all other models on unseen species classification using DNA barcodes, though fine-grained classification and open-world generalization remain challenging.

Conclusion: The framework provides a structure-aware foundation for biodiversity modeling with potential applications to species discovery, ecological monitoring, and conservation efforts.

Abstract: Taxonomic classification in biodiversity research involves organizing
biological specimens into structured hierarchies based on evidence, which can
come from multiple modalities such as images and genetic information. We
investigate whether hyperbolic networks can provide a better embedding space
for such hierarchical models. Our method embeds multimodal inputs into a shared
hyperbolic space using contrastive and a novel stacked entailment-based
objective. Experiments on the BIOSCAN-1M dataset show that hyperbolic embedding
achieves competitive performance with Euclidean baselines, and outperforms all
other models on unseen species classification using DNA barcodes. However,
fine-grained classification and open-world generalization remain challenging.
Our framework offers a structure-aware foundation for biodiversity modelling,
with potential applications to species discovery, ecological monitoring, and
conservation efforts.

</details>


### [350] [Interpreting the Effects of Quantization on LLMs](https://arxiv.org/abs/2508.16785)
*Manpreet Singh,Hassan Sajjad*

Main category: cs.LG

TL;DR: Quantization has minimal impact on LLM reliability - model calibration remains stable, dead neuron counts unchanged, and no drastic changes observed that would discourage quantization use.


<details>
  <summary>Details</summary>
Motivation: To investigate how quantization affects internal representations and reliability of LLMs, as this impact remains understudied despite quantization being a practical solution for resource-constrained deployment.

Method: Employed interpretability techniques to analyze multiple LLMs under 4-bit and 8-bit quantization, examining model calibration, neuron activations (dead neurons), and neuron contribution to predictions.

Result: Quantization impact on model calibration is minor; dead neuron counts remain consistent; smaller models have fewer salient neurons while larger models have more (except Llama-2-7B); neuron redundancy effects vary by model.

Conclusion: Quantization effects vary by model and tasks, but no drastic changes were observed that would discourage using quantization as a reliable model compression technique.

Abstract: Quantization offers a practical solution to deploy LLMs in
resource-constraint environments. However, its impact on internal
representations remains understudied, raising questions about the reliability
of quantized models. In this study, we employ a range of interpretability
techniques to investigate how quantization affects model and neuron behavior.
We analyze multiple LLMs under 4-bit and 8-bit quantization. Our findings
reveal that the impact of quantization on model calibration is generally minor.
Analysis of neuron activations indicates that the number of dead neurons, i.e.,
those with activation values close to 0 across the dataset, remains consistent
regardless of quantization. In terms of neuron contribution to predictions, we
observe that smaller full precision models exhibit fewer salient neurons,
whereas larger models tend to have more, with the exception of Llama-2-7B. The
effect of quantization on neuron redundancy varies across models. Overall, our
findings suggest that effect of quantization may vary by model and tasks,
however, we did not observe any drastic change which may discourage the use of
quantization as a reliable model compression technique.

</details>


### [351] [Attention Layers Add Into Low-Dimensional Residual Subspaces](https://arxiv.org/abs/2508.16929)
*Junxuan Wang,Xuyang Ge,Wentao Shu,Zhengfu He,Xipeng Qiu*

Main category: cs.LG

TL;DR: Attention outputs in transformers are surprisingly low-dimensional (60% directions account for 99% variance), causing dead feature problems in sparse dictionary learning. Proposed subspace-constrained training reduces dead features from 87% to below 1%.


<details>
  <summary>Details</summary>
Motivation: To understand why sparse dictionary learning suffers from dead feature problems and to develop methods that align with the intrinsic low-dimensional geometry of attention outputs.

Method: Analyzed attention output geometry, identified low-rank structure induced by projection matrices, and proposed subspace-constrained training that initializes feature directions into the active subspace of activations.

Result: Reduced dead features from 87% to below 1% in Attention Output SAEs with 1M features. Consistently observed low-rank structure across diverse models and datasets.

Conclusion: Attention outputs have intrinsic low-dimensional geometry that causes dead feature problems. Subspace-constrained initialization provides practical solution and new insights into transformer geometry.

Abstract: While transformer models are widely believed to operate in high-dimensional
hidden spaces, we show that attention outputs are confined to a surprisingly
low-dimensional subspace, where about 60\% of the directions account for 99\%
of the variance--a phenomenon that is induced by the attention output
projection matrix and consistently observed across diverse model families and
datasets. Critically, we find this low-rank structure as a fundamental cause of
the prevalent dead feature problem in sparse dictionary learning, where it
creates a mismatch between randomly initialized features and the intrinsic
geometry of the activation space. Building on this insight, we propose a
subspace-constrained training method for sparse autoencoders (SAEs),
initializing feature directions into the active subspace of activations. Our
approach reduces dead features from 87\% to below 1\% in Attention Output SAEs
with 1M features, and can further extend to other sparse dictionary learning
methods. Our findings provide both new insights into the geometry of attention
and practical tools for improving sparse dictionary learning in large language
models.

</details>


### [352] [LLM Assertiveness can be Mechanistically Decomposed into Emotional and Logical Components](https://arxiv.org/abs/2508.17182)
*Hikaru Tsujimura,Arush Tagade*

Main category: cs.LG

TL;DR: Mechanistic analysis reveals LLM assertiveness decomposes into emotional and logical components, with steering vectors showing distinct causal effects on prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: LLMs often display overconfidence with unwarranted certainty in high-stakes contexts, requiring investigation into the internal mechanisms behind this assertive behavior.

Method: Used open-sourced Llama 3.2 models fine-tuned on human-annotated assertiveness datasets, extracted residual activations across all layers, computed similarity metrics to localize assertive representations, and derived steering vectors from identified sub-components.

Result: Identified layers most sensitive to assertiveness contrasts, revealed that high-assertive representations decompose into orthogonal emotional and logical clusters (paralleling psychological dual-route models), and showed emotional vectors broadly influence prediction accuracy while logical vectors have localized effects.

Conclusion: Provides mechanistic evidence for multi-component structure of LLM assertiveness and highlights avenues for mitigating overconfident behavior through targeted interventions.

Abstract: Large Language Models (LLMs) often display overconfidence, presenting
information with unwarranted certainty in high-stakes contexts. We investigate
the internal basis of this behavior via mechanistic interpretability. Using
open-sourced Llama 3.2 models fine-tuned on human annotated assertiveness
datasets, we extract residual activations across all layers, and compute
similarity metrics to localize assertive representations. Our analysis
identifies layers most sensitive to assertiveness contrasts and reveals that
high-assertive representations decompose into two orthogonal sub-components of
emotional and logical clusters-paralleling the dual-route Elaboration
Likelihood Model in Psychology. Steering vectors derived from these
sub-components show distinct causal effects: emotional vectors broadly
influence prediction accuracy, while logical vectors exert more localized
effects. These findings provide mechanistic evidence for the multi-component
structure of LLM assertiveness and highlight avenues for mitigating
overconfident behavior.

</details>


### [353] [TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling](https://arxiv.org/abs/2508.17445)
*Yizhi Li,Qingshui Gu,Zhoufutu Wen,Ziniu Li,Tianshun Xing,Shuyue Guo,Tianyu Zheng,Xin Zhou,Xingwei Qu,Wangchunshu Zhou,Zheng Zhang,Wei Shen,Qian Liu,Chenghua Lin,Jian Yang,Ge Zhang,Wenhao Huang*

Main category: cs.LG

TL;DR: TreePO introduces a tree-structured search approach for RL-based language model alignment that reduces computational costs by 22-43% while maintaining or improving reasoning performance through dynamic branching and efficient path pruning.


<details>
  <summary>Details</summary>
Motivation: Current RL methods for aligning large language models require expensive on-policy rollouts and have limited exploration of diverse reasoning paths, creating computational bottlenecks.

Method: TreePO uses a self-guided rollout algorithm with dynamic tree sampling policy, fixed-length segment decoding, and early pruning of low-value paths. It includes segment-wise sampling to reduce KV cache burden, tree-based segment-level advantage estimation, and probability/quality-driven divergence strategies.

Result: TreePO achieves 22-43% GPU hour savings, 40% reduction in trajectory-level sampling compute, and 35% reduction in token-level sampling compute while maintaining or enhancing performance on reasoning benchmarks.

Conclusion: TreePO provides a practical path for scaling RL-based post-training with fewer samples and less computation, offering inference efficiency improvements without sacrificing performance.

Abstract: Recent advancements in aligning large language models via reinforcement
learning have achieved remarkable gains in solving complex reasoning problems,
but at the cost of expensive on-policy rollouts and limited exploration of
diverse reasoning paths. In this work, we introduce TreePO, involving a
self-guided rollout algorithm that views sequence generation as a
tree-structured searching process. Composed of dynamic tree sampling policy and
fixed-length segment decoding, TreePO leverages local uncertainty to warrant
additional branches. By amortizing computation across common prefixes and
pruning low-value paths early, TreePO essentially reduces the per-update
compute burden while preserving or enhancing exploration diversity. Key
contributions include: (1) a segment-wise sampling algorithm that alleviates
the KV cache burden through contiguous segments and spawns new branches along
with an early-stop mechanism; (2) a tree-based segment-level advantage
estimation that considers both global and local proximal policy optimization.
and (3) analysis on the effectiveness of probability and quality-driven dynamic
divergence and fallback strategy. We empirically validate the performance gain
of TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours
from 22\% up to 43\% of the sampling design for the trained models, meanwhile
showing up to 40\% reduction at trajectory-level and 35\% at token-level
sampling compute for the existing models. While offering a free lunch of
inference efficiency, TreePO reveals a practical path toward scaling RL-based
post-training with fewer samples and less compute. Home page locates at
https://m-a-p.ai/TreePO.

</details>


### [354] [Activation Transport Operators](https://arxiv.org/abs/2508.17540)
*Andrzej Szablewski,Marek Masiak*

Main category: cs.LG

TL;DR: ATO method analyzes linear feature transport in transformer residual streams, showing how features move between layers and enabling better model safety and debugging.


<details>
  <summary>Details</summary>
Motivation: Understanding how features flow through transformer residual streams can improve jailbreaking protections, enable early detection of model mistakes, and facilitate their correction.

Method: Proposed Activation Transport Operators (ATO) - linear maps from upstream to downstream residuals evaluated using downstream SAE decoder projections to track feature transport across layers.

Result: Empirically demonstrated that ATO can distinguish between linearly transported features and those synthesized from non-linear computation, with transport efficiency providing bounds on residual stream subspace involved in linear transport.

Conclusion: ATO provides a compute-light method (<50 GPU-h) offering practical tools for safety, debugging, and understanding linear computation behavior in LLMs.

Abstract: The residual stream mediates communication between transformer decoder layers
via linear reads and writes of non-linear computations. While sparse-dictionary
learning-based methods locate features in the residual stream, and activation
patching methods discover circuits within the model, the mechanism by which
features flow through the residual stream remains understudied. Understanding
this dynamic can better inform jailbreaking protections, enable early detection
of model mistakes, and their correction. In this work, we propose Activation
Transport Operators (ATO), linear maps from upstream to downstream residuals
$k$ layers later, evaluated in feature space using downstream SAE decoder
projections. We empirically demonstrate that these operators can determine
whether a feature has been linearly transported from a previous layer or
synthesised from non-linear layer computation. We develop the notion of
transport efficiency, for which we provide an upper bound, and use it to
estimate the size of the residual stream subspace that corresponds to linear
transport. We empirically demonstrate the linear transport, report transport
efficiency and the size of the residual stream's subspace involved in linear
transport. This compute-light (no finetuning, <50 GPU-h) method offers
practical tools for safety, debugging, and a clearer picture of where
computation in LLMs behaves linearly.

</details>


### [355] [Characterizing the Behavior of Training Mamba-based State Space Models on GPUs](https://arxiv.org/abs/2508.17679)
*Trinayan Baruah,Kaustubh Shivdikar,Sara Prescott,David Kaeli*

Main category: cs.LG

TL;DR: This paper analyzes Mamba-based State Space Models (SSMs) as alternatives to transformers, characterizing their GPU behavior during training and identifying optimization opportunities for scaling performance.


<details>
  <summary>Details</summary>
Motivation: Transformers face quadratic complexity issues with attention computation as sequence length increases. SSMs offer reduced computational complexity and have shown promise across various domains, making it crucial to understand their GPU requirements for architectural design.

Method: The authors constructed a workload suite with representative Mamba-based SSM models spanning different architectures, then used this suite to analyze architectural implications and behavior during GPU training.

Result: The study provides new insights into how Mamba-based SSMs perform on GPUs during training, revealing their computational patterns and requirements.

Conclusion: The analysis sheds light on potential optimizations needed to continue scaling performance for Mamba-based state space models on GPU architectures.

Abstract: Mamba-based State Space Models (SSM) have emerged as a promising alternative
to the ubiquitous transformers. Despite the expressive power of transformers,
the quadratic complexity of computing attention is a major impediment to
scaling performance as we increase the sequence length. SSMs provide an
alternative path that addresses this problem, reducing the computational
complexity requirements of self-attention with novel model architectures for
different domains and fields such as video, text generation and graphs. Thus,
it is important to characterize the behavior of these emerging workloads on
GPUs and understand their requirements during GPU microarchitectural design. In
this work we evaluate Mamba-based SSMs and characterize their behavior during
training on GPUs. We construct a workload suite that offers representative
models that span different model architectures. We then use this suite to
analyze the architectural implications of running Mamba-based SSMs on GPUs. Our
work sheds new light on potential optimizations to continue scaling the
performance for such models.

</details>


### [356] [Proximal Supervised Fine-Tuning](https://arxiv.org/abs/2508.17784)
*Wenhong Zhu,Ruobing Xie,Rui Wang,Xingwu Sun,Di Wang,Pengfei Liu*

Main category: cs.LG

TL;DR: Proximal SFT (PSFT) is a new fine-tuning method that prevents capability deterioration in foundation models by incorporating trust-region constraints inspired by RL optimization techniques.


<details>
  <summary>Details</summary>
Motivation: Supervised fine-tuning often causes poor generalization and deterioration of prior capabilities when adapting foundation models to new tasks or domains.

Method: PSFT incorporates trust-region principles from TRPO and PPO in RL, viewing SFT as a special case of policy gradient methods with constant positive advantages to constrain policy drift.

Result: PSFT matches SFT performance in-domain, outperforms it in out-of-domain generalization, remains stable under prolonged training without entropy collapse, and provides a stronger foundation for subsequent optimization.

Conclusion: PSFT effectively stabilizes optimization and improves generalization while maintaining competitive tuning performance, making it a superior alternative to standard SFT for foundation models.

Abstract: Supervised fine-tuning (SFT) of foundation models often leads to poor
generalization, where prior capabilities deteriorate after tuning on new tasks
or domains. Inspired by trust-region policy optimization (TRPO) and proximal
policy optimization (PPO) in reinforcement learning (RL), we propose Proximal
SFT (PSFT). This fine-tuning objective incorporates the benefits of
trust-region, effectively constraining policy drift during SFT while
maintaining competitive tuning. By viewing SFT as a special case of policy
gradient methods with constant positive advantages, we derive PSFT that
stabilizes optimization and leads to generalization, while leaving room for
further optimization in subsequent post-training stages. Experiments across
mathematical and human-value domains show that PSFT matches SFT in-domain,
outperforms it in out-of-domain generalization, remains stable under prolonged
training without causing entropy collapse, and provides a stronger foundation
for the subsequent optimization.

</details>


### [357] [A Laplace diffusion-based transformer model for heart rate forecasting within daily activity context](https://arxiv.org/abs/2508.16655)
*Andrei Mateescu,Ioana Hadarau,Ionut Anghel,Tudor Cioara,Ovidiu Anchidin,Ancuta Nemes*

Main category: cs.LG

TL;DR: Transformer model with Laplace diffusion integrates physical activity context to improve heart rate monitoring accuracy in remote patient care, achieving 43% error reduction.


<details>
  <summary>Details</summary>
Motivation: Current remote heart rate monitoring lacks integration of physical activity context, making it difficult to assess whether heart rate fluctuations are clinically significant.

Method: Transformer model combined with Laplace diffusion technique that conditions the entire modeling process on activity context using specialized embeddings and attention mechanisms to prioritize activity-specific historical patterns.

Result: 43% reduction in mean absolute error compared to baseline models, with R2 coefficient of 0.97 indicating strong agreement between predicted and actual heart rates.

Conclusion: The proposed model is a practical and effective tool for supporting healthcare providers and remote patient monitoring systems by accurately modeling activity-driven heart rate fluctuations.

Abstract: With the advent of wearable Internet of Things (IoT) devices, remote patient
monitoring (RPM) emerged as a promising solution for managing heart failure.
However, the heart rate can fluctuate significantly due to various factors, and
without correlating it to the patient's actual physical activity, it becomes
difficult to assess whether changes are significant. Although Artificial
Intelligence (AI) models may enhance the accuracy and contextual understanding
of remote heart rate monitoring, the integration of activity data is still
rarely addressed. In this paper, we propose a Transformer model combined with a
Laplace diffusion technique to model heart rate fluctuations driven by physical
activity of the patient. Unlike prior models that treat activity as secondary,
our approach conditions the entire modeling process on activity context using
specialized embeddings and attention mechanisms to prioritize activity specific
historical patents. The model captures both long-term patterns and
activity-specific heart rate dynamics by incorporating contextualized
embeddings and dedicated encoder. The Transformer model was validated on a
real-world dataset collected from 29 patients over a 4-month period.
Experimental results show that our model outperforms current state-of-the-art
methods, achieving a 43% reduction in mean absolute error compared to the
considered baseline models. Moreover, the coefficient of determination R2 is
0.97 indicating the model predicted heart rate is in strong agreement with
actual heart rate values. These findings suggest that the proposed model is a
practical and effective tool for supporting both healthcare providers and
remote patient monitoring systems.

</details>


### [358] [UM3: Unsupervised Map to Map Matching](https://arxiv.org/abs/2508.16874)
*Chaolong Ying,Yinan Zhang,Lei Zhang,Jiazhuang Wang,Shujun Jia,Tianshu Yu*

Main category: cs.LG

TL;DR: Unsupervised graph-based framework for map-to-map matching that uses pseudo coordinates, adaptive similarity balancing, and tile-based processing to achieve state-of-the-art accuracy without training data.


<details>
  <summary>Details</summary>
Motivation: Map-to-map matching faces challenges due to lack of ground truth correspondences, sparse node features, and scalability demands in large-scale spatial data alignment.

Method: Unsupervised learning with pseudo coordinates for spatial layout, adaptive feature-geometric similarity balancing, geometric-consistent loss function, and tile-based post-processing with overlapping regions and majority voting for scalability.

Result: Achieves state-of-the-art accuracy in matching tasks, significantly outperforming existing methods, especially in high-noise and large-scale scenarios.

Conclusion: Provides a scalable, practical solution for map alignment that is robust to noisy data and offers efficient parallel processing capabilities.

Abstract: Map-to-map matching is a critical task for aligning spatial data across
heterogeneous sources, yet it remains challenging due to the lack of ground
truth correspondences, sparse node features, and scalability demands. In this
paper, we propose an unsupervised graph-based framework that addresses these
challenges through three key innovations. First, our method is an unsupervised
learning approach that requires no training data, which is crucial for
large-scale map data where obtaining labeled training samples is challenging.
Second, we introduce pseudo coordinates that capture the relative spatial
layout of nodes within each map, which enhances feature discriminability and
enables scale-invariant learning. Third, we design an mechanism to adaptively
balance feature and geometric similarity, as well as a geometric-consistent
loss function, ensuring robustness to noisy or incomplete coordinate data. At
the implementation level, to handle large-scale maps, we develop a tile-based
post-processing pipeline with overlapping regions and majority voting, which
enables parallel processing while preserving boundary coherence. Experiments on
real-world datasets demonstrate that our method achieves state-of-the-art
accuracy in matching tasks, surpassing existing methods by a large margin,
particularly in high-noise and large-scale scenarios. Our framework provides a
scalable and practical solution for map alignment, offering a robust and
efficient alternative to traditional approaches.

</details>


### [359] [Disentangling Polysemantic Neurons with a Null-Calibrated Polysemanticity Index and Causal Patch Interventions](https://arxiv.org/abs/2508.16950)
*Manan Gupta,Dhruv Kumar*

Main category: cs.LG

TL;DR: PSI is a new metric to quantify polysemantic neurons by measuring semantic clustering in activation patterns across geometric quality, category alignment, and semantic distinctness.


<details>
  <summary>Details</summary>
Motivation: Neural networks contain polysemantic neurons that respond to multiple unrelated features, making mechanistic interpretability challenging.

Method: PSI multiplies three calibrated components: geometric cluster quality (S), alignment to labeled categories (Q), and open-vocabulary semantic distinctness via CLIP (D). Evaluated on ResNet-50 with Tiny-ImageNet.

Result: PSI identifies neurons with coherent, nameable prototypes and reveals higher polysemanticity in later layers. Validated through robustness checks and causal interventions showing aligned patch replacements increase target activation.

Conclusion: PSI provides a principled and practical method for discovering, quantifying, and studying polysemantic units in neural networks.

Abstract: Neural networks often contain polysemantic neurons that respond to multiple,
sometimes unrelated, features, complicating mechanistic interpretability. We
introduce the Polysemanticity Index (PSI), a null-calibrated metric that
quantifies when a neuron's top activations decompose into semantically distinct
clusters. PSI multiplies three independently calibrated components: geometric
cluster quality (S), alignment to labeled categories (Q), and open-vocabulary
semantic distinctness via CLIP (D). On a pretrained ResNet-50 evaluated with
Tiny-ImageNet images, PSI identifies neurons whose activation sets split into
coherent, nameable prototypes, and reveals strong depth trends: later layers
exhibit substantially higher PSI than earlier layers. We validate our approach
with robustness checks (varying hyperparameters, random seeds, and
cross-encoder text heads), breadth analyses (comparing class-only vs.
open-vocabulary concepts), and causal patch-swap interventions. In particular,
aligned patch replacements increase target-neuron activation significantly more
than non-aligned, random, shuffled-position, or ablate-elsewhere controls. PSI
thus offers a principled and practical lever for discovering, quantifying, and
studying polysemantic units in neural networks.

</details>


### [360] [SACA: Selective Attention-Based Clustering Algorithm](https://arxiv.org/abs/2508.17150)
*Meysam Shirdel Bilehsavar,Razieh Ghaedi,Samira Seyed Taheri,Xinqi Fan,Christian O'Reilly*

Main category: cs.LG

TL;DR: Novel density-based clustering method inspired by selective attention that minimizes user-defined parameters, requiring only a single integer parameter when needed, with robust performance across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional density-based clustering methods like DBSCAN require user-defined parameters that pose optimization challenges and demand domain expertise, creating barriers to accessibility.

Method: The algorithm initially operates without user-defined parameters. When needed, it introduces a single integer parameter. It computes a threshold to filter sparse points and outliers, forms preliminary clusters, then reintegrates excluded points to finalize results.

Result: Experimental evaluations on diverse datasets demonstrate the method's accessibility and robust performance, showing it provides an effective alternative for density-based clustering tasks.

Conclusion: The proposed selective attention-inspired clustering method successfully addresses parameter optimization challenges in density-based clustering, offering improved accessibility while maintaining strong performance across various datasets.

Abstract: Clustering algorithms are widely used in various applications, with
density-based methods such as Density-Based Spatial Clustering of Applications
with Noise (DBSCAN) being particularly prominent. These algorithms identify
clusters in high-density regions while treating sparser areas as noise.
However, reliance on user-defined parameters often poses optimization
challenges that require domain expertise. This paper presents a novel
density-based clustering method inspired by the concept of selective attention,
which minimizes the need for user-defined parameters under standard conditions.
Initially, the algorithm operates without requiring user-defined parameters. If
parameter adjustment is needed, the method simplifies the process by
introducing a single integer parameter that is straightforward to tune. The
approach computes a threshold to filter out the most sparsely distributed
points and outliers, forms a preliminary cluster structure, and then
reintegrates the excluded points to finalize the results. Experimental
evaluations on diverse data sets highlight the accessibility and robust
performance of the method, providing an effective alternative for density-based
clustering tasks.

</details>


### [361] [Curvature Learning for Generalization of Hyperbolic Neural Networks](https://arxiv.org/abs/2508.17232)
*Xiaomeng Fan,Yuwei Wu,Zhi Gao,Mehrtash Harandi,Yunde Jia*

Main category: cs.LG

TL;DR: The paper develops a theoretical foundation for curvature's role in hyperbolic neural networks (HNNs) and proposes a sharpness-aware curvature learning method to improve generalization by smoothing the loss landscape.


<details>
  <summary>Details</summary>
Motivation: Curvature plays a crucial role in optimizing HNNs but inappropriate curvatures can cause suboptimal performance. The theoretical foundation of curvature's effect on HNNs has not been developed.

Method: Derived a PAC-Bayesian generalization bound for HNNs, then proposed a sharpness-aware curvature learning method with scope sharpness measure minimized through bi-level optimization, using implicit differentiation for efficient gradient approximation.

Result: The method shows approximation error is upper-bounded and can converge by bounding gradients. Experiments on classification, long-tailed data, noisy data, and few-shot learning demonstrate improved HNN performance.

Conclusion: The proposed curvature learning method effectively improves HNN generalization by smoothing the loss landscape, with theoretical guarantees and empirical validation across multiple learning scenarios.

Abstract: Hyperbolic neural networks (HNNs) have demonstrated notable efficacy in
representing real-world data with hierarchical structures via exploiting the
geometric properties of hyperbolic spaces characterized by negative curvatures.
Curvature plays a crucial role in optimizing HNNs. Inappropriate curvatures may
cause HNNs to converge to suboptimal parameters, degrading overall performance.
So far, the theoretical foundation of the effect of curvatures on HNNs has not
been developed. In this paper, we derive a PAC-Bayesian generalization bound of
HNNs, highlighting the role of curvatures in the generalization of HNNs via
their effect on the smoothness of the loss landscape. Driven by the derived
bound, we propose a sharpness-aware curvature learning method to smooth the
loss landscape, thereby improving the generalization of HNNs. In our method,
  we design a scope sharpness measure for curvatures, which is minimized
through a bi-level optimization process. Then, we introduce an implicit
differentiation algorithm that efficiently solves the bi-level optimization by
approximating gradients of curvatures. We present the approximation error and
convergence analyses of the proposed method, showing that the approximation
error is upper-bounded, and the proposed method can converge by bounding
gradients of HNNs. Experiments on four settings: classification, learning from
long-tailed data, learning from noisy data, and few-shot learning show that our
method can improve the performance of HNNs.

</details>


### [362] [ShaLa: Multimodal Shared Latent Space Modelling](https://arxiv.org/abs/2508.17376)
*Jiali Cui,Yan-Ying Chen,Yanxia Zhang,Matthew Klenk*

Main category: cs.LG

TL;DR: ShaLa is a novel multimodal generative framework that integrates architectural inference and diffusion prior to learn shared latent representations across modalities, overcoming limitations of traditional multimodal VAEs in inference and synthesis quality.


<details>
  <summary>Details</summary>
Motivation: Traditional multimodal VAEs struggle with designing expressive joint variational posteriors and suffer from low-quality synthesis, while often obscuring high-level semantic concepts shared across modalities.

Method: ShaLa integrates a novel architectural inference model and a second-stage expressive diffusion prior to facilitate effective inference of shared latent representation and improve multimodal synthesis quality.

Result: Extensive validation across multiple benchmarks demonstrates ShaLa's superior coherence and synthesis quality compared to state-of-the-art multimodal VAEs, with scalability to many more modalities.

Conclusion: ShaLa successfully addresses key challenges in multimodal representation learning by enabling effective shared latent space inference and high-quality multimodal synthesis across diverse modalities.

Abstract: This paper presents a novel generative framework for learning shared latent
representations across multimodal data. Many advanced multimodal methods focus
on capturing all combinations of modality-specific details across inputs, which
can inadvertently obscure the high-level semantic concepts that are shared
across modalities. Notably, Multimodal VAEs with low-dimensional latent
variables are designed to capture shared representations, enabling various
tasks such as joint multimodal synthesis and cross-modal inference. However,
multimodal VAEs often struggle to design expressive joint variational
posteriors and suffer from low-quality synthesis. In this work, ShaLa addresses
these challenges by integrating a novel architectural inference model and a
second-stage expressive diffusion prior, which not only facilitates effective
inference of shared latent representation but also significantly improves the
quality of downstream multimodal synthesis. We validate ShaLa extensively
across multiple benchmarks, demonstrating superior coherence and synthesis
quality compared to state-of-the-art multimodal VAEs. Furthermore, ShaLa scales
to many more modalities while prior multimodal VAEs have fallen short in
capturing the increasing complexity of the shared latent space.

</details>


### [363] [Robustness Feature Adapter for Efficient Adversarial Training](https://arxiv.org/abs/2508.17680)
*Quanwei Wu,Jun Guo,Wei Wang,Yi Wang*

Main category: cs.LG

TL;DR: Adapter-based approach for efficient adversarial training that eliminates robust overfitting and improves computational efficiency while generalizing robustness to unseen attacks.


<details>
  <summary>Details</summary>
Motivation: Adversarial training with projected gradient descent has high computational overhead for large models and suffers from robust overfitting, making it impractical for foundation models.

Method: Proposed a new adapter-based approach for efficient adversarial training directly in the feature space, eliminating robust overfitting and improving inner-loop convergence.

Result: Significantly increased computational efficiency, improved model accuracy, and demonstrated generalization of adversarial robustness to unseen attacks across different backbone architectures.

Conclusion: The adapter-based approach successfully solves both computational efficiency and robust overfitting problems simultaneously, enabling more trustworthy foundation models through efficient adversarial training.

Abstract: Adversarial training (AT) with projected gradient descent is the most popular
method to improve model robustness under adversarial attacks. However,
computational overheads become prohibitively large when AT is applied to large
backbone models. AT is also known to have the issue of robust overfitting. This
paper contributes to solving both problems simultaneously towards building more
trustworthy foundation models. In particular, we propose a new adapter-based
approach for efficient AT directly in the feature space. We show that the
proposed adapter-based approach can improve the inner-loop convergence quality
by eliminating robust overfitting. As a result, it significantly increases
computational efficiency and improves model accuracy by generalizing
adversarial robustness to unseen attacks. We demonstrate the effectiveness of
the new adapter-based approach in different backbone architectures and in AT at
scale.

</details>


### [364] [Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets](https://arxiv.org/abs/2508.17930)
*Sarina Penquitt,Tobias Riedlinger,Timo Heller,Markus Reischl,Matthias Rottmann*

Main category: cs.LG

TL;DR: A unified learning-based method for detecting label errors across object detection, semantic segmentation, and instance segmentation datasets by injecting artificial errors and framing detection as an instance segmentation problem.


<details>
  <summary>Details</summary>
Motivation: Current label error detection methods are task-specific (focus on single vision tasks) and not learning-based, leading to reduced model performance, biased benchmarks, and lower accuracy due to poor label quality.

Method: Inject different types of label errors into ground truth data, then frame label error detection across multiple tasks as an instance segmentation problem using composite input.

Result: Compared performance with various baselines and state-of-the-art approaches on simulated errors across multiple tasks, datasets, and base models, plus generalization study on real-world errors. Released 459 real label errors in Cityscapes and provided benchmark.

Conclusion: The method successfully addresses the research gap by providing a unified, learning-based approach for label error detection across multiple computer vision tasks, demonstrating effectiveness on both simulated and real-world label errors.

Abstract: Recently, detection of label errors and improvement of label quality in
datasets for supervised learning tasks has become an increasingly important
goal in both research and industry. The consequences of incorrectly annotated
data include reduced model performance, biased benchmark results, and lower
overall accuracy. Current state-of-the-art label error detection methods often
focus on a single computer vision task and, consequently, a specific type of
dataset, containing, for example, either bounding boxes or pixel-wise
annotations. Furthermore, previous methods are not learning-based. In this
work, we overcome this research gap. We present a unified method for detecting
label errors in object detection, semantic segmentation, and instance
segmentation datasets. In a nutshell, our approach - learning to detect label
errors by making them - works as follows: we inject different kinds of label
errors into the ground truth. Then, the detection of label errors, across all
mentioned primary tasks, is framed as an instance segmentation problem based on
a composite input. In our experiments, we compare the label error detection
performance of our method with various baselines and state-of-the-art
approaches of each task's domain on simulated label errors across multiple
tasks, datasets, and base models. This is complemented by a generalization
study on real-world label errors. Additionally, we release 459 real label
errors identified in the Cityscapes dataset and provide a benchmark for real
label error detection in Cityscapes.

</details>


### [365] [Generative Feature Imputing - A Technique for Error-resilient Semantic Communication](https://arxiv.org/abs/2508.17957)
*Jianhao Huang,Qunsong Zeng,Hongyang Du,Kaibin Huang*

Main category: cs.LG

TL;DR: Proposes generative feature imputing framework for robust semantic communication, using spatial error concentration, diffusion-based feature reconstruction, and semantic-aware power allocation to improve transmission reliability.


<details>
  <summary>Details</summary>
Motivation: Semantic communication in 6G networks faces robustness challenges against transmission errors that can distort semantically critical content, requiring new error resilience techniques.

Method: Three key techniques: 1) Spatial error concentration packetization strategy, 2) Generative feature imputing using diffusion models to reconstruct missing features, 3) Semantic-aware power allocation for unequal error protection based on semantic importance.

Result: Outperforms conventional approaches like DJSCC and JPEG2000 under block fading conditions, achieving higher semantic accuracy and lower LPIPS scores.

Conclusion: The proposed framework effectively addresses robustness challenges in semantic communication by combining spatial error concentration, generative feature reconstruction, and semantic-aware power allocation.

Abstract: Semantic communication (SemCom) has emerged as a promising paradigm for
achieving unprecedented communication efficiency in sixth-generation (6G)
networks by leveraging artificial intelligence (AI) to extract and transmit the
underlying meanings of source data. However, deploying SemCom over digital
systems presents new challenges, particularly in ensuring robustness against
transmission errors that may distort semantically critical content. To address
this issue, this paper proposes a novel framework, termed generative feature
imputing, which comprises three key techniques. First, we introduce a spatial
error concentration packetization strategy that spatially concentrates feature
distortions by encoding feature elements based on their channel mappings, a
property crucial for both the effectiveness and reduced complexity of the
subsequent techniques. Second, building on this strategy, we propose a
generative feature imputing method that utilizes a diffusion model to
efficiently reconstruct missing features caused by packet losses. Finally, we
develop a semantic-aware power allocation scheme that enables unequal error
protection by allocating transmission power according to the semantic
importance of each packet. Experimental results demonstrate that the proposed
framework outperforms conventional approaches, such as Deep Joint
Source-Channel Coding (DJSCC) and JPEG2000, under block fading conditions,
achieving higher semantic accuracy and lower Learned Perceptual Image Patch
Similarity (LPIPS) scores.

</details>


### [366] [Topology Aware Neural Interpolation of Scalar Fields](https://arxiv.org/abs/2508.17995)
*Mohamed Kissi,Keanu Sisouk,Joshua A. Levine,Julien Tierny*

Main category: cs.LG

TL;DR: Neural network approach for topology-aware interpolation of time-varying scalar fields using persistence diagrams and keyframes to estimate missing data at non-keyframe time steps.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of interpolating time-varying scalar fields with topological accuracy, particularly when only sparse temporal samples (keyframes) are available, while preserving the topological structure represented by persistence diagrams.

Method: A neural architecture that learns the relationship between time values and corresponding scalar fields from keyframe examples, enhanced with topological losses that exploit input persistence diagrams to improve both geometric and topological reconstruction at non-keyframe time steps.

Result: The approach produces instantaneous outputs via single network propagation, showing superiority over reference interpolation schemes in both data fitting and topological accuracy for 2D and 3D time-varying datasets.

Conclusion: The neural scheme with topological losses effectively inverts non-keyframe persistence diagrams to produce plausible scalar field estimations, demonstrating improved interpolation quality while maintaining topological consistency across time steps.

Abstract: This paper presents a neural scheme for the topology-aware interpolation of
time-varying scalar fields. Given a time-varying sequence of persistence
diagrams, along with a sparse temporal sampling of the corresponding scalar
fields, denoted as keyframes, our interpolation approach aims at "inverting"
the non-keyframe diagrams to produce plausible estimations of the
corresponding, missing data. For this, we rely on a neural architecture which
learns the relation from a time value to the corresponding scalar field, based
on the keyframe examples, and reliably extends this relation to the
non-keyframe time steps. We show how augmenting this architecture with specific
topological losses exploiting the input diagrams both improves the geometrical
and topological reconstruction of the non-keyframe time steps. At query time,
given an input time value for which an interpolation is desired, our approach
instantaneously produces an output, via a single propagation of the time input
through the network. Experiments interpolating 2D and 3D time-varying datasets
show our approach superiority, both in terms of data and topological fitting,
with regard to reference interpolation schemes.

</details>


### [367] [AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration](https://arxiv.org/abs/2508.18025)
*Aditri Paul,Archan Paul*

Main category: cs.LG

TL;DR: AQ-PCDSys is a quantized neural network framework with adaptive sensor fusion for real-time crater detection on resource-constrained planetary exploration hardware.


<details>
  <summary>Details</summary>
Motivation: Planetary exploration requires real-time environmental perception but faces computational constraints that limit deep learning deployment.

Method: Combines Quantized Neural Network (QNN) with Quantization-Aware Training and Adaptive Multi-Sensor Fusion module that dynamically weights optical imagery and elevation data.

Result: Achieves optimized model size and inference latency while maintaining high accuracy for crater detection across diverse planetary conditions.

Conclusion: Provides computationally efficient and reliable crater detection solution enabling autonomous planetary landing and navigation missions.

Abstract: Autonomous planetary exploration missions are critically dependent on
real-time, accurate environmental perception for navigation and hazard
avoidance. However, deploying deep learning models on the resource-constrained
computational hardware of planetary exploration platforms remains a significant
challenge. This paper introduces the Adaptive Quantized Planetary Crater
Detection System (AQ-PCDSys), a novel framework specifically engineered for
real-time, onboard deployment in the computationally constrained environments
of space exploration missions. AQ-PCDSys synergistically integrates a Quantized
Neural Network (QNN) architecture, trained using Quantization-Aware Training
(QAT), with an Adaptive Multi-Sensor Fusion (AMF) module. The QNN architecture
significantly optimizes model size and inference latency suitable for real-time
onboard deployment in space exploration missions, while preserving high
accuracy. The AMF module intelligently fuses data from Optical Imagery (OI) and
Digital Elevation Models (DEMs) at the feature level, utilizing an Adaptive
Weighting Mechanism (AWM) to dynamically prioritize the most relevant and
reliable sensor modality based on planetary ambient conditions. This approach
enhances detection robustness across diverse planetary landscapes. Paired with
Multi-Scale Detection Heads specifically designed for robust and efficient
detection of craters across a wide range of sizes, AQ-PCDSys provides a
computationally efficient, reliable and accurate solution for planetary crater
detection, a critical capability for enabling the next generation of autonomous
planetary landing, navigation, and scientific exploration.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [368] [Revisiting Rule-Based Stuttering Detection: A Comprehensive Analysis of Interpretable Models for Clinical Applications](https://arxiv.org/abs/2508.16681)
*Eric Zhang*

Main category: cs.AI

TL;DR: Enhanced rule-based stuttering detection framework achieves competitive performance with complete interpretability, excelling in prolongation detection and stable across speaking rates, suitable for clinical applications.


<details>
  <summary>Details</summary>
Motivation: Stuttering affects 1% of global population and while deep learning advances exist, rule-based approaches remain crucial for clinical applications requiring interpretability and transparency.

Method: Proposed enhanced rule-based framework with speaking-rate normalization, multi-level acoustic feature analysis, and hierarchical decision structures, analyzed across multiple corpora including UCLASS, FluencyBank, and SEP-28k.

Result: Achieves competitive performance with 97-99% accuracy in prolongation detection, stable performance across varying speaking rates, and maintains complete interpretability for clinical adoption.

Conclusion: Rule-based methods offer unique advantages in clinical contexts for decision auditability, patient-specific tuning, and real-time feedback, and can be integrated with modern ML pipelines as proposal generators or constraint modules.

Abstract: Stuttering affects approximately 1% of the global population, impacting
communication and quality of life. While recent advances in deep learning have
pushed the boundaries of automatic speech dysfluency detection, rule-based
approaches remain crucial for clinical applications where interpretability and
transparency are paramount. This paper presents a comprehensive analysis of
rule-based stuttering detection systems, synthesizing insights from multiple
corpora including UCLASS, FluencyBank, and SEP-28k. We propose an enhanced
rule-based framework that incorporates speaking-rate normalization, multi-level
acoustic feature analysis, and hierarchical decision structures. Our approach
achieves competitive performance while maintaining complete
interpretability-critical for clinical adoption. We demonstrate that rule-based
systems excel particularly in prolongation detection (97-99% accuracy) and
provide stable performance across varying speaking rates. Furthermore, we show
how these interpretable models can be integrated with modern machine learning
pipelines as proposal generators or constraint modules, bridging the gap
between traditional speech pathology practices and contemporary AI systems. Our
analysis reveals that while neural approaches may achieve marginally higher
accuracy in unconstrained settings, rule-based methods offer unique advantages
in clinical contexts where decision auditability, patient-specific tuning, and
real-time feedback are essential.

</details>


### [369] [Quantifying Sycophancy as Deviations from Bayesian Rationality in LLMs](https://arxiv.org/abs/2508.16846)
*Katherine Atwell,Pedram Heydari,Anthony Sicilia,Malihe Alikhani*

Main category: cs.AI

TL;DR: This paper introduces a Bayesian framework to quantify sycophancy in LLMs as deviations from rational behavior when presented with user perspectives, distinguishing between rational and irrational updates.


<details>
  <summary>Details</summary>
Motivation: Existing sycophancy metrics focus on behavior shifts or accuracy impacts but don't characterize rationality shifts, and accuracy measures only work with known ground truth. A better method is needed to study sycophancy in uncertain or ground-truth-lacking scenarios.

Method: Using Bayesian framework to measure deviations from rational behavior when LLMs are presented with user perspectives. Studied 3 different tasks, multiple LLMs (open-source and closed), two probing methods, and multiple probability judgment elicitation techniques.

Result: 1) LLMs are not Bayesian rational, 2) sycophancy probing causes significant increases in predicted posteriors favoring steered outcomes, 3) sycophancy sometimes increases Bayesian error but occasionally decreases it, 4) Bayesian error changes are not strongly correlated with Brier score.

Conclusion: Studying sycophancy's impact on ground truth alone doesn't fully capture reasoning errors; the Bayesian framework provides a more comprehensive way to quantify irrational sycophantic behavior in LLMs across various scenarios.

Abstract: Sycophancy, or overly agreeable or flattering behavior, is a documented issue
in large language models (LLMs), and is critical to understand in the context
of human/AI collaboration. Prior works typically quantify sycophancy by
measuring shifts in behavior or impacts on accuracy, but neither metric
characterizes shifts in rationality, and accuracy measures can only be used in
scenarios with a known ground truth. In this work, we utilize a Bayesian
framework to quantify sycophancy as deviations from rational behavior when
presented with user perspectives, thus distinguishing between rational and
irrational updates based on the introduction of user perspectives. In
comparison to other methods, this approach allows us to characterize excessive
behavioral shifts, even for tasks that involve inherent uncertainty or do not
have a ground truth. We study sycophancy for 3 different tasks, a combination
of open-source and closed LLMs, and two different methods for probing
sycophancy. We also experiment with multiple methods for eliciting probability
judgments from LLMs. We hypothesize that probing LLMs for sycophancy will cause
deviations in LLMs' predicted posteriors that will lead to increased Bayesian
error. Our findings indicate that: 1) LLMs are not Bayesian rational, 2)
probing for sycophancy results in significant increases to the predicted
posterior in favor of the steered outcome, 3) sycophancy sometimes results in
increased Bayesian error, and in a small number of cases actually decreases
error, and 4) changes in Bayesian error due to sycophancy are not strongly
correlated in Brier score, suggesting that studying the impact of sycophancy on
ground truth alone does not fully capture errors in reasoning due to
sycophancy.

</details>


### [370] [Large Language Models as Universal Predictors? An Empirical Study on Small Tabular Datasets](https://arxiv.org/abs/2508.17391)
*Nikolaos Pavlidis,Vasilis Perifanis,Symeon Symeonidis,Pavlos S. Efraimidis*

Main category: cs.AI

TL;DR: LLMs show strong classification performance on structured data with few-shot learning but poor regression and clustering results compared to traditional ML models, making them useful for rapid data exploration but with modality-specific limitations.


<details>
  <summary>Details</summary>
Motivation: To investigate the empirical function approximation capability of LLMs on structured datasets for classification, regression and clustering tasks, leveraging their in-context learning capabilities without explicit fine-tuning.

Method: Evaluated state-of-the-art LLMs (GPT-5, GPT-4o, GPT-o3, Gemini-2.5-Flash, DeepSeek-R1) under few-shot prompting on small-scale structured datasets, comparing against traditional ML baselines including linear models, ensemble methods and tabular foundation models.

Result: LLMs achieve strong performance in classification tasks under limited data availability, establishing practical zero-training baselines. However, performance in regression with continuous-valued outputs is poor compared to ML models, and clustering results are similarly limited due to absence of genuine ICL in this setting.

Conclusion: LLMs can serve as general-purpose predictive engines for structured data with clear strengths in classification but significant limitations in regression and clustering, enabling rapid data exploration and offering alternatives to traditional ML pipelines in business intelligence contexts.

Abstract: Large Language Models (LLMs), originally developed for natural language
processing (NLP), have demonstrated the potential to generalize across
modalities and domains. With their in-context learning (ICL) capabilities, LLMs
can perform predictive tasks over structured inputs without explicit
fine-tuning on downstream tasks. In this work, we investigate the empirical
function approximation capability of LLMs on small-scale structured datasets
for classification, regression and clustering tasks. We evaluate the
performance of state-of-the-art LLMs (GPT-5, GPT-4o, GPT-o3, Gemini-2.5-Flash,
DeepSeek-R1) under few-shot prompting and compare them against established
machine learning (ML) baselines, including linear models, ensemble methods and
tabular foundation models (TFMs). Our results show that LLMs achieve strong
performance in classification tasks under limited data availability,
establishing practical zero-training baselines. In contrast, the performance in
regression with continuous-valued outputs is poor compared to ML models, likely
because regression demands outputs in a large (often infinite) space, and
clustering results are similarly limited, which we attribute to the absence of
genuine ICL in this setting. Nonetheless, this approach enables rapid,
low-overhead data exploration and offers a viable alternative to traditional ML
pipelines in business intelligence and exploratory analytics contexts. We
further analyze the influence of context size and prompt structure on
approximation quality, identifying trade-offs that affect predictive
performance. Our findings suggest that LLMs can serve as general-purpose
predictive engines for structured data, with clear strengths in classification
and significant limitations in regression and clustering.

</details>


### [371] [LLM-based Agentic Reasoning Frameworks: A Survey from Methods to Scenarios](https://arxiv.org/abs/2508.17692)
*Bingxi Zhao,Lin Geng Foo,Ping Hu,Christian Theobalt,Hossein Rahmani,Jun Liu*

Main category: cs.AI

TL;DR: A survey paper proposing a systematic taxonomy for LLM-based agent reasoning frameworks, classifying them into single-agent, tool-based, and multi-agent methods, with comprehensive review of applications across various domains.


<details>
  <summary>Details</summary>
Motivation: Recent advances in LLM reasoning capabilities have led to diverse agent systems with different reasoning frameworks, but there's a lack of systematic classification and analysis of how these frameworks guide reasoning processes across different scenarios.

Method: The authors propose a unified formal language to classify agentic reasoning systems into three categories: single-agent methods, tool-based methods, and multi-agent methods. They then conduct a comprehensive review of applications in scientific discovery, healthcare, software engineering, social simulation, and economics.

Result: The survey provides a systematic taxonomy that decomposes agentic reasoning frameworks and analyzes how different frameworks dominate reasoning processes. It identifies characteristic features of each framework and summarizes evaluation strategies.

Conclusion: This survey offers the research community a panoramic view to understand the strengths, suitable scenarios, and evaluation practices of different agentic reasoning frameworks, facilitating better selection and application of appropriate frameworks for specific tasks.

Abstract: Recent advances in the intrinsic reasoning capabilities of large language
models (LLMs) have given rise to LLM-based agent systems that exhibit
near-human performance on a variety of automated tasks. However, although these
systems share similarities in terms of their use of LLMs, different reasoning
frameworks of the agent system steer and organize the reasoning process in
different ways. In this survey, we propose a systematic taxonomy that
decomposes agentic reasoning frameworks and analyze how these frameworks
dominate framework-level reasoning by comparing their applications across
different scenarios. Specifically, we propose an unified formal language to
further classify agentic reasoning systems into single-agent methods,
tool-based methods, and multi-agent methods. After that, we provide a
comprehensive review of their key application scenarios in scientific
discovery, healthcare, software engineering, social simulation, and economics.
We also analyze the characteristic features of each framework and summarize
different evaluation strategies. Our survey aims to provide the research
community with a panoramic view to facilitate understanding of the strengths,
suitable scenarios, and evaluation practices of different agentic reasoning
frameworks.

</details>


### [372] [The AI Data Scientist](https://arxiv.org/abs/2508.18113)
*Farkhad Akimov,Munachiso Samuel Nwadike,Zangir Iklassov,Martin TakÃ¡Ä*

Main category: cs.AI

TL;DR: AI Data Scientist is an autonomous LLM-powered agent that provides rapid, end-to-end data analysis and actionable insights through specialized subagents handling data cleaning, statistical testing, and plain-language communication.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between data evidence and actionable decisions by automating the entire data science workflow, making deep data analysis accessible and fast for decision-makers.

Method: Uses a team of specialized LLM subagents that each handle distinct tasks (data cleaning, statistical testing, validation, communication), write their own code, reason about causality, and follow scientific hypothesis principles to uncover explanatory patterns.

Result: Achieves in minutes what traditionally takes days or weeks, delivering rigorous statistical analysis, predictive modeling, and accessible recommendations directly to users.

Conclusion: The AI Data Scientist enables a new paradigm of rapid, accessible data science that transforms raw data into actionable insights through autonomous LLM-powered analysis, revolutionizing decision-making workflows.

Abstract: Imagine decision-makers uploading data and, within minutes, receiving clear,
actionable insights delivered straight to their fingertips. That is the promise
of the AI Data Scientist, an autonomous Agent powered by large language models
(LLMs) that closes the gap between evidence and action. Rather than simply
writing code or responding to prompts, it reasons through questions, tests
ideas, and delivers end-to-end insights at a pace far beyond traditional
workflows. Guided by the scientific tenet of the hypothesis, this Agent
uncovers explanatory patterns in data, evaluates their statistical
significance, and uses them to inform predictive modeling. It then translates
these results into recommendations that are both rigorous and accessible. At
the core of the AI Data Scientist is a team of specialized LLM Subagents, each
responsible for a distinct task such as data cleaning, statistical testing,
validation, and plain-language communication. These Subagents write their own
code, reason about causality, and identify when additional data is needed to
support sound conclusions. Together, they achieve in minutes what might
otherwise take days or weeks, enabling a new kind of interaction that makes
deep data science both accessible and actionable.

</details>


### [373] [Unraveling the cognitive patterns of Large Language Models through module communities](https://arxiv.org/abs/2508.18192)
*Kushal Raj Bhandari,Pin-Yu Chen,Jianxi Gao*

Main category: cs.AI

TL;DR: A network-based framework that analyzes LLM cognition by linking cognitive skills, architectures, and datasets, revealing distributed yet interconnected cognitive patterns similar to biological systems but with key differences in learning dynamics.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding about LLM inner mechanisms hidden within billions of parameters and complex structures, by adopting approaches from biological cognition studies.

Method: Developed a network-based framework that connects cognitive skills, LLM architectures, and datasets, analyzing module communities and skill distribution patterns.

Result: LLMs exhibit unique module communities with emergent skill patterns that partially mirror distributed cognitive organization in avian and small mammalian brains, but with key divergence - skill acquisition benefits more from dynamic cross-regional interactions and neural plasticity.

Conclusion: The framework provides new insights into LLM interpretability and suggests fine-tuning strategies should leverage distributed learning dynamics rather than rigid modular interventions, integrating cognitive science with machine learning.

Abstract: Large Language Models (LLMs) have reshaped our world with significant
advancements in science, engineering, and society through applications ranging
from scientific discoveries and medical diagnostics to Chatbots. Despite their
ubiquity and utility, the underlying mechanisms of LLM remain concealed within
billions of parameters and complex structures, making their inner architecture
and cognitive processes challenging to comprehend. We address this gap by
adopting approaches to understanding emerging cognition in biology and
developing a network-based framework that links cognitive skills, LLM
architectures, and datasets, ushering in a paradigm shift in foundation model
analysis. The skill distribution in the module communities demonstrates that
while LLMs do not strictly parallel the focalized specialization observed in
specific biological systems, they exhibit unique communities of modules whose
emergent skill patterns partially mirror the distributed yet interconnected
cognitive organization seen in avian and small mammalian brains. Our numerical
results highlight a key divergence from biological systems to LLMs, where skill
acquisition benefits substantially from dynamic, cross-regional interactions
and neural plasticity. By integrating cognitive science principles with machine
learning, our framework provides new insights into LLM interpretability and
suggests that effective fine-tuning strategies should leverage distributed
learning dynamics rather than rigid modular interventions.

</details>


### [374] [WebSight: A Vision-First Architecture for Robust Web Agents](https://arxiv.org/abs/2508.16987)
*Tanvir Bhathal,Asanshay Gupta*

Main category: cs.AI

TL;DR: WebSight is a vision-based autonomous web agent that interacts with web environments purely through visual perception, eliminating HTML/DOM dependencies. It uses WebSight-7B, a fine-tuned vision-language model, and achieves state-of-the-art performance on web navigation benchmarks.


<details>
  <summary>Details</summary>
Motivation: To create a web agent that can interact with web environments through visual perception alone, eliminating the need for HTML or DOM-based inputs, which makes the system more robust and interpretable.

Method: Developed WebSight-7B, a fine-tuned vision-language model using LoRA on a web-focused subset of Wave-UI-25K dataset. Integrated into a modular multi-agent architecture with planning, reasoning, vision-action, and verification agents coordinated through episodic memory.

Result: WebSight-7B achieves 58.84% top-1 accuracy on Showdown Clicks benchmark, outperforming larger generalist models with lower latency. Full WebSight agent achieves 68.0% success rate on WebVoyager benchmark, surpassing OpenAI (61.0%) and HCompany (67.0%). 97.14% correct answer rate on completed tasks.

Conclusion: WebSight and WebSight-7B establish a new standard for interpretable, robust, and efficient visual web navigation, demonstrating superior performance over existing systems while maintaining visual-only interaction capabilities.

Abstract: We introduce WebSight, a vision-based autonomous web agent, designed to
interact with web environments purely through visual perception, eliminating
dependence on HTML or DOM-based inputs. Central to our approach we introduce
our new model, WebSight-7B, a fine-tuned vision-language model optimized for UI
element interaction, trained using LoRA on a web-focused subset of the
Wave-UI-25K dataset. WebSight integrates this model into a modular multi-agent
architecture, comprising planning, reasoning, vision-action, and verification
agents, coordinated through an episodic memory mechanism.
  WebSight-7B achieves a top-1 accuracy of 58.84% on the Showdown Clicks
benchmark, outperforming several larger generalist models while maintaining
lower latency. The full WebSight agent achieves a 68.0% success rate on the
WebVoyager benchmark, surpassing systems from labs such as OpenAI (61.0%) and
HCompany (Runner H, 67.0%). Among tasks completed, WebSight answers correctly
97.14% of the time, indicating high precision. Together, WebSight and
WebSight-7B establish a new standard for interpretable, robust, and efficient
visual web navigation.

</details>


### [375] [MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapes](https://arxiv.org/abs/2508.17180)
*Nilay Pande,Sahiti Yerramilli,Jayant Sravan Tamarapalli,Rynaa Grover*

Main category: cs.AI

TL;DR: MaRVL-QA is a new benchmark for evaluating mathematical and spatial reasoning in MLLMs through topological counting and transformation recognition tasks on surface plots, revealing current models' limitations.


<details>
  <summary>Details</summary>
Motivation: To push MLLMs beyond semantic description and test their deep mathematical/spatial reasoning capabilities using mathematical surface plots that eliminate semantic noise from natural images.

Method: Created MaRVL-QA benchmark with two tasks: Topological Counting (identifying/enumerating features like local maxima) and Transformation Recognition (recognizing geometric transformations), generated from curated functions with ambiguity filtering.

Result: State-of-the-art MLLMs struggle significantly, often resorting to superficial heuristics rather than robust spatial reasoning.

Conclusion: MaRVL-QA provides a challenging tool to measure progress, expose model limitations, and guide development of MLLMs with more profound reasoning abilities.

Abstract: A key frontier for Multimodal Large Language Models (MLLMs) is the ability to
perform deep mathematical and spatial reasoning directly from images, moving
beyond their established success in semantic description. Mathematical surface
plots provide a rigorous testbed for this capability, as they isolate the task
of reasoning from the semantic noise common in natural images. To measure
progress on this frontier, we introduce MaRVL-QA (Mathematical Reasoning over
Visual Landscapes), a new benchmark designed to quantitatively evaluate these
core reasoning skills. The benchmark comprises two novel tasks: Topological
Counting, identifying and enumerating features like local maxima; and
Transformation Recognition, recognizing applied geometric transformations.
Generated from a curated library of functions with rigorous ambiguity
filtering, our evaluation on MaRVL-QA reveals that even state-of-the-art MLLMs
struggle significantly, often resorting to superficial heuristics instead of
robust spatial reasoning. MaRVL-QA provides a challenging new tool for the
research community to measure progress, expose model limitations, and guide the
development of MLLMs with more profound reasoning abilities.

</details>


### [376] [SEAM: Semantically Equivalent Across Modalities Benchmark for Vision-Language Models](https://arxiv.org/abs/2508.18179)
*Zhenwei Tang,Difan Jiao,Blair Yang,Ashton Anderson*

Main category: cs.AI

TL;DR: SEAM benchmark evaluates vision-language models' cross-modal reasoning consistency using semantically equivalent inputs across four domains with standardized textual and visual notations, revealing systematic modality imbalance where vision lags behind language.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of evaluating VLMs' consistent reasoning across modalities without task differences and asymmetric information confounding the comparison.

Method: Introduces SEAM benchmark that pairs semantically equivalent inputs across four domains using distinct notation systems across modalities (not OCR-based), enabling rigorous comparative assessment of textual-symbolic vs visual-spatial reasoning.

Result: Across 21 contemporary models, systematic modality imbalance observed: vision frequently lags language despite semantically equivalent information, with low cross-modal agreement. Error analysis reveals textual perception failures from tokenization and visual perception failures causing hallucinations.

Conclusion: SEAM provides a controlled, semantically equivalent setting for measuring and improving modality-agnostic reasoning in VLMs, with results robust to visual transformations.

Abstract: Evaluating whether vision-language models (VLMs) reason consistently across
representations is challenging because modality comparisons are typically
confounded by task differences and asymmetric information. We introduce SEAM, a
benchmark that pairs semantically equivalent inputs across four domains that
have existing standardized textual and visual notations. By employing distinct
notation systems across modalities, in contrast to OCR-based image-text
pairing, SEAM provides a rigorous comparative assessment of the
textual-symbolic and visual-spatial reasoning capabilities of VLMs. Across 21
contemporary models, we observe systematic modality imbalance: vision
frequently lags language in overall performance, despite the problems
containing semantically equivalent information, and cross-modal agreement is
relatively low. Our error analysis reveals two main drivers: textual perception
failures from tokenization in domain notation and visual perception failures
that induce hallucinations. We also show that our results are largely robust to
visual transformations. SEAM establishes a controlled, semantically equivalent
setting for measuring and improving modality-agnostic reasoning.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [377] [VGGSounder: Audio-Visual Evaluations for Foundation Models](https://arxiv.org/abs/2508.08237)
*Daniil Zverev,ThaddÃ¤us Wiedemer,Ameya Prabhu,Matthias Bethge,Wieland Brendel,A. Sophia Koepke*

Main category: cs.MM

TL;DR: VGGSounder is a re-annotated multi-label test set that fixes limitations in VGGSound for better evaluation of audio-visual foundation models, with detailed modality annotations and a new confusion metric.


<details>
  <summary>Details</summary>
Motivation: VGGSound dataset has limitations including incomplete labeling, overlapping classes, and misaligned modalities that distort evaluation of audio-visual models.

Method: Created VGGSounder by comprehensively re-annotating VGGSound with multi-label classification, detailed modality annotations, and introduced a modality confusion metric to analyze performance degradation.

Result: Provides a more reliable benchmark for evaluating audio-visual foundation models with precise modality-specific performance analysis capabilities.

Conclusion: VGGSounder addresses critical limitations in existing benchmarks and enables more accurate assessment of multi-modal understanding in audio-visual models.

Abstract: The emergence of audio-visual foundation models underscores the importance of
reliably assessing their multi-modal understanding. The VGGSound dataset is
commonly used as a benchmark for evaluation audio-visual classification.
However, our analysis identifies several limitations of VGGSound, including
incomplete labelling, partially overlapping classes, and misaligned modalities.
These lead to distorted evaluations of auditory and visual capabilities. To
address these limitations, we introduce VGGSounder, a comprehensively
re-annotated, multi-label test set that extends VGGSound and is specifically
designed to evaluate audio-visual foundation models. VGGSounder features
detailed modality annotations, enabling precise analyses of modality-specific
performance. Furthermore, we reveal model limitations by analysing performance
degradation when adding another input modality with our new modality confusion
metric.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [378] [Named Entity Recognition of Historical Text via Large Language Model](https://arxiv.org/abs/2508.18090)
*Shibingfeng Zhang,Giovanni Colavizza*

Main category: cs.DL

TL;DR: LLMs can achieve reasonably strong NER performance on historical documents using zero-shot and few-shot prompting, offering a viable alternative when annotated training data is scarce.


<details>
  <summary>Details</summary>
Motivation: Historical texts present challenges for NER due to scarce annotated datasets and language variability (inconsistent spelling, archaic vocabulary), making traditional supervised approaches infeasible.

Method: Applied large language models to NER using zero-shot and few-shot prompting strategies on the HIPE-2022 dataset, requiring little to no task-specific training data.

Result: LLMs achieved reasonably strong performance on historical NER tasks, though performance fell short of fully supervised models trained on domain-specific annotations.

Conclusion: LLMs offer a viable and efficient alternative for information extraction in low-resource or historically significant corpora where traditional supervised methods cannot be applied.

Abstract: Large language models have demonstrated remarkable versatility across a wide
range of natural language processing tasks and domains. One such task is Named
Entity Recognition (NER), which involves identifying and classifying proper
names in text, such as people, organizations, locations, dates, and other
specific entities. NER plays a crucial role in extracting information from
unstructured textual data, enabling downstream applications such as information
retrieval from unstructured text.
  Traditionally, NER is addressed using supervised machine learning approaches,
which require large amounts of annotated training data. However, historical
texts present a unique challenge, as the annotated datasets are often scarce or
nonexistent, due to the high cost and expertise required for manual labeling.
In addition, the variability and noise inherent in historical language, such as
inconsistent spelling and archaic vocabulary, further complicate the
development of reliable NER systems for these sources.
  In this study, we explore the feasibility of applying LLMs to NER in
historical documents using zero-shot and few-shot prompting strategies, which
require little to no task-specific training data. Our experiments, conducted on
the HIPE-2022 (Identifying Historical People, Places and other Entities)
dataset, show that LLMs can achieve reasonably strong performance on NER tasks
in this setting. While their performance falls short of fully supervised models
trained on domain-specific annotations, the results are nevertheless promising.
These findings suggest that LLMs offer a viable and efficient alternative for
information extraction in low-resource or historically significant corpora,
where traditional supervised methods are infeasible.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [379] [3D latent diffusion models for parameterizing and history matching multiscenario facies systems](https://arxiv.org/abs/2508.16621)
*Guido Di Federico,Louis J. Durlofsky*

Main category: physics.geo-ph

TL;DR: Developed a generative latent diffusion model (LDM) parameterization method for 3D channel-levee-mud geological systems that enables efficient history matching by reducing dimensionality while preserving geological realism.


<details>
  <summary>Details</summary>
Motivation: To enable efficient history matching in geological modeling by reducing the high-dimensional geomodel space to a low-dimensional latent representation while automatically preserving geological realism and handling scenario uncertainty.

Method: Used generative latent diffusion models (LDMs) with perceptual loss during training to parameterize 3D geological models with variable mud fraction, channel orientation, and channel width. Applied the parameterization for ensemble-based history matching with updates performed in the LDM latent space.

Result: Generated realizations closely resembled reference geomodels visually and statistically. Flow response distributions showed close agreement between LDM-generated and reference models. History matching demonstrated clear uncertainty reduction in production forecasts and geological scenario parameters across three different geological scenarios.

Conclusion: The LDM-based parameterization method effectively handles geological scenario uncertainty and provides posterior geomodels consistent with true models, making it a powerful tool for ensemble-based history matching while preserving geological realism.

Abstract: Geological parameterization procedures entail the mapping of a
high-dimensional geomodel to a low-dimensional latent variable. These
parameterizations can be very useful for history matching because the number of
variables to be calibrated is greatly reduced, and the mapping can be
constructed such that geological realism is automatically preserved. In this
work, a parameterization method based on generative latent diffusion models
(LDMs) is developed for 3D channel-levee-mud systems. Geomodels with variable
scenario parameters, specifically mud fraction, channel orientation, and
channel width, are considered. A perceptual loss term is included during
training to improve geological realism. For any set of scenario parameters, an
(essentially) infinite number of realizations can be generated, so our LDM
parameterizes over a very wide model space. New realizations constructed using
the LDM procedure are shown to closely resemble reference geomodels, both
visually and in terms of one- and two-point spatial statistics. Flow response
distributions, for a specified set of injection and production wells, are also
shown to be in close agreement between the two sets of models. The
parameterization method is applied for ensemble-based history matching, with
model updates performed in the LDM latent space, for cases involving geological
scenario uncertainty. For three synthetic true models corresponding to
different geological scenarios, we observe clear uncertainty reduction in both
production forecasts and geological scenario parameters. The overall method is
additionally shown to provide posterior geomodels consistent with the synthetic
true model in each case.

</details>
